# Phase 2: ë°ì´í„° ë¶„ì„ & ì»¨ì„¤íŒ… (8ì£¼)

> ëª©í‘œ: ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ ì •ì˜í•˜ê³ , ë°ì´í„°ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ë©°, ML ëª¨ë¸ë¡œ ì˜ˆì¸¡/ë¶„ë¥˜/ì´ìƒíƒì§€ë¥¼ ìˆ˜í–‰í•˜ê³ , ê²½ì˜ì§„ì—ê²Œ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤.
>
> ê¸°ê°„: 2ê°œì›” (8ì£¼)
>
> í¬íŠ¸í´ë¦¬ì˜¤: ë°ì´í„° ë¶„ì„ í”„ë¡œì íŠ¸ + ê²½ì˜ì§„ ë³´ê³ ì„œ

---

## ğŸš€ Phase 2ë¥¼ ì‹œì‘í•˜ë©°

Phase 1ì—ì„œ ë°ì´í„°ë¥¼ **ìˆ˜ì§‘, ì²˜ë¦¬, ì €ì¥**í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ FDEëŠ” ë‹¨ìˆœíˆ "ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì‚¬ëŒ"ì´ ì•„ë‹™ë‹ˆë‹¤.
**ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ ì •ì˜í•˜ê³ , ë°ì´í„°ë¡œ í•´ê²°ì±…ì„ ì°¾ëŠ” ì‚¬ëŒ**ì…ë‹ˆë‹¤.

Phase 2ì—ì„œëŠ”:
- **"ë¬´ì—‡ì„"** ë¶„ì„í• ì§€ ì •ì˜í•˜ëŠ” ë°©ë²• (5 Whys, MECE)
- **"ì–´ë–»ê²Œ"** ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí• ì§€ (EDA, ML)
- **"ì–´ë–»ê²Œ"** ì „ë‹¬í• ì§€ (Pyramid Principle, ê²½ì˜ì§„ ë°œí‘œ)

ë¥¼ ë°°ì›ë‹ˆë‹¤.

> **ê¸°ìˆ ì  ì—­ëŸ‰(Phase 1) + ë¹„ì¦ˆë‹ˆìŠ¤ ì—­ëŸ‰(Phase 2) = FDEì˜ ì°¨ë³„í™”ëœ ê°€ì¹˜**

---

## ğŸ’¡ Phase 2ì˜ í•µì‹¬: "ì™œ" + "ì–´ë–»ê²Œ"

Phase 1ì—ì„œ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆë¥¼ ë°°ì› ë‹¤ë©´, Phase 2ì—ì„œëŠ” **ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ í•´ê²°** ê´€ì ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

```
Phase 1: "ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë‹¤ë£¨ëŠ”ê°€?" (ê¸°ìˆ )
Phase 2: "ì™œ ë¶„ì„í•˜ëŠ”ê°€?" + "ì–´ë–»ê²Œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ëŠ”ê°€?" (ê¸°ìˆ  + ë¹„ì¦ˆë‹ˆìŠ¤)
```

### FDE vs ì¼ë°˜ ë°ì´í„° ë¶„ì„ê°€

| ì—­í•  | ì‹œì‘ì  | í•µì‹¬ ì§ˆë¬¸ |
|------|--------|----------|
| **ë°ì´í„° ë¶„ì„ê°€** | "ì´ ë°ì´í„° ë¶„ì„í•´ì£¼ì„¸ìš”" | How? |
| **FDE** | "ë§¤ì¶œì´ ë–¨ì–´ì§€ê³  ìˆì–´ìš”" | Why? What? Then How? |

---

## Month 3: ë¬¸ì œ ì •ì˜ & EDA

---

### Week 9: ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ ì •ì˜ & EDA ê¸°ì´ˆ

#### í•™ìŠµ ëª©í‘œ
- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œì™€ ì¦ìƒì„ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤
- [ ] 5 Whys, MECEë¡œ ë¬¸ì œë¥¼ êµ¬ì¡°í™”í•  ìˆ˜ ìˆë‹¤
- [ ] ë°ì´í„°ì˜ ë¶„í¬, ì¤‘ì‹¬ ê²½í–¥, ì‚°í¬ë„ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆë‹¤
- [ ] íš¨ê³¼ì ì¸ ì‹œê°í™”ë¥¼ í†µí•´ ì¸ì‚¬ì´íŠ¸ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤

#### í•µì‹¬ ê°œë…

**1. ë¬¸ì œ ì •ì˜ (ë¶„ì„ ì „ í•„ìˆ˜!)**

```
âŒ ì˜ëª»ëœ ì ‘ê·¼:
"Python ë°°ì› ì–´ìš”" â†’ "Pandas ë°°ì› ì–´ìš”" â†’ "ê·¼ë° ë­˜ ë¶„ì„í•´ì•¼ í•˜ì£ ?"

âœ… ì˜¬ë°”ë¥¸ ì ‘ê·¼:
"ê³ ê° ì´íƒˆì´ ë¬¸ì œì˜ˆìš”" â†’ "ì–´ë–¤ ë°ì´í„°ê°€ ìˆì£ ?" â†’ "ê°€ì„¤ì„ ì„¸ì›Œë´…ì‹œë‹¤"
â†’ "ì´ê±¸ ê²€ì¦í•˜ë ¤ë©´ ì´ëŸ° ë¶„ì„ì´ í•„ìš”í•´ìš”" â†’ "Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤"
```

**ë¬¸ì œ vs ì¦ìƒ**
```
ì¦ìƒ: "ë§¤ì¶œì´ 20% ê°ì†Œí–ˆì–´ìš”"
     â†“ Why?
ë¬¸ì œ: "ì‹ ê·œ ê³ ê° ìœ ì…ì€ ë™ì¼í•œë° ì¬êµ¬ë§¤ìœ¨ì´ ê¸‰ê°"
     â†“ Why?
ê·¼ë³¸ ì›ì¸: "ê²½ìŸì‚¬ ëŒ€ë¹„ ë°°ì†¡ ì‹œê°„ì´ 2ë°° â†’ ê³ ê° ë§Œì¡±ë„ í•˜ë½"
```

**5 Whys ê¸°ë²•**
```markdown
1. Why: ëŒ€ì‹œë³´ë“œ ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ ì•ˆ ë¨ â†’ ETL ì‘ì—… ì‹¤íŒ¨
2. Why: ETL ì‘ì—…ì´ ì™œ ì‹¤íŒ¨? â†’ ì†ŒìŠ¤ DB ì—°ê²° íƒ€ì„ì•„ì›ƒ
3. Why: ì™œ íƒ€ì„ì•„ì›ƒ? â†’ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ 30ë¶„ ì´ˆê³¼
4. Why: ì™œ ì¿¼ë¦¬ê°€ ëŠë ¤ì¡Œë‚˜? â†’ í…Œì´ë¸” 10ë°° ì¦ê°€, ì¸ë±ìŠ¤ ì—†ìŒ
5. Why: ì™œ ì¸ë±ìŠ¤ê°€ ì—†ë‚˜? â†’ ì´ˆê¸° ì„¤ê³„ ì‹œ ì˜ˆì¸¡ ì‹¤íŒ¨

â†’ ê·¼ë³¸ ì›ì¸: ë°ì´í„° ëª¨ë¸ë§/ìš©ëŸ‰ ê³„íš ë¶€ì¬
â†’ í•´ê²°ì±…: ì¸ë±ìŠ¤ ì¶”ê°€ + íŒŒí‹°ì…”ë‹ + ëª¨ë‹ˆí„°ë§
```

**MECE (Mutually Exclusive, Collectively Exhaustive)**
```
ë¬¸ì œ: "ì™œ ê³ ê°ì´ ì´íƒˆí•˜ëŠ”ê°€?"

MECE ë¶„í•´:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ê³ ê° ì´íƒˆ ì›ì¸                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ì œí’ˆ ë¬¸ì œ  â”‚  ì„œë¹„ìŠ¤ ë¬¸ì œ â”‚   ì™¸ë¶€ ìš”ì¸     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ í’ˆì§ˆ ë¶ˆë§Œ  â”‚ â€¢ ë°°ì†¡ ì§€ì—°  â”‚ â€¢ ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜â”‚
â”‚ â€¢ ê°€ê²© ë¶ˆë§Œ  â”‚ â€¢ CS ë¶ˆë§Œì¡±  â”‚ â€¢ ê²½ê¸° ì¹¨ì²´     â”‚
â”‚ â€¢ ê¸°ëŠ¥ ë¶€ì¡±  â”‚ â€¢ ë°˜í’ˆ ì–´ë ¤ì›€â”‚ â€¢ ê³„ì ˆì  ìš”ì¸   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… ìƒí˜¸ ë°°íƒ€ì : ê° ì¹´í…Œê³ ë¦¬ê°€ ê²¹ì¹˜ì§€ ì•ŠìŒ
âœ… ì „ì²´ í¬ê´„ì : ëª¨ë“  ê°€ëŠ¥í•œ ì›ì¸ í¬í•¨
```

**2. í†µê³„ ê¸°ì´ˆ**
```python
import pandas as pd
import numpy as np

# ì¤‘ì‹¬ ê²½í–¥
df['value'].mean()      # í‰ê·  - ì´ìƒì¹˜ì— ë¯¼ê°
df['value'].median()    # ì¤‘ì•™ê°’ - ì´ìƒì¹˜ì— ê°•ê±´
df['value'].mode()      # ìµœë¹ˆê°’ - ë²”ì£¼í˜•ì— ìœ ìš©

# ì‚°í¬ë„
df['value'].std()       # í‘œì¤€í¸ì°¨
df['value'].var()       # ë¶„ì‚°
df['value'].quantile([0.25, 0.5, 0.75])  # ì‚¬ë¶„ìœ„ìˆ˜
df['value'].max() - df['value'].min()    # ë²”ìœ„

# ë¶„í¬ í˜•íƒœ
from scipy.stats import skew, kurtosis
skew(df['value'])       # ì™œë„ (0: ëŒ€ì¹­, >0: ì˜¤ë¥¸ìª½ ê¼¬ë¦¬)
kurtosis(df['value'])   # ì²¨ë„ (0: ì •ê·œë¶„í¬)

# ìƒê´€ê´€ê³„
df.corr()               # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
df.corr(method='spearman')  # ìŠ¤í”¼ì–´ë§Œ (ìˆœìœ„ ê¸°ë°˜)
```

| ì§€í‘œ | ì˜ë¯¸ | ì‚¬ìš© ì‹œì  |
|------|------|----------|
| í‰ê·  vs ì¤‘ì•™ê°’ | ì°¨ì´ í¬ë©´ ì™œê³¡ëœ ë¶„í¬ | ì´ìƒì¹˜ ì¡´ì¬ íŒŒì•… |
| í‘œì¤€í¸ì°¨ | ë°ì´í„° í¼ì§ ì •ë„ | ë³€ë™ì„± ë¶„ì„ |
| ì™œë„ | ë¶„í¬ ë¹„ëŒ€ì¹­ì„± | ë¡œê·¸ ë³€í™˜ í•„ìš”ì„± íŒë‹¨ |
| ìƒê´€ê³„ìˆ˜ | ë³€ìˆ˜ ê°„ ì„ í˜• ê´€ê³„ | í”¼ì²˜ ì„ íƒ |

**2. ì‹œê°í™” (matplotlib, seaborn, plotly)**
```python
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# ë¶„í¬ ì‹œê°í™”
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
sns.histplot(df['value'], kde=True, ax=axes[0])  # íˆìŠ¤í† ê·¸ë¨
sns.boxplot(x=df['value'], ax=axes[1])           # ë°•ìŠ¤í”Œë¡¯
sns.violinplot(x=df['value'], ax=axes[2])        # ë°”ì´ì˜¬ë¦°

# ê´€ê³„ ì‹œê°í™”
sns.scatterplot(x='feature1', y='target', hue='category', data=df)
sns.pairplot(df[numerical_cols])  # ëª¨ë“  ì¡°í•© ì‚°ì ë„

# ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='RdBu_r', center=0)

# ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™” (Plotly)
fig = px.scatter(df, x='feature1', y='target', color='category',
                 hover_data=['id', 'date'], title='Feature vs Target')
fig.show()
```

| ì°¨íŠ¸ ìœ í˜• | ìš©ë„ | ë³€ìˆ˜ ìœ í˜• |
|----------|------|----------|
| íˆìŠ¤í† ê·¸ë¨ | ë¶„í¬ | ìˆ˜ì¹˜í˜• 1ê°œ |
| ë°•ìŠ¤í”Œë¡¯ | ë¶„í¬ + ì´ìƒì¹˜ | ìˆ˜ì¹˜í˜• 1ê°œ |
| ì‚°ì ë„ | ê´€ê³„ | ìˆ˜ì¹˜í˜• 2ê°œ |
| íˆíŠ¸ë§µ | ìƒê´€ê´€ê³„ | ìˆ˜ì¹˜í˜• ë‹¤ìˆ˜ |
| ë§‰ëŒ€ ê·¸ë˜í”„ | ì¹´ìš´íŠ¸/ë¹„êµ | ë²”ì£¼í˜• |
| ë¼ì¸ ì°¨íŠ¸ | ì¶”ì„¸ | ì‹œê³„ì—´ |

**3. ê²°ì¸¡ì¹˜ ë¶„ì„**
```python
# ê²°ì¸¡ì¹˜ í˜„í™©
df.isnull().sum()
df.isnull().sum() / len(df) * 100  # ë¹„ìœ¨

# ê²°ì¸¡ì¹˜ ì‹œê°í™”
import missingno as msno
msno.matrix(df)        # ê²°ì¸¡ íŒ¨í„´ ë§¤íŠ¸ë¦­ìŠ¤
msno.heatmap(df)       # ê²°ì¸¡ ìƒê´€ê´€ê³„
msno.dendrogram(df)    # ê²°ì¸¡ í´ëŸ¬ìŠ¤í„°ë§

# ê²°ì¸¡ ìœ í˜• íŒë‹¨
# MCAR (ì™„ì „ ë¬´ì‘ìœ„): ê²°ì¸¡ì´ ë‹¤ë¥¸ ë³€ìˆ˜ì™€ ë¬´ê´€
# MAR (ë¬´ì‘ìœ„): ê´€ì¸¡ëœ ë³€ìˆ˜ì™€ ê´€ë ¨
# MNAR (ë¹„ë¬´ì‘ìœ„): ê²°ì¸¡ê°’ ìì²´ì™€ ê´€ë ¨
```

| ê²°ì¸¡ ìœ í˜• | íŠ¹ì§• | ì²˜ë¦¬ ë°©ë²• |
|----------|------|----------|
| MCAR | ì™„ì „ ë¬´ì‘ìœ„ | ì‚­ì œ ë˜ëŠ” ëŒ€ì²´ |
| MAR | ë‹¤ë¥¸ ë³€ìˆ˜ì™€ ê´€ë ¨ | ì¡°ê±´ë¶€ ëŒ€ì²´ |
| MNAR | ê²°ì¸¡ê°’ ìì²´ì™€ ê´€ë ¨ | ë„ë©”ì¸ ì§€ì‹ í•„ìš” |

**4. ì´ìƒì¹˜ íƒì§€ (ê¸°ì´ˆ)**
```python
# IQR ë°©ë²•
Q1 = df['value'].quantile(0.25)
Q3 = df['value'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
outliers = df[(df['value'] < lower) | (df['value'] > upper)]

# Z-score ë°©ë²•
from scipy import stats
z_scores = np.abs(stats.zscore(df['value']))
outliers = df[z_scores > 3]

# ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
sns.boxplot(x=df['value'], ax=axes[0])
axes[0].set_title('Boxplot - Outliers')
sns.scatterplot(x=range(len(df)), y=df['value'], ax=axes[1])
axes[1].axhline(upper, color='r', linestyle='--')
axes[1].axhline(lower, color='r', linestyle='--')
```

#### ì‹¤ìŠµ ê³¼ì œ

**ê³¼ì œ: ì‹¤ì œ ë°ì´í„°ì…‹ EDA ë¦¬í¬íŠ¸**

```
ë°ì´í„° ì„ íƒ (1ê°œ):
1. Kaggle: House Prices (ì£¼íƒ ê°€ê²© ì˜ˆì¸¡)
2. Kaggle: Titanic (ìƒì¡´ ì˜ˆì¸¡)
3. UCI: Adult Income (ì†Œë“ ë¶„ë¥˜)
4. ìì²´ ìˆ˜ì§‘ ë°ì´í„°

ìš”êµ¬ì‚¬í•­:
1. ë°ì´í„° ê°œìš” (shape, dtypes, ìƒ˜í”Œ)
2. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„ì„:
   - ê¸°ìˆ  í†µê³„ëŸ‰ (mean, std, min, max, percentiles)
   - ë¶„í¬ ì‹œê°í™” (íˆìŠ¤í† ê·¸ë¨, ë°•ìŠ¤í”Œë¡¯)
   - ì™œë„/ì²¨ë„ ë¶„ì„
3. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„:
   - ì¹´í…Œê³ ë¦¬ë³„ ë¹ˆë„
   - ë§‰ëŒ€ ê·¸ë˜í”„
4. ê²°ì¸¡ì¹˜ ë¶„ì„:
   - ê²°ì¸¡ ë¹„ìœ¨
   - ê²°ì¸¡ íŒ¨í„´ (missingno)
   - ê²°ì¸¡ ìœ í˜• ì¶”ì •
5. ì´ìƒì¹˜ ë¶„ì„:
   - IQR / Z-score íƒì§€
   - ì´ìƒì¹˜ íŠ¹ì„± ë¶„ì„
6. ë³€ìˆ˜ ê°„ ê´€ê³„:
   - ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
   - íƒ€ê²Ÿê³¼ì˜ ê´€ê³„ ì‹œê°í™”
7. ì¸ì‚¬ì´íŠ¸ ìš”ì•½ (3-5ê°œ)

ì‚°ì¶œë¬¼:
- Jupyter Notebook (ì½”ë“œ + ë§ˆí¬ë‹¤ìš´ ì„¤ëª…)
- ìµœì†Œ 10ê°œ ì‹œê°í™”
- ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ì„¹ì…˜
```

#### í‰ê°€ ê¸°ì¤€

| í•­ëª© | í†µê³¼ ê¸°ì¤€ | ë°°ì  |
|------|----------|------|
| ì™„ì„±ë„ | 7ê°œ ë¶„ì„ í•­ëª© ëª¨ë‘ í¬í•¨ | 30% |
| ì‹œê°í™” í’ˆì§ˆ | ëª…í™•í•œ ì œëª©, ë ˆì´ë¸”, í•´ì„ | 25% |
| ì¸ì‚¬ì´íŠ¸ | ì‹¤í–‰ ê°€ëŠ¥í•œ ë°œê²¬ 3ê°œ ì´ìƒ | 25% |
| ì½”ë“œ í’ˆì§ˆ | í•¨ìˆ˜í™”, ì£¼ì„, ì¬ì‚¬ìš©ì„± | 10% |
| í”„ë ˆì  í…Œì´ì…˜ | ë…¼ë¦¬ì  íë¦„ | 10% |

#### ì¶”ì²œ ìë£Œ

| ìœ í˜• | ì œëª© | ë§í¬ |
|------|------|------|
| ì±… | Python for Data Analysis (Wes McKinney) | - |
| ì½”ìŠ¤ | Kaggle EDA Course | https://www.kaggle.com/learn/data-visualization |
| ì˜ìƒ | StatQuest - Statistics | https://www.youtube.com/c/joshstarmer |
| ë„êµ¬ | ydata-profiling (ìë™ EDA) | https://github.com/ydataai/ydata-profiling |
| ì°¸ê³  | Kaggle ìš°ìŠ¹ì EDA ë…¸íŠ¸ë¶ | https://www.kaggle.com/notebooks?sortBy=voteCount |

---

### Week 10: ë°ì´í„° ì´í•´ & ì „ì²˜ë¦¬

#### í•™ìŠµ ëª©í‘œ
- [ ] ì¡°ì§ ë‚´ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ë§¤í•‘í•  ìˆ˜ ìˆë‹¤
- [ ] ë°ì´í„° í’ˆì§ˆ 6ì°¨ì›ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆë‹¤
- [ ] ê²°ì¸¡ì¹˜ë¥¼ ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤
- [ ] ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¤ì–‘í•œ ì „ëµì„ ì ìš©í•  ìˆ˜ ìˆë‹¤

#### í•µì‹¬ ê°œë…

**1. ë°ì´í„° ì†ŒìŠ¤ ë§¤í•‘**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ì¡°ì§ ë°ì´í„° ë§µ                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [ìš´ì˜ ì‹œìŠ¤í…œ]                                               â”‚
â”‚  â”œâ”€â”€ ERP (SAP, Oracle)        â†’ ì¬ë¬´, ì¬ê³ , êµ¬ë§¤            â”‚
â”‚  â”œâ”€â”€ CRM (Salesforce)         â†’ ê³ ê°, ì˜ì—…, ë§ˆì¼€íŒ…          â”‚
â”‚  â”œâ”€â”€ WMS (ì°½ê³ ê´€ë¦¬)           â†’ ë¬¼ë¥˜, ë°°ì†¡                  â”‚
â”‚  â””â”€â”€ POS (ë§¤ì¥)               â†’ ê±°ë˜, ê²°ì œ                  â”‚
â”‚                                                              â”‚
â”‚  [ì›¹/ì•±]                                                     â”‚
â”‚  â”œâ”€â”€ ì›¹ ë¡œê·¸ (GA, Adobe)      â†’ í–‰ë™, ì „í™˜                  â”‚
â”‚  â”œâ”€â”€ ì•± ì´ë²¤íŠ¸ (Amplitude)    â†’ ì‚¬ìš© íŒ¨í„´                   â”‚
â”‚  â””â”€â”€ A/B í…ŒìŠ¤íŠ¸ (Optimizely)  â†’ ì‹¤í—˜ ê²°ê³¼                   â”‚
â”‚                                                              â”‚
â”‚  [ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤]                                         â”‚
â”‚  â”œâ”€â”€ Snowflake / BigQuery     â†’ í†µí•© ë¶„ì„                   â”‚
â”‚  â””â”€â”€ Data Lake (S3)           â†’ ì›ì‹œ ë°ì´í„°                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**2. ë°ì´í„° í’ˆì§ˆ 6ì°¨ì›**

| ì°¨ì› | ì§ˆë¬¸ | ì²´í¬ ë°©ë²• |
|------|------|----------|
| **ì™„ì „ì„±** (Completeness) | í•„ìš”í•œ ë°ì´í„°ê°€ ë‹¤ ìˆë‚˜? | NULL ë¹„ìœ¨ ì²´í¬ |
| **ì •í™•ì„±** (Accuracy) | ë°ì´í„°ê°€ ì‹¤ì œì™€ ì¼ì¹˜í•˜ë‚˜? | ìƒ˜í”Œë§ ê²€ì¦ |
| **ì¼ê´€ì„±** (Consistency) | ì‹œìŠ¤í…œ ê°„ ë°ì´í„°ê°€ ì¼ì¹˜í•˜ë‚˜? | A vs B ë¹„êµ |
| **ì ì‹œì„±** (Timeliness) | ë°ì´í„°ê°€ ìµœì‹ ì¸ê°€? | ìµœì¢… ì—…ë°ì´íŠ¸ í™•ì¸ |
| **ìœ íš¨ì„±** (Validity) | ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€? | ì •ê·œì‹ ì²´í¬ |
| **ìœ ì¼ì„±** (Uniqueness) | ì¤‘ë³µì´ ì—†ë‚˜? | ì¤‘ë³µ ë ˆì½”ë“œ íƒì§€ |

```python
"""ë°ì´í„° í’ˆì§ˆ ê¸°ë³¸ í”„ë¡œíŒŒì¼ë§"""
def profile_dataframe(df):
    profile = {
        'row_count': len(df),
        'column_count': len(df.columns),
        'columns': {}
    }

    for col in df.columns:
        col_profile = {
            'dtype': str(df[col].dtype),
            'null_pct': df[col].isnull().mean() * 100,
            'unique_count': df[col].nunique(),
        }

        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¶”ê°€ í†µê³„
        if df[col].dtype in ['int64', 'float64']:
            col_profile.update({
                'min': df[col].min(),
                'max': df[col].max(),
                'mean': df[col].mean(),
            })

        profile['columns'][col] = col_profile

    return profile

# í’ˆì§ˆ ì´ìŠˆ íƒì§€
for col, stats in profile['columns'].items():
    if stats['null_pct'] > 10:
        print(f"âš ï¸ {col}: NULL ë¹„ìœ¨ {stats['null_pct']:.1f}%")
```

**3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬**
```python
# ì‚­ì œ
df.dropna()                        # ì „ì²´ í–‰ ì‚­ì œ
df.dropna(thresh=len(df)*0.5)      # 50% ì´ìƒ ê²°ì¸¡ ì‹œ ì‚­ì œ
df.drop(columns=['col_with_many_na'])  # ì»¬ëŸ¼ ì‚­ì œ

# ëŒ€ì²´ - ë‹¨ìˆœ
df['num'].fillna(df['num'].mean())     # í‰ê· 
df['num'].fillna(df['num'].median())   # ì¤‘ì•™ê°’
df['cat'].fillna(df['cat'].mode()[0])  # ìµœë¹ˆê°’
df['num'].fillna(method='ffill')       # ì• ê°’ìœ¼ë¡œ (ì‹œê³„ì—´)

# ëŒ€ì²´ - ê³ ê¸‰
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# KNN ëŒ€ì²´
imputer = KNNImputer(n_neighbors=5)
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# ë‹¤ì¤‘ ëŒ€ì²´ (MICE)
imputer = IterativeImputer(random_state=42)
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# ê·¸ë£¹ë³„ ëŒ€ì²´
df['value'] = df.groupby('category')['value'].transform(
    lambda x: x.fillna(x.median())
)
```

| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œì  |
|------|------|------|----------|
| ì‚­ì œ | ë‹¨ìˆœ | ë°ì´í„° ì†ì‹¤ | ê²°ì¸¡ 5% ë¯¸ë§Œ, MCAR |
| í‰ê· /ì¤‘ì•™ê°’ | ë‹¨ìˆœ | ë¶„ì‚° ê°ì†Œ | ìˆ˜ì¹˜í˜•, ì ì€ ê²°ì¸¡ |
| KNN | ê´€ê³„ ê³ ë ¤ | ëŠë¦¼ | ì¤‘ê°„ ê·œëª¨ ë°ì´í„° |
| MICE | ë¶ˆí™•ì‹¤ì„± ë°˜ì˜ | ë³µì¡ | ë‹¤ë³€ëŸ‰ ê²°ì¸¡ |

**2. ì´ìƒì¹˜ ì²˜ë¦¬**
```python
# Winsorizing (ê·¹ë‹¨ê°’ í´ë¦¬í•‘)
from scipy.stats import mstats
df['value_winsorized'] = mstats.winsorize(df['value'], limits=[0.05, 0.05])

# Clipping (ê²½ê³„ê°’ìœ¼ë¡œ ì œí•œ)
lower = df['value'].quantile(0.01)
upper = df['value'].quantile(0.99)
df['value_clipped'] = df['value'].clip(lower, upper)

# ë¡œê·¸ ë³€í™˜ (ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ ì¶•ì†Œ)
df['value_log'] = np.log1p(df['value'])  # log(1+x) - 0 ì²˜ë¦¬

# ì‚­ì œ (ëª…í™•í•œ ì˜¤ë¥˜)
df = df[df['age'] <= 120]  # ë¹„í˜„ì‹¤ì  ê°’ ì œê±°

# ë³„ë„ ì²˜ë¦¬ (ë¶„ë¦¬ ëª¨ë¸ë§)
outliers = df[df['value'] > threshold]
normal = df[df['value'] <= threshold]
```

| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œì  |
|------|------|------|----------|
| ì œê±° | ë‹¨ìˆœ | ë°ì´í„° ì†ì‹¤ | ëª…í™•í•œ ì˜¤ë¥˜ |
| Winsorize | ë°ì´í„° ë³´ì¡´ | ì •ë³´ ì†ì‹¤ | ê·¹ë‹¨ê°’ ì¶•ì†Œ |
| ë¡œê·¸ ë³€í™˜ | ë¶„í¬ ì •ê·œí™” | í•´ì„ ë³€í™” | ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ |
| ë³„ë„ ëª¨ë¸ | ì •ë³´ ë³´ì¡´ | ë³µì¡ | ì´ìƒì¹˜ê°€ ì˜ë¯¸ ìˆì„ ë•Œ |

**3. ë°ì´í„° ë³€í™˜**
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import PowerTransformer

# í‘œì¤€í™” (Z-score): í‰ê·  0, í‘œì¤€í¸ì°¨ 1
scaler = StandardScaler()
df['value_std'] = scaler.fit_transform(df[['value']])

# ì •ê·œí™” (Min-Max): 0~1 ë²”ìœ„
scaler = MinMaxScaler()
df['value_norm'] = scaler.fit_transform(df[['value']])

# ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§: ì´ìƒì¹˜ì— ê°•ê±´ (ì¤‘ì•™ê°’, IQR ì‚¬ìš©)
scaler = RobustScaler()
df['value_robust'] = scaler.fit_transform(df[['value']])

# ë¡œê·¸ ë³€í™˜
df['value_log'] = np.log1p(df['value'])

# Box-Cox / Yeo-Johnson: ì •ê·œë¶„í¬ì— ê°€ê¹ê²Œ
pt = PowerTransformer(method='yeo-johnson')  # ìŒìˆ˜ í¬í•¨ ê°€ëŠ¥
df['value_transformed'] = pt.fit_transform(df[['value']])
```

| ë³€í™˜ | ì‚¬ìš© ì‹œì  | ì•Œê³ ë¦¬ì¦˜ |
|------|----------|----------|
| StandardScaler | ëŒ€ë¶€ë¶„ | SVM, ì„ í˜• ëª¨ë¸, ì‹ ê²½ë§ |
| MinMaxScaler | 0-1 ë²”ìœ„ í•„ìš” | ì´ë¯¸ì§€, ì‹ ê²½ë§ |
| RobustScaler | ì´ìƒì¹˜ ì¡´ì¬ | ëŒ€ë¶€ë¶„ |
| Log/Box-Cox | ì™œê³¡ëœ ë¶„í¬ | ì„ í˜• ëª¨ë¸ |

**4. ë²”ì£¼í˜• ì¸ì½”ë”©**
```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import category_encoders as ce

# Label Encoding (ìˆœì„œ ìˆëŠ” ë²”ì£¼)
le = LabelEncoder()
df['size_encoded'] = le.fit_transform(df['size'])  # S=0, M=1, L=2

# One-Hot Encoding (ìˆœì„œ ì—†ëŠ” ë²”ì£¼)
df_encoded = pd.get_dummies(df, columns=['color'], drop_first=True)

# Target Encoding (íƒ€ê²Ÿê³¼ì˜ ê´€ê³„ í™œìš©)
encoder = ce.TargetEncoder(cols=['category'])
df['category_encoded'] = encoder.fit_transform(df['category'], df['target'])

# Frequency Encoding (ë¹ˆë„ ê¸°ë°˜)
freq = df['category'].value_counts(normalize=True)
df['category_freq'] = df['category'].map(freq)

# Binary Encoding (ê³ ì¹´ë””ë„ë¦¬í‹°)
encoder = ce.BinaryEncoder(cols=['high_cardinality_col'])
df_encoded = encoder.fit_transform(df)
```

| ì¸ì½”ë”© | ì¹´ë””ë„ë¦¬í‹° | ì¥ì  | ë‹¨ì  |
|--------|-----------|------|------|
| One-Hot | ë‚®ìŒ (<10) | ì •ë³´ ë³´ì¡´ | ì°¨ì› í­ë°œ |
| Label | ìˆœì„œí˜• | ë‹¨ìˆœ | ìˆœì„œ ê°€ì • |
| Target | ë†’ìŒ | ì˜ˆì¸¡ë ¥ ì¢‹ìŒ | ê³¼ì í•© ìœ„í—˜ |
| Binary | ë†’ìŒ | ì°¨ì› ì ˆì•½ | ì •ë³´ ì†ì‹¤ |

#### ì‹¤ìŠµ ê³¼ì œ

**ê³¼ì œ: ì§€ì €ë¶„í•œ ë°ì´í„° ì •ì œ íŒŒì´í”„ë¼ì¸**

```
ë°ì´í„°: Kaggle "Spaceship Titanic" ë˜ëŠ” ìœ ì‚¬í•œ ì§€ì €ë¶„í•œ ë°ì´í„°

ìš”êµ¬ì‚¬í•­:
1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬:
   - ê²°ì¸¡ íŒ¨í„´ ë¶„ì„ í›„ ì „ëµ ì„ íƒ
   - ìˆ˜ì¹˜í˜•: KNN ë˜ëŠ” ê·¸ë£¹ë³„ ì¤‘ì•™ê°’
   - ë²”ì£¼í˜•: ìµœë¹ˆê°’ ë˜ëŠ” ë³„ë„ ì¹´í…Œê³ ë¦¬
2. ì´ìƒì¹˜ ì²˜ë¦¬:
   - íƒì§€ (IQR, Z-score)
   - ì „ëµ ì„ íƒ ë° ì ìš© (ê·¼ê±° í¬í•¨)
3. ë°ì´í„° ë³€í™˜:
   - ì™œê³¡ëœ ë³€ìˆ˜ ì‹ë³„
   - ì ì ˆí•œ ë³€í™˜ ì ìš©
   - ìŠ¤ì¼€ì¼ë§ ì ìš©
4. ì¸ì½”ë”©:
   - ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„
   - ì ì ˆí•œ ì¸ì½”ë”© ë°©ë²• ì„ íƒ
5. sklearn Pipelineìœ¼ë¡œ êµ¬ì„±

ì‚°ì¶œë¬¼:
- Jupyter Notebook
- ì²˜ë¦¬ ì „/í›„ ë¹„êµ ì‹œê°í™”
- sklearn Pipeline ì½”ë“œ
- ê° ê²°ì •ì— ëŒ€í•œ ê·¼ê±° ë¬¸ì„œí™”
```

#### í‰ê°€ ê¸°ì¤€

| í•­ëª© | í†µê³¼ ê¸°ì¤€ | ë°°ì  |
|------|----------|------|
| ê²°ì¸¡ì¹˜ ì²˜ë¦¬ | ì ì ˆí•œ ì „ëµ + ê·¼ê±° | 25% |
| ì´ìƒì¹˜ ì²˜ë¦¬ | íƒì§€ + ì²˜ë¦¬ + ê·¼ê±° | 25% |
| ë³€í™˜/ì¸ì½”ë”© | ì˜¬ë°”ë¥¸ ë°©ë²• ì„ íƒ | 20% |
| Pipeline | ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ë“œ | 20% |
| ë¬¸ì„œí™” | ê²°ì • ê·¼ê±° ëª…í™• | 10% |

#### ì¶”ì²œ ìë£Œ

| ìœ í˜• | ì œëª© | ë§í¬ |
|------|------|------|
| ì±… | Feature Engineering for ML (Zheng & Casari) | O'Reilly |
| ì½”ìŠ¤ | Kaggle Feature Engineering | https://www.kaggle.com/learn/feature-engineering |
| ë¬¸ì„œ | sklearn Preprocessing | https://scikit-learn.org/stable/modules/preprocessing.html |
| ë¼ì´ë¸ŒëŸ¬ë¦¬ | category_encoders | https://contrib.scikit-learn.org/category_encoders/ |

---

### Week 11: Feature Engineering

#### í•™ìŠµ ëª©í‘œ
- [ ] ìˆ˜ì¹˜í˜• í”¼ì²˜ì—ì„œ ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤
- [ ] ë²”ì£¼í˜• í”¼ì²˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤
- [ ] ì‹œê°„ í”¼ì²˜ë¥¼ ì¶”ì¶œí•˜ê³  í™œìš©í•  ìˆ˜ ìˆë‹¤
- [ ] í…ìŠ¤íŠ¸ì—ì„œ í”¼ì²˜ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆë‹¤

#### í•µì‹¬ ê°œë…

**1. ìˆ˜ì¹˜í˜• í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§**
```python
# Binning (êµ¬ê°„í™”)
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 55, 100],
                         labels=['ì²­ì†Œë…„', 'ì²­ë…„', 'ì¤‘ë…„', 'ë…¸ë…„'])

# Polynomial Features (ë‹¤í•­ íŠ¹ì„±)
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(df[['x1', 'x2']])
# x1, x2, x1^2, x1*x2, x2^2

# ìƒí˜¸ì‘ìš© í”¼ì²˜
df['price_per_sqft'] = df['price'] / df['sqft']
df['room_ratio'] = df['bedrooms'] / df['total_rooms']

# ì§‘ê³„ í”¼ì²˜ (Groupby)
df['user_avg_spend'] = df.groupby('user_id')['amount'].transform('mean')
df['user_order_count'] = df.groupby('user_id')['order_id'].transform('count')
df['category_median_price'] = df.groupby('category')['price'].transform('median')
```

**Kaggle Grandmaster íŒ (ìë£Œì¡°ì‚¬ ë°˜ì˜):**
```python
# Groupby Aggregations - ê°€ì¥ ê°•ë ¥í•œ ê¸°ë²•
agg_features = df.groupby('user_id').agg({
    'amount': ['mean', 'std', 'min', 'max', 'sum', 'count'],
    'category': ['nunique'],
    'timestamp': ['min', 'max']
}).reset_index()
agg_features.columns = ['user_id'] + [f'user_{a}_{b}' for a, b in agg_features.columns[1:]]

# ìˆœìœ„ í”¼ì²˜
df['amount_rank'] = df.groupby('category')['amount'].rank(pct=True)
```

**2. ë²”ì£¼í˜• í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§**
```python
# í¬ê·€ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
freq = df['category'].value_counts(normalize=True)
rare_categories = freq[freq < 0.01].index
df['category_grouped'] = df['category'].replace(rare_categories, 'Other')

# ì¹´í…Œê³ ë¦¬ ì¡°í•©
df['location_type'] = df['city'] + '_' + df['property_type']

# Target í†µê³„ (ì£¼ì˜: ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
from sklearn.model_selection import KFold

def target_encode_cv(df, col, target, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    df[f'{col}_target_enc'] = 0

    for train_idx, val_idx in kf.split(df):
        means = df.iloc[train_idx].groupby(col)[target].mean()
        df.loc[val_idx, f'{col}_target_enc'] = df.loc[val_idx, col].map(means)

    return df
```

**3. ì‹œê°„ í”¼ì²˜**
```python
df['datetime'] = pd.to_datetime(df['timestamp'])

# ê¸°ë³¸ ì¶”ì¶œ
df['year'] = df['datetime'].dt.year
df['month'] = df['datetime'].dt.month
df['day'] = df['datetime'].dt.day
df['dayofweek'] = df['datetime'].dt.dayofweek  # 0=ì›”ìš”ì¼
df['hour'] = df['datetime'].dt.hour
df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)

# ì£¼ê¸°ì„± ì¸ì½”ë”© (ìˆœí™˜ í”¼ì²˜)
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)

# Lag í”¼ì²˜ (ì‹œê³„ì—´)
df['sales_lag_1'] = df.groupby('product')['sales'].shift(1)
df['sales_lag_7'] = df.groupby('product')['sales'].shift(7)
df['sales_rolling_7'] = df.groupby('product')['sales'].transform(
    lambda x: x.shift(1).rolling(7).mean()
)
```

**4. í…ìŠ¤íŠ¸ í”¼ì²˜**
```python
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

# ê¸°ë³¸ í†µê³„
df['text_len'] = df['text'].str.len()
df['word_count'] = df['text'].str.split().str.len()
df['avg_word_len'] = df['text'].apply(lambda x: np.mean([len(w) for w in x.split()]))

# TF-IDF
tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 2))
tfidf_features = tfidf.fit_transform(df['text'])
tfidf_df = pd.DataFrame(tfidf_features.toarray(),
                        columns=[f'tfidf_{w}' for w in tfidf.get_feature_names_out()])

# íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
keywords = ['urgent', 'sale', 'free']
for kw in keywords:
    df[f'has_{kw}'] = df['text'].str.lower().str.contains(kw).astype(int)
```

#### ì‹¤ìŠµ ê³¼ì œ

**ê³¼ì œ: Kaggle ìŠ¤íƒ€ì¼ Feature Engineering**

```
ë°ì´í„°: Kaggle "House Prices" ë˜ëŠ” ìœ ì‚¬ í…Œì´ë¸” ë°ì´í„°

ìš”êµ¬ì‚¬í•­:
1. ì›ë³¸ í”¼ì²˜ ë¶„ì„ (EDA ê°„ëµíˆ)
2. ìˆ˜ì¹˜í˜• í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§:
   - ë¹„ìœ¨/ì¡°í•© í”¼ì²˜ 5ê°œ ì´ìƒ
   - Binning 2ê°œ ì´ìƒ
   - Groupby ì§‘ê³„ 3ê°œ ì´ìƒ
3. ë²”ì£¼í˜• í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§:
   - í¬ê·€ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
   - ì¹´í…Œê³ ë¦¬ ì¡°í•©
   - Target Encoding (CV ë°©ì‹)
4. ì‹œê°„ í”¼ì²˜ (í•´ë‹¹ ì‹œ):
   - ê¸°ë³¸ ì¶”ì¶œ
   - ìˆœí™˜ ì¸ì½”ë”©
5. í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„:
   - ìƒê´€ê´€ê³„
   - RandomForest feature_importances_
6. ìµœì¢… í”¼ì²˜ ì…‹ ì„ ì • ë° ê·¼ê±°

ì‚°ì¶œë¬¼:
- Jupyter Notebook
- í”¼ì²˜ 50ê°œ ì´ìƒ ìƒì„±
- ìµœì¢… ì„ ì • í”¼ì²˜ ëª©ë¡ + ê·¼ê±°
- ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (í”¼ì²˜ ì „/í›„)
```

#### í‰ê°€ ê¸°ì¤€

| í•­ëª© | í†µê³¼ ê¸°ì¤€ | ë°°ì  |
|------|----------|------|
| í”¼ì²˜ ìˆ˜ | 50ê°œ ì´ìƒ ìƒì„± | 20% |
| í”¼ì²˜ í’ˆì§ˆ | ì˜ë¯¸ ìˆëŠ” í”¼ì²˜ ë¹„ìœ¨ | 25% |
| ê¸°ë²• ë‹¤ì–‘ì„± | 4ê°€ì§€ ìœ í˜• ì´ìƒ í™œìš© | 20% |
| ì„±ëŠ¥ ê°œì„  | ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ê°œì„  | 25% |
| ë¬¸ì„œí™” | í”¼ì²˜ ì„¤ëª… ë° ê·¼ê±° | 10% |

#### ì¶”ì²œ ìë£Œ

| ìœ í˜• | ì œëª© | ë§í¬ |
|------|------|------|
| ë…¸íŠ¸ë¶ | Kaggle ìš°ìŠ¹ FE ì‚¬ë¡€ | https://www.kaggle.com/code |
| ì±… | Feature Engineering for ML | O'Reilly |
| ì˜ìƒ | Abhishek Thakur - FE Tips | https://www.youtube.com/c/AbhishekThakurAbhi |
| ë¸”ë¡œê·¸ | NVIDIA Kaggle Grandmaster Tips | https://developer.nvidia.com/blog/kaggle-grandmasters-unveil-winning-strategies/ |

---

### Week 12: Feature Selection & ì°¨ì› ì¶•ì†Œ

#### í•™ìŠµ ëª©í‘œ
- [ ] Filter, Wrapper, Embedded ë°©ë²•ìœ¼ë¡œ í”¼ì²˜ë¥¼ ì„ íƒí•  ìˆ˜ ìˆë‹¤
- [ ] PCAë¥¼ ì ìš©í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•  ìˆ˜ ìˆë‹¤
- [ ] t-SNE, UMAPìœ¼ë¡œ ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤
- [ ] ì ì ˆí•œ í”¼ì²˜ ìˆ˜ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤

#### í•µì‹¬ ê°œë…

**1. Filter Methods**
```python
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

# ë¶„ì‚° ê¸°ë°˜
from sklearn.feature_selection import VarianceThreshold
selector = VarianceThreshold(threshold=0.01)
X_filtered = selector.fit_transform(X)

# ìƒê´€ê´€ê³„ ê¸°ë°˜
correlation_matrix = X.corr().abs()
upper = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
to_drop = [col for col in upper.columns if any(upper[col] > 0.95)]

# í†µê³„ ê²€ì • (ANOVA F-value)
selector = SelectKBest(f_classif, k=20)
X_selected = selector.fit_transform(X, y)

# Mutual Information
selector = SelectKBest(mutual_info_classif, k=20)
X_selected = selector.fit_transform(X, y)
```

**2. Wrapper Methods**
```python
from sklearn.feature_selection import RFE, RFECV
from sklearn.ensemble import RandomForestClassifier

# Recursive Feature Elimination
estimator = RandomForestClassifier(n_estimators=100, random_state=42)
selector = RFE(estimator, n_features_to_select=20, step=1)
selector.fit(X, y)
selected_features = X.columns[selector.support_]

# RFE with Cross-Validation (ìµœì  í”¼ì²˜ ìˆ˜ ìë™ ê²°ì •)
selector = RFECV(estimator, step=1, cv=5, scoring='accuracy')
selector.fit(X, y)
print(f"ìµœì  í”¼ì²˜ ìˆ˜: {selector.n_features_}")

# Sequential Feature Selection (Forward/Backward)
from sklearn.feature_selection import SequentialFeatureSelector
sfs = SequentialFeatureSelector(estimator, n_features_to_select=20, direction='forward')
sfs.fit(X, y)
```

**3. Embedded Methods**
```python
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestClassifier

# Lasso (L1 Regularization)
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X, y)
selected_features = X.columns[lasso.coef_ != 0]

# Tree-based Feature Importance
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)
importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

# Permutation Importance (ë” ì‹ ë¢°ì„± ìˆìŒ)
from sklearn.inspection import permutation_importance
perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10)
```

| ë°©ë²• | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œì  |
|------|------|------|----------|
| Filter | ë¹ ë¦„, ëª¨ë¸ ë¬´ê´€ | í”¼ì²˜ ìƒí˜¸ì‘ìš© ë¬´ì‹œ | ì „ì²˜ë¦¬, ë¹ ë¥¸ ê°ì†Œ |
| Wrapper | ì •í™• | ëŠë¦¼, ê³¼ì í•© ê°€ëŠ¥ | ì‘ì€ í”¼ì²˜ì…‹ |
| Embedded | ë¹ ë¦„, ì •í™• | ëª¨ë¸ ì˜ì¡´ | ëŒ€ë¶€ë¶„ ìƒí™© |

**4. ì°¨ì› ì¶•ì†Œ**
```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap

# PCA
pca = PCA(n_components=0.95)  # 95% ë¶„ì‚° ìœ ì§€
X_pca = pca.fit_transform(X_scaled)
print(f"ì›ë³¸: {X.shape[1]} â†’ PCA: {X_pca.shape[1]}")

# Explained Variance
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')

# t-SNE (ì‹œê°í™”ìš©, 2D/3D)
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')

# UMAP (t-SNEë³´ë‹¤ ë¹ ë¥´ê³  êµ¬ì¡° ë³´ì¡´ ì¢‹ìŒ)
reducer = umap.UMAP(n_components=2, random_state=42)
X_umap = reducer.fit_transform(X_scaled)
```

| ë°©ë²• | ìš©ë„ | íŠ¹ì§• |
|------|------|------|
| PCA | í”¼ì²˜ ì••ì¶•, ë…¸ì´ì¦ˆ ì œê±° | ì„ í˜•, ë¹ ë¦„ |
| t-SNE | ì‹œê°í™” (2D/3D) | ë¹„ì„ í˜•, ëŠë¦¼, í´ëŸ¬ìŠ¤í„° ê°•ì¡° |
| UMAP | ì‹œê°í™” + í”¼ì²˜ | ë¹„ì„ í˜•, ë¹ ë¦„, ì „ì—­ êµ¬ì¡° ë³´ì¡´ |

#### ì‹¤ìŠµ ê³¼ì œ

**ê³¼ì œ: ê³ ì°¨ì› ë°ì´í„° í”¼ì²˜ ì„ íƒ & ì‹œê°í™”**

```
ë°ì´í„°: ê³ ì°¨ì› ë°ì´í„° (í”¼ì²˜ 50ê°œ ì´ìƒ)
- Kaggle ëŒ€íšŒ ë°ì´í„°
- ìœ ì „ì ë°œí˜„ ë°ì´í„°
- ì„¼ì„œ ë°ì´í„°

ìš”êµ¬ì‚¬í•­:
1. Filter Methods:
   - ë¶„ì‚°, ìƒê´€ê´€ê³„ ê¸°ë°˜ í•„í„°ë§
   - ê²°ê³¼ ë¹„êµ (ì œê±°ëœ í”¼ì²˜ ëª©ë¡)
2. Wrapper Methods:
   - RFE ë˜ëŠ” SFS ì ìš©
   - ìµœì  í”¼ì²˜ ìˆ˜ ê²°ì •
3. Embedded Methods:
   - Lasso ë˜ëŠ” Tree Importance
   - Permutation Importance ë¹„êµ
4. ì°¨ì› ì¶•ì†Œ:
   - PCA ì ìš© (explained variance plot)
   - t-SNE / UMAP ì‹œê°í™”
   - í´ëŸ¬ìŠ¤í„° í•´ì„
5. ì¢…í•© ë¹„êµ:
   - ê° ë°©ë²•ìœ¼ë¡œ ì„ íƒëœ í”¼ì²˜ ë²¤ ë‹¤ì´ì–´ê·¸ë¨
   - ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (ì „ì²´ vs ì„ íƒëœ í”¼ì²˜)

ì‚°ì¶œë¬¼:
- Jupyter Notebook
- í”¼ì²˜ ì„ íƒ ë¹„êµ í‘œ
- 2D ì‹œê°í™” (t-SNE/UMAP)
- ìµœì¢… ì¶”ì²œ í”¼ì²˜ì…‹
```

#### í‰ê°€ ê¸°ì¤€

| í•­ëª© | í†µê³¼ ê¸°ì¤€ | ë°°ì  |
|------|----------|------|
| Filter ì ìš© | 2ê°€ì§€ ì´ìƒ ë°©ë²• | 20% |
| Wrapper ì ìš© | RFE/SFS ì¤‘ 1ê°œ | 20% |
| Embedded ì ìš© | Importance ë¶„ì„ | 20% |
| ì°¨ì› ì¶•ì†Œ | PCA + ì‹œê°í™” | 25% |
| ì¢…í•© ë¶„ì„ | ì„±ëŠ¥ ë¹„êµ ë° ê²°ë¡  | 15% |

#### ì¶”ì²œ ìë£Œ

| ìœ í˜• | ì œëª© | ë§í¬ |
|------|------|------|
| ë¬¸ì„œ | sklearn Feature Selection | https://scikit-learn.org/stable/modules/feature_selection.html |
| ì˜ìƒ | StatQuest - PCA | https://www.youtube.com/watch?v=FgakZw6K1QQ |
| ë…¼ë¬¸ | UMAP Paper | https://arxiv.org/abs/1802.03426 |
| ë„êµ¬ | UMAP Documentation | https://umap-learn.readthedocs.io/ |

---

## Month 4: ML ëª¨ë¸ë§ & ì´ìƒíƒì§€

---

### Week 13: ê°€ì„¤ ê¸°ë°˜ ë¶„ì„ & ë¶„ë¥˜/íšŒê·€

#### í•™ìŠµ ëª©í‘œ
- [ ] ê°€ì„¤ ê¸°ë°˜ ë¶„ì„ ì ‘ê·¼ë²•ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤
- [ ] ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤
- [ ] ë¶„ë¥˜/íšŒê·€ ë¬¸ì œë¥¼ êµ¬ë¶„í•˜ê³  ì ì ˆí•œ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤
- [ ] XGBoost, LightGBMì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤

#### í•µì‹¬ ê°œë…

**1. ê°€ì„¤ ê¸°ë°˜ ë¶„ì„ (Hypothesis-Driven)**

ëª¨ë¸ë§ ì „ì— "ì™œ ì´ ëª¨ë¸ì„ ë§Œë“œëŠ”ê°€?"ë¥¼ ëª…í™•íˆ í•´ì•¼ í•©ë‹ˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ê°€ì„¤ ê¸°ë°˜ ë¶„ì„ í”„ë¡œì„¸ìŠ¤                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. ë¬¸ì œ ì •ì˜                                                â”‚
â”‚     "ì¬êµ¬ë§¤ìœ¨ì´ 30% í•˜ë½í•œ ì´ìœ ëŠ”?"                          â”‚
â”‚              â”‚                                               â”‚
â”‚              â–¼                                               â”‚
â”‚  2. ê°€ì„¤ ìˆ˜ë¦½                                                â”‚
â”‚     H1: ë°°ì†¡ ì‹œê°„ ì¦ê°€ â†’ ë§Œì¡±ë„ í•˜ë½ â†’ ì¬êµ¬ë§¤ ê°ì†Œ          â”‚
â”‚     H2: ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜ â†’ ê³ ê° ì´íƒˆ                         â”‚
â”‚     H3: ì œí’ˆ í’ˆì§ˆ í•˜ë½ â†’ ë°˜í’ˆ ì¦ê°€ â†’ ì¬êµ¬ë§¤ ê°ì†Œ            â”‚
â”‚              â”‚                                               â”‚
â”‚              â–¼                                               â”‚
â”‚  3. ë°ì´í„° ë¶„ì„ (ê²€ì¦)                                       â”‚
â”‚     ê° ê°€ì„¤ì— ëŒ€í•œ ì¦ê±° ìˆ˜ì§‘                                 â”‚
â”‚              â”‚                                               â”‚
â”‚              â–¼                                               â”‚
â”‚  4. ê²°ë¡  ë„ì¶œ                                                â”‚
â”‚     "H1 ì§€ì§€: ë°°ì†¡ ì‹œê°„ 3ì¼â†’5ì¼, ì¬êµ¬ë§¤ìœ¨ -25%p ìƒê´€"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì¢‹ì€ ê°€ì„¤ì˜ ì¡°ê±´**
- âœ… êµ¬ì²´ì : "ë°°ì†¡ 3ì¼ ì´ìƒì´ë©´ ì¬êµ¬ë§¤ìœ¨ 20% í•˜ë½"
- âœ… ì¸¡ì • ê°€ëŠ¥: ë°ì´í„°ë¡œ ê²€ì¦ ê°€ëŠ¥
- âœ… í–‰ë™ ê°€ëŠ¥: ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ í–‰ë™ ê°€ëŠ¥
- âŒ ë‚˜ìœ ì˜ˆ: "ê³ ê°ì´ ë¶ˆë§Œì¡±í•´ì„œ ê·¸ë ‡ë‹¤" (ë„ˆë¬´ ëª¨í˜¸)

**2. ìƒê´€ê´€ê³„ vs ì¸ê³¼ê´€ê³„**

```python
# ìƒê´€ê´€ê³„ â‰  ì¸ê³¼ê´€ê³„
# ì˜ˆì‹œ: ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰ê³¼ ìµì‚¬ ì‚¬ê³  (ìƒê´€ê³„ìˆ˜ 0.95)
# í•˜ì§€ë§Œ ì¸ê³¼ê´€ê³„? âŒ
# ì§„ì§œ ì›ì¸: ì—¬ë¦„ (ë”ìš´ ë‚ ì”¨ â†’ ìˆ˜ì˜ ì¦ê°€ + ì•„ì´ìŠ¤í¬ë¦¼ ì†Œë¹„ ì¦ê°€)

# ì¸ê³¼ê´€ê³„ íŒë‹¨ ê¸°ì¤€
# 1. ì‹œê°„ì  ì„ í›„ê´€ê³„: ì›ì¸ì´ ê²°ê³¼ë³´ë‹¤ ë¨¼ì € ë°œìƒ
# 2. ìƒê´€ê´€ê³„: í†µê³„ì  ì—°ê´€ì„± ì¡´ì¬
# 3. ë‹¤ë¥¸ ì„¤ëª… ë°°ì œ: êµë€ë³€ìˆ˜ í†µì œ
# 4. ì‹¤í—˜: ë¬´ì‘ìœ„ í†µì œ ì‹¤í—˜ (Gold Standard)
```

**3. ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜**
```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb

# Logistic Regression
lr = LogisticRegression(max_iter=1000, random_state=42)
lr.fit(X_train, y_train)

# Random Forest
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)

# XGBoost
xgb_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# LightGBM (ê°€ì¥ ë¹ ë¦„)
lgb_model = lgb.LGBMClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    num_leaves=31,
    random_state=42
)
```

**2. íšŒê·€ ì•Œê³ ë¦¬ì¦˜**
```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# Ridge Regression (L2)
ridge = Ridge(alpha=1.0)

# Lasso (L1, í”¼ì²˜ ì„ íƒ íš¨ê³¼)
lasso = Lasso(alpha=0.1)

# ElasticNet (L1 + L2)
elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)

# XGBoost Regressor
xgb_reg = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)
```

**3. êµì°¨ ê²€ì¦ & íŠœë‹**
```python
from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import StratifiedKFold
import optuna

# K-Fold Cross Validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=cv, scoring='f1')
print(f"F1: {scores.mean():.4f} (+/- {scores.std()*2:.4f})")

# Grid Search
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, 15],
    'learning_rate': [0.01, 0.1]
}
grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='f1', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Optuna (ë” íš¨ìœ¨ì )
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
    }
    model = lgb.LGBMClassifier(**params, random_state=42)
    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)
```

**4. í‰ê°€ ì§€í‘œ**
```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    mean_squared_error, mean_absolute_error, r2_score
)

# ë¶„ë¥˜ ì§€í‘œ
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print(classification_report(y_test, y_pred))
print(f"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}")

# Confusion Matrix ì‹œê°í™”
from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_predictions(y_test, y_pred)

# íšŒê·€ ì§€í‘œ
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.4f}")
print(f"RÂ²: {r2_score(y_test, y_pred):.4f}")
```

| ë¬¸ì œ ìœ í˜• | ì§€í‘œ | ì‚¬ìš© ì‹œì  |
|----------|------|----------|
| ë¶„ë¥˜ (ê· í˜•) | Accuracy, F1 | í´ë˜ìŠ¤ ê· í˜• |
| ë¶„ë¥˜ (ë¶ˆê· í˜•) | F1, AUC, Recall | ì´ìƒíƒì§€, ì‚¬ê¸°íƒì§€ |
| íšŒê·€ | RMSE, MAE | ì—°ì†ê°’ ì˜ˆì¸¡ |
| ë­í‚¹ | NDCG, MRR | ì¶”ì²œ, ê²€ìƒ‰ |

#### ì‹¤ìŠµ ê³¼ì œ

**ê³¼ì œ: ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸**

```
ë°ì´í„°: Telco Customer Churn ë˜ëŠ” ìœ ì‚¬ ë°ì´í„°

ìš”êµ¬ì‚¬í•­:
1. ë°ì´í„° ì „ì²˜ë¦¬ (Week 10-12 ê¸°ë²• ì ìš©)
2. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ (Logistic Regression)
3. ì•™ìƒë¸” ëª¨ë¸:
   - Random Forest
   - XGBoost
   - LightGBM
4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Optuna ê¶Œì¥)
5. ëª¨ë¸ ë¹„êµ:
   - 5-Fold CV ì„±ëŠ¥
   - ROC Curve ë¹„êµ
   - í˜¼ë™ í–‰ë ¬
6. í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
7. ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ê·¼ê±°

ì‚°ì¶œë¬¼:
- Jupyter Notebook
- ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í‘œ
- ROC Curve ê·¸ë˜í”„
- ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ (ì´íƒˆ ìš”ì¸ ë¶„ì„)
```

#### í‰ê°€ ê¸°ì¤€

| í•­ëª© | í†µê³¼ ê¸°ì¤€ | ë°°ì  |
|------|----------|------|
| ì „ì²˜ë¦¬ | ì²´ê³„ì  íŒŒì´í”„ë¼ì¸ | 15% |
| ëª¨ë¸ë§ | 3ê°œ ì´ìƒ ëª¨ë¸ | 25% |
| íŠœë‹ | CV + í•˜ì´í¼íŒŒë¼ë¯¸í„° | 25% |
| í‰ê°€ | ì ì ˆí•œ ì§€í‘œ + ë¹„êµ | 20% |
| í•´ì„ | ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ | 15% |

#### ì¶”ì²œ ìë£Œ

| ìœ í˜• | ì œëª© | ë§í¬ |
|------|------|------|
| ë¬¸ì„œ | XGBoost Documentation | https://xgboost.readthedocs.io/ |
| ë¬¸ì„œ | LightGBM Documentation | https://lightgbm.readthedocs.io/ |
| ë¬¸ì„œ | Optuna Tutorial | https://optuna.org/ |
| ì˜ìƒ | StatQuest - XGBoost | https://www.youtube.com/watch?v=OtD8wVaFm6E |

---

### Week 14: í´ëŸ¬ìŠ¤í„°ë§ & ì„¸ê·¸ë©˜í…Œì´ì…˜

#### í•™ìŠµ ëª©í‘œ
- [ ] K-means í´ëŸ¬ìŠ¤í„°ë§ì„ ì ìš©í•˜ê³  ìµœì  Kë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤
- [ ] ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§ê³¼ ë´ë“œë¡œê·¸ë¨ì„ í•´ì„í•  ìˆ˜ ìˆë‹¤
- [ ] DBSCANìœ¼ë¡œ ë°€ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- [ ] RFM ë¶„ì„ìœ¼ë¡œ ê³ ê°ì„ ì„¸ê·¸ë©˜í…Œì´ì…˜í•  ìˆ˜ ìˆë‹¤

#### í•µì‹¬ ê°œë…

**1. K-means**
```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Elbow Method
inertias = []
silhouettes = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)
    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))

# ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].plot(K_range, inertias, 'bo-')
axes[0].set_title('Elbow Method')
axes[1].plot(K_range, silhouettes, 'ro-')
axes[1].set_title('Silhouette Score')

# ìµœì¢… ëª¨ë¸
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
df['cluster'] = kmeans.fit_predict(X_scaled)
```

**2. ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§**
```python
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.cluster import AgglomerativeClustering

# Dendrogram
Z = linkage(X_scaled, method='ward')
plt.figure(figsize=(12, 6))
dendrogram(Z, truncate_mode='lastp', p=30)
plt.title('Hierarchical Clustering Dendrogram')

# í´ëŸ¬ìŠ¤í„° ì¶”ì¶œ
clusters = fcluster(Z, t=4, criterion='maxclust')  # 4ê°œ í´ëŸ¬ìŠ¤í„°

# sklearn ë°©ì‹
agg = AgglomerativeClustering(n_clusters=4, linkage='ward')
df['cluster'] = agg.fit_predict(X_scaled)
```

**3. DBSCAN**
```python
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors

# eps ê²°ì • (k-distance plot)
k = 5
nn = NearestNeighbors(n_neighbors=k)
nn.fit(X_scaled)
distances, _ = nn.kneighbors(X_scaled)
distances = np.sort(distances[:, k-1])
plt.plot(distances)
plt.title('K-Distance Plot (Elbow = eps)')

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
df['cluster'] = dbscan.fit_predict(X_scaled)

# ë…¸ì´ì¦ˆ í¬ì¸íŠ¸
noise_points = df[df['cluster'] == -1]
print(f"ë…¸ì´ì¦ˆ: {len(noise_points)} ({len(noise_points)/len(df)*100:.1f}%)")
```

| ì•Œê³ ë¦¬ì¦˜ | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œì  |
|----------|------|------|----------|
| K-means | ë¹ ë¦„, í™•ì¥ì„± | K ì‚¬ì „ ì§€ì •, êµ¬í˜• ê°€ì • | ëŒ€ë¶€ë¶„ ìƒí™© |
| ê³„ì¸µì  | ë´ë“œë¡œê·¸ë¨, K ë¶ˆí•„ìš” | ëŠë¦¼, ëŒ€ìš©ëŸ‰ ë¶ˆê°€ | ì†Œê·œëª¨, íƒìƒ‰ì  |
| DBSCAN | ì„ì˜ í˜•íƒœ, ì´ìƒì¹˜ íƒì§€ | ë°€ë„ ì°¨ì´ì— ì•½í•¨ | ë…¸ì´ì¦ˆ ìˆëŠ” ë°ì´í„° |

**4. RFM ë¶„ì„**
```python
# RFM ê³„ì‚°
snapshot_date = df['order_date'].max() + pd.Timedelta(days=1)

rfm = df.groupby('customer_id').agg({
    'order_date': lambda x: (snapshot_date - x.max()).days,  # Recency
    'order_id': 'count',                                      # Frequency
    'amount': 'sum'                                           # Monetary
}).rename(columns={
    'order_date': 'Recency',
    'order_id': 'Frequency',
    'amount': 'Monetary'
})

# ì ìˆ˜í™” (1-5)
rfm['R_Score'] = pd.qcut(rfm['Recency'], 5, labels=[5,4,3,2,1])  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])
rfm['M_Score'] = pd.qcut(rfm['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5])

rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)

# ì„¸ê·¸ë¨¼íŠ¸ ì •ì˜
def segment(row):
    r, f, m = int(row['R_Score']), int(row['F_Score']), int(row['M_Score'])
    if r >= 4 and f >= 4:
        return 'Champions'
    elif r >= 3 and f >= 3:
        return 'Loyal'
    elif r >= 4 and f <= 2:
        return 'New Customers'
    elif r <= 2 and f >= 3:
        return 'At Risk'
    elif r <= 2 and f <= 2:
        return 'Lost'
    else:
        return 'Others'

rfm['Segment'] = rfm.apply(segment, axis=1)
```

#### ì‹¤ìŠµ ê³¼ì œ

**ê³¼ì œ: ê³ ê° ì„¸ê·¸ë©˜í…Œì´ì…˜ í”„ë¡œì íŠ¸**

```
ë°ì´í„°: ì´ì»¤ë¨¸ìŠ¤ ì£¼ë¬¸ ë°ì´í„°

ìš”êµ¬ì‚¬í•­:
1. RFM ë¶„ì„:
   - RFM ì ìˆ˜ ê³„ì‚°
   - ì„¸ê·¸ë¨¼íŠ¸ ì •ì˜ (5ê°œ ì´ìƒ)
   - ì„¸ê·¸ë¨¼íŠ¸ë³„ íŠ¹ì„± ë¶„ì„
2. í´ëŸ¬ìŠ¤í„°ë§:
   - K-means (Elbow, Silhouette)
   - ê³„ì¸µì  (Dendrogram)
   - DBSCAN (eps ê²°ì •)
3. í´ëŸ¬ìŠ¤í„° í”„ë¡œíŒŒì¼ë§:
   - ê° í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ìš”ì•½
   - ì‹œê°í™” (PCA/t-SNE 2D)
4. RFM vs í´ëŸ¬ìŠ¤í„°ë§ ë¹„êµ
5. ë¹„ì¦ˆë‹ˆìŠ¤ ì•¡ì…˜ ì œì•ˆ:
   - ì„¸ê·¸ë¨¼íŠ¸ë³„ ë§ˆì¼€íŒ… ì „ëµ
   - ìš°ì„ ìˆœìœ„ ê³ ê° ê·¸ë£¹

ì‚°ì¶œë¬¼:
- Jupyter Notebook
- ì„¸ê·¸ë¨¼íŠ¸ í”„ë¡œíŒŒì¼ í…Œì´ë¸”
- í´ëŸ¬ìŠ¤í„° ì‹œê°í™”
- ë§ˆì¼€íŒ… ì „ëµ ì œì•ˆì„œ
```

#### í‰ê°€ ê¸°ì¤€

| í•­ëª© | í†µê³¼ ê¸°ì¤€ | ë°°ì  |
|------|----------|------|
| RFM ë¶„ì„ | ì ìˆ˜í™” + ì„¸ê·¸ë¨¼íŠ¸ | 25% |
| í´ëŸ¬ìŠ¤í„°ë§ | 3ê°€ì§€ ë°©ë²• ë¹„êµ | 25% |
| í”„ë¡œíŒŒì¼ë§ | í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ëª…í™• | 20% |
| ì‹œê°í™” | 2D ì‹œê°í™” + í•´ì„ | 15% |
| ì „ëµ ì œì•ˆ | ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ | 15% |

#### ì¶”ì²œ ìë£Œ

| ìœ í˜• | ì œëª© | ë§í¬ |
|------|------|------|
| ì˜ìƒ | StatQuest - Clustering | https://www.youtube.com/watch?v=4b5d3muPQmA |
| ë¬¸ì„œ | sklearn Clustering | https://scikit-learn.org/stable/modules/clustering.html |
| ë¸”ë¡œê·¸ | RFM Analysis Guide | https://clevertap.com/blog/rfm-analysis/ |

---

### Week 15: ì´ìƒ íƒì§€ (Anomaly Detection)

#### í•™ìŠµ ëª©í‘œ
- [ ] ë‹¤ì–‘í•œ ì´ìƒíƒì§€ ê¸°ë²•ì„ ì´í•´í•˜ê³  ì ìš©í•  ìˆ˜ ìˆë‹¤
- [ ] Isolation Forestë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•  ìˆ˜ ìˆë‹¤
- [ ] Autoencoder ê¸°ë°˜ ì´ìƒíƒì§€ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
- [ ] ì ì ˆí•œ ì„ê³„ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆë‹¤

#### í•µì‹¬ ê°œë…

**1. í†µê³„ì  ë°©ë²•**
```python
# Z-score
from scipy import stats
z_scores = np.abs(stats.zscore(df[numerical_cols]))
outliers = (z_scores > 3).any(axis=1)

# IQR
Q1 = df[numerical_cols].quantile(0.25)
Q3 = df[numerical_cols].quantile(0.75)
IQR = Q3 - Q1
outliers = ((df[numerical_cols] < (Q1 - 1.5 * IQR)) |
            (df[numerical_cols] > (Q3 + 1.5 * IQR))).any(axis=1)

# Mahalanobis Distance (ë‹¤ë³€ëŸ‰)
from scipy.spatial.distance import mahalanobis
mean = df[numerical_cols].mean()
cov = df[numerical_cols].cov()
cov_inv = np.linalg.inv(cov)
df['mahalanobis'] = df[numerical_cols].apply(
    lambda x: mahalanobis(x, mean, cov_inv), axis=1
)
```

**2. ë°€ë„ ê¸°ë°˜ (LOF, DBSCAN)**
```python
from sklearn.neighbors import LocalOutlierFactor

# Local Outlier Factor
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)
df['lof_outlier'] = lof.fit_predict(X_scaled)  # -1: ì´ìƒ, 1: ì •ìƒ
df['lof_score'] = -lof.negative_outlier_factor_  # ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì´ìƒ)

# DBSCAN (ë…¸ì´ì¦ˆ = ì´ìƒ)
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
df['dbscan_outlier'] = dbscan.fit_predict(X_scaled)
outliers = df[df['dbscan_outlier'] == -1]
```

**3. Isolation Forest**
```python
from sklearn.ensemble import IsolationForest

# í•™ìŠµ
iso_forest = IsolationForest(
    n_estimators=100,
    contamination=0.05,  # ì˜ˆìƒ ì´ìƒì¹˜ ë¹„ìœ¨
    random_state=42
)
df['iso_outlier'] = iso_forest.fit_predict(X_scaled)  # -1: ì´ìƒ
df['iso_score'] = -iso_forest.decision_function(X_scaled)  # ì ìˆ˜

# ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(14, 5))
axes[0].scatter(X_scaled[:, 0], X_scaled[:, 1],
                c=df['iso_outlier'], cmap='RdBu')
axes[0].set_title('Isolation Forest Results')

# ì ìˆ˜ ë¶„í¬
axes[1].hist(df['iso_score'], bins=50)
axes[1].axvline(np.percentile(df['iso_score'], 95), color='r', linestyle='--')
axes[1].set_title('Anomaly Score Distribution')
```

**4. Autoencoder**
```python
import tensorflow as tf
from tensorflow.keras import layers, Model

# Autoencoder ì •ì˜
class Autoencoder(Model):
    def __init__(self, input_dim, latent_dim=8):
        super().__init__()
        self.encoder = tf.keras.Sequential([
            layers.Dense(32, activation='relu'),
            layers.Dense(16, activation='relu'),
            layers.Dense(latent_dim, activation='relu')
        ])
        self.decoder = tf.keras.Sequential([
            layers.Dense(16, activation='relu'),
            layers.Dense(32, activation='relu'),
            layers.Dense(input_dim, activation='sigmoid')
        ])

    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# í•™ìŠµ (ì •ìƒ ë°ì´í„°ë§Œ)
autoencoder = Autoencoder(input_dim=X_train.shape[1])
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.fit(X_train_normal, X_train_normal,
                epochs=50, batch_size=32, validation_split=0.1)

# ì´ìƒ íƒì§€ (ì¬êµ¬ì„± ì˜¤ì°¨)
reconstructed = autoencoder.predict(X_test)
mse = np.mean((X_test - reconstructed) ** 2, axis=1)
threshold = np.percentile(mse, 95)
df['ae_outlier'] = (mse > threshold).astype(int)
```

**ì‹¤ë¬´ ì‚¬ë¡€ (ìë£Œì¡°ì‚¬ ë°˜ì˜):**

| ê¸°ì—… | ë°©ë²• | ì„±ê³¼ |
|------|------|------|
| Mastercard | Decision Intelligence | íƒì§€ìœ¨ 300% í–¥ìƒ, ì˜¤íƒ 85% ê°ì†Œ |
| PayPal | Isolation Forest | ìˆ˜ë°±ë§Œ ì‚¬ìš©ì ë³´í˜¸ |
| ì œì¡°ì—… | Autoencoder | ë‹¤ìš´íƒ€ì„ 40% ê°ì†Œ |

#### ì‹¤ìŠµ ê³¼ì œ

**ê³¼ì œ: ê¸ˆìœµ ì‚¬ê¸° íƒì§€ ì‹œìŠ¤í…œ**

```
ë°ì´í„°: Kaggle Credit Card Fraud Detection

ìš”êµ¬ì‚¬í•­:
1. ë°ì´í„° ë¶„ì„:
   - í´ë˜ìŠ¤ ë¶ˆê· í˜• í™•ì¸
   - ì •ìƒ vs ì‚¬ê¸° íŠ¹ì„± ë¹„êµ
2. ì´ìƒíƒì§€ ëª¨ë¸:
   - Z-score / IQR (ë² ì´ìŠ¤ë¼ì¸)
   - Isolation Forest
   - LOF
   - Autoencoder
3. ëª¨ë¸ ë¹„êµ:
   - Precision, Recall, F1 (íŠ¹íˆ Recall ì¤‘ìš”)
   - ROC-AUC
   - Precision-Recall Curve
4. ì„ê³„ê°’ ì¡°ì •:
   - ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì„ê³„ê°’
   - Precision-Recall íŠ¸ë ˆì´ë“œì˜¤í”„
5. ì•™ìƒë¸”:
   - ì—¬ëŸ¬ ëª¨ë¸ ê²°í•©
   - íˆ¬í‘œ ë˜ëŠ” ìŠ¤íƒœí‚¹

ì‚°ì¶œë¬¼:
- Jupyter Notebook
- ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í‘œ
- ì„ê³„ê°’ ë¶„ì„ ê·¸ë˜í”„
- ìš´ì˜ ê°€ì´ë“œ (ì„ê³„ê°’ ì¡°ì • ë°©ë²•)
```

#### í‰ê°€ ê¸°ì¤€

| í•­ëª© | í†µê³¼ ê¸°ì¤€ | ë°°ì  |
|------|----------|------|
| ë°ì´í„° ë¶„ì„ | ë¶ˆê· í˜• ì²˜ë¦¬ ì „ëµ | 15% |
| ëª¨ë¸ êµ¬í˜„ | 4ê°œ ë°©ë²• ëª¨ë‘ | 30% |
| í‰ê°€ | ì ì ˆí•œ ì§€í‘œ ì‚¬ìš© | 20% |
| ì„ê³„ê°’ | ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì  ë¶„ì„ | 20% |
| ì•™ìƒë¸” | ì„±ëŠ¥ ê°œì„  | 15% |

#### ì¶”ì²œ ìë£Œ

| ìœ í˜• | ì œëª© | ë§í¬ |
|------|------|------|
| ë°ì´í„° | Credit Card Fraud | https://www.kaggle.com/mlg-ulb/creditcardfraud |
| ë¬¸ì„œ | PyOD Library | https://pyod.readthedocs.io/ |
| ë…¼ë¬¸ | Isolation Forest | https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf |
| ì˜ìƒ | Autoencoder for AD | https://www.youtube.com/watch?v=2K3ScZp1dXQ |

---

### Week 16: ì‹œê³„ì—´ ë¶„ì„

#### í•™ìŠµ ëª©í‘œ
- [ ] ì‹œê³„ì—´ ë°ì´í„°ì˜ êµ¬ì„± ìš”ì†Œë¥¼ ë¶„í•´í•  ìˆ˜ ìˆë‹¤
- [ ] ì •ìƒì„±ì„ ê²€ì •í•˜ê³  ë³€í™˜í•  ìˆ˜ ìˆë‹¤
- [ ] Prophetìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤
- [ ] ML ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤

#### í•µì‹¬ ê°œë…

**1. ì‹œê³„ì—´ ë¶„í•´**
```python
from statsmodels.tsa.seasonal import seasonal_decompose

# ì‹œê³„ì—´ ë¶„í•´
result = seasonal_decompose(df['value'], model='additive', period=7)

fig, axes = plt.subplots(4, 1, figsize=(12, 10))
result.observed.plot(ax=axes[0], title='Original')
result.trend.plot(ax=axes[1], title='Trend')
result.seasonal.plot(ax=axes[2], title='Seasonal')
result.resid.plot(ax=axes[3], title='Residual')
plt.tight_layout()
```

**2. ì •ìƒì„± ê²€ì • & ë³€í™˜**
```python
from statsmodels.tsa.stattools import adfuller, kpss

# ADF ê²€ì • (ê·€ë¬´ê°€ì„¤: ë¹„ì •ìƒ)
result = adfuller(df['value'])
print(f"ADF Statistic: {result[0]:.4f}")
print(f"p-value: {result[1]:.4f}")  # < 0.05ë©´ ì •ìƒ

# ì°¨ë¶„ (differencing)
df['value_diff'] = df['value'].diff()
df['value_diff2'] = df['value'].diff().diff()

# ë¡œê·¸ ë³€í™˜ + ì°¨ë¶„
df['value_log_diff'] = np.log(df['value']).diff()
```

**3. Prophet**
```python
from prophet import Prophet

# ë°ì´í„° ì¤€ë¹„ (Prophet í˜•ì‹)
df_prophet = df.rename(columns={'date': 'ds', 'value': 'y'})

# ëª¨ë¸ í•™ìŠµ
model = Prophet(
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=False,
    changepoint_prior_scale=0.05  # íŠ¸ë Œë“œ ìœ ì—°ì„±
)
model.fit(df_prophet)

# ì˜ˆì¸¡
future = model.make_future_dataframe(periods=30)
forecast = model.predict(future)

# ì‹œê°í™”
model.plot(forecast)
model.plot_components(forecast)

# íœ´ì¼ íš¨ê³¼ ì¶”ê°€
holidays = pd.DataFrame({
    'holiday': 'custom_holiday',
    'ds': pd.to_datetime(['2024-01-01', '2024-12-25']),
    'lower_window': 0,
    'upper_window': 1
})
model = Prophet(holidays=holidays)
```

**4. ML ê¸°ë°˜ ì‹œê³„ì—´ (Lag Features)**
```python
# Lag Features ìƒì„±
def create_lag_features(df, target_col, lags):
    for lag in lags:
        df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
    return df

df = create_lag_features(df, 'value', [1, 7, 14, 30])

# Rolling Features
df['value_rolling_7_mean'] = df['value'].shift(1).rolling(7).mean()
df['value_rolling_7_std'] = df['value'].shift(1).rolling(7).std()
df['value_rolling_30_mean'] = df['value'].shift(1).rolling(30).mean()

# ì‹œê°„ íŠ¹ì„±
df['dayofweek'] = df['date'].dt.dayofweek
df['month'] = df['date'].dt.month
df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)

# TimeSeriesSplit
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

scores = []
for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

    model.fit(X_train, y_train)
    pred = model.predict(X_val)
    scores.append(mean_absolute_error(y_val, pred))
```

**ìë£Œì¡°ì‚¬ ë°˜ì˜ - ë„êµ¬ ë¹„êµ:**

| ë„êµ¬ | íŠ¹ì§• | ì¶”ì²œ ìƒí™© |
|------|------|----------|
| Prophet | ì‰¬ì›€, íœ´ì¼/ì´ë²¤íŠ¸ ì²˜ë¦¬ | ë¹ ë¥¸ ë² ì´ìŠ¤ë¼ì¸ |
| NeuralProphet | Prophet + Deep Learning | ë” ì •í™•í•œ ì˜ˆì¸¡ |
| TimeGPT | Zero-shot, ì‚¬ì „í•™ìŠµ | ë§¤ìš° ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… |
| LightGBM + Lag | ìœ ì—°ì„±, í•´ì„ ê°€ëŠ¥ | ë³µì¡í•œ í”¼ì²˜ í•„ìš” ì‹œ |

#### ì‹¤ìŠµ ê³¼ì œ

**ê³¼ì œ: ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ (í¬íŠ¸í´ë¦¬ì˜¤ #2)**

```
ë°ì´í„°: ì†Œë§¤ íŒë§¤ ë°ì´í„° (Kaggle Store Sales ë˜ëŠ” ìœ ì‚¬)

ìš”êµ¬ì‚¬í•­:
1. ì‹œê³„ì—´ ë¶„ì„:
   - ë¶„í•´ (ì¶”ì„¸, ê³„ì ˆì„±, ì”ì°¨)
   - ì •ìƒì„± ê²€ì •
   - ACF/PACF ë¶„ì„
2. ì „í†µ ëª¨ë¸:
   - Prophet (ê¸°ë³¸)
   - Prophet + íœ´ì¼/ì´ë²¤íŠ¸
3. ML ëª¨ë¸:
   - Lag/Rolling í”¼ì²˜ ìƒì„±
   - LightGBM + TimeSeriesSplit
4. ëª¨ë¸ ë¹„êµ:
   - MAE, MAPE
   - ì‹œê°í™” (ì˜ˆì¸¡ vs ì‹¤ì œ)
5. ë¹„ì¦ˆë‹ˆìŠ¤ í™œìš©:
   - ì¬ê³  ê´€ë¦¬ ê´€ì  í•´ì„
   - ì‹ ë¢° êµ¬ê°„ í™œìš©

ì‚°ì¶œë¬¼:
- **í¬íŠ¸í´ë¦¬ì˜¤ #2: ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸**
- Jupyter Notebook
- ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
- ì˜ˆì¸¡ ëŒ€ì‹œë³´ë“œ (Streamlit ì„ íƒ)
```

#### í‰ê°€ ê¸°ì¤€

| í•­ëª© | í†µê³¼ ê¸°ì¤€ | ë°°ì  |
|------|----------|------|
| EDA | ë¶„í•´ + ì •ìƒì„± ë¶„ì„ | 20% |
| Prophet | ê¸°ë³¸ + íœ´ì¼ ëª¨ë¸ | 25% |
| ML ëª¨ë¸ | Lag í”¼ì²˜ + êµì°¨ê²€ì¦ | 25% |
| ë¹„êµ ë¶„ì„ | ì ì ˆí•œ ì§€í‘œ + ì‹œê°í™” | 20% |
| ë¹„ì¦ˆë‹ˆìŠ¤ | ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ | 10% |

#### ì¶”ì²œ ìë£Œ

| ìœ í˜• | ì œëª© | ë§í¬ |
|------|------|------|
| ë¬¸ì„œ | Prophet Documentation | https://facebook.github.io/prophet/ |
| ë¬¸ì„œ | NeuralProphet | https://neuralprophet.com/ |
| ì˜ìƒ | StatQuest - Time Series | https://www.youtube.com/watch?v=DeORzP0go5I |
| Kaggle | Store Sales Competition | https://www.kaggle.com/c/store-sales-time-series-forecasting |

---

## ğŸ’¬ ë°ì´í„° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ (Phase 2 ì „ì²´ ì ìš©)

Phase 2ì˜ ëª¨ë“  ì‹¤ìŠµ ê³¼ì œì—ëŠ” **ë°œí‘œ/ë³´ê³ ì„œ** ìš”ì†Œê°€ í¬í•¨ë©ë‹ˆë‹¤.

### ê¸°ìˆ  â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ë²ˆì—­

```
âŒ ê¸°ìˆ  ì–¸ì–´:
"Random Forest ëª¨ë¸ì˜ AUCê°€ 0.85ë¡œ Logistic Regression ëŒ€ë¹„
0.12 í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤."

âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ì–¸ì–´:
"ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ì •í™•ë„ê°€ 85%ì…ë‹ˆë‹¤. ì´íƒˆ ìœ„í—˜ ê³ ê° 10ëª… ì¤‘ 8.5ëª…ì„
ì •í™•íˆ ì‹ë³„í•  ìˆ˜ ìˆì–´, ì„ ì œì  ë¦¬í…ì…˜ ìº í˜ì¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
```

### Pyramid Principle (ê²°ë¡  ë¨¼ì €)

```
ì „í†µì  ë°©ì‹ âŒ:
"ë°ì´í„°ë¥¼ ìˆ˜ì§‘í–ˆê³ , ì •ì œí–ˆê³ , ë¶„ì„í–ˆê³ ... ê²°ë¡ ì€ Xì…ë‹ˆë‹¤."

Pyramid ë°©ì‹ âœ…:
"ê²°ë¡ : ë°°ì†¡ ì‹œê°„ì„ 2ì¼ ë‹¨ì¶•í•˜ë©´ ì¬êµ¬ë§¤ìœ¨ì´ 15% ì¦ê°€í•©ë‹ˆë‹¤.

ì™œëƒí•˜ë©´:
1. ë°°ì†¡ ì‹œê°„ê³¼ ì¬êµ¬ë§¤ìœ¨ì˜ ìƒê´€ê´€ê³„ -0.6 (ê°•í•œ ìŒì˜ ìƒê´€)
2. ê³ ê° ì„¤ë¬¸: ë¶ˆë§Œì¡± 1ìœ„ = ë°°ì†¡ ì§€ì—° (43%)
3. ê²½ìŸì‚¬ ë¹„êµ: ìš°ë¦¬ 5ì¼ vs ê²½ìŸì‚¬ 3ì¼"
```

### ì´í•´ê´€ê³„ìë³„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜

| ëŒ€ìƒ | ê´€ì‹¬ì‚¬ | í˜•ì‹ |
|------|--------|------|
| **ê²½ì˜ì§„** | ROI, ë¦¬ìŠ¤í¬ | 1í˜ì´ì§€ ìš”ì•½, 3ë¶„ ë°œí‘œ |
| **í˜„ì—… íŒ€** | ì‹¤í–‰ ê°€ëŠ¥ì„± | êµ¬ì²´ì  ê°€ì´ë“œ, ë°ëª¨ |
| **ê¸°ìˆ  íŒ€** | êµ¬í˜„ ë°©ë²• | ê¸°ìˆ  ë¬¸ì„œ, ì•„í‚¤í…ì²˜ |

---

## Phase 2 ì™„ë£Œ ê¸°ì¤€

### í•„ìˆ˜ ì‚°ì¶œë¬¼
1. [ ] Week 9: ë¬¸ì œ ì •ì˜ì„œ + EDA ë¦¬í¬íŠ¸
2. [ ] Week 10: ë°ì´í„° í’ˆì§ˆ í‰ê°€ + ì •ì œ íŒŒì´í”„ë¼ì¸
3. [ ] Week 11: Feature Engineering ë…¸íŠ¸ë¶
4. [ ] Week 12: í”¼ì²˜ ì„ íƒ & ì°¨ì› ì¶•ì†Œ ë¶„ì„
5. [ ] Week 13: ê°€ì„¤ ê²€ì¦ + ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸
6. [ ] Week 14: ê³ ê° ì„¸ê·¸ë©˜í…Œì´ì…˜ + ë§ˆì¼€íŒ… ì „ëµ ì œì•ˆ
7. [ ] Week 15: ì´ìƒíƒì§€ ì‹œìŠ¤í…œ
8. [ ] **Week 16: í¬íŠ¸í´ë¦¬ì˜¤ #2 - ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ + ê²½ì˜ì§„ ë³´ê³ ì„œ**

### ì—­ëŸ‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

**ë¶„ì„ ì—­ëŸ‰**
- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ì •ì˜í•  ìˆ˜ ìˆë‹¤ (5 Whys, MECE)
- [ ] ê°€ì„¤ ê¸°ë°˜ ë¶„ì„ ì ‘ê·¼ë²•ì„ ì ìš©í•  ìˆ˜ ìˆë‹¤
- [ ] ë°ì´í„°ì—ì„œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤
- [ ] ê²°ì¸¡ì¹˜, ì´ìƒì¹˜ë¥¼ ì ì ˆíˆ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤
- [ ] í”¼ì²˜ë¥¼ ìƒì„±í•˜ê³  ì„ íƒí•  ìˆ˜ ìˆë‹¤

**ëª¨ë¸ë§ ì—­ëŸ‰**
- [ ] ë¶„ë¥˜/íšŒê·€ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í‰ê°€í•  ìˆ˜ ìˆë‹¤
- [ ] í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤
- [ ] ì´ìƒíƒì§€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤
- [ ] ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤

**ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì—­ëŸ‰**
- [ ] ê¸°ìˆ  ë‚´ìš©ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ì–¸ì–´ë¡œ ë²ˆì—­í•  ìˆ˜ ìˆë‹¤
- [ ] Pyramid Principleë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆë‹¤
- [ ] ì´í•´ê´€ê³„ì ë§ì¶¤ ë°œí‘œë¥¼ í•  ìˆ˜ ìˆë‹¤

---

*Phase 2 ì™„ë£Œ â†’ Phase 3: Knowledge Graphë¡œ ì´ë™*
