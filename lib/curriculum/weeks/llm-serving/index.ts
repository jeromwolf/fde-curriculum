// Phase 4, Week 7: LLM ì„œë¹™
import type { Week, Day } from '../../types'

const day1: Day = {
  slug: 'llm-serving-intro',
  title: 'LLM ì„œë¹™ ê¸°ì´ˆ',
  totalDuration: 180,
  tasks: [
    {
      id: 'llm-serving-intro-video',
      type: 'video',
      title: 'LLM ì„œë¹™ ì•„í‚¤í…ì²˜',
      duration: 30,
      content: {
        objectives: ['LLM ì„œë¹™ ì˜µì…˜ ì´í•´', 'API vs Self-hosted ë¹„êµ', 'ë¹„ìš© ìµœì í™” ì „ëµ'],
        videoUrl: 'https://www.youtube.com/watch?v=llm-serving-intro-placeholder',
        transcript: `
## LLM ì„œë¹™ ì˜µì…˜

### API vs Self-hosted

\`\`\`
API ì„œë¹„ìŠ¤ (OpenAI, Anthropic)
â”œâ”€â”€ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
â”œâ”€â”€ ê´€ë¦¬ ë¶ˆí•„ìš”
â”œâ”€â”€ í† í°ë‹¹ ê³¼ê¸ˆ
â””â”€â”€ ë°ì´í„° ì™¸ë¶€ ì „ì†¡

Self-hosted (vLLM, TGI)
â”œâ”€â”€ ì´ˆê¸° ì„¤ì • í•„ìš”
â”œâ”€â”€ ì¸í”„ë¼ ê´€ë¦¬
â”œâ”€â”€ ì¸ìŠ¤í„´ìŠ¤ë‹¹ ê³¼ê¸ˆ
â””â”€â”€ ë°ì´í„° ë‚´ë¶€ ìœ ì§€
\`\`\`

### ë¹„ìš© ë¹„êµ (ì›” 1ì–µ í† í° ê¸°ì¤€)

| ì˜µì…˜ | ë¹„ìš© |
|------|------|
| GPT-4o | ~$500 |
| GPT-4o-mini | ~$15 |
| Claude 3.5 Sonnet | ~$300 |
| Llama 3 70B (self) | ~$1,500 (GPU) |
| Llama 3 8B (self) | ~$500 (GPU) |

### ì£¼ìš” ì„œë¹™ í”„ë ˆì„ì›Œí¬

| í”„ë ˆì„ì›Œí¬ | íŠ¹ì§• |
|------------|------|
| **vLLM** | PagedAttention, ê³ ì²˜ë¦¬ëŸ‰ |
| **TGI** | Hugging Face, ì‰¬ìš´ ë°°í¬ |
| **TensorRT-LLM** | NVIDIA ìµœì í™” |
| **llama.cpp** | CPU/ì €ì‚¬ì–‘ GPU |
| **Ollama** | ë¡œì»¬ ì‹¤í–‰ |
        `
      }
    },
    {
      id: 'api-gateway-video',
      type: 'video',
      title: 'LLM API Gateway íŒ¨í„´',
      duration: 30,
      content: {
        objectives: ['API Gateway ì„¤ê³„', 'Rate Limiting', 'Fallback ì „ëµ'],
        videoUrl: 'https://www.youtube.com/watch?v=api-gateway-placeholder',
        transcript: `
## LLM API Gateway

### ì•„í‚¤í…ì²˜

\`\`\`
í´ë¼ì´ì–¸íŠ¸
    â”‚
    â–¼
API Gateway (Kong, AWS API Gateway)
â”œâ”€â”€ ì¸ì¦/ì¸ê°€
â”œâ”€â”€ Rate Limiting
â”œâ”€â”€ ë¡œê¹…
â””â”€â”€ ë¼ìš°íŒ…
    â”‚
    â–¼
LLM Router
â”œâ”€â”€ OpenAI (ê¸°ë³¸)
â”œâ”€â”€ Anthropic (Fallback 1)
â””â”€â”€ Self-hosted (Fallback 2)
\`\`\`

### LiteLLM í†µí•© ì¸í„°í˜ì´ìŠ¤

\`\`\`python
from litellm import completion

# ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ì—¬ëŸ¬ ëª¨ë¸ í˜¸ì¶œ
response = completion(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Hello"}]
)

# Fallback ì„¤ì •
response = completion(
    model="gpt-4o",
    messages=[...],
    fallbacks=["claude-3-sonnet", "llama-3-70b"]
)
\`\`\`

### Rate Limiting

\`\`\`python
from fastapi import FastAPI
from slowapi import Limiter

limiter = Limiter(key_func=get_user_id)
app = FastAPI()

@app.post("/chat")
@limiter.limit("60/minute")  # ë¶„ë‹¹ 60íšŒ
async def chat(request: ChatRequest):
    return await llm_call(request)
\`\`\`
        `
      }
    }
  ]
}

const day2: Day = {
  slug: 'vllm-deployment',
  title: 'vLLM ë°°í¬',
  totalDuration: 180,
  tasks: [
    {
      id: 'vllm-video',
      type: 'video',
      title: 'vLLM ì„¤ì¹˜ & ì„¤ì •',
      duration: 35,
      content: {
        objectives: ['vLLM ì•„í‚¤í…ì²˜ ì´í•´', 'GPU ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •', 'OpenAI í˜¸í™˜ ì„œë²„'],
        videoUrl: 'https://www.youtube.com/watch?v=vllm-placeholder',
        transcript: `
## vLLM

### vLLMì´ë€?

**ê³ ì²˜ë¦¬ëŸ‰ LLM ì¶”ë¡  ì—”ì§„**ì…ë‹ˆë‹¤.

\`\`\`
íŠ¹ì§•:
â”œâ”€â”€ PagedAttention: ë©”ëª¨ë¦¬ íš¨ìœ¨ ìµœì í™”
â”œâ”€â”€ Continuous Batching: ë™ì  ë°°ì¹˜
â”œâ”€â”€ OpenAI í˜¸í™˜ API
â””â”€â”€ ë‹¤ì–‘í•œ ëª¨ë¸ ì§€ì›
\`\`\`

### ì„¤ì¹˜ ë° ì‹¤í–‰

\`\`\`bash
# ì„¤ì¹˜
pip install vllm

# ì„œë²„ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server \\
    --model meta-llama/Meta-Llama-3-8B-Instruct \\
    --port 8000 \\
    --gpu-memory-utilization 0.9
\`\`\`

### OpenAI SDKë¡œ í˜¸ì¶œ

\`\`\`python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"
)

response = client.chat.completions.create(
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    messages=[{"role": "user", "content": "Hello"}]
)
\`\`\`

### Docker ë°°í¬

\`\`\`dockerfile
FROM vllm/vllm-openai:latest

CMD ["--model", "meta-llama/Meta-Llama-3-8B-Instruct", \\
     "--port", "8000", \\
     "--gpu-memory-utilization", "0.9"]
\`\`\`
        `
      }
    }
  ]
}

const day3: Day = {
  slug: 'bedrock',
  title: 'AWS Bedrock',
  totalDuration: 180,
  tasks: [
    {
      id: 'bedrock-video',
      type: 'video',
      title: 'AWS Bedrock í™œìš©',
      duration: 30,
      content: {
        objectives: ['Bedrock ëª¨ë¸ ì„ íƒ', 'API í˜¸ì¶œ ë°©ë²•', 'Knowledge Bases ì—°ë™'],
        videoUrl: 'https://www.youtube.com/watch?v=bedrock-placeholder',
        transcript: `
## AWS Bedrock

### Bedrockì´ë€?

**AWS ê´€ë¦¬í˜• ìƒì„±í˜• AI ì„œë¹„ìŠ¤**ì…ë‹ˆë‹¤.

\`\`\`
ì§€ì› ëª¨ë¸:
â”œâ”€â”€ Anthropic Claude 3.5 Sonnet
â”œâ”€â”€ Meta Llama 3
â”œâ”€â”€ Amazon Titan
â”œâ”€â”€ Mistral
â””â”€â”€ Cohere
\`\`\`

### API í˜¸ì¶œ

\`\`\`python
import boto3
import json

bedrock = boto3.client('bedrock-runtime', region_name='us-east-1')

response = bedrock.invoke_model(
    modelId='anthropic.claude-3-sonnet-20240229-v1:0',
    body=json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1024,
        "messages": [
            {"role": "user", "content": "Hello"}
        ]
    })
)

result = json.loads(response['body'].read())
print(result['content'][0]['text'])
\`\`\`

### Knowledge Bases

\`\`\`
ìë™ RAG íŒŒì´í”„ë¼ì¸:
S3 ë¬¸ì„œ â†’ ìë™ ì²­í‚¹ â†’ ì„ë² ë”© â†’ OpenSearch/Pinecone
                                    â”‚
                                    â–¼
                              ì¿¼ë¦¬ API
\`\`\`
        `
      }
    }
  ]
}

const day4: Day = {
  slug: 'llm-monitoring',
  title: 'LLM ëª¨ë‹ˆí„°ë§ & ìµœì í™”',
  totalDuration: 180,
  tasks: [
    {
      id: 'monitoring-video',
      type: 'video',
      title: 'LLM ì„œë¹™ ëª¨ë‹ˆí„°ë§',
      duration: 30,
      content: {
        objectives: ['í•µì‹¬ ë©”íŠ¸ë¦­ ì •ì˜', 'LangSmith/Langfuse í™œìš©', 'ë¹„ìš© ì¶”ì '],
        videoUrl: 'https://www.youtube.com/watch?v=llm-monitoring-placeholder',
        transcript: `
## LLM ëª¨ë‹ˆí„°ë§

### í•µì‹¬ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ì„¤ëª… | ëª©í‘œ |
|--------|------|------|
| **Latency** | ì‘ë‹µ ì‹œê°„ | < 2ì´ˆ |
| **TTFT** | ì²« í† í° ì‹œê°„ | < 500ms |
| **TPS** | ì´ˆë‹¹ í† í° | > 50 |
| **Token Cost** | í† í°ë‹¹ ë¹„ìš© | ìµœì†Œí™” |
| **Error Rate** | ì˜¤ë¥˜ìœ¨ | < 1% |

### Langfuse ì—°ë™

\`\`\`python
from langfuse.decorators import observe
from langfuse import Langfuse

langfuse = Langfuse()

@observe()
def generate_response(prompt: str):
    response = openai.chat.completions.create(...)
    return response

# ìë™ ì¶”ì :
# - ì…ë ¥/ì¶œë ¥
# - ì§€ì—° ì‹œê°„
# - í† í° ì‚¬ìš©ëŸ‰
# - ë¹„ìš©
\`\`\`

### ë¹„ìš© ìµœì í™”

\`\`\`
ì „ëµ:
â”œâ”€â”€ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ (ë³µì¡ë„ì— ë§ê²Œ)
â”œâ”€â”€ í”„ë¡¬í”„íŠ¸ ìºì‹±
â”œâ”€â”€ ë°°ì¹˜ ì²˜ë¦¬ (ë¹„ë™ê¸°)
â”œâ”€â”€ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
â””â”€â”€ Fallback ì²´ì¸
\`\`\`
        `
      }
    }
  ]
}

const day5: Day = {
  slug: 'llm-project',
  title: 'ğŸ† Week 7 í”„ë¡œì íŠ¸',
  totalDuration: 180,
  tasks: [
    {
      id: 'week7-challenge',
      type: 'challenge',
      title: 'Production LLM ì„œë¹™ ì‹œìŠ¤í…œ',
      duration: 120,
      content: {
        objectives: ['í™•ì¥ ê°€ëŠ¥í•œ LLM ì„œë¹™ êµ¬ì¶•', 'ë¹„ìš© íš¨ìœ¨ì  ì•„í‚¤í…ì²˜', 'ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ'],
        requirements: [
          'LLM API Gateway (ì¸ì¦, Rate Limiting)',
          'ë‹¤ì¤‘ ëª¨ë¸ ë¼ìš°íŒ… (OpenAI + Bedrock + Self-hosted)',
          'Fallback ë° Retry ë¡œì§',
          'Langfuse ëª¨ë‹ˆí„°ë§ ì—°ë™',
          'CloudWatch ëŒ€ì‹œë³´ë“œ',
          'ë¹„ìš© ì¶”ì  ë° ì•Œë¦¼'
        ],
        evaluationCriteria: ['ì‹œìŠ¤í…œ ì•ˆì •ì„± (30%)', 'ë¹„ìš© íš¨ìœ¨ì„± (25%)', 'ëª¨ë‹ˆí„°ë§ ì™„ì„±ë„ (25%)', 'ë¬¸ì„œí™” (20%)'],
        bonusPoints: ['ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ', 'í”„ë¡¬í”„íŠ¸ ìºì‹±', 'A/B í…ŒìŠ¤íŠ¸']
      }
    }
  ]
}

export const llmServingWeek: Week = {
  slug: 'llm-serving',
  week: 7,
  phase: 4,
  month: 9,
  access: 'pro',
  title: 'LLM ì„œë¹™',
  topics: ['vLLM', 'AWS Bedrock', 'API Gateway', 'LiteLLM', 'Langfuse', 'ë¹„ìš© ìµœì í™”'],
  practice: 'Production LLM ì„œë¹™ ì‹œìŠ¤í…œ',
  totalDuration: 900,
  days: [day1, day2, day3, day4, day5]
}
