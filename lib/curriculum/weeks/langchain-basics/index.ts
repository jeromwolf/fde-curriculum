// Phase 5, Week 2: LangChain ê¸°ì´ˆ
import type { Week, Day } from '../../types'

const day1: Day = {
  slug: 'langchain-intro',
  title: 'LangChain ì†Œê°œ',
  totalDuration: 180,
  tasks: [
    {
      id: 'langchain-overview-video',
      type: 'video',
      title: 'LangChainì´ë€?',
      duration: 30,
      content: {
        objectives: [
          'LangChainì˜ ëª©ì ê³¼ ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•œë‹¤',
          'í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¥¼ íŒŒì•…í•œë‹¤',
          'LangChain vs ì§ì ‘ êµ¬í˜„ì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤'
        ],
        videoUrl: 'https://www.youtube.com/watch?v=langchain-intro-placeholder',
        transcript: `
## LangChain ê°œìš”

### LangChainì´ë€?

\`\`\`
LangChain = LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬

í•´ê²°í•˜ëŠ” ë¬¸ì œ:
â”œâ”€â”€ LLM API í˜¸ì¶œ ì¶”ìƒí™”
â”œâ”€â”€ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
â”œâ”€â”€ ì²´ì´ë‹ (ì—¬ëŸ¬ LLM í˜¸ì¶œ ì—°ê²°)
â”œâ”€â”€ ë©”ëª¨ë¦¬ (ëŒ€í™” íˆìŠ¤í† ë¦¬)
â”œâ”€â”€ ë„êµ¬ í†µí•© (ê²€ìƒ‰, DB, API)
â””â”€â”€ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
\`\`\`

### ì•„í‚¤í…ì²˜

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LangChain                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Models        â”‚  Prompts      â”‚  Memory        â”‚
â”‚  â”œâ”€â”€ ChatModelsâ”‚  â”œâ”€â”€ Templatesâ”‚  â”œâ”€â”€ Buffer    â”‚
â”‚  â”œâ”€â”€ LLMs      â”‚  â”œâ”€â”€ Examples â”‚  â”œâ”€â”€ Summary   â”‚
â”‚  â””â”€â”€ Embeddingsâ”‚  â””â”€â”€ Selectorsâ”‚  â””â”€â”€ Vector    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Chains        â”‚  Agents       â”‚  Tools         â”‚
â”‚  â”œâ”€â”€ LLMChain  â”‚  â”œâ”€â”€ ReAct    â”‚  â”œâ”€â”€ Search    â”‚
â”‚  â”œâ”€â”€ Sequentialâ”‚  â”œâ”€â”€ OpenAI   â”‚  â”œâ”€â”€ Calculatorâ”‚
â”‚  â””â”€â”€ Router    â”‚  â””â”€â”€ Custom   â”‚  â””â”€â”€ Custom    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Retrievers    â”‚  Vector Storesâ”‚  Document      â”‚
â”‚  â”œâ”€â”€ Vector    â”‚  â”œâ”€â”€ FAISS    â”‚  Loaders       â”‚
â”‚  â”œâ”€â”€ BM25      â”‚  â”œâ”€â”€ Chroma   â”‚  â”œâ”€â”€ PDF       â”‚
â”‚  â””â”€â”€ Hybrid    â”‚  â””â”€â”€ Pinecone â”‚  â””â”€â”€ Web       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### LangChain vs ì§ì ‘ êµ¬í˜„

| ì¸¡ë©´ | ì§ì ‘ êµ¬í˜„ | LangChain |
|------|----------|-----------|
| **ì´ˆê¸° ê°œë°œ** | ë¹ ë¦„ | í•™ìŠµ í•„ìš” |
| **ìœ ì§€ë³´ìˆ˜** | ëª¨ë“  ê²ƒ ì§ì ‘ | ì¶”ìƒí™” í™œìš© |
| **í™•ì¥ì„±** | ì§ì ‘ ì„¤ê³„ | íŒ¨í„´ ì œê³µ |
| **ë””ë²„ê¹…** | ëª…í™•í•¨ | ì¶”ìƒí™” ë ˆì´ì–´ |
| **ìƒíƒœê³„** | ì§ì ‘ í†µí•© | í’ë¶€í•œ í†µí•© |

### ì–¸ì œ LangChainì„ ì‚¬ìš©í•˜ë‚˜?

\`\`\`
âœ… LangChain ê¶Œì¥:
â”œâ”€â”€ RAG ì‹œìŠ¤í…œ êµ¬ì¶•
â”œâ”€â”€ ë³µì¡í•œ ì—ì´ì „íŠ¸ ê°œë°œ
â”œâ”€â”€ ë‹¤ì–‘í•œ LLM êµì²´ í•„ìš”
â”œâ”€â”€ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘
â””â”€â”€ ë‹¤ì–‘í•œ ë„êµ¬ í†µí•©

âŒ ì§ì ‘ êµ¬í˜„ ê¶Œì¥:
â”œâ”€â”€ ë‹¨ìˆœ API í˜¸ì¶œ
â”œâ”€â”€ ì„±ëŠ¥ ìµœì í™” í•„ìˆ˜
â”œâ”€â”€ íŠ¹ìˆ˜í•œ ìš”êµ¬ì‚¬í•­
â””â”€â”€ ì˜ì¡´ì„± ìµœì†Œí™” í•„ìš”
\`\`\`

### ì„¤ì¹˜ ë° ì„¤ì •

\`\`\`bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install langchain langchain-openai langchain-anthropic

# ë²¡í„° ìŠ¤í† ì–´
pip install langchain-chroma faiss-cpu

# ë¬¸ì„œ ë¡œë”
pip install pypdf python-docx

# í™˜ê²½ ë³€ìˆ˜
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
\`\`\`
        `
      }
    },
    {
      id: 'langchain-models-video',
      type: 'video',
      title: 'Chat Models & LLMs',
      duration: 25,
      content: {
        objectives: [
          'ë‹¤ì–‘í•œ LLM í”„ë¡œë°”ì´ë”ë¥¼ ì—°ë™í•œë‹¤',
          'ChatModelê³¼ LLMì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤',
          'ëª¨ë¸ ì„¤ì •ê³¼ ì½œë°±ì„ í™œìš©í•œë‹¤'
        ],
        videoUrl: 'https://www.youtube.com/watch?v=langchain-models-placeholder',
        transcript: `
## Chat Models & LLMs

### ChatModel ê¸°ë³¸ ì‚¬ìš©

\`\`\`python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage

# OpenAI
chat_openai = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    max_tokens=500
)

# Anthropic
chat_anthropic = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.7,
    max_tokens=500
)

# ë©”ì‹œì§€ êµ¬ì¡°
messages = [
    SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AIì…ë‹ˆë‹¤."),
    HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”!")
]

response = chat_openai.invoke(messages)
print(response.content)
\`\`\`

### ìŠ¤íŠ¸ë¦¬ë°

\`\`\`python
# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
for chunk in chat_openai.stream(messages):
    print(chunk.content, end="", flush=True)

# ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
async for chunk in chat_openai.astream(messages):
    print(chunk.content, end="", flush=True)
\`\`\`

### ë°°ì¹˜ ì²˜ë¦¬

\`\`\`python
# ì—¬ëŸ¬ ì…ë ¥ ë™ì‹œ ì²˜ë¦¬
batch_messages = [
    [HumanMessage(content="Pythonì´ë€?")],
    [HumanMessage(content="JavaScriptë€?")],
    [HumanMessage(content="Rustë€?")]
]

responses = chat_openai.batch(batch_messages)
for r in responses:
    print(r.content[:100])
\`\`\`

### ì½œë°±

\`\`\`python
from langchain_core.callbacks import BaseCallbackHandler

class TokenCounter(BaseCallbackHandler):
    def __init__(self):
        self.total_tokens = 0

    def on_llm_end(self, response, **kwargs):
        if response.llm_output:
            usage = response.llm_output.get("token_usage", {})
            self.total_tokens += usage.get("total_tokens", 0)

counter = TokenCounter()
response = chat_openai.invoke(messages, config={"callbacks": [counter]})
print(f"ì‚¬ìš© í† í°: {counter.total_tokens}")
\`\`\`
        `
      }
    },
    {
      id: 'langchain-basic-practice',
      type: 'code',
      title: 'LangChain ê¸°ë³¸ ì‹¤ìŠµ',
      duration: 60,
      content: {
        objectives: [
          'ChatModelì„ ì‚¬ìš©í•œ ëŒ€í™”ë¥¼ êµ¬í˜„í•œë‹¤',
          'ë‹¤ì–‘í•œ í”„ë¡œë°”ì´ë”ë¥¼ í…ŒìŠ¤íŠ¸í•œë‹¤',
          'ìŠ¤íŠ¸ë¦¬ë°ê³¼ ë°°ì¹˜ë¥¼ í™œìš©í•œë‹¤'
        ],
        instructions: `
## ì‹¤ìŠµ: LangChain Chat Models

### ê³¼ì œ
1. OpenAIì™€ Anthropic ëª¨ë¸ ì—°ë™
2. ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ êµ¬í˜„
3. í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
4. ëª¨ë¸ ì „í™˜ ê°€ëŠ¥í•œ êµ¬ì¡° ì„¤ê³„
        `,
        starterCode: `from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage

class MultiModelChat:
    def __init__(self, default_provider: str = "openai"):
        self.models = {}
        self.current_provider = default_provider
        # TODO: ëª¨ë¸ ì´ˆê¸°í™”

    def set_provider(self, provider: str):
        """í”„ë¡œë°”ì´ë” ì „í™˜"""
        # TODO: êµ¬í˜„
        pass

    def chat(self, message: str, system: str = None) -> str:
        """ì¼ë°˜ ëŒ€í™”"""
        # TODO: êµ¬í˜„
        pass

    def stream_chat(self, message: str, system: str = None):
        """ìŠ¤íŠ¸ë¦¬ë° ëŒ€í™”"""
        # TODO: êµ¬í˜„
        pass

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    chat = MultiModelChat()
    print(chat.chat("Pythonì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”."))
`,
        solutionCode: `from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.callbacks import BaseCallbackHandler
from typing import Optional, Generator

class TokenTracker(BaseCallbackHandler):
    def __init__(self):
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    def on_llm_end(self, response, **kwargs):
        if response.llm_output:
            usage = response.llm_output.get("token_usage", {})
            self.total_tokens += usage.get("total_tokens", 0)
            self.prompt_tokens += usage.get("prompt_tokens", 0)
            self.completion_tokens += usage.get("completion_tokens", 0)

class MultiModelChat:
    def __init__(self, default_provider: str = "openai"):
        self.models = {
            "openai": ChatOpenAI(model="gpt-4o-mini", temperature=0.7),
            "anthropic": ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=0.7)
        }
        self.current_provider = default_provider
        self.token_tracker = TokenTracker()

    def set_provider(self, provider: str):
        """í”„ë¡œë°”ì´ë” ì „í™˜"""
        if provider not in self.models:
            raise ValueError(f"Unknown provider: {provider}. Available: {list(self.models.keys())}")
        self.current_provider = provider
        print(f"Switched to {provider}")

    def _get_model(self):
        return self.models[self.current_provider]

    def _build_messages(self, message: str, system: Optional[str] = None):
        messages = []
        if system:
            messages.append(SystemMessage(content=system))
        messages.append(HumanMessage(content=message))
        return messages

    def chat(self, message: str, system: Optional[str] = None) -> str:
        """ì¼ë°˜ ëŒ€í™”"""
        messages = self._build_messages(message, system)
        response = self._get_model().invoke(
            messages,
            config={"callbacks": [self.token_tracker]}
        )
        return response.content

    def stream_chat(self, message: str, system: Optional[str] = None) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ëŒ€í™”"""
        messages = self._build_messages(message, system)
        for chunk in self._get_model().stream(messages):
            yield chunk.content

    def batch_chat(self, messages: list[str], system: Optional[str] = None) -> list[str]:
        """ë°°ì¹˜ ëŒ€í™”"""
        batch = [self._build_messages(msg, system) for msg in messages]
        responses = self._get_model().batch(batch)
        return [r.content for r in responses]

    def get_token_usage(self) -> dict:
        """í† í° ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        return {
            "total": self.token_tracker.total_tokens,
            "prompt": self.token_tracker.prompt_tokens,
            "completion": self.token_tracker.completion_tokens
        }

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    chat = MultiModelChat()

    # ê¸°ë³¸ ëŒ€í™”
    print("=== OpenAI ===")
    response = chat.chat(
        "Pythonì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”.",
        system="ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."
    )
    print(response)
    print(f"\\ní† í° ì‚¬ìš©: {chat.get_token_usage()}")

    # í”„ë¡œë°”ì´ë” ì „í™˜
    print("\\n=== Anthropic ===")
    chat.set_provider("anthropic")
    response = chat.chat("ê°™ì€ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”: Pythonì˜ ì¥ì  3ê°€ì§€")
    print(response)

    # ìŠ¤íŠ¸ë¦¬ë°
    print("\\n=== ìŠ¤íŠ¸ë¦¬ë° ===")
    chat.set_provider("openai")
    for chunk in chat.stream_chat("AIì˜ ë¯¸ë˜ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ"):
        print(chunk, end="", flush=True)
    print()

    # ë°°ì¹˜
    print("\\n=== ë°°ì¹˜ ===")
    questions = ["Pythonì´ë€?", "JavaScriptë€?", "Goë€?"]
    answers = chat.batch_chat(questions, system="10ë‹¨ì–´ ì´ë‚´ë¡œ ë‹µë³€")
    for q, a in zip(questions, answers):
        print(f"Q: {q}")
        print(f"A: {a}\\n")
`
      }
    },
    {
      id: 'langchain-intro-quiz',
      type: 'quiz',
      title: 'LangChain ê¸°ì´ˆ í€´ì¦ˆ',
      duration: 15,
      content: {
        objectives: ['LangChain ê¸°ì´ˆ ê°œë…ì„ ë³µìŠµí•œë‹¤'],
        questions: [
          {
            question: 'LangChainì˜ ì£¼ìš” ëª©ì ì´ ì•„ë‹Œ ê²ƒì€?',
            options: [
              'LLM API í˜¸ì¶œ ì¶”ìƒí™”',
              'í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ë° í…œí”Œë¦¿',
              'LLM ëª¨ë¸ í•™ìŠµ ë° íŒŒì¸íŠœë‹',
              'RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•'
            ],
            answer: 2,
            explanation: 'LangChainì€ LLM í™œìš©ì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì´ë©°, ëª¨ë¸ í•™ìŠµ/íŒŒì¸íŠœë‹ì€ ë³„ë„ ë„êµ¬(ì˜ˆ: Hugging Face)ì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.'
          },
          {
            question: 'LangChainì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë©”ì„œë“œëŠ”?',
            options: [
              'invoke()',
              'stream()',
              'batch()',
              'run()'
            ],
            answer: 1,
            explanation: 'stream() ë©”ì„œë“œëŠ” í† í° ë‹¨ìœ„ë¡œ ì ì§„ì  ì¶œë ¥ì„ ì œê³µí•˜ì—¬ ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.'
          }
        ]
      }
    }
  ]
}

const day2: Day = {
  slug: 'prompts-chains',
  title: 'Prompts & Chains',
  totalDuration: 180,
  tasks: [
    {
      id: 'prompt-templates-video',
      type: 'video',
      title: 'Prompt Templates',
      duration: 30,
      content: {
        objectives: [
          'PromptTemplateê³¼ ChatPromptTemplateì„ ì´í•´í•œë‹¤',
          'ë³€ìˆ˜ ì¹˜í™˜ê³¼ ë¶€ë¶„ ì ìš©ì„ í™œìš©í•œë‹¤',
          'Few-shot í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•œë‹¤'
        ],
        videoUrl: 'https://www.youtube.com/watch?v=prompt-templates-placeholder',
        transcript: `
## Prompt Templates

### ê¸°ë³¸ PromptTemplate

\`\`\`python
from langchain_core.prompts import PromptTemplate

# ë°©ë²• 1: from_template
template = PromptTemplate.from_template(
    "{product}ì˜ ì¥ì ì„ {count}ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”."
)

prompt = template.format(product="Python", count=3)
print(prompt)  # "Pythonì˜ ì¥ì ì„ 3ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”."

# ë°©ë²• 2: ëª…ì‹œì  ë³€ìˆ˜ ì§€ì •
template = PromptTemplate(
    template="{product}ì˜ ì¥ì ì„ {count}ê°€ì§€ ì•Œë ¤ì£¼ì„¸ìš”.",
    input_variables=["product", "count"]
)
\`\`\`

### ChatPromptTemplate

\`\`\`python
from langchain_core.prompts import ChatPromptTemplate

# ë©”ì‹œì§€ ê¸°ë°˜ í…œí”Œë¦¿
chat_template = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ {role} ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
    ("human", "{question}")
])

messages = chat_template.format_messages(
    role="Python",
    question="ë°ì½”ë ˆì´í„°ë€ ë¬´ì—‡ì¸ê°€ìš”?"
)
\`\`\`

### ë¶€ë¶„ ì ìš© (Partial)

\`\`\`python
# ì¼ë¶€ ë³€ìˆ˜ë§Œ ë¯¸ë¦¬ ì±„ìš°ê¸°
partial_template = template.partial(count=3)
prompt = partial_template.format(product="Python")
\`\`\`

### Few-shot Prompt

\`\`\`python
from langchain_core.prompts import FewShotPromptTemplate

examples = [
    {"input": "ì¢‹ì•„ìš”", "output": "positive"},
    {"input": "ë³„ë¡œì˜ˆìš”", "output": "negative"},
    {"input": "ê·¸ëƒ¥ìš”", "output": "neutral"}
]

example_prompt = PromptTemplate(
    template="ì…ë ¥: {input}\\nì¶œë ¥: {output}",
    input_variables=["input", "output"]
)

few_shot = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ë¥˜í•˜ì„¸ìš”.",
    suffix="ì…ë ¥: {text}\\nì¶œë ¥:",
    input_variables=["text"]
)

prompt = few_shot.format(text="ì •ë§ ìµœê³ ì˜ˆìš”!")
\`\`\`
        `
      }
    },
    {
      id: 'chains-video',
      type: 'video',
      title: 'Chains & LCEL',
      duration: 30,
      content: {
        objectives: [
          'LCEL (LangChain Expression Language)ì„ ì´í•´í•œë‹¤',
          'ì²´ì¸ì„ êµ¬ì„±í•˜ê³  ì—°ê²°í•œë‹¤',
          'RunnablePassthroughì™€ RunnableLambdaë¥¼ í™œìš©í•œë‹¤'
        ],
        videoUrl: 'https://www.youtube.com/watch?v=chains-lcel-placeholder',
        transcript: `
## Chains & LCEL

### LCEL ê¸°ë³¸

\`\`\`python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# íŒŒì´í”„ ì—°ì‚°ìë¡œ ì²´ì¸ êµ¬ì„±
prompt = ChatPromptTemplate.from_template("{topic}ì— ëŒ€í•œ ë†ë‹´ì„ í•´ì£¼ì„¸ìš”.")
model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()

chain = prompt | model | parser

result = chain.invoke({"topic": "í”„ë¡œê·¸ë˜ë°"})
print(result)
\`\`\`

### ì²´ì¸ êµ¬ì„± ìš”ì†Œ

\`\`\`
LCEL ì²´ì¸ = Runnable ê°ì²´ë“¤ì˜ ì—°ê²°

[Input]
    â†“
[Prompt Template] â†’ í”„ë¡¬í”„íŠ¸ ìƒì„±
    â†“
[Chat Model] â†’ LLM í˜¸ì¶œ
    â†“
[Output Parser] â†’ ê²°ê³¼ íŒŒì‹±
    â†“
[Output]
\`\`\`

### RunnablePassthrough

\`\`\`python
from langchain_core.runnables import RunnablePassthrough

# ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ë©´ì„œ ì¶”ê°€ ì²˜ë¦¬
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | parser
)
\`\`\`

### RunnableLambda

\`\`\`python
from langchain_core.runnables import RunnableLambda

# ì»¤ìŠ¤í…€ í•¨ìˆ˜ë¥¼ ì²´ì¸ì— í¬í•¨
def format_docs(docs):
    return "\\n".join(doc.page_content for doc in docs)

chain = (
    {"context": retriever | RunnableLambda(format_docs), "question": RunnablePassthrough()}
    | prompt
    | model
    | parser
)
\`\`\`

### ë³‘ë ¬ ì‹¤í–‰

\`\`\`python
from langchain_core.runnables import RunnableParallel

# ì—¬ëŸ¬ ì²´ì¸ ë™ì‹œ ì‹¤í–‰
parallel = RunnableParallel(
    joke=joke_chain,
    poem=poem_chain,
    summary=summary_chain
)

results = parallel.invoke({"topic": "AI"})
# {"joke": "...", "poem": "...", "summary": "..."}
\`\`\`

### ì¡°ê±´ë¶€ ë¼ìš°íŒ…

\`\`\`python
from langchain_core.runnables import RunnableBranch

branch = RunnableBranch(
    (lambda x: x["type"] == "joke", joke_chain),
    (lambda x: x["type"] == "poem", poem_chain),
    default_chain  # ê¸°ë³¸
)
\`\`\`
        `
      }
    },
    {
      id: 'chains-practice',
      type: 'code',
      title: 'Chains ì‹¤ìŠµ',
      duration: 90,
      content: {
        objectives: [
          'LCELë¡œ ë‹¤ì–‘í•œ ì²´ì¸ì„ êµ¬ì„±í•œë‹¤',
          'ë³‘ë ¬ ë° ì¡°ê±´ë¶€ ì²´ì¸ì„ êµ¬í˜„í•œë‹¤',
          'ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì²´ì¸ ëª¨ë“ˆì„ ë§Œë“ ë‹¤'
        ],
        instructions: `
## ì‹¤ìŠµ: ë‹¤ëª©ì  ì½˜í…ì¸  ìƒì„±ê¸°

### ëª©í‘œ
í•˜ë‚˜ì˜ ì£¼ì œë¡œ ì—¬ëŸ¬ í˜•íƒœì˜ ì½˜í…ì¸ ë¥¼ ìƒì„±í•˜ëŠ” ì²´ì¸ì„ êµ¬ì¶•í•˜ì„¸ìš”.

### ìš”êµ¬ì‚¬í•­
1. ì£¼ì œ ì…ë ¥ â†’ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸, SNS ê²Œì‹œê¸€, ì´ë©”ì¼ ë™ì‹œ ìƒì„±
2. í†¤ ì„ íƒ (formal/casual) ê°€ëŠ¥
3. ì¶œë ¥ í˜•ì‹ ì§€ì • (JSON)
        `,
        starterCode: `from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnableParallel, RunnableLambda

model = ChatOpenAI(model="gpt-4o-mini")

# TODO: ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì²´ì¸
blog_chain = None

# TODO: SNS ê²Œì‹œê¸€ ì²´ì¸
sns_chain = None

# TODO: ì´ë©”ì¼ ì²´ì¸
email_chain = None

# TODO: ë³‘ë ¬ ì‹¤í–‰ ì²´ì¸
content_generator = None

def generate_content(topic: str, tone: str = "casual"):
    """ë‹¤ì–‘í•œ ì½˜í…ì¸  ë™ì‹œ ìƒì„±"""
    # TODO: êµ¬í˜„
    pass

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    result = generate_content("ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜", tone="formal")
    print(result)
`,
        solutionCode: `from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnableLambda
import json

model = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
parser = StrOutputParser()

# ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì²´ì¸
blog_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ {tone} í†¤ì˜ ë¸”ë¡œê·¸ ì‘ê°€ì…ë‹ˆë‹¤."),
    ("human", "'{topic}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”. ì œëª©ê³¼ 3ê°œì˜ ì„¹ì…˜ì„ í¬í•¨í•˜ì„¸ìš”.")
])
blog_chain = blog_prompt | model | parser

# SNS ê²Œì‹œê¸€ ì²´ì¸
sns_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ SNS ë§ˆì¼€í„°ì…ë‹ˆë‹¤. {tone} í†¤ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤."),
    ("human", "'{topic}'ì— ëŒ€í•œ íŠ¸ìœ„í„°/ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œê¸€ì„ ì‘ì„±í•˜ì„¸ìš”. í•´ì‹œíƒœê·¸ í¬í•¨, 280ì ì´ë‚´.")
])
sns_chain = sns_prompt | model | parser

# ì´ë©”ì¼ ì²´ì¸
email_prompt = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¤ë®¤ë‹ˆì¼€ì´í„°ì…ë‹ˆë‹¤. {tone} í†¤ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤."),
    ("human", "'{topic}'ì— ëŒ€í•œ ë‰´ìŠ¤ë ˆí„° ì´ë©”ì¼ì„ ì‘ì„±í•˜ì„¸ìš”. ì œëª©, ì¸ì‚¬ë§, ë³¸ë¬¸, ë§ˆë¬´ë¦¬ë¥¼ í¬í•¨í•˜ì„¸ìš”.")
])
email_chain = email_prompt | model | parser

# ë³‘ë ¬ ì‹¤í–‰ ì²´ì¸
content_generator = RunnableParallel(
    blog=blog_chain,
    sns=sns_chain,
    email=email_chain
)

def generate_content(topic: str, tone: str = "casual") -> dict:
    """ë‹¤ì–‘í•œ ì½˜í…ì¸  ë™ì‹œ ìƒì„±"""
    tone_map = {
        "casual": "ì¹œê·¼í•˜ê³  ëŒ€í™”ì²´ì˜",
        "formal": "ê³µì‹ì ì´ê³  ì „ë¬¸ì ì¸"
    }

    input_data = {
        "topic": topic,
        "tone": tone_map.get(tone, tone_map["casual"])
    }

    result = content_generator.invoke(input_data)
    return result

def generate_with_metadata(topic: str, tone: str = "casual") -> dict:
    """ë©”íƒ€ë°ì´í„° í¬í•¨ ìƒì„±"""
    content = generate_content(topic, tone)

    return {
        "metadata": {
            "topic": topic,
            "tone": tone,
            "types": ["blog", "sns", "email"]
        },
        "content": content
    }

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("=== ì½˜í…ì¸  ìƒì„±ê¸° í…ŒìŠ¤íŠ¸ ===\\n")

    result = generate_content("ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜", tone="formal")

    print("ğŸ“ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸:")
    print("-" * 40)
    print(result["blog"][:500] + "...\\n")

    print("ğŸ“± SNS ê²Œì‹œê¸€:")
    print("-" * 40)
    print(result["sns"] + "\\n")

    print("ğŸ“§ ì´ë©”ì¼:")
    print("-" * 40)
    print(result["email"][:500] + "...")
`
      }
    }
  ]
}

const day3: Day = {
  slug: 'output-parsers',
  title: 'Output Parsers & êµ¬ì¡°í™”',
  totalDuration: 180,
  tasks: [
    {
      id: 'output-parsers-video',
      type: 'video',
      title: 'Output Parsers',
      duration: 30,
      content: {
        objectives: [
          'ë‹¤ì–‘í•œ Output Parserë¥¼ ì´í•´í•œë‹¤',
          'Pydanticê³¼ ì—°ë™í•˜ì—¬ íƒ€ì… ì•ˆì „ ì¶œë ¥ì„ êµ¬í˜„í•œë‹¤',
          'íŒŒì‹± ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ í•™ìŠµí•œë‹¤'
        ],
        videoUrl: 'https://www.youtube.com/watch?v=output-parsers-placeholder',
        transcript: `
## Output Parsers

### Parser ì¢…ë¥˜

\`\`\`python
from langchain_core.output_parsers import (
    StrOutputParser,      # ë¬¸ìì—´
    JsonOutputParser,     # JSON
    CommaSeparatedListOutputParser,  # ë¦¬ìŠ¤íŠ¸
    PydanticOutputParser  # Pydantic ëª¨ë¸
)
\`\`\`

### JsonOutputParser

\`\`\`python
from langchain_core.output_parsers import JsonOutputParser

parser = JsonOutputParser()

chain = prompt | model | parser
result = chain.invoke({"text": "..."})
# dict ë°˜í™˜
\`\`\`

### PydanticOutputParser

\`\`\`python
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

class MovieReview(BaseModel):
    title: str = Field(description="ì˜í™” ì œëª©")
    rating: float = Field(ge=0, le=10, description="í‰ì  0-10")
    summary: str = Field(description="í•œ ì¤„ ìš”ì•½")
    pros: list[str] = Field(description="ì¥ì  ëª©ë¡")
    cons: list[str] = Field(description="ë‹¨ì  ëª©ë¡")

parser = PydanticOutputParser(pydantic_object=MovieReview)

# format_instructionsë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
prompt = ChatPromptTemplate.from_messages([
    ("system", "ì˜í™” ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ì„¸ìš”."),
    ("human", "{review}\\n\\n{format_instructions}")
])

chain = prompt.partial(
    format_instructions=parser.get_format_instructions()
) | model | parser

result = chain.invoke({"review": "ì¸ì…‰ì…˜ì€ ì •ë§ ëŒ€ë‹¨í•œ ì˜í™”ë‹¤..."})
print(result.title)  # "ì¸ì…‰ì…˜"
print(result.rating)  # 9.5
\`\`\`

### ì˜¤ë¥˜ ì²˜ë¦¬

\`\`\`python
from langchain.output_parsers import OutputFixingParser

# ìë™ ì˜¤ë¥˜ ìˆ˜ì • íŒŒì„œ
fixing_parser = OutputFixingParser.from_llm(
    parser=parser,
    llm=model
)

# íŒŒì‹± ì‹¤íŒ¨ ì‹œ LLMìœ¼ë¡œ ì¬ì‹œë„
\`\`\`
        `
      }
    },
    {
      id: 'structured-data-practice',
      type: 'code',
      title: 'êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ ì‹¤ìŠµ',
      duration: 90,
      content: {
        objectives: [
          'ë¹„ì •í˜• í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì¶”ì¶œí•œë‹¤',
          'Pydantic ëª¨ë¸ë¡œ ê²€ì¦í•œë‹¤',
          'ë³µì¡í•œ ì¤‘ì²© êµ¬ì¡°ë¥¼ ì²˜ë¦¬í•œë‹¤'
        ],
        instructions: `
## ì‹¤ìŠµ: ì´ë ¥ì„œ ì •ë³´ ì¶”ì¶œê¸°

### ëª©í‘œ
ë¹„ì •í˜• ì´ë ¥ì„œ í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

### ì¶”ì¶œ í•­ëª©
- ì´ë¦„, ì—°ë½ì²˜
- í•™ë ¥ (í•™êµ, ì „ê³µ, ì¡¸ì—…ë…„ë„)
- ê²½ë ¥ (íšŒì‚¬, ì§ì±…, ê¸°ê°„, ë‹´ë‹¹ ì—…ë¬´)
- ìŠ¤í‚¬
        `,
        starterCode: `from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List, Optional

# TODO: Pydantic ëª¨ë¸ ì •ì˜
class Education(BaseModel):
    pass

class Experience(BaseModel):
    pass

class Resume(BaseModel):
    pass

# TODO: íŒŒì„œ ë° ì²´ì¸ êµ¬ì„±
def extract_resume_info(resume_text: str) -> Resume:
    pass

# í…ŒìŠ¤íŠ¸
sample_resume = """
ê¹€ì² ìˆ˜
ì´ë©”ì¼: kim@example.com | ì „í™”: 010-1234-5678

í•™ë ¥:
ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ í•™ì‚¬ (2018ë…„ ì¡¸ì—…)

ê²½ë ¥:
ë„¤ì´ë²„ - ë°±ì—”ë“œ ê°œë°œì (2020.03 - í˜„ì¬)
- Python/Django ê¸°ë°˜ API ê°œë°œ
- ì¼ 1ì–µ ê±´ íŠ¸ë˜í”½ ì²˜ë¦¬

ì¹´ì¹´ì˜¤ - ì¸í„´ (2019.06 - 2019.12)
- ë°ì´í„° ë¶„ì„ ë³´ì¡°

ìŠ¤í‚¬: Python, Django, PostgreSQL, Docker, AWS
"""

if __name__ == "__main__":
    result = extract_resume_info(sample_resume)
    print(result)
`,
        solutionCode: `from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List, Optional

# Pydantic ëª¨ë¸ ì •ì˜
class Education(BaseModel):
    school: str = Field(description="í•™êµëª…")
    major: str = Field(description="ì „ê³µ")
    degree: str = Field(description="í•™ìœ„ (í•™ì‚¬, ì„ì‚¬, ë°•ì‚¬)")
    graduation_year: Optional[int] = Field(description="ì¡¸ì—…ë…„ë„")

class Experience(BaseModel):
    company: str = Field(description="íšŒì‚¬ëª…")
    position: str = Field(description="ì§ì±…")
    start_date: str = Field(description="ì‹œì‘ì¼ (YYYY.MM)")
    end_date: str = Field(description="ì¢…ë£Œì¼ (YYYY.MM ë˜ëŠ” 'í˜„ì¬')")
    responsibilities: List[str] = Field(description="ë‹´ë‹¹ ì—…ë¬´ ëª©ë¡")

class Resume(BaseModel):
    name: str = Field(description="ì´ë¦„")
    email: Optional[str] = Field(description="ì´ë©”ì¼")
    phone: Optional[str] = Field(description="ì „í™”ë²ˆí˜¸")
    education: List[Education] = Field(description="í•™ë ¥ ëª©ë¡")
    experience: List[Experience] = Field(description="ê²½ë ¥ ëª©ë¡")
    skills: List[str] = Field(description="ìŠ¤í‚¬ ëª©ë¡")

# íŒŒì„œ ë° ì²´ì¸ êµ¬ì„±
model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
parser = PydanticOutputParser(pydantic_object=Resume)

prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ì´ë ¥ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì´ë ¥ì„œì—ì„œ ì •ë³´ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.
ì •ë³´ê°€ ì—†ëŠ” í•„ë“œëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ë‚˜ nullë¡œ ì²˜ë¦¬í•˜ì„¸ìš”."""),
    ("human", """ë‹¤ìŒ ì´ë ¥ì„œì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

{resume_text}

{format_instructions}""")
])

chain = (
    prompt.partial(format_instructions=parser.get_format_instructions())
    | model
    | parser
)

def extract_resume_info(resume_text: str) -> Resume:
    return chain.invoke({"resume_text": resume_text})

# í…ŒìŠ¤íŠ¸
sample_resume = """
ê¹€ì² ìˆ˜
ì´ë©”ì¼: kim@example.com | ì „í™”: 010-1234-5678

í•™ë ¥:
ì„œìš¸ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ í•™ì‚¬ (2018ë…„ ì¡¸ì—…)

ê²½ë ¥:
ë„¤ì´ë²„ - ë°±ì—”ë“œ ê°œë°œì (2020.03 - í˜„ì¬)
- Python/Django ê¸°ë°˜ API ê°œë°œ
- ì¼ 1ì–µ ê±´ íŠ¸ë˜í”½ ì²˜ë¦¬

ì¹´ì¹´ì˜¤ - ì¸í„´ (2019.06 - 2019.12)
- ë°ì´í„° ë¶„ì„ ë³´ì¡°

ìŠ¤í‚¬: Python, Django, PostgreSQL, Docker, AWS
"""

if __name__ == "__main__":
    result = extract_resume_info(sample_resume)

    print("=== ì´ë ¥ì„œ ë¶„ì„ ê²°ê³¼ ===\\n")
    print(f"ì´ë¦„: {result.name}")
    print(f"ì´ë©”ì¼: {result.email}")
    print(f"ì „í™”: {result.phone}")

    print("\\ní•™ë ¥:")
    for edu in result.education:
        print(f"  - {edu.school} {edu.major} ({edu.degree}, {edu.graduation_year})")

    print("\\nê²½ë ¥:")
    for exp in result.experience:
        print(f"  - {exp.company} / {exp.position} ({exp.start_date} ~ {exp.end_date})")
        for resp in exp.responsibilities:
            print(f"    * {resp}")

    print(f"\\nìŠ¤í‚¬: {', '.join(result.skills)}")
`
      }
    }
  ]
}

const day4: Day = {
  slug: 'memory-history',
  title: 'Memory & ëŒ€í™” íˆìŠ¤í† ë¦¬',
  totalDuration: 180,
  tasks: [
    {
      id: 'memory-video',
      type: 'video',
      title: 'Memory ì‹œìŠ¤í…œ',
      duration: 30,
      content: {
        objectives: [
          'LangChain Memory ìœ í˜•ì„ ì´í•´í•œë‹¤',
          'ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤',
          'ë©”ëª¨ë¦¬ ìµœì í™” ì „ëµì„ í•™ìŠµí•œë‹¤'
        ],
        videoUrl: 'https://www.youtube.com/watch?v=langchain-memory-placeholder',
        transcript: `
## Memory ì‹œìŠ¤í…œ

### Memory ìœ í˜•

\`\`\`
ConversationBufferMemory
â”œâ”€â”€ ì „ì²´ ëŒ€í™” ì €ì¥
â”œâ”€â”€ ë‹¨ìˆœí•˜ì§€ë§Œ í† í° ì¦ê°€
â””â”€â”€ ì§§ì€ ëŒ€í™”ì— ì í•©

ConversationBufferWindowMemory
â”œâ”€â”€ ìµœê·¼ Nê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
â”œâ”€â”€ í† í° ì œí•œ
â””â”€â”€ ê¸´ ëŒ€í™”ì— ì í•©

ConversationSummaryMemory
â”œâ”€â”€ ëŒ€í™” ìš”ì•½ ì €ì¥
â”œâ”€â”€ í† í° íš¨ìœ¨ì 
â””â”€â”€ ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤ ê°€ëŠ¥

ConversationSummaryBufferMemory
â”œâ”€â”€ ìµœê·¼ ëŒ€í™” + ê³¼ê±° ìš”ì•½
â”œâ”€â”€ ê· í˜• ì¡íŒ ì ‘ê·¼
â””â”€â”€ ì‹¤ë¬´ì—ì„œ ìì£¼ ì‚¬ìš©
\`\`\`

### ChatMessageHistory

\`\`\`python
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ
store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# íˆìŠ¤í† ë¦¬ í¬í•¨ ì²´ì¸
chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="question",
    history_messages_key="history"
)

# ì„¸ì…˜ë³„ ëŒ€í™”
response = chain_with_history.invoke(
    {"question": "ì•ˆë…•í•˜ì„¸ìš”"},
    config={"configurable": {"session_id": "user123"}}
)
\`\`\`

### ì˜êµ¬ ì €ì¥

\`\`\`python
from langchain_community.chat_message_histories import RedisChatMessageHistory

def get_redis_history(session_id: str):
    return RedisChatMessageHistory(
        session_id=session_id,
        url="redis://localhost:6379"
    )
\`\`\`
        `
      }
    },
    {
      id: 'chatbot-practice',
      type: 'code',
      title: 'ëŒ€í™”í˜• ì±—ë´‡ ì‹¤ìŠµ',
      duration: 90,
      content: {
        objectives: [
          'Memoryë¥¼ í™œìš©í•œ ì±—ë´‡ì„ êµ¬í˜„í•œë‹¤',
          'ì„¸ì…˜ ê´€ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤',
          'ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•œë‹¤'
        ],
        instructions: `
## ì‹¤ìŠµ: ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì±—ë´‡

### ëª©í‘œ
ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ëŠ” ì±—ë´‡ì„ êµ¬í˜„í•˜ì„¸ìš”.

### ìš”êµ¬ì‚¬í•­
1. ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
2. ì´ì „ ëŒ€í™” ì°¸ì¡° ê°€ëŠ¥
3. íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ (ìµœê·¼ 10ê°œ)
4. ì„¸ì…˜ ì´ˆê¸°í™” ê¸°ëŠ¥
        `,
        starterCode: `from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

class ChatBot:
    def __init__(self, system_prompt: str = "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AIì…ë‹ˆë‹¤."):
        self.model = ChatOpenAI(model="gpt-4o-mini")
        self.sessions = {}
        # TODO: ì²´ì¸ êµ¬ì„±

    def get_session_history(self, session_id: str):
        # TODO: êµ¬í˜„
        pass

    def chat(self, message: str, session_id: str = "default") -> str:
        # TODO: êµ¬í˜„
        pass

    def clear_session(self, session_id: str):
        # TODO: êµ¬í˜„
        pass

    def get_history(self, session_id: str) -> list:
        # TODO: êµ¬í˜„
        pass

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    bot = ChatBot()

    print(bot.chat("ë‚´ ì´ë¦„ì€ ì² ìˆ˜ì•¼", "user1"))
    print(bot.chat("ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?", "user1"))
`,
        solutionCode: `from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser

class ChatBot:
    def __init__(self, system_prompt: str = "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AIì…ë‹ˆë‹¤.", max_history: int = 10):
        self.model = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
        self.sessions = {}
        self.max_history = max_history

        # í”„ë¡¬í”„íŠ¸ (íˆìŠ¤í† ë¦¬ í¬í•¨)
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}")
        ])

        # ê¸°ë³¸ ì²´ì¸
        base_chain = prompt | self.model | StrOutputParser()

        # íˆìŠ¤í† ë¦¬ í¬í•¨ ì²´ì¸
        self.chain = RunnableWithMessageHistory(
            base_chain,
            self.get_session_history,
            input_messages_key="question",
            history_messages_key="history"
        )

    def get_session_history(self, session_id: str) -> ChatMessageHistory:
        if session_id not in self.sessions:
            self.sessions[session_id] = ChatMessageHistory()
        return self.sessions[session_id]

    def _trim_history(self, session_id: str):
        """íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ"""
        history = self.sessions.get(session_id)
        if history and len(history.messages) > self.max_history * 2:
            # ìµœê·¼ ë©”ì‹œì§€ë§Œ ìœ ì§€ (ì§ˆë¬¸+ì‘ë‹µ ìŒ)
            history.messages = history.messages[-self.max_history * 2:]

    def chat(self, message: str, session_id: str = "default") -> str:
        response = self.chain.invoke(
            {"question": message},
            config={"configurable": {"session_id": session_id}}
        )
        self._trim_history(session_id)
        return response

    def clear_session(self, session_id: str):
        if session_id in self.sessions:
            self.sessions[session_id].clear()
            print(f"ì„¸ì…˜ '{session_id}' ì´ˆê¸°í™”ë¨")

    def get_history(self, session_id: str) -> list:
        history = self.sessions.get(session_id)
        if not history:
            return []
        return [(msg.type, msg.content) for msg in history.messages]

    def list_sessions(self) -> list:
        return list(self.sessions.keys())

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    bot = ChatBot(system_prompt="ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì´ë¦„ì„ ê¸°ì–µí•˜ì„¸ìš”.")

    print("=== ì„¸ì…˜ 1 (user1) ===")
    print(f"User: ë‚´ ì´ë¦„ì€ ì² ìˆ˜ì•¼")
    print(f"Bot: {bot.chat('ë‚´ ì´ë¦„ì€ ì² ìˆ˜ì•¼', 'user1')}")

    print(f"\\nUser: ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?")
    print(f"Bot: {bot.chat('ë‚´ ì´ë¦„ì´ ë­ì˜€ì§€?', 'user1')}")

    print("\\n=== ì„¸ì…˜ 2 (user2) ===")
    print(f"User: ì•ˆë…•! ë‚˜ëŠ” ì˜í¬ì•¼")
    print(f"Bot: {bot.chat('ì•ˆë…•! ë‚˜ëŠ” ì˜í¬ì•¼', 'user2')}")

    print("\\n=== íˆìŠ¤í† ë¦¬ í™•ì¸ ===")
    print(f"user1 íˆìŠ¤í† ë¦¬: {bot.get_history('user1')}")
    print(f"user2 íˆìŠ¤í† ë¦¬: {bot.get_history('user2')}")

    print("\\n=== ì„¸ì…˜ ì´ˆê¸°í™” ===")
    bot.clear_session('user1')
    print(f"User: ë‚´ ì´ë¦„ì´ ë­ì•¼?")
    print(f"Bot: {bot.chat('ë‚´ ì´ë¦„ì´ ë­ì•¼?', 'user1')}")
`
      }
    }
  ]
}

const day5: Day = {
  slug: 'langchain-project',
  title: 'LangChain ë¯¸ë‹ˆ í”„ë¡œì íŠ¸',
  totalDuration: 180,
  tasks: [
    {
      id: 'langchain-project-challenge',
      type: 'challenge',
      title: 'ë¬¸ì„œ Q&A ì‹œìŠ¤í…œ',
      duration: 180,
      content: {
        objectives: [
          'LangChain ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•©í•œë‹¤',
          'ì‹¤ì „ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•œë‹¤',
          'í”„ë¡œë•ì…˜ ê³ ë ¤ì‚¬í•­ì„ ì ìš©í•œë‹¤'
        ],
        requirements: [
          '**ê¸°ë³¸ ìš”êµ¬ì‚¬í•­**',
          '- PDF ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹',
          '- ë²¡í„° ìŠ¤í† ì–´ì— ì„ë² ë”© ì €ì¥',
          '- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±',
          '- ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ',
          '',
          '**LangChain ì»´í¬ë„ŒíŠ¸ í™œìš©**',
          '- Document Loaders (PDF)',
          '- Text Splitters',
          '- Embeddings & Vector Store',
          '- Retriever + Chain',
          '',
          '**ì¶”ê°€ ê¸°ëŠ¥**',
          '- ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€',
          '- ë‹µë³€ ë¶ˆê°€ ì‹œ ì²˜ë¦¬',
          '- ì‹ ë¢°ë„ ì ìˆ˜ í‘œì‹œ'
        ],
        evaluationCriteria: [
          'LangChain í™œìš©ë„ (25%)',
          'ë‹µë³€ í’ˆì§ˆ (25%)',
          'ì½”ë“œ êµ¬ì¡° (25%)',
          'UX/ì—ëŸ¬ ì²˜ë¦¬ (25%)'
        ],
        bonusPoints: [
          'ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì§€ì›',
          'í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í‚¤ì›Œë“œ + ë²¡í„°)',
          'ë‹µë³€ ìºì‹±',
          'Streamlit UI'
        ]
      }
    }
  ]
}

export const langchainBasicsWeek: Week = {
  slug: 'langchain-basics',
  week: 2,
  phase: 5,
  month: 10,
  access: 'pro',
  title: 'LangChain ê¸°ì´ˆ',
  topics: ['LangChain', 'LCEL', 'Chains', 'Memory', 'Output Parsers'],
  practice: 'ë¬¸ì„œ Q&A ì‹œìŠ¤í…œ',
  totalDuration: 900,
  days: [day1, day2, day3, day4, day5]
}
