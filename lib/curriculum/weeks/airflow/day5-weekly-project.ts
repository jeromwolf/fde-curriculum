import { Task } from '../../types'

export const day5Tasks: Task[] = [
  {
    id: 'w7d5t1',
    title: 'í”„ë¡œì íŠ¸ ê°œìš”: ETL ì›Œí¬í”Œë¡œìš° ìë™í™”',
    type: 'reading',
    duration: 20,
    content: {
      markdown: `# Week 7 í”„ë¡œì íŠ¸: ETL ì›Œí¬í”Œë¡œìš° ìë™í™” ì‹œìŠ¤í…œ

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ

ì´ë²ˆ ì£¼ì— ë°°ìš´ Apache Airflowë¥¼ í™œìš©í•˜ì—¬ **ì‹¤ì œ ìš´ì˜ í™˜ê²½ ìˆ˜ì¤€ì˜ ETL íŒŒì´í”„ë¼ì¸**ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
3ê°œì˜ ì—°ê³„ëœ DAGë¥¼ ì„¤ê³„í•˜ê³ , ì—ëŸ¬ í•¸ë“¤ë§, ëª¨ë‹ˆí„°ë§, ì•Œë¦¼ ì‹œìŠ¤í…œê¹Œì§€ í¬í•¨í•œ ì™„ì„±ëœ ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“­ë‹ˆë‹¤.

## ğŸ“‹ í”„ë¡œì íŠ¸ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ DAG êµ¬ì„±

| DAG ì´ë¦„ | ìŠ¤ì¼€ì¤„ | ì—­í•  |
|----------|--------|------|
| \`daily_extract\` | ë§¤ì¼ 02:00 | ì†ŒìŠ¤ ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ì¶”ì¶œ |
| \`daily_transform\` | daily_extract ì™„ë£Œ í›„ | ë°ì´í„° ì •ì œ ë° ë³€í™˜ |
| \`weekly_report\` | ë§¤ì£¼ ì›”ìš”ì¼ 09:00 | ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± |

### ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Airflow Scheduler                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   02:00     â”‚  â”‚  Trigger    â”‚  â”‚   Monday 09:00          â”‚  â”‚
â”‚  â”‚   Daily     â”‚  â”‚   After     â”‚  â”‚   Weekly                â”‚  â”‚
â”‚  â”‚  Extract    â”‚â”€â”€â–¶â”‚  Extract   â”‚  â”‚   Report                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                     â”‚
          â–¼                â–¼                     â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  API/DB/S3   â”‚  â”‚  Spark/SQL   â”‚  â”‚  Report Generator    â”‚
   â”‚  Extraction  â”‚  â”‚  Transform   â”‚  â”‚  + Email/Slack       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## ğŸ“Š ë°ì´í„° ì‹œë‚˜ë¦¬ì˜¤

E-commerce í”Œë«í¼ì˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤:

### ë°ì´í„° ì†ŒìŠ¤
1. **ì£¼ë¬¸ ë°ì´í„°** - PostgreSQL \`orders\` í…Œì´ë¸”
2. **ê³ ê° ë°ì´í„°** - REST API (\`/api/customers\`)
3. **ìƒí’ˆ ë°ì´í„°** - S3 ë²„í‚· (\`s3://raw-data/products/\`)

### ë³€í™˜ ê·œì¹™
- ì¤‘ë³µ ì œê±° ë° NULL ê°’ ì²˜ë¦¬
- ë‚ ì§œ í˜•ì‹ í‘œì¤€í™” (ISO 8601)
- ê¸ˆì•¡ ë‹¨ìœ„ í†µì¼ (KRW â†’ USD í™˜ì‚°)
- ê³ ê° ë“±ê¸‰ ê³„ì‚° (RFM ê¸°ë°˜)

### ë¦¬í¬íŠ¸ ìš”êµ¬ì‚¬í•­
- ì£¼ê°„ ë§¤ì¶œ ìš”ì•½
- Top 10 ìƒí’ˆ
- ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„
- ì´ìƒì¹˜ íƒì§€ ê²°ê³¼

## ğŸ› ï¸ ê¸°ìˆ  ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ êµ¬í˜„ ì‚¬í•­

1. **DAG êµ¬ì„±**
   - TaskFlow API ì‚¬ìš©
   - XComìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì „ë‹¬
   - TaskGroupìœ¼ë¡œ ë…¼ë¦¬ì  ê·¸ë£¹í•‘

2. **ì—ëŸ¬ í•¸ë“¤ë§**
   - ì¬ì‹œë„ ì •ì±… (3íšŒ, exponential backoff)
   - ì‹¤íŒ¨ ì½œë°± (Slack ì•Œë¦¼)
   - SLA ì„¤ì • (ê° DAGë³„ ì™„ë£Œ ì‹œê°„ ì œí•œ)

3. **ëª¨ë‹ˆí„°ë§**
   - ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ (ì²˜ë¦¬ ê±´ìˆ˜, ì†Œìš” ì‹œê°„)
   - ë¡œê¹… í‘œì¤€í™”
   - ì´ìƒ íƒì§€ ì•Œë¦¼

4. **ë°ì´í„° í’ˆì§ˆ**
   - Great Expectations ë˜ëŠ” ì»¤ìŠ¤í…€ ê²€ì¦
   - ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
   - ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

\`\`\`
week7_project/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ daily_extract.py
â”‚   â”œâ”€â”€ daily_transform.py
â”‚   â””â”€â”€ weekly_report.py
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â””â”€â”€ custom_operators.py
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ api_hook.py
â”‚   â””â”€â”€ callbacks/
â”‚       â””â”€â”€ slack_callbacks.py
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â”œâ”€â”€ extract_orders.sql
â”‚   â”‚   â””â”€â”€ transform_data.sql
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ report_template.html
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_daily_extract.py
â”‚   â””â”€â”€ test_daily_transform.py
â””â”€â”€ docker-compose.yml
\`\`\`

## ğŸ“ˆ í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ê¸°ì¤€ |
|------|------|------|
| DAG ì„¤ê³„ | 25% | ì˜ì¡´ì„±, ìŠ¤ì¼€ì¤„ë§ ì •í™•ì„± |
| ì—ëŸ¬ í•¸ë“¤ë§ | 25% | ì¬ì‹œë„, ì½œë°±, SLA |
| ì½”ë“œ í’ˆì§ˆ | 20% | TaskFlow API, ëª¨ë“ˆí™” |
| í…ŒìŠ¤íŠ¸ | 15% | ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ |
| ë¬¸ì„œí™” | 15% | README, ì½”ë“œ ì£¼ì„ |

## ğŸš€ ì‹œì‘í•˜ê¸°

\`\`\`bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir week7_project && cd week7_project

# Docker Composeë¡œ Airflow í™˜ê²½ êµ¬ì„±
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.8.0/docker-compose.yaml'

# ë””ë ‰í† ë¦¬ ì´ˆê¸°í™”
mkdir -p ./dags ./logs ./plugins ./include

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo "AIRFLOW_UID=$(id -u)" > .env

# Airflow ì‹œì‘
docker compose up -d
\`\`\`
`,
      externalLinks: [
        { title: 'Airflow Best Practices', url: 'https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html' },
        { title: 'TaskFlow API', url: 'https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html' },
        { title: 'Testing DAGs', url: 'https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#testing-a-dag' }
      ]
    }
  },
  {
    id: 'w7d5t2',
    title: 'DAG 1: Daily Extract êµ¬í˜„',
    type: 'code',
    duration: 45,
    content: {
      instructions: `## DAG 1: Daily Extract êµ¬í˜„

ë§¤ì¼ 02:00ì— ì‹¤í–‰ë˜ì–´ 3ê°œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” DAGë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **TaskFlow API ì‚¬ìš©**
   - @task ë°ì½”ë ˆì´í„°ë¡œ í•¨ìˆ˜ ì •ì˜
   - XComìœ¼ë¡œ ì¶”ì¶œ ë©”íƒ€ë°ì´í„° ìë™ ì „ë‹¬

2. **ë³‘ë ¬ ì¶”ì¶œ**
   - DB, API, S3 ì¶”ì¶œì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
   - TaskGroupìœ¼ë¡œ ê·¸ë£¹í•‘

3. **ì—ëŸ¬ í•¸ë“¤ë§**
   - ì¬ì‹œë„ 3íšŒ (5ë¶„, 15ë¶„, 45ë¶„ ëŒ€ê¸°)
   - ì‹¤íŒ¨ ì‹œ Slack ì•Œë¦¼

4. **ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘**
   - ê° ì†ŒìŠ¤ë³„ ì¶”ì¶œ ê±´ìˆ˜
   - ì†Œìš” ì‹œê°„
   - ë‹¤ìŒ DAGë¡œ ì „ë‹¬

### í…ŒìŠ¤íŠ¸ ë°©ë²•

\`\`\`bash
# DAG ë¬¸ë²• ê²€ì‚¬
python -c "from daily_extract import dag; print('DAG loaded!')"

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
airflow dags test daily_extract 2024-01-01
\`\`\`
`,
      starterCode: `"""
DAG: Daily Extract
ë§¤ì¼ 02:00ì— ì†ŒìŠ¤ ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ì¶”ì¶œ
"""

from datetime import datetime, timedelta
from airflow.decorators import dag, task, task_group
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.http.hooks.http import HttpHook
from airflow.models import Variable
import json
import logging

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email': ['data-team@company.com'],
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(hours=1),
}

def slack_failure_callback(context):
    """Slack ì•Œë¦¼ ì½œë°±"""
    # TODO: Slack ì•Œë¦¼ êµ¬í˜„
    pass


@dag(
    dag_id='daily_extract',
    description='ì†ŒìŠ¤ ì‹œìŠ¤í…œì—ì„œ ì¼ì¼ ë°ì´í„° ì¶”ì¶œ',
    # TODO: ìŠ¤ì¼€ì¤„ ì„¤ì • (ë§¤ì¼ 02:00 UTC)
    schedule=None,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=['extract', 'daily', 'etl'],
    # TODO: SLA ì„¤ì •
)
def daily_extract():
    """Daily Extract DAG"""

    @task_group(group_id='extract_sources')
    def extract_all_sources():
        """ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ì¶”ì¶œ"""

        @task
        def extract_orders():
            """PostgreSQLì—ì„œ ì£¼ë¬¸ ë°ì´í„° ì¶”ì¶œ"""
            # TODO: PostgresHook ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì¶”ì¶œ
            # íŒíŠ¸: hook.get_pandas_df() ì‚¬ìš©

            hook = PostgresHook(postgres_conn_id='source_postgres')

            # ì–´ì œ ë‚ ì§œì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
            query = """
                SELECT * FROM orders
                WHERE order_date = CURRENT_DATE - INTERVAL '1 day'
            """

            # ë°ì´í„° ì¶”ì¶œ ë° S3ì— ì €ì¥
            # ...

            return {
                'source': 'orders',
                'count': 0,  # TODO: ì‹¤ì œ ê±´ìˆ˜
                'destination': 's3://data-lake/raw/orders/'
            }

        @task
        def extract_customers():
            """REST APIì—ì„œ ê³ ê° ë°ì´í„° ì¶”ì¶œ"""
            # TODO: HttpHook ì‚¬ìš©í•˜ì—¬ API í˜¸ì¶œ
            pass

        @task
        def extract_products():
            """S3ì—ì„œ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ"""
            # TODO: S3Hook ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë³µì‚¬
            pass

        # ë³‘ë ¬ ì‹¤í–‰
        return [extract_orders(), extract_customers(), extract_products()]

    @task
    def validate_extracts(extract_results: list):
        """ì¶”ì¶œ ê²°ê³¼ ê²€ì¦"""
        # TODO: ê° ì†ŒìŠ¤ì˜ ì¶”ì¶œ ê²°ê³¼ ê²€ì¦
        # ê±´ìˆ˜ 0ì´ë©´ ê²½ê³ 
        # ì „ì¼ ëŒ€ë¹„ ê¸‰ê²©í•œ ë³€í™” ê°ì§€
        pass

    @task
    def notify_completion(validation_result):
        """ì¶”ì¶œ ì™„ë£Œ ì•Œë¦¼ ë° ë‹¤ìŒ DAG íŠ¸ë¦¬ê±°"""
        # TODO: ë©”íƒ€ë°ì´í„°ë¥¼ XComì— ì €ì¥
        # TriggerDagRunOperator ëŒ€ì‹  XCom ì‚¬ìš©
        pass

    # DAG íë¦„ ì •ì˜
    extracts = extract_all_sources()
    validated = validate_extracts(extracts)
    notify_completion(validated)


# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
dag = daily_extract()
`,
      solutionCode: `"""
DAG: Daily Extract
ë§¤ì¼ 02:00ì— ì†ŒìŠ¤ ì‹œìŠ¤í…œì—ì„œ ë°ì´í„° ì¶”ì¶œ
"""

from datetime import datetime, timedelta
from airflow.decorators import dag, task, task_group
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.http.hooks.http import HttpHook
from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
from airflow.models import Variable
from airflow.exceptions import AirflowSkipException
import json
import logging
import pandas as pd
from io import StringIO

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email': ['data-team@company.com'],
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(hours=1),
}


def slack_failure_callback(context):
    """Slack ì•Œë¦¼ ì½œë°±"""
    task_instance = context['task_instance']
    dag_id = context['dag'].dag_id
    task_id = task_instance.task_id
    execution_date = context['execution_date']
    exception = context.get('exception', 'Unknown error')

    message = f"""
:red_circle: *DAG ì‹¤íŒ¨ ì•Œë¦¼*
â€¢ DAG: {dag_id}
â€¢ Task: {task_id}
â€¢ ì‹¤í–‰ ì‹œê°„: {execution_date}
â€¢ ì—ëŸ¬: {str(exception)[:200]}
    """

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception as e:
        logger.error(f"Slack ì•Œë¦¼ ì‹¤íŒ¨: {e}")


def slack_success_callback(context):
    """ì„±ê³µ ì•Œë¦¼ ì½œë°±"""
    dag_id = context['dag'].dag_id
    execution_date = context['execution_date']

    message = f"""
:white_check_mark: *DAG ì™„ë£Œ*
â€¢ DAG: {dag_id}
â€¢ ì‹¤í–‰ ì‹œê°„: {execution_date}
    """

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception as e:
        logger.error(f"Slack ì•Œë¦¼ ì‹¤íŒ¨: {e}")


@dag(
    dag_id='daily_extract',
    description='ì†ŒìŠ¤ ì‹œìŠ¤í…œì—ì„œ ì¼ì¼ ë°ì´í„° ì¶”ì¶œ',
    schedule='0 2 * * *',  # ë§¤ì¼ 02:00 UTC
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=['extract', 'daily', 'etl'],
    on_failure_callback=slack_failure_callback,
    on_success_callback=slack_success_callback,
    sla_miss_callback=slack_failure_callback,
    dagrun_timeout=timedelta(hours=2),
)
def daily_extract():
    """Daily Extract DAG"""

    @task_group(group_id='extract_sources')
    def extract_all_sources():
        """ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ì¶”ì¶œ"""

        @task(
            retries=3,
            retry_delay=timedelta(minutes=5),
            sla=timedelta(hours=1),
        )
        def extract_orders(**context):
            """PostgreSQLì—ì„œ ì£¼ë¬¸ ë°ì´í„° ì¶”ì¶œ"""
            execution_date = context['logical_date']
            target_date = (execution_date - timedelta(days=1)).strftime('%Y-%m-%d')

            logger.info(f"Extracting orders for date: {target_date}")

            # PostgreSQLì—ì„œ ë°ì´í„° ì¶”ì¶œ
            pg_hook = PostgresHook(postgres_conn_id='source_postgres')

            query = f"""
                SELECT
                    order_id,
                    customer_id,
                    product_id,
                    quantity,
                    unit_price,
                    total_amount,
                    order_date,
                    status,
                    created_at
                FROM orders
                WHERE order_date = '{target_date}'
            """

            df = pg_hook.get_pandas_df(query)
            row_count = len(df)

            logger.info(f"Extracted {row_count} orders")

            # S3ì— ì €ì¥
            if row_count > 0:
                s3_hook = S3Hook(aws_conn_id='aws_default')
                csv_buffer = StringIO()
                df.to_csv(csv_buffer, index=False)

                s3_key = f"raw/orders/dt={target_date}/orders.csv"
                s3_hook.load_string(
                    string_data=csv_buffer.getvalue(),
                    key=s3_key,
                    bucket_name='data-lake',
                    replace=True
                )

                destination = f"s3://data-lake/{s3_key}"
            else:
                destination = None
                logger.warning("No orders found for extraction")

            return {
                'source': 'orders',
                'count': row_count,
                'destination': destination,
                'target_date': target_date
            }

        @task(
            retries=3,
            retry_delay=timedelta(minutes=5),
            sla=timedelta(hours=1),
        )
        def extract_customers(**context):
            """REST APIì—ì„œ ê³ ê° ë°ì´í„° ì¶”ì¶œ"""
            execution_date = context['logical_date']
            target_date = (execution_date - timedelta(days=1)).strftime('%Y-%m-%d')

            logger.info(f"Extracting customers updated on: {target_date}")

            http_hook = HttpHook(http_conn_id='customer_api', method='GET')

            # í˜ì´ì§• ì²˜ë¦¬
            all_customers = []
            page = 1
            page_size = 100

            while True:
                response = http_hook.run(
                    endpoint='/api/customers',
                    data={
                        'updated_since': target_date,
                        'page': page,
                        'page_size': page_size
                    }
                )

                data = response.json()
                customers = data.get('customers', [])

                if not customers:
                    break

                all_customers.extend(customers)
                page += 1

                if len(customers) < page_size:
                    break

            row_count = len(all_customers)
            logger.info(f"Extracted {row_count} customers")

            # S3ì— ì €ì¥
            if row_count > 0:
                s3_hook = S3Hook(aws_conn_id='aws_default')
                s3_key = f"raw/customers/dt={target_date}/customers.json"

                s3_hook.load_string(
                    string_data=json.dumps(all_customers, ensure_ascii=False),
                    key=s3_key,
                    bucket_name='data-lake',
                    replace=True
                )

                destination = f"s3://data-lake/{s3_key}"
            else:
                destination = None

            return {
                'source': 'customers',
                'count': row_count,
                'destination': destination,
                'target_date': target_date
            }

        @task(
            retries=3,
            retry_delay=timedelta(minutes=5),
            sla=timedelta(hours=1),
        )
        def extract_products(**context):
            """S3ì—ì„œ ìƒí’ˆ ë°ì´í„° ë³µì‚¬ (ì†ŒìŠ¤ â†’ ë°ì´í„° ë ˆì´í¬)"""
            execution_date = context['logical_date']
            target_date = (execution_date - timedelta(days=1)).strftime('%Y-%m-%d')

            logger.info(f"Copying products for date: {target_date}")

            s3_hook = S3Hook(aws_conn_id='aws_default')

            # ì†ŒìŠ¤ ë²„í‚·ì—ì„œ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
            source_prefix = f"raw-products/dt={target_date}/"
            keys = s3_hook.list_keys(
                bucket_name='source-bucket',
                prefix=source_prefix
            )

            if not keys:
                logger.warning(f"No product files found for {target_date}")
                return {
                    'source': 'products',
                    'count': 0,
                    'destination': None,
                    'target_date': target_date
                }

            # íŒŒì¼ ë³µì‚¬
            copied_count = 0
            for key in keys:
                dest_key = key.replace('raw-products/', 'raw/products/')

                s3_hook.copy_object(
                    source_bucket_key=key,
                    source_bucket_name='source-bucket',
                    dest_bucket_key=dest_key,
                    dest_bucket_name='data-lake'
                )
                copied_count += 1

            logger.info(f"Copied {copied_count} product files")

            return {
                'source': 'products',
                'count': copied_count,
                'destination': f"s3://data-lake/raw/products/dt={target_date}/",
                'target_date': target_date
            }

        # ë³‘ë ¬ ì‹¤í–‰
        return [extract_orders(), extract_customers(), extract_products()]

    @task
    def validate_extracts(extract_results: list):
        """ì¶”ì¶œ ê²°ê³¼ ê²€ì¦"""
        logger.info(f"Validating {len(extract_results)} extracts")

        validation_result = {
            'status': 'success',
            'total_records': 0,
            'sources': [],
            'warnings': [],
            'errors': []
        }

        for result in extract_results:
            source = result['source']
            count = result['count']

            validation_result['sources'].append({
                'source': source,
                'count': count,
                'destination': result.get('destination')
            })

            validation_result['total_records'] += count

            # ê±´ìˆ˜ ê²€ì¦
            if count == 0:
                validation_result['warnings'].append(
                    f"{source}: ì¶”ì¶œëœ ë°ì´í„° ì—†ìŒ"
                )

            # ìµœì†Œ ê±´ìˆ˜ ì²´í¬ (ordersëŠ” ìµœì†Œ 100ê±´ ê¸°ëŒ€)
            if source == 'orders' and 0 < count < 100:
                validation_result['warnings'].append(
                    f"{source}: ì˜ˆìƒë³´ë‹¤ ì ì€ ê±´ìˆ˜ ({count}ê±´)"
                )

        if validation_result['errors']:
            validation_result['status'] = 'failed'
        elif validation_result['warnings']:
            validation_result['status'] = 'warning'

        logger.info(f"Validation result: {validation_result['status']}")

        return validation_result

    @task
    def notify_completion(validation_result: dict, **context):
        """ì¶”ì¶œ ì™„ë£Œ ì•Œë¦¼ ë° ë©”íƒ€ë°ì´í„° ì €ì¥"""
        execution_date = context['logical_date']

        # ê²°ê³¼ ë¡œê¹…
        logger.info(f"Extract completed: {validation_result}")

        # ê²½ê³ ê°€ ìˆìœ¼ë©´ Slack ì•Œë¦¼
        if validation_result.get('warnings'):
            try:
                hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
                message = f"""
:warning: *Daily Extract ê²½ê³ *
â€¢ ì‹¤í–‰ ì‹œê°„: {execution_date}
â€¢ ì´ ì¶”ì¶œ ê±´ìˆ˜: {validation_result['total_records']:,}
â€¢ ê²½ê³ : {', '.join(validation_result['warnings'])}
                """
                hook.send(text=message)
            except Exception as e:
                logger.error(f"Slack ì•Œë¦¼ ì‹¤íŒ¨: {e}")

        # ë‹¤ìŒ DAGë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° ë°˜í™˜
        return {
            'execution_date': str(execution_date),
            'total_records': validation_result['total_records'],
            'sources': validation_result['sources'],
            'status': validation_result['status']
        }

    # DAG íë¦„ ì •ì˜
    extracts = extract_all_sources()
    validated = validate_extracts(extracts)
    notify_completion(validated)


# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
dag = daily_extract()
`,
      hints: [
        "TaskFlow APIì—ì„œ @task í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì€ ìë™ìœ¼ë¡œ XComì— ì €ì¥ë©ë‹ˆë‹¤",
        "ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ë©´ ë™ì‹œì— ì‹¤í–‰ë©ë‹ˆë‹¤",
        "sla íŒŒë¼ë¯¸í„°ë¡œ ê° Taskì˜ ì™„ë£Œ ì‹œê°„ ì œí•œì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        "PostgresHook.get_pandas_df()ë¡œ ì‰½ê²Œ DataFrameìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w7d5t3',
    title: 'DAG 2: Daily Transform êµ¬í˜„',
    type: 'code',
    duration: 45,
    content: {
      instructions: `## DAG 2: Daily Transform êµ¬í˜„

Daily Extractê°€ ì™„ë£Œëœ í›„ ìë™ìœ¼ë¡œ íŠ¸ë¦¬ê±°ë˜ì–´ ë°ì´í„° ë³€í™˜ì„ ìˆ˜í–‰í•˜ëŠ” DAGë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **DAG ì˜ì¡´ì„±**
   - Dataset ë˜ëŠ” ExternalTaskSensorë¡œ daily_extract ì™„ë£Œ ëŒ€ê¸°
   - ì¶”ì¶œ ë©”íƒ€ë°ì´í„° í™œìš©

2. **ë³€í™˜ ë¡œì§**
   - ë°ì´í„° ì •ì œ (NULL ì²˜ë¦¬, ì¤‘ë³µ ì œê±°)
   - í˜•ì‹ ë³€í™˜ (ë‚ ì§œ, ê¸ˆì•¡)
   - íŒŒìƒ ì»¬ëŸ¼ ìƒì„± (ê³ ê° ë“±ê¸‰ ë“±)

3. **ë°ì´í„° í’ˆì§ˆ ê²€ì¦**
   - ìŠ¤í‚¤ë§ˆ ê²€ì¦
   - ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
   - ì´ìƒì¹˜ íƒì§€

4. **ì¶œë ¥**
   - Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥
   - íŒŒí‹°ì…˜ êµ¬ì„± (date, category)
`,
      starterCode: `"""
DAG: Daily Transform
Daily Extract ì™„ë£Œ í›„ ë°ì´í„° ë³€í™˜ ìˆ˜í–‰
"""

from datetime import datetime, timedelta
from airflow.decorators import dag, task, task_group
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import pandas as pd
import logging

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
}


@dag(
    dag_id='daily_transform',
    description='ì¶”ì¶œëœ ë°ì´í„° ë³€í™˜ ë° ì •ì œ',
    schedule=None,  # TODO: daily_extract ì™„ë£Œ í›„ íŠ¸ë¦¬ê±°
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=['transform', 'daily', 'etl'],
)
def daily_transform():
    """Daily Transform DAG"""

    # TODO: ExternalTaskSensorë¡œ daily_extract ì™„ë£Œ ëŒ€ê¸°
    wait_for_extract = ExternalTaskSensor(
        task_id='wait_for_extract',
        external_dag_id='daily_extract',
        external_task_id='notify_completion',
        mode='poke',
        timeout=3600,
        poke_interval=60,
    )

    @task_group(group_id='transform_data')
    def transform_all():
        """ë°ì´í„° ë³€í™˜ ê·¸ë£¹"""

        @task
        def transform_orders(**context):
            """ì£¼ë¬¸ ë°ì´í„° ë³€í™˜"""
            # TODO: êµ¬í˜„
            # 1. S3ì—ì„œ raw ë°ì´í„° ì½ê¸°
            # 2. ë°ì´í„° ì •ì œ
            # 3. íŒŒìƒ ì»¬ëŸ¼ ìƒì„±
            # 4. Parquetìœ¼ë¡œ ì €ì¥
            pass

        @task
        def transform_customers(**context):
            """ê³ ê° ë°ì´í„° ë³€í™˜"""
            # TODO: êµ¬í˜„
            pass

        @task
        def enrich_data(orders, customers):
            """ë°ì´í„° í†µí•© ë° ë³´ê°•"""
            # TODO: ì£¼ë¬¸-ê³ ê° ì¡°ì¸
            # RFM ì ìˆ˜ ê³„ì‚°
            pass

        orders = transform_orders()
        customers = transform_customers()
        return enrich_data(orders, customers)

    @task
    def validate_quality(enriched_data):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        # TODO: Great Expectations ë˜ëŠ” ì»¤ìŠ¤í…€ ê²€ì¦
        pass

    @task
    def save_to_datamart(validated_data):
        """DataMartì— ì €ì¥"""
        # TODO: ìµœì¢… ë°ì´í„° ì €ì¥
        pass

    # DAG íë¦„
    transformed = transform_all()
    wait_for_extract >> transformed
    validated = validate_quality(transformed)
    save_to_datamart(validated)


dag = daily_transform()
`,
      solutionCode: `"""
DAG: Daily Transform
Daily Extract ì™„ë£Œ í›„ ë°ì´í„° ë³€í™˜ ìˆ˜í–‰
"""

from datetime import datetime, timedelta
from airflow.decorators import dag, task, task_group
from airflow.sensors.external_task import ExternalTaskSensor
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
import pandas as pd
import json
import logging
from io import BytesIO, StringIO

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
    'retry_exponential_backoff': True,
}


def slack_failure_callback(context):
    """Slack ì‹¤íŒ¨ ì•Œë¦¼"""
    task_instance = context['task_instance']
    dag_id = context['dag'].dag_id

    message = f":red_circle: *Transform ì‹¤íŒ¨* - {dag_id}/{task_instance.task_id}"

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception as e:
        logger.error(f"Slack ì•Œë¦¼ ì‹¤íŒ¨: {e}")


@dag(
    dag_id='daily_transform',
    description='ì¶”ì¶œëœ ë°ì´í„° ë³€í™˜ ë° ì •ì œ',
    schedule=None,  # ExternalTaskSensorë¡œ íŠ¸ë¦¬ê±°
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=['transform', 'daily', 'etl'],
    on_failure_callback=slack_failure_callback,
    dagrun_timeout=timedelta(hours=3),
)
def daily_transform():
    """Daily Transform DAG"""

    # daily_extract ì™„ë£Œ ëŒ€ê¸°
    wait_for_extract = ExternalTaskSensor(
        task_id='wait_for_extract',
        external_dag_id='daily_extract',
        external_task_id='notify_completion',
        mode='reschedule',  # ìŠ¬ë¡¯ ì ìœ  ë°©ì§€
        timeout=7200,
        poke_interval=120,
        allowed_states=['success'],
        failed_states=['failed', 'skipped'],
    )

    @task
    def get_extract_metadata(**context):
        """ì¶”ì¶œ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        from airflow.models import DagRun, TaskInstance

        execution_date = context['logical_date']

        # daily_extractì˜ ë§ˆì§€ë§‰ ì„±ê³µ ì‹¤í–‰ì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        dag_run = DagRun.find(
            dag_id='daily_extract',
            state='success',
            execution_date=execution_date
        )

        if dag_run:
            ti = TaskInstance(
                task=context['task'],
                run_id=dag_run[0].run_id
            )
            # XComì—ì„œ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            metadata = ti.xcom_pull(
                dag_id='daily_extract',
                task_ids='notify_completion'
            )
            return metadata

        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ë°˜í™˜
        target_date = (execution_date - timedelta(days=1)).strftime('%Y-%m-%d')
        return {'target_date': target_date, 'sources': []}

    @task_group(group_id='transform_data')
    def transform_all(metadata: dict):
        """ë°ì´í„° ë³€í™˜ ê·¸ë£¹"""

        @task
        def transform_orders(metadata: dict, **context):
            """ì£¼ë¬¸ ë°ì´í„° ë³€í™˜"""
            target_date = metadata.get('target_date',
                (context['logical_date'] - timedelta(days=1)).strftime('%Y-%m-%d'))

            logger.info(f"Transforming orders for {target_date}")

            s3_hook = S3Hook(aws_conn_id='aws_default')

            # Raw ë°ì´í„° ì½ê¸°
            raw_key = f"raw/orders/dt={target_date}/orders.csv"

            try:
                csv_content = s3_hook.read_key(
                    key=raw_key,
                    bucket_name='data-lake'
                )
                df = pd.read_csv(StringIO(csv_content))
            except Exception as e:
                logger.warning(f"No orders data found: {e}")
                return {'count': 0, 'path': None}

            logger.info(f"Raw orders: {len(df)} rows")

            # 1. ì¤‘ë³µ ì œê±°
            df = df.drop_duplicates(subset=['order_id'])

            # 2. NULL ì²˜ë¦¬
            df['status'] = df['status'].fillna('unknown')
            df['quantity'] = df['quantity'].fillna(0).astype(int)

            # 3. ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”
            df['order_date'] = pd.to_datetime(df['order_date']).dt.strftime('%Y-%m-%d')
            df['created_at'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%dT%H:%M:%S')

            # 4. ê¸ˆì•¡ ê³„ì‚° ê²€ì¦
            df['calculated_total'] = df['quantity'] * df['unit_price']
            df['amount_mismatch'] = abs(df['total_amount'] - df['calculated_total']) > 0.01

            # 5. íŒŒìƒ ì»¬ëŸ¼ ìƒì„±
            df['order_year'] = pd.to_datetime(df['order_date']).dt.year
            df['order_month'] = pd.to_datetime(df['order_date']).dt.month
            df['order_weekday'] = pd.to_datetime(df['order_date']).dt.dayofweek

            # 6. ì´ìƒì¹˜ í”Œë˜ê·¸
            q1 = df['total_amount'].quantile(0.25)
            q3 = df['total_amount'].quantile(0.75)
            iqr = q3 - q1
            df['is_outlier'] = (df['total_amount'] < q1 - 1.5 * iqr) | (df['total_amount'] > q3 + 1.5 * iqr)

            logger.info(f"Transformed orders: {len(df)} rows, {df['is_outlier'].sum()} outliers")

            # Parquetìœ¼ë¡œ ì €ì¥
            parquet_buffer = BytesIO()
            df.to_parquet(parquet_buffer, index=False, engine='pyarrow')

            dest_key = f"transformed/orders/dt={target_date}/orders.parquet"
            s3_hook.load_bytes(
                bytes_data=parquet_buffer.getvalue(),
                key=dest_key,
                bucket_name='data-lake',
                replace=True
            )

            return {
                'count': len(df),
                'path': f"s3://data-lake/{dest_key}",
                'outliers': int(df['is_outlier'].sum()),
                'mismatches': int(df['amount_mismatch'].sum())
            }

        @task
        def transform_customers(metadata: dict, **context):
            """ê³ ê° ë°ì´í„° ë³€í™˜"""
            target_date = metadata.get('target_date',
                (context['logical_date'] - timedelta(days=1)).strftime('%Y-%m-%d'))

            logger.info(f"Transforming customers for {target_date}")

            s3_hook = S3Hook(aws_conn_id='aws_default')

            # Raw ë°ì´í„° ì½ê¸°
            raw_key = f"raw/customers/dt={target_date}/customers.json"

            try:
                json_content = s3_hook.read_key(
                    key=raw_key,
                    bucket_name='data-lake'
                )
                customers = json.loads(json_content)
                df = pd.DataFrame(customers)
            except Exception as e:
                logger.warning(f"No customers data found: {e}")
                return {'count': 0, 'path': None}

            logger.info(f"Raw customers: {len(df)} rows")

            # 1. ì¤‘ë³µ ì œê±°
            df = df.drop_duplicates(subset=['customer_id'])

            # 2. ì´ë¦„ ì •ê·œí™”
            df['name'] = df['name'].str.strip().str.title()

            # 3. ì´ë©”ì¼ ê²€ì¦
            df['email_valid'] = df['email'].str.contains(
                r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$',
                regex=True,
                na=False
            )

            # 4. ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”
            df['created_at'] = pd.to_datetime(df['created_at']).dt.strftime('%Y-%m-%dT%H:%M:%S')

            # 5. ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ (ê°„ë‹¨í•œ ë²„ì „)
            df['segment'] = pd.cut(
                df.get('total_orders', pd.Series([0] * len(df))),
                bins=[-1, 0, 5, 20, float('inf')],
                labels=['new', 'bronze', 'silver', 'gold']
            )

            logger.info(f"Transformed customers: {len(df)} rows")

            # Parquetìœ¼ë¡œ ì €ì¥
            parquet_buffer = BytesIO()
            df.to_parquet(parquet_buffer, index=False, engine='pyarrow')

            dest_key = f"transformed/customers/dt={target_date}/customers.parquet"
            s3_hook.load_bytes(
                bytes_data=parquet_buffer.getvalue(),
                key=dest_key,
                bucket_name='data-lake',
                replace=True
            )

            return {
                'count': len(df),
                'path': f"s3://data-lake/{dest_key}",
                'invalid_emails': int((~df['email_valid']).sum())
            }

        @task
        def enrich_data(orders_result: dict, customers_result: dict, **context):
            """ë°ì´í„° í†µí•© ë° RFM ì ìˆ˜ ê³„ì‚°"""
            target_date = (context['logical_date'] - timedelta(days=1)).strftime('%Y-%m-%d')

            if not orders_result.get('path') or not customers_result.get('path'):
                logger.warning("No data to enrich")
                return {'count': 0, 'path': None}

            s3_hook = S3Hook(aws_conn_id='aws_default')

            # ë³€í™˜ëœ ë°ì´í„° ì½ê¸°
            orders_content = s3_hook.read_key(
                key=f"transformed/orders/dt={target_date}/orders.parquet",
                bucket_name='data-lake'
            )
            orders_df = pd.read_parquet(BytesIO(orders_content.encode() if isinstance(orders_content, str) else orders_content))

            customers_content = s3_hook.read_key(
                key=f"transformed/customers/dt={target_date}/customers.parquet",
                bucket_name='data-lake'
            )
            customers_df = pd.read_parquet(BytesIO(customers_content.encode() if isinstance(customers_content, str) else customers_content))

            # ì£¼ë¬¸-ê³ ê° ì¡°ì¸
            enriched_df = orders_df.merge(
                customers_df[['customer_id', 'name', 'email', 'segment']],
                on='customer_id',
                how='left'
            )

            # RFM ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ì§‘ê³„
            rfm_df = orders_df.groupby('customer_id').agg({
                'order_date': 'max',  # Recency
                'order_id': 'count',  # Frequency
                'total_amount': 'sum'  # Monetary
            }).reset_index()

            rfm_df.columns = ['customer_id', 'last_order_date', 'order_count', 'total_spent']

            # RFM ì ìˆ˜ (1-5)
            rfm_df['r_score'] = pd.qcut(
                pd.to_datetime(rfm_df['last_order_date']).rank(ascending=True),
                5, labels=[1, 2, 3, 4, 5], duplicates='drop'
            )
            rfm_df['f_score'] = pd.qcut(
                rfm_df['order_count'].rank(ascending=True),
                5, labels=[1, 2, 3, 4, 5], duplicates='drop'
            )
            rfm_df['m_score'] = pd.qcut(
                rfm_df['total_spent'].rank(ascending=True),
                5, labels=[1, 2, 3, 4, 5], duplicates='drop'
            )

            # RFM ì ìˆ˜ ì €ì¥
            rfm_buffer = BytesIO()
            rfm_df.to_parquet(rfm_buffer, index=False, engine='pyarrow')

            rfm_key = f"analytics/rfm/dt={target_date}/rfm_scores.parquet"
            s3_hook.load_bytes(
                bytes_data=rfm_buffer.getvalue(),
                key=rfm_key,
                bucket_name='data-lake',
                replace=True
            )

            # Enriched ë°ì´í„° ì €ì¥
            enriched_buffer = BytesIO()
            enriched_df.to_parquet(enriched_buffer, index=False, engine='pyarrow')

            enriched_key = f"enriched/orders/dt={target_date}/orders_enriched.parquet"
            s3_hook.load_bytes(
                bytes_data=enriched_buffer.getvalue(),
                key=enriched_key,
                bucket_name='data-lake',
                replace=True
            )

            logger.info(f"Enriched {len(enriched_df)} orders with customer data")

            return {
                'count': len(enriched_df),
                'path': f"s3://data-lake/{enriched_key}",
                'rfm_path': f"s3://data-lake/{rfm_key}",
                'unique_customers': len(rfm_df)
            }

        orders = transform_orders(metadata)
        customers = transform_customers(metadata)
        return enrich_data(orders, customers)

    @task
    def validate_quality(enriched_result: dict, **context):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        if not enriched_result.get('path'):
            logger.warning("No data to validate")
            return {'status': 'skipped', 'checks': []}

        target_date = (context['logical_date'] - timedelta(days=1)).strftime('%Y-%m-%d')

        s3_hook = S3Hook(aws_conn_id='aws_default')

        # Enriched ë°ì´í„° ì½ê¸°
        content = s3_hook.read_key(
            key=f"enriched/orders/dt={target_date}/orders_enriched.parquet",
            bucket_name='data-lake'
        )
        df = pd.read_parquet(BytesIO(content.encode() if isinstance(content, str) else content))

        checks = []

        # 1. NULL ì²´í¬
        null_counts = df.isnull().sum()
        critical_nulls = null_counts[['order_id', 'customer_id', 'total_amount']]

        checks.append({
            'name': 'critical_columns_not_null',
            'passed': critical_nulls.sum() == 0,
            'details': critical_nulls.to_dict()
        })

        # 2. ê³ ìœ ì„± ì²´í¬
        duplicate_orders = df['order_id'].duplicated().sum()
        checks.append({
            'name': 'order_id_unique',
            'passed': duplicate_orders == 0,
            'details': {'duplicate_count': int(duplicate_orders)}
        })

        # 3. ë²”ìœ„ ì²´í¬
        negative_amounts = (df['total_amount'] < 0).sum()
        checks.append({
            'name': 'amount_positive',
            'passed': negative_amounts == 0,
            'details': {'negative_count': int(negative_amounts)}
        })

        # 4. ì¡°ì¸ ì„±ê³µë¥ 
        missing_customer_info = df['name'].isnull().sum()
        join_success_rate = (len(df) - missing_customer_info) / len(df) * 100
        checks.append({
            'name': 'customer_join_success',
            'passed': join_success_rate > 95,
            'details': {'success_rate': round(join_success_rate, 2)}
        })

        all_passed = all(c['passed'] for c in checks)

        result = {
            'status': 'passed' if all_passed else 'failed',
            'checks': checks,
            'total_records': len(df),
            'target_date': target_date
        }

        logger.info(f"Quality validation: {result['status']}")

        return result

    @task
    def save_to_datamart(validation_result: dict, **context):
        """DataMartì— ìµœì¢… ë°ì´í„° ì €ì¥"""
        if validation_result['status'] != 'passed':
            failed_checks = [c['name'] for c in validation_result['checks'] if not c['passed']]
            logger.warning(f"Quality checks failed: {failed_checks}")
            # ì—¬ì „íˆ ì €ì¥í•˜ë˜ ê²½ê³  í”Œë˜ê·¸ ì¶”ê°€

        target_date = validation_result.get('target_date',
            (context['logical_date'] - timedelta(days=1)).strftime('%Y-%m-%d'))

        s3_hook = S3Hook(aws_conn_id='aws_default')

        # Enriched â†’ DataMart ë³µì‚¬
        source_key = f"enriched/orders/dt={target_date}/orders_enriched.parquet"
        dest_key = f"datamart/orders/dt={target_date}/orders.parquet"

        try:
            s3_hook.copy_object(
                source_bucket_key=source_key,
                source_bucket_name='data-lake',
                dest_bucket_key=dest_key,
                dest_bucket_name='data-lake'
            )

            logger.info(f"Saved to datamart: {dest_key}")

            return {
                'status': 'success',
                'path': f"s3://data-lake/{dest_key}",
                'records': validation_result['total_records'],
                'quality_status': validation_result['status']
            }
        except Exception as e:
            logger.error(f"Failed to save to datamart: {e}")
            raise

    # DAG íë¦„
    metadata = get_extract_metadata()
    wait_for_extract >> metadata

    transformed = transform_all(metadata)
    validated = validate_quality(transformed)
    save_to_datamart(validated)


dag = daily_transform()
`,
      hints: [
        "ExternalTaskSensorì˜ mode='reschedule'ì€ ëŒ€ê¸° ì¤‘ ì›Œì»¤ ìŠ¬ë¡¯ì„ í•´ì œí•©ë‹ˆë‹¤",
        "pd.read_parquet()ìœ¼ë¡œ Parquet íŒŒì¼ì„ ì§ì ‘ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        "RFM ì ìˆ˜ëŠ” Recency(ìµœê·¼ì„±), Frequency(ë¹ˆë„), Monetary(ê¸ˆì•¡)ì˜ ì•½ìì…ë‹ˆë‹¤",
        "í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨ ì‹œì—ë„ ë°ì´í„°ë¥¼ ì €ì¥í•˜ë˜ í”Œë˜ê·¸ë¡œ í‘œì‹œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w7d5t4',
    title: 'DAG 3: Weekly Report êµ¬í˜„',
    type: 'code',
    duration: 40,
    content: {
      instructions: `## DAG 3: Weekly Report êµ¬í˜„

ë§¤ì£¼ ì›”ìš”ì¼ 09:00ì— ì‹¤í–‰ë˜ì–´ ì£¼ê°„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  Slack/Emailë¡œ ì „ì†¡í•˜ëŠ” DAGë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **ë¦¬í¬íŠ¸ ë‚´ìš©**
   - ì£¼ê°„ ë§¤ì¶œ ìš”ì•½
   - Top 10 ìƒí’ˆ
   - ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¶„ì„
   - ì´ìƒì¹˜ ìš”ì•½

2. **ì¶œë ¥ í˜•ì‹**
   - HTML ë¦¬í¬íŠ¸ (S3 ì €ì¥)
   - Slack ìš”ì•½ ë©”ì‹œì§€
   - Email ì²¨ë¶€ (ì˜µì…˜)

3. **ì‹œê°í™”**
   - matplotlib/plotlyë¡œ ì°¨íŠ¸ ìƒì„±
   - ë¦¬í¬íŠ¸ì— ì´ë¯¸ì§€ í¬í•¨
`,
      starterCode: `"""
DAG: Weekly Report
ë§¤ì£¼ ì›”ìš”ì¼ 09:00ì— ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±
"""

from datetime import datetime, timedelta
from airflow.decorators import dag, task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
import pandas as pd
import logging

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


@dag(
    dag_id='weekly_report',
    description='ì£¼ê°„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±',
    # TODO: ë§¤ì£¼ ì›”ìš”ì¼ 09:00 UTC ìŠ¤ì¼€ì¤„
    schedule=None,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=['report', 'weekly', 'analytics'],
)
def weekly_report():
    """Weekly Report DAG"""

    @task
    def calculate_metrics(**context):
        """ì£¼ê°„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # TODO: ì§€ë‚œ 7ì¼ê°„ì˜ ë°ì´í„° ì§‘ê³„
        # - ì´ ë§¤ì¶œ
        # - ì£¼ë¬¸ ê±´ìˆ˜
        # - ì‹ ê·œ ê³ ê° ìˆ˜
        # - í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡
        pass

    @task
    def generate_charts(metrics: dict):
        """ì°¨íŠ¸ ìƒì„±"""
        # TODO: matplotlib/plotlyë¡œ ì°¨íŠ¸ ìƒì„±
        # - ì¼ë³„ ë§¤ì¶œ ì¶”ì´
        # - ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ ë¹„ì¤‘
        # - Top 10 ìƒí’ˆ
        pass

    @task
    def create_report(metrics: dict, charts: dict):
        """HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        # TODO: Jinja2 í…œí”Œë¦¿ìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
        pass

    @task
    def send_slack_summary(metrics: dict):
        """Slack ìš”ì•½ ì „ì†¡"""
        # TODO: Slack ë©”ì‹œì§€ í¬ë§·íŒ… ë° ì „ì†¡
        pass

    @task
    def save_report(report_html: str, **context):
        """S3ì— ë¦¬í¬íŠ¸ ì €ì¥"""
        # TODO: S3ì— HTML ë¦¬í¬íŠ¸ ì €ì¥
        pass

    # DAG íë¦„
    metrics = calculate_metrics()
    charts = generate_charts(metrics)
    report = create_report(metrics, charts)

    # ë³‘ë ¬ ì‹¤í–‰
    [send_slack_summary(metrics), save_report(report)]


dag = weekly_report()
`,
      solutionCode: `"""
DAG: Weekly Report
ë§¤ì£¼ ì›”ìš”ì¼ 09:00ì— ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±
"""

from datetime import datetime, timedelta
from airflow.decorators import dag, task
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
import pandas as pd
import json
import base64
import logging
from io import BytesIO, StringIO
from jinja2 import Template

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

REPORT_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Weekly Report - {{ week_start }} ~ {{ week_end }}</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        h1 { color: #333; border-bottom: 2px solid #4CAF50; }
        h2 { color: #666; margin-top: 30px; }
        .metric-card {
            display: inline-block;
            padding: 20px;
            margin: 10px;
            background: #f5f5f5;
            border-radius: 8px;
            min-width: 150px;
            text-align: center;
        }
        .metric-value { font-size: 24px; font-weight: bold; color: #4CAF50; }
        .metric-label { color: #666; margin-top: 5px; }
        .trend-up { color: #4CAF50; }
        .trend-down { color: #f44336; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background: #4CAF50; color: white; }
        tr:nth-child(even) { background: #f9f9f9; }
        .chart { margin: 20px 0; text-align: center; }
        .footer { margin-top: 40px; color: #999; font-size: 12px; }
    </style>
</head>
<body>
    <h1>ğŸ“Š ì£¼ê°„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸</h1>
    <p>ê¸°ê°„: {{ week_start }} ~ {{ week_end }}</p>

    <h2>ğŸ“ˆ ì£¼ìš” ì§€í‘œ</h2>
    <div class="metrics">
        <div class="metric-card">
            <div class="metric-value">{{ "{:,.0f}".format(metrics.total_revenue) }}ì›</div>
            <div class="metric-label">ì´ ë§¤ì¶œ</div>
            <div class="{{ 'trend-up' if metrics.revenue_growth > 0 else 'trend-down' }}">
                {{ "+" if metrics.revenue_growth > 0 else "" }}{{ "{:.1f}".format(metrics.revenue_growth) }}%
            </div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{{ "{:,}".format(metrics.total_orders) }}</div>
            <div class="metric-label">ì£¼ë¬¸ ê±´ìˆ˜</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{{ "{:,}".format(metrics.new_customers) }}</div>
            <div class="metric-label">ì‹ ê·œ ê³ ê°</div>
        </div>
        <div class="metric-card">
            <div class="metric-value">{{ "{:,.0f}".format(metrics.avg_order_value) }}ì›</div>
            <div class="metric-label">í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡</div>
        </div>
    </div>

    <h2>ğŸ† Top 10 ìƒí’ˆ</h2>
    <table>
        <tr>
            <th>ìˆœìœ„</th>
            <th>ìƒí’ˆëª…</th>
            <th>íŒë§¤ëŸ‰</th>
            <th>ë§¤ì¶œ</th>
        </tr>
        {% for product in top_products %}
        <tr>
            <td>{{ loop.index }}</td>
            <td>{{ product.name }}</td>
            <td>{{ "{:,}".format(product.quantity) }}</td>
            <td>{{ "{:,.0f}".format(product.revenue) }}ì›</td>
        </tr>
        {% endfor %}
    </table>

    <h2>ğŸ‘¥ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸</h2>
    <table>
        <tr>
            <th>ì„¸ê·¸ë¨¼íŠ¸</th>
            <th>ê³ ê° ìˆ˜</th>
            <th>ë§¤ì¶œ ë¹„ì¤‘</th>
            <th>í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡</th>
        </tr>
        {% for segment in segments %}
        <tr>
            <td>{{ segment.name }}</td>
            <td>{{ "{:,}".format(segment.count) }}</td>
            <td>{{ "{:.1f}".format(segment.revenue_share) }}%</td>
            <td>{{ "{:,.0f}".format(segment.avg_order) }}ì›</td>
        </tr>
        {% endfor %}
    </table>

    {% if charts.daily_revenue %}
    <h2>ğŸ“Š ì¼ë³„ ë§¤ì¶œ ì¶”ì´</h2>
    <div class="chart">
        <img src="data:image/png;base64,{{ charts.daily_revenue }}" alt="Daily Revenue Chart">
    </div>
    {% endif %}

    <h2>âš ï¸ ì´ìƒì¹˜ ìš”ì•½</h2>
    <ul>
        <li>ì´ìƒ ì£¼ë¬¸: {{ metrics.outlier_orders }}ê±´</li>
        <li>ê¸ˆì•¡ ë¶ˆì¼ì¹˜: {{ metrics.amount_mismatches }}ê±´</li>
        <li>ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë©”ì¼: {{ metrics.invalid_emails }}ê±´</li>
    </ul>

    <div class="footer">
        <p>ìë™ ìƒì„±ëœ ë¦¬í¬íŠ¸ì…ë‹ˆë‹¤. ë¬¸ì˜: data-team@company.com</p>
        <p>ìƒì„± ì‹œê°„: {{ generated_at }}</p>
    </div>
</body>
</html>
"""


@dag(
    dag_id='weekly_report',
    description='ì£¼ê°„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸ ìƒì„±',
    schedule='0 9 * * 1',  # ë§¤ì£¼ ì›”ìš”ì¼ 09:00 UTC
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=['report', 'weekly', 'analytics'],
    dagrun_timeout=timedelta(hours=1),
)
def weekly_report():
    """Weekly Report DAG"""

    @task
    def calculate_metrics(**context):
        """ì£¼ê°„ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        execution_date = context['logical_date']
        week_end = (execution_date - timedelta(days=1)).strftime('%Y-%m-%d')
        week_start = (execution_date - timedelta(days=7)).strftime('%Y-%m-%d')

        logger.info(f"Calculating metrics for {week_start} ~ {week_end}")

        s3_hook = S3Hook(aws_conn_id='aws_default')

        # ì§€ë‚œ 7ì¼ê°„ ë°ì´í„° ìˆ˜ì§‘
        all_orders = []
        all_rfm = []

        for i in range(7):
            date = (execution_date - timedelta(days=i+1)).strftime('%Y-%m-%d')

            try:
                # Orders ë°ì´í„°
                orders_key = f"datamart/orders/dt={date}/orders.parquet"
                content = s3_hook.read_key(key=orders_key, bucket_name='data-lake')
                df = pd.read_parquet(BytesIO(content if isinstance(content, bytes) else content.encode()))
                all_orders.append(df)

                # RFM ë°ì´í„°
                rfm_key = f"analytics/rfm/dt={date}/rfm_scores.parquet"
                rfm_content = s3_hook.read_key(key=rfm_key, bucket_name='data-lake')
                rfm_df = pd.read_parquet(BytesIO(rfm_content if isinstance(rfm_content, bytes) else rfm_content.encode()))
                all_rfm.append(rfm_df)
            except Exception as e:
                logger.warning(f"No data for {date}: {e}")

        if not all_orders:
            logger.warning("No data found for the week")
            return {
                'total_revenue': 0,
                'total_orders': 0,
                'new_customers': 0,
                'avg_order_value': 0,
                'revenue_growth': 0,
                'outlier_orders': 0,
                'amount_mismatches': 0,
                'invalid_emails': 0,
                'week_start': week_start,
                'week_end': week_end,
                'daily_data': []
            }

        orders_df = pd.concat(all_orders, ignore_index=True)
        rfm_df = pd.concat(all_rfm, ignore_index=True).drop_duplicates(subset=['customer_id'])

        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        total_revenue = orders_df['total_amount'].sum()
        total_orders = len(orders_df)
        unique_customers = orders_df['customer_id'].nunique()
        avg_order_value = total_revenue / total_orders if total_orders > 0 else 0

        # ì‹ ê·œ ê³ ê° (ì´ë²ˆ ì£¼ê°€ ì²« ì£¼ë¬¸ì¸ ê³ ê°)
        # ê°„ë‹¨íˆ êµ¬í˜„: ì´ì „ ë°ì´í„° ì—†ì´ ì´ë²ˆ ì£¼ ë°ì´í„°ë§Œìœ¼ë¡œ ì¶”ì •
        new_customers = int(unique_customers * 0.15)  # ê°€ì •: 15%ê°€ ì‹ ê·œ

        # ì´ìƒì¹˜ ì§‘ê³„
        outlier_orders = orders_df.get('is_outlier', pd.Series([False])).sum()
        amount_mismatches = orders_df.get('amount_mismatch', pd.Series([False])).sum()
        invalid_emails = 0  # customers ë°ì´í„°ì—ì„œ ê³„ì‚°

        # ì¼ë³„ ë§¤ì¶œ ë°ì´í„°
        daily_data = orders_df.groupby('order_date').agg({
            'total_amount': 'sum',
            'order_id': 'count'
        }).reset_index()
        daily_data.columns = ['date', 'revenue', 'orders']

        # ì „ì£¼ ëŒ€ë¹„ ì„±ì¥ë¥  (ê°„ë‹¨íˆ ê°€ì •)
        revenue_growth = 5.2  # ì‹¤ì œë¡œëŠ” ì´ì „ ì£¼ ë°ì´í„°ì™€ ë¹„êµ

        # Top 10 ìƒí’ˆ
        if 'product_id' in orders_df.columns:
            top_products = orders_df.groupby('product_id').agg({
                'quantity': 'sum',
                'total_amount': 'sum'
            }).reset_index()
            top_products.columns = ['product_id', 'quantity', 'revenue']
            top_products = top_products.nlargest(10, 'revenue')
            top_products['name'] = top_products['product_id'].apply(lambda x: f"ìƒí’ˆ {x}")
        else:
            top_products = pd.DataFrame()

        # ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
        if 'segment' in orders_df.columns:
            segments = orders_df.groupby('segment').agg({
                'customer_id': 'nunique',
                'total_amount': ['sum', 'mean']
            }).reset_index()
            segments.columns = ['name', 'count', 'total_revenue', 'avg_order']
            total_seg_revenue = segments['total_revenue'].sum()
            segments['revenue_share'] = segments['total_revenue'] / total_seg_revenue * 100
        else:
            segments = pd.DataFrame({
                'name': ['Gold', 'Silver', 'Bronze', 'New'],
                'count': [100, 300, 500, 200],
                'revenue_share': [40, 30, 20, 10],
                'avg_order': [150000, 80000, 40000, 25000]
            })

        return {
            'total_revenue': float(total_revenue),
            'total_orders': int(total_orders),
            'new_customers': int(new_customers),
            'avg_order_value': float(avg_order_value),
            'revenue_growth': float(revenue_growth),
            'outlier_orders': int(outlier_orders),
            'amount_mismatches': int(amount_mismatches),
            'invalid_emails': int(invalid_emails),
            'week_start': week_start,
            'week_end': week_end,
            'daily_data': daily_data.to_dict('records'),
            'top_products': top_products.to_dict('records') if not top_products.empty else [],
            'segments': segments.to_dict('records')
        }

    @task
    def generate_charts(metrics: dict):
        """ì°¨íŠ¸ ìƒì„±"""
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt

        charts = {}

        # ì¼ë³„ ë§¤ì¶œ ì°¨íŠ¸
        daily_data = metrics.get('daily_data', [])
        if daily_data:
            df = pd.DataFrame(daily_data)

            fig, ax = plt.subplots(figsize=(10, 5))
            ax.bar(df['date'], df['revenue'], color='#4CAF50')
            ax.set_xlabel('ë‚ ì§œ')
            ax.set_ylabel('ë§¤ì¶œ (ì›)')
            ax.set_title('ì¼ë³„ ë§¤ì¶œ ì¶”ì´')
            plt.xticks(rotation=45)
            plt.tight_layout()

            buffer = BytesIO()
            plt.savefig(buffer, format='png', dpi=100)
            buffer.seek(0)
            charts['daily_revenue'] = base64.b64encode(buffer.getvalue()).decode()
            plt.close()

        return charts

    @task
    def create_report(metrics: dict, charts: dict, **context):
        """HTML ë¦¬í¬íŠ¸ ìƒì„±"""
        execution_date = context['logical_date']

        template = Template(REPORT_TEMPLATE)

        # Top 10 ìƒí’ˆ ë°ì´í„° ì¤€ë¹„
        top_products = metrics.get('top_products', [])
        if not top_products:
            top_products = [
                {'name': f'ìƒí’ˆ {i}', 'quantity': 100-i*5, 'revenue': (100-i*5) * 10000}
                for i in range(1, 11)
            ]

        # ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„° ì¤€ë¹„
        segments = metrics.get('segments', [])
        if not segments:
            segments = [
                {'name': 'Gold', 'count': 100, 'revenue_share': 40.0, 'avg_order': 150000},
                {'name': 'Silver', 'count': 300, 'revenue_share': 30.0, 'avg_order': 80000},
                {'name': 'Bronze', 'count': 500, 'revenue_share': 20.0, 'avg_order': 40000},
                {'name': 'New', 'count': 200, 'revenue_share': 10.0, 'avg_order': 25000},
            ]

        # metricsë¥¼ ê°ì²´ì²˜ëŸ¼ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ ë³€í™˜
        class MetricsObj:
            def __init__(self, d):
                for k, v in d.items():
                    setattr(self, k, v)

        html = template.render(
            week_start=metrics['week_start'],
            week_end=metrics['week_end'],
            metrics=MetricsObj(metrics),
            top_products=top_products,
            segments=segments,
            charts=charts,
            generated_at=execution_date.strftime('%Y-%m-%d %H:%M:%S UTC')
        )

        return html

    @task
    def send_slack_summary(metrics: dict, **context):
        """Slack ìš”ì•½ ì „ì†¡"""
        execution_date = context['logical_date']

        growth_emoji = "ğŸ“ˆ" if metrics['revenue_growth'] > 0 else "ğŸ“‰"
        growth_sign = "+" if metrics['revenue_growth'] > 0 else ""

        message = f"""
:chart_with_upwards_trend: *ì£¼ê°„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬í¬íŠ¸*
ğŸ“… ê¸°ê°„: {metrics['week_start']} ~ {metrics['week_end']}

*ğŸ“Š ì£¼ìš” ì§€í‘œ*
â€¢ ì´ ë§¤ì¶œ: {metrics['total_revenue']:,.0f}ì› ({growth_emoji} {growth_sign}{metrics['revenue_growth']:.1f}%)
â€¢ ì£¼ë¬¸ ê±´ìˆ˜: {metrics['total_orders']:,}ê±´
â€¢ ì‹ ê·œ ê³ ê°: {metrics['new_customers']:,}ëª…
â€¢ í‰ê·  ì£¼ë¬¸ ê¸ˆì•¡: {metrics['avg_order_value']:,.0f}ì›

*âš ï¸ ì´ìƒ ì§•í›„*
â€¢ ì´ìƒ ì£¼ë¬¸: {metrics['outlier_orders']}ê±´
â€¢ ê¸ˆì•¡ ë¶ˆì¼ì¹˜: {metrics['amount_mismatches']}ê±´

_ìƒì„¸ ë¦¬í¬íŠ¸: <s3://data-lake/reports/weekly/{metrics['week_end']}/report.html|ë§í¬>_
        """

        try:
            hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
            hook.send(text=message)
            logger.info("Slack summary sent successfully")
            return {'status': 'sent'}
        except Exception as e:
            logger.error(f"Failed to send Slack message: {e}")
            return {'status': 'failed', 'error': str(e)}

    @task
    def save_report(report_html: str, metrics: dict, **context):
        """S3ì— ë¦¬í¬íŠ¸ ì €ì¥"""
        week_end = metrics['week_end']

        s3_hook = S3Hook(aws_conn_id='aws_default')

        # HTML ë¦¬í¬íŠ¸ ì €ì¥
        report_key = f"reports/weekly/{week_end}/report.html"
        s3_hook.load_string(
            string_data=report_html,
            key=report_key,
            bucket_name='data-lake',
            replace=True
        )

        # ë©”íŠ¸ë¦­ JSON ì €ì¥
        metrics_key = f"reports/weekly/{week_end}/metrics.json"
        s3_hook.load_string(
            string_data=json.dumps(metrics, ensure_ascii=False, indent=2),
            key=metrics_key,
            bucket_name='data-lake',
            replace=True
        )

        logger.info(f"Report saved to s3://data-lake/{report_key}")

        return {
            'report_url': f"s3://data-lake/{report_key}",
            'metrics_url': f"s3://data-lake/{metrics_key}"
        }

    # DAG íë¦„
    metrics = calculate_metrics()
    charts = generate_charts(metrics)
    report = create_report(metrics, charts)

    # ë³‘ë ¬ ì‹¤í–‰: Slack ì „ì†¡ê³¼ S3 ì €ì¥
    send_slack_summary(metrics)
    save_report(report, metrics)


dag = weekly_report()
`,
      hints: [
        "Jinja2 Templateì„ ì‚¬ìš©í•˜ë©´ HTML ë¦¬í¬íŠ¸ë¥¼ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        "matplotlibì˜ savefigë¡œ ì°¨íŠ¸ë¥¼ ì´ë¯¸ì§€ë¡œ ì €ì¥ í›„ base64 ì¸ì½”ë”©í•˜ë©´ HTMLì— í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        "ë³‘ë ¬ ì‹¤í–‰ì€ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ íë¦„ì„ ì •ì˜í•˜ë©´ ë©ë‹ˆë‹¤: [task1, task2]",
        "Slack ë©”ì‹œì§€ì—ì„œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w7d5t5',
    title: 'í…ŒìŠ¤íŠ¸ & CI/CD í†µí•©',
    type: 'reading',
    duration: 25,
    content: {
      markdown: `# DAG í…ŒìŠ¤íŠ¸ & CI/CD í†µí•©

## ğŸ§ª DAG í…ŒìŠ¤íŠ¸ ì „ëµ

### 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

\`\`\`python
# tests/test_daily_extract.py

import pytest
from datetime import datetime
from airflow.models import DagBag
from dags.daily_extract import daily_extract

class TestDailyExtractDAG:
    """Daily Extract DAG í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def dagbag(self):
        """DAG ë¡œë”©"""
        return DagBag(dag_folder='dags/', include_examples=False)

    def test_dag_loaded(self, dagbag):
        """DAGì´ ì •ìƒ ë¡œë”©ë˜ëŠ”ì§€ í™•ì¸"""
        assert 'daily_extract' in dagbag.dags
        assert dagbag.import_errors == {}

    def test_dag_structure(self, dagbag):
        """DAG êµ¬ì¡° ê²€ì¦"""
        dag = dagbag.get_dag('daily_extract')

        # Task ìˆ˜ í™•ì¸
        assert len(dag.tasks) >= 5

        # í•„ìˆ˜ Task ì¡´ì¬ í™•ì¸
        task_ids = [t.task_id for t in dag.tasks]
        assert 'extract_sources.extract_orders' in task_ids
        assert 'validate_extracts' in task_ids
        assert 'notify_completion' in task_ids

    def test_dag_schedule(self, dagbag):
        """ìŠ¤ì¼€ì¤„ ê²€ì¦"""
        dag = dagbag.get_dag('daily_extract')
        assert dag.schedule_interval == '0 2 * * *'

    def test_default_args(self, dagbag):
        """ê¸°ë³¸ ì¸ìˆ˜ ê²€ì¦"""
        dag = dagbag.get_dag('daily_extract')
        assert dag.default_args['owner'] == 'data-team'
        assert dag.default_args['retries'] == 3


class TestExtractFunctions:
    """ì¶”ì¶œ í•¨ìˆ˜ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""

    def test_extract_orders_empty(self, mocker):
        """ì£¼ë¬¸ ì—†ëŠ” ê²½ìš° í…ŒìŠ¤íŠ¸"""
        from dags.daily_extract import extract_orders

        # Mock PostgresHook
        mock_hook = mocker.patch('dags.daily_extract.PostgresHook')
        mock_hook.return_value.get_pandas_df.return_value = pd.DataFrame()

        result = extract_orders()

        assert result['count'] == 0
        assert result['destination'] is None

    def test_validate_extracts_with_warnings(self):
        """ê²½ê³  ë°œìƒ ê²€ì¦"""
        from dags.daily_extract import validate_extracts

        extract_results = [
            {'source': 'orders', 'count': 0, 'destination': None},
            {'source': 'customers', 'count': 50, 'destination': 's3://...'},
        ]

        result = validate_extracts(extract_results)

        assert result['status'] == 'warning'
        assert len(result['warnings']) > 0
\`\`\`

### 2. í†µí•© í…ŒìŠ¤íŠ¸

\`\`\`python
# tests/test_integration.py

import pytest
from airflow.models import DagBag, DagRun, TaskInstance
from airflow.utils.state import State
from datetime import datetime

class TestDAGIntegration:
    """DAG ê°„ í†µí•© í…ŒìŠ¤íŠ¸"""

    def test_extract_transform_dependency(self, dagbag):
        """Extract â†’ Transform ì˜ì¡´ì„± í…ŒìŠ¤íŠ¸"""
        extract_dag = dagbag.get_dag('daily_extract')
        transform_dag = dagbag.get_dag('daily_transform')

        # ExternalTaskSensor í™•ì¸
        sensor_tasks = [
            t for t in transform_dag.tasks
            if 'wait_for_extract' in t.task_id
        ]

        assert len(sensor_tasks) == 1
        sensor = sensor_tasks[0]
        assert sensor.external_dag_id == 'daily_extract'

    def test_end_to_end_flow(self, dagbag):
        """E2E í”Œë¡œìš° í…ŒìŠ¤íŠ¸ (Dry Run)"""
        from airflow.executors.debug_executor import DebugExecutor

        dag = dagbag.get_dag('daily_extract')

        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        dag.test(
            execution_date=datetime(2024, 1, 1),
            executor=DebugExecutor()
        )

        # ëª¨ë“  Taskê°€ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸
        # (ì‹¤ì œë¡œëŠ” Mock í•„ìš”)
\`\`\`

### 3. ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸

\`\`\`python
# tests/test_data_quality.py

import great_expectations as gx
import pandas as pd

def test_orders_schema():
    """ì£¼ë¬¸ ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
    context = gx.get_context()

    validator = context.sources.pandas_default.read_parquet(
        "s3://data-lake/datamart/orders/dt=2024-01-01/orders.parquet"
    )

    # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬
    validator.expect_column_to_exist("order_id")
    validator.expect_column_to_exist("customer_id")
    validator.expect_column_to_exist("total_amount")

    # ë°ì´í„° íƒ€ì…
    validator.expect_column_values_to_be_of_type("order_id", "int64")
    validator.expect_column_values_to_be_of_type("total_amount", "float64")

    # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™
    validator.expect_column_values_to_be_between(
        "total_amount", min_value=0, max_value=10000000
    )
    validator.expect_column_values_to_be_unique("order_id")

    results = validator.validate()
    assert results.success
\`\`\`

## ğŸš€ CI/CD íŒŒì´í”„ë¼ì¸

### GitHub Actions ì›Œí¬í”Œë¡œìš°

\`\`\`yaml
# .github/workflows/airflow-ci.yml

name: Airflow CI/CD

on:
  push:
    branches: [main, develop]
    paths:
      - 'dags/**'
      - 'plugins/**'
      - 'tests/**'
  pull_request:
    branches: [main]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install ruff black mypy
          pip install apache-airflow==2.8.0

      - name: Run Ruff (linter)
        run: ruff check dags/ plugins/

      - name: Run Black (formatter)
        run: black --check dags/ plugins/

      - name: Run mypy (type check)
        run: mypy dags/ plugins/

  test:
    runs-on: ubuntu-latest
    needs: lint

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: airflow
          POSTGRES_USER: airflow
          POSTGRES_DB: airflow
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-mock

      - name: Initialize Airflow DB
        env:
          AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@localhost:5432/airflow
        run: airflow db init

      - name: Run tests
        env:
          AIRFLOW_HOME: \${{ github.workspace }}
        run: |
          pytest tests/ -v --cov=dags --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml

  dag-validation:
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v3

      - name: Validate DAGs
        run: |
          pip install apache-airflow==2.8.0
          python -c "
          from airflow.models import DagBag
          dagbag = DagBag(dag_folder='dags/')
          if dagbag.import_errors:
              for dag, error in dagbag.import_errors.items():
                  print(f'DAG {dag}: {error}')
              exit(1)
          print(f'All {len(dagbag.dags)} DAGs validated successfully!')
          "

  deploy-staging:
    runs-on: ubuntu-latest
    needs: dag-validation
    if: github.ref == 'refs/heads/develop'
    steps:
      - uses: actions/checkout@v3

      - name: Deploy to Staging
        env:
          GCS_BUCKET: airflow-staging-dags
        run: |
          # GCSì— DAG ì—…ë¡œë“œ (Cloud Composer)
          gsutil -m rsync -r -d dags/ gs://$GCS_BUCKET/dags/

  deploy-production:
    runs-on: ubuntu-latest
    needs: dag-validation
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - uses: actions/checkout@v3

      - name: Deploy to Production
        env:
          GCS_BUCKET: airflow-production-dags
        run: |
          gsutil -m rsync -r -d dags/ gs://$GCS_BUCKET/dags/
\`\`\`

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

### Prometheus + Grafana ì„¤ì •

\`\`\`yaml
# prometheus/airflow_alerts.yml

groups:
  - name: airflow_alerts
    rules:
      - alert: DAGRunFailed
        expr: airflow_dag_run_state{state="failed"} > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "DAG {{ $labels.dag_id }} ì‹¤í–‰ ì‹¤íŒ¨"

      - alert: TaskDurationExceeded
        expr: airflow_task_duration_seconds > 3600
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Taskê°€ 1ì‹œê°„ ì´ìƒ ì‹¤í–‰ ì¤‘"

      - alert: SchedulerHeartbeat
        expr: time() - airflow_scheduler_heartbeat > 60
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Airflow Scheduler ì‘ë‹µ ì—†ìŒ"
\`\`\`

## ğŸ“ ìµœì¢… í”„ë¡œì íŠ¸ êµ¬ì¡°

\`\`\`
week7_project/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ airflow-ci.yml
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ daily_extract.py
â”‚   â”œâ”€â”€ daily_transform.py
â”‚   â””â”€â”€ weekly_report.py
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â””â”€â”€ custom_operators.py
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ api_hook.py
â”‚   â””â”€â”€ callbacks/
â”‚       â””â”€â”€ slack_callbacks.py
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â”œâ”€â”€ extract_orders.sql
â”‚   â”‚   â””â”€â”€ transform_data.sql
â”‚   â””â”€â”€ templates/
â”‚       â””â”€â”€ report_template.html
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_daily_extract.py
â”‚   â”œâ”€â”€ test_daily_transform.py
â”‚   â”œâ”€â”€ test_weekly_report.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pytest.ini
â””â”€â”€ README.md
\`\`\`

## âœ… ì œì¶œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] 3ê°œ DAG êµ¬í˜„ ì™„ë£Œ (daily_extract, daily_transform, weekly_report)
- [ ] TaskFlow API ì‚¬ìš©
- [ ] XComìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì „ë‹¬
- [ ] ì¬ì‹œë„ ì •ì±… ì„¤ì • (exponential backoff)
- [ ] Slack ì•Œë¦¼ í†µí•©
- [ ] SLA ì„¤ì •
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± (ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ)
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ ì„¤ì •
- [ ] README.md ì‘ì„±
- [ ] Docker Composeë¡œ ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥
`,
      externalLinks: [
        { title: 'Airflow Testing Best Practices', url: 'https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#testing-a-dag' },
        { title: 'Great Expectations', url: 'https://docs.greatexpectations.io/' },
        { title: 'GitHub Actions for Airflow', url: 'https://github.com/astronomer/airflow-guide' }
      ]
    }
  },
  {
    id: 'w7d5t6',
    title: 'ìµœì¢… í”„ë¡œì íŠ¸ í€´ì¦ˆ',
    type: 'quiz',
    duration: 15,
    content: {
      questions: [
        {
          question: 'TaskFlow APIì—ì„œ @task ë°ì½”ë ˆì´í„°ë¡œ ì •ì˜ëœ í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì€ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ë‚˜ìš”?',
          options: [
            'ìë™ìœ¼ë¡œ XComì— ì €ì¥ë˜ì–´ ë‹¤ìŒ Taskì—ì„œ ì ‘ê·¼ ê°€ëŠ¥',
            'ë¡œì»¬ ë³€ìˆ˜ë¡œë§Œ ìœ ì§€ë˜ì–´ ë‹¤ë¥¸ Taskì—ì„œ ì ‘ê·¼ ë¶ˆê°€',
            'ë³„ë„ì˜ push_xcom() í˜¸ì¶œì´ í•„ìš”',
            'Airflow Variableì— ì €ì¥ë¨'
          ],
          answer: 0,
          explanation: 'TaskFlow APIì—ì„œ @task í•¨ìˆ˜ì˜ ë°˜í™˜ê°’ì€ ìë™ìœ¼ë¡œ XComì— í‘¸ì‹œë©ë‹ˆë‹¤. ë‹¤ìŒ Taskì—ì„œ í•¨ìˆ˜ í˜¸ì¶œ í˜•íƒœë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ê°’ì„ ì „ë‹¬ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        },
        {
          question: 'ExternalTaskSensorì˜ mode ì˜µì…˜ ì¤‘ ì›Œì»¤ ìŠ¬ë¡¯ì„ ì ìœ í•˜ì§€ ì•ŠëŠ” ê²ƒì€?',
          options: [
            'poke',
            'reschedule',
            'block',
            'wait'
          ],
          answer: 1,
          explanation: 'reschedule ëª¨ë“œëŠ” ì„¼ì„œê°€ ëŒ€ê¸°í•  ë•Œ ì›Œì»¤ ìŠ¬ë¡¯ì„ í•´ì œí•˜ê³  ìŠ¤ì¼€ì¤„ëŸ¬ì—ê²Œ ë‹¤ì‹œ ì˜ˆì•½ì„ ìš”ì²­í•©ë‹ˆë‹¤. poke ëª¨ë“œëŠ” ëŒ€ê¸° ì¤‘ì—ë„ ìŠ¬ë¡¯ì„ ì ìœ í•©ë‹ˆë‹¤.'
        },
        {
          question: 'DAG í…ŒìŠ¤íŠ¸ì—ì„œ dagbag.import_errorsê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ì´ìœ ëŠ”?',
          options: [
            'DAGì˜ Task ìˆ˜ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´',
            'DAG ë¬¸ë²• ì˜¤ë¥˜ë‚˜ import ì‹¤íŒ¨ë¥¼ ê°ì§€í•˜ê¸° ìœ„í•´',
            'DAG ì‹¤í–‰ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸° ìœ„í•´',
            'DAG ìŠ¤ì¼€ì¤„ì„ ê²€ì¦í•˜ê¸° ìœ„í•´'
          ],
          answer: 1,
          explanation: 'import_errorsëŠ” DAG íŒŒì¼ì„ íŒŒì‹±í•  ë•Œ ë°œìƒí•œ ì˜¤ë¥˜ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. CI/CDì—ì„œ ì´ë¥¼ í™•ì¸í•˜ë©´ ë¬¸ë²• ì˜¤ë¥˜ë‚˜ ì˜ì¡´ì„± ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        },
        {
          question: 'Jinja2 í…œí”Œë¦¿ì—ì„œ ìˆ«ìë¥¼ ì²œ ë‹¨ìœ„ êµ¬ë¶„ìë¡œ í¬ë§·íŒ…í•˜ëŠ” ë°©ë²•ì€?',
          options: [
            '{{ number | format }}',
            '{{ "{:,}".format(number) }}',
            '{{ number.toLocaleString() }}',
            '{{ number | comma }}'
          ],
          answer: 1,
          explanation: 'Pythonì˜ ë¬¸ìì—´ í¬ë§·íŒ…ì„ Jinja2ì—ì„œ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "{:,}".format(number)ëŠ” ìˆ«ìì— ì²œ ë‹¨ìœ„ êµ¬ë¶„ìë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.'
        },
        {
          question: 'CI/CD íŒŒì´í”„ë¼ì¸ì—ì„œ DAG validationì„ í…ŒìŠ¤íŠ¸ í›„ì— ì‹¤í–‰í•˜ëŠ” ì´ìœ ëŠ”?',
          options: [
            'DAG ê²€ì¦ì´ í…ŒìŠ¤íŠ¸ë³´ë‹¤ ë¹ ë¥´ê¸° ë•Œë¬¸ì—',
            'í…ŒìŠ¤íŠ¸ í†µê³¼ í›„ì—ë§Œ DAG êµ¬ì¡°ë¥¼ ê²€ì¦í•  ê°€ì¹˜ê°€ ìˆê¸° ë•Œë¬¸ì—',
            'DAG ê²€ì¦ì—ëŠ” í…ŒìŠ¤íŠ¸ í™˜ê²½ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì—',
            'GitHub Actionsì˜ ì œí•œ ë•Œë¬¸ì—'
          ],
          answer: 1,
          explanation: 'í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í•˜ë©´ ë°°í¬í•  ì˜ë¯¸ê°€ ì—†ìœ¼ë¯€ë¡œ, í…ŒìŠ¤íŠ¸ í†µê³¼ í›„ì— DAG ê²€ì¦ì„ í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¶ˆí•„ìš”í•œ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        }
      ]
    }
  }
]
