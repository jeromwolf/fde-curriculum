// Week 7 Day 1: Airflow ê¸°ì´ˆ & ì•„í‚¤í…ì²˜
import type { Task } from '../../types'

export const day1Tasks: Task[] = [
  {
    id: 'w7d1t1',
    title: 'Apache Airflow ì†Œê°œ',
    type: 'video',
    duration: 25,
    content: {
      videoUrl: '',
      transcript: `
# Apache Airflow ì†Œê°œ

## 1. Airflowë€?

Apache AirflowëŠ” **ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** í”Œë«í¼ì…ë‹ˆë‹¤.
ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ì‘ì„±, ìŠ¤ì¼€ì¤„ë§, ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### íƒ„ìƒ ë°°ê²½
- 2014ë…„ Airbnbì—ì„œ ê°œë°œ
- 2016ë…„ Apache ì¸íë² ì´í„°
- 2019ë…„ Apache Top-Level Project
- í˜„ì¬: ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ í‘œì¤€ ë„êµ¬

## 2. ì™œ Airflowì¸ê°€?

### Cronì˜ í•œê³„
\`\`\`bash
# crontab - ë‹¨ìˆœ ìŠ¤ì¼€ì¤„ë§ë§Œ ê°€ëŠ¥
0 6 * * * python /scripts/extract.py
30 6 * * * python /scripts/transform.py  # extract ì™„ë£Œ ë³´ì¥?
0 7 * * * python /scripts/load.py        # transform ì™„ë£Œ ë³´ì¥?
\`\`\`

### Airflowì˜ í•´ê²°ì±…
\`\`\`python
# DAG - ì˜ì¡´ì„± ëª…ì‹œì  ê´€ë¦¬
extract >> transform >> load
# extract ì™„ë£Œ í›„ transform ì‹¤í–‰
# transform ì™„ë£Œ í›„ load ì‹¤í–‰
\`\`\`

## 3. í•µì‹¬ ê°œë…

### DAG (Directed Acyclic Graph)
\`\`\`
ë°©í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„
- ë°©í–¥ì„±: ì‘ì—… ìˆœì„œê°€ ìˆìŒ
- ë¹„ìˆœí™˜: ìˆœí™˜ ì˜ì¡´ì„± ì—†ìŒ

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Extract â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚Transformâ”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  Load   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Task
- DAG ë‚´ì˜ ê°œë³„ ì‘ì—… ë‹¨ìœ„
- Operatorë¡œ ì •ì˜

### Operator
| Operator | ìš©ë„ |
|----------|------|
| PythonOperator | Python í•¨ìˆ˜ ì‹¤í–‰ |
| BashOperator | Shell ëª…ë ¹ ì‹¤í–‰ |
| PostgresOperator | SQL ì¿¼ë¦¬ ì‹¤í–‰ |
| S3Operator | AWS S3 ì‘ì—… |
| EmailOperator | ì´ë©”ì¼ ë°œì†¡ |

### Sensor
- ì¡°ê±´ì„ ê¸°ë‹¤ë¦¬ëŠ” íŠ¹ìˆ˜ Operator
- FileSensor: íŒŒì¼ ì¡´ì¬ ëŒ€ê¸°
- S3KeySensor: S3 ê°ì²´ ëŒ€ê¸°
- ExternalTaskSensor: ë‹¤ë¥¸ DAG ì™„ë£Œ ëŒ€ê¸°

## 4. Airflow ì•„í‚¤í…ì²˜

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Web Server                            â”‚
â”‚                    (UI, API, ëª¨ë‹ˆí„°ë§)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Scheduler                             â”‚
â”‚              (DAG íŒŒì‹±, íƒœìŠ¤í¬ ìŠ¤ì¼€ì¤„ë§)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼               â–¼               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Worker 1 â”‚   â”‚ Worker 2 â”‚   â”‚ Worker 3 â”‚
        â”‚ (Executor)â”‚   â”‚ (Executor)â”‚   â”‚ (Executor)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Metadata Database                        â”‚
â”‚              (PostgreSQL, MySQL - ìƒíƒœ ì €ì¥)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Executor ì¢…ë¥˜
| Executor | íŠ¹ì§• | ì‚¬ìš© í™˜ê²½ |
|----------|------|----------|
| SequentialExecutor | ìˆœì°¨ ì‹¤í–‰, ë””ë²„ê¹…ìš© | ê°œë°œ |
| LocalExecutor | ë¡œì»¬ ë³‘ë ¬ ì‹¤í–‰ | ì†Œê·œëª¨ |
| CeleryExecutor | ë¶„ì‚° ì‹¤í–‰ (Celery) | ì¤‘/ëŒ€ê·œëª¨ |
| KubernetesExecutor | K8s Podìœ¼ë¡œ ì‹¤í–‰ | í´ë¼ìš°ë“œ |

## 5. Airflowì˜ ì¥ë‹¨ì 

### ì¥ì 
- âœ… Python ê¸°ë°˜ (ê°œë°œì ì¹œí™”ì )
- âœ… í’ë¶€í•œ UI (ëª¨ë‹ˆí„°ë§, ë””ë²„ê¹…)
- âœ… ë°©ëŒ€í•œ ìƒíƒœê³„ (Provider íŒ¨í‚¤ì§€)
- âœ… ì»¤ë®¤ë‹ˆí‹° ì§€ì›

### ë‹¨ì 
- âŒ í•™ìŠµ ê³¡ì„  ë†’ìŒ
- âŒ ì´ˆê¸° ì„¤ì • ë³µì¡
- âŒ ìŠ¤íŠ¸ë¦¬ë° ë¯¸ì§€ì› (ë°°ì¹˜ ì „ìš©)
- âŒ í…ŒìŠ¤íŠ¸ ì–´ë ¤ì›€
      `,
      objectives: [
        'Airflowì˜ í•„ìš”ì„±ê³¼ ì—­í•  ì´í•´',
        'DAG, Task, Operator ê°œë… íŒŒì•…',
        'Airflow ì•„í‚¤í…ì²˜ ì»´í¬ë„ŒíŠ¸ ì´í•´',
        'Executor ì¢…ë¥˜ë³„ íŠ¹ì§• íŒŒì•…'
      ],
      keyPoints: [
        'AirflowëŠ” ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬',
        'DAGë¡œ ì‘ì—… ê°„ ì˜ì¡´ì„± ì •ì˜',
        'Schedulerê°€ íƒœìŠ¤í¬ ì‹¤í–‰ ê´€ë¦¬'
      ]
    }
  },
  {
    id: 'w7d1t2',
    title: 'Airflow í™˜ê²½ ì„¤ì •',
    type: 'reading',
    duration: 25,
    content: {
      markdown: `
# Airflow í™˜ê²½ ì„¤ì •

## 1. ì„¤ì¹˜ ë°©ë²•

### ë°©ë²• 1: pip ì„¤ì¹˜ (ê°œë°œìš©)
\`\`\`bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv airflow_venv
source airflow_venv/bin/activate

# Airflow ì„¤ì¹˜ (constraint íŒŒì¼ ì‚¬ìš© ê¶Œì¥)
AIRFLOW_VERSION=2.8.0
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-\${AIRFLOW_VERSION}/constraints-\${PYTHON_VERSION}.txt"

pip install "apache-airflow==\${AIRFLOW_VERSION}" --constraint "\${CONSTRAINT_URL}"
\`\`\`

### ë°©ë²• 2: Docker Compose (ê¶Œì¥)
\`\`\`yaml
# docker-compose.yaml
version: '3.8'
x-airflow-common:
  &airflow-common
  image: apache/airflow:2.8.0
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - 8080:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname HOSTNAME_VAR']
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create \\
          --username admin \\
          --firstname Admin \\
          --lastname User \\
          --role Admin \\
          --email admin@example.com \\
          --password admin

volumes:
  postgres-db-volume:
\`\`\`

### ë°©ë²• 3: Astronomer (í”„ë¡œë•ì…˜)
\`\`\`bash
# Astro CLI ì„¤ì¹˜
curl -sSL https://install.astronomer.io | sudo bash

# í”„ë¡œì íŠ¸ ìƒì„±
astro dev init

# ë¡œì»¬ ì‹¤í–‰
astro dev start
\`\`\`

## 2. ë””ë ‰í† ë¦¬ êµ¬ì¡°

\`\`\`
airflow_project/
â”œâ”€â”€ dags/                    # DAG íŒŒì¼ë“¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ daily_etl.py
â”‚   â””â”€â”€ weekly_report.py
â”œâ”€â”€ plugins/                 # ì»¤ìŠ¤í…€ í”ŒëŸ¬ê·¸ì¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ operators/
â”‚   â””â”€â”€ hooks/
â”œâ”€â”€ include/                 # SQL, ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ sql/
â”‚   â””â”€â”€ config/
â”œâ”€â”€ tests/                   # í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ dags/
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
\`\`\`

## 3. ì£¼ìš” ì„¤ì • (airflow.cfg)

\`\`\`ini
[core]
# DAG í´ë”
dags_folder = /opt/airflow/dags

# Executor ì„¤ì •
executor = LocalExecutor

# ë³‘ë ¬ ì‹¤í–‰ ìˆ˜
parallelism = 32
dag_concurrency = 16
max_active_runs_per_dag = 16

[webserver]
# ì›¹ ì„œë²„ í¬íŠ¸
web_server_port = 8080

# ì¸ì¦
authenticate = True
auth_backend = airflow.api.auth.backend.basic_auth

[scheduler]
# DAG íŒŒì‹± ì£¼ê¸°
dag_dir_list_interval = 30

# ìŠ¤ì¼€ì¤„ëŸ¬ í•˜íŠ¸ë¹„íŠ¸
scheduler_heartbeat_sec = 5
\`\`\`

## 4. ì²« ë²ˆì§¸ ì‹¤í–‰

\`\`\`bash
# Docker Compose ì‹œì‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f airflow-scheduler

# ì›¹ UI ì ‘ì†
# http://localhost:8080
# ID: admin / PW: admin
\`\`\`

## 5. ìœ ìš©í•œ CLI ëª…ë ¹ì–´

\`\`\`bash
# DAG ëª©ë¡
airflow dags list

# DAG íŠ¸ë¦¬ê±°
airflow dags trigger my_dag

# íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸
airflow tasks test my_dag my_task 2024-01-01

# DAG íŒŒì‹± í…ŒìŠ¤íŠ¸
airflow dags test my_dag

# ì—°ê²° ëª©ë¡
airflow connections list

# ë³€ìˆ˜ ì„¤ì •
airflow variables set my_var "value"
\`\`\`
      `,
      externalLinks: [
        {
          title: 'Airflow Official Documentation',
          url: 'https://airflow.apache.org/docs/'
        },
        {
          title: 'Airflow Docker Compose',
          url: 'https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html'
        },
        {
          title: 'Astronomer Academy',
          url: 'https://academy.astronomer.io/'
        }
      ]
    }
  },
  {
    id: 'w7d1t3',
    title: 'ì²« ë²ˆì§¸ DAG ì‘ì„±',
    type: 'code',
    duration: 30,
    content: {
      instructions: `
# ì²« ë²ˆì§¸ DAG ì‘ì„±

ê°„ë‹¨í•œ Hello World DAGë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰í•´ë´…ë‹ˆë‹¤.

## ëª©í‘œ
1. DAG ê¸°ë³¸ êµ¬ì¡° ì´í•´
2. Task ì •ì˜ ë°©ë²• í•™ìŠµ
3. ì˜ì¡´ì„± ì„¤ì • ë°©ë²• í•™ìŠµ

## í™˜ê²½
- Docker Composeë¡œ ì‹¤í–‰ëœ Airflow ë˜ëŠ”
- ë¡œì»¬ Airflow (standalone)
      `,
      starterCode: `
# dags/hello_world.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

# TODO 1: default_args ì •ì˜
# - owner: 'data_team'
# - retries: 2
# - retry_delay: 5ë¶„


# TODO 2: DAG ì •ì˜
# - dag_id: 'hello_world'
# - schedule_interval: ë§¤ì¼ 09:00 (cron: '0 9 * * *')
# - start_date: 2024-01-01
# - catchup: False


# TODO 3: Python í•¨ìˆ˜ ì •ì˜
def print_hello():
    print("Hello, Airflow!")
    return "Hello returned"

def print_date(**context):
    # execution_date ì¶œë ¥
    print(f"Execution date: {context['ds']}")


# TODO 4: Task ì •ì˜
# - task1: BashOperatorë¡œ 'echo "Starting..."' ì‹¤í–‰
# - task2: PythonOperatorë¡œ print_hello ì‹¤í–‰
# - task3: PythonOperatorë¡œ print_date ì‹¤í–‰
# - task4: BashOperatorë¡œ 'echo "Done!"' ì‹¤í–‰


# TODO 5: ì˜ì¡´ì„± ì„¤ì •
# task1 >> task2 >> task3 >> task4
`,
      solutionCode: `
# dags/hello_world.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator

# 1. default_args ì •ì˜
default_args = {
    'owner': 'data_team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': False,
    'email_on_retry': False,
}

# 2. DAG ì •ì˜
with DAG(
    dag_id='hello_world',
    default_args=default_args,
    description='My first Airflow DAG',
    schedule_interval='0 9 * * *',  # ë§¤ì¼ 09:00
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['example', 'tutorial'],
) as dag:

    # 3. Python í•¨ìˆ˜ ì •ì˜
    def print_hello():
        print("Hello, Airflow!")
        return "Hello returned"

    def print_date(**context):
        execution_date = context['ds']
        logical_date = context['logical_date']
        print(f"Execution date (ds): {execution_date}")
        print(f"Logical date: {logical_date}")
        return execution_date

    # 4. Task ì •ì˜
    task1 = BashOperator(
        task_id='start',
        bash_command='echo "Starting pipeline at $(date)"',
    )

    task2 = PythonOperator(
        task_id='print_hello',
        python_callable=print_hello,
    )

    task3 = PythonOperator(
        task_id='print_date',
        python_callable=print_date,
        provide_context=True,  # Airflow 2.0+ì—ì„œëŠ” ìë™
    )

    task4 = BashOperator(
        task_id='end',
        bash_command='echo "Pipeline completed successfully!"',
    )

    # 5. ì˜ì¡´ì„± ì„¤ì •
    task1 >> task2 >> task3 >> task4

# í…ŒìŠ¤íŠ¸ ëª…ë ¹ì–´:
# airflow tasks test hello_world start 2024-01-01
# airflow dags test hello_world 2024-01-01
`,
      hints: [
        'with DAG() as dag: ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©',
        'default_argsëŠ” ëª¨ë“  Taskì— ì ìš©ë¨',
        '>> ì—°ì‚°ìë¡œ ì˜ì¡´ì„± ì„¤ì •',
        'provide_context=Trueë¡œ Airflow ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬'
      ]
    }
  },
  {
    id: 'w7d1t4',
    title: 'Airflow UI íƒìƒ‰',
    type: 'reading',
    duration: 15,
    content: {
      markdown: `
# Airflow UI íƒìƒ‰

## 1. ë©”ì¸ í™”ë©´ (DAGs View)

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DAG                    â”‚ Schedule â”‚ Last Run â”‚ Recent Tasks     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â—‹ daily_etl            â”‚ 0 6 * * *â”‚ Success  â”‚ â—â—â—â—â— (5 tasks)  â”‚
â”‚ â— weekly_report        â”‚ 0 9 * * 1â”‚ Running  â”‚ â—â—â—â—‹â—‹ (3/5)      â”‚
â”‚ â—‹ monthly_cleanup      â”‚ 0 0 1 * *â”‚ Failed   â”‚ â—â—â—‹â—‹â—‹ (2/5)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### ì£¼ìš” ê¸°ëŠ¥
- **Toggle**: DAG í™œì„±í™”/ë¹„í™œì„±í™”
- **Trigger**: ìˆ˜ë™ ì‹¤í–‰
- **Refresh**: DAG ìƒˆë¡œê³ ì¹¨
- **Delete**: DAG ì‚­ì œ

## 2. Graph View

DAGì˜ íƒœìŠ¤í¬ ì˜ì¡´ì„±ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

\`\`\`
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ extract â”‚ (success)
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚transformâ”‚ (running)
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
    â”‚  load   â”‚ (queued)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### íƒœìŠ¤í¬ ìƒíƒœ ìƒ‰ìƒ
| ìƒ‰ìƒ | ìƒíƒœ |
|------|------|
| ğŸŸ¢ ë…¹ìƒ‰ | Success |
| ğŸŸ¡ ë…¸ë‘ | Running |
| ğŸ”´ ë¹¨ê°• | Failed |
| ğŸŸ£ ë³´ë¼ | Queued |
| âšª í°ìƒ‰ | No status |
| ğŸŸ  ì£¼í™© | Upstream failed |

## 3. Tree View (Grid View)

ì‹œê°„ì— ë”°ë¥¸ ì‹¤í–‰ ì´ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

\`\`\`
           â”‚ Jan 1 â”‚ Jan 2 â”‚ Jan 3 â”‚ Jan 4 â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
extract    â”‚   â—   â”‚   â—   â”‚   â—   â”‚   â—‹   â”‚
transform  â”‚   â—   â”‚   â—   â”‚   âœ—   â”‚   -   â”‚
load       â”‚   â—   â”‚   â—   â”‚   -   â”‚   -   â”‚

â— Success  âœ— Failed  â—‹ Running  - Skipped
\`\`\`

## 4. Task Instance Details

íƒœìŠ¤í¬ í´ë¦­ ì‹œ ìƒì„¸ ì •ë³´:

\`\`\`
Task Instance: transform
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
State: success
Start: 2024-01-15 06:05:00
End: 2024-01-15 06:08:30
Duration: 3m 30s
Retries: 0

[Log] [Rendered] [XCom] [Clear] [Mark Success]
\`\`\`

### ì£¼ìš” ì•¡ì…˜
- **Log**: ì‹¤í–‰ ë¡œê·¸ í™•ì¸
- **Rendered**: í…œí”Œë¦¿ ë Œë”ë§ ê²°ê³¼
- **XCom**: íƒœìŠ¤í¬ ê°„ ì „ë‹¬ ë°ì´í„°
- **Clear**: íƒœìŠ¤í¬ ì¬ì‹¤í–‰
- **Mark Success/Failed**: ìƒíƒœ ìˆ˜ë™ ë³€ê²½

## 5. Admin ë©”ë‰´

### Connections
ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ê²° ì •ë³´ ê´€ë¦¬

\`\`\`
Connection ID: postgres_default
Connection Type: Postgres
Host: localhost
Schema: airflow
Login: airflow
Password: ****
Port: 5432
\`\`\`

### Variables
ì „ì—­ ë³€ìˆ˜ ê´€ë¦¬

\`\`\`
Key: environment
Value: production

Key: slack_webhook
Value: https://hooks.slack.com/...
\`\`\`

### Pools
ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì •

\`\`\`
Pool: default_pool
Slots: 128
Running: 5
Queued: 0
\`\`\`

## 6. ìœ ìš©í•œ ë‹¨ì¶•í‚¤

| ë‹¨ì¶•í‚¤ | ê¸°ëŠ¥ |
|--------|------|
| t | Task ì„ íƒ |
| g | Graph View |
| l | Tree View |
| c | Calendar View |
| d | Details |
| r | Refresh |
      `,
      externalLinks: [
        {
          title: 'Airflow UI Overview',
          url: 'https://airflow.apache.org/docs/apache-airflow/stable/ui.html'
        }
      ]
    }
  },
  {
    id: 'w7d1t5',
    title: 'Airflow ê¸°ì´ˆ í€´ì¦ˆ',
    type: 'quiz',
    duration: 10,
    content: {
      questions: [
        {
          question: 'DAGì—ì„œ "Acyclic"ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì€?',
          options: [
            'ë¹„ë™ê¸° ì‹¤í–‰',
            'ìˆœí™˜ ì˜ì¡´ì„± ì—†ìŒ',
            'ìë™ ìŠ¤ì¼€ì¤„ë§',
            'ë¶„ì‚° ì‹¤í–‰'
          ],
          answer: 1,
          explanation: 'Acyclicì€ "ë¹„ìˆœí™˜"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. DAGì—ì„œëŠ” Aâ†’Bâ†’Câ†’Aì™€ ê°™ì€ ìˆœí™˜ ì˜ì¡´ì„±ì´ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.'
        },
        {
          question: 'Airflowì—ì„œ ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ê²° ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ê³³ì€?',
          options: [
            'Variables',
            'Connections',
            'Pools',
            'XCom'
          ],
          answer: 1,
          explanation: 'ConnectionsëŠ” ë°ì´í„°ë² ì´ìŠ¤, API, í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ë“± ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ê²° ì •ë³´ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.'
        },
        {
          question: 'Schedulerì˜ ì£¼ìš” ì—­í• ì€?',
          options: [
            'UI ì œê³µ',
            'DAG íŒŒì‹± ë° íƒœìŠ¤í¬ ìŠ¤ì¼€ì¤„ë§',
            'ë°ì´í„° ì €ì¥',
            'ë¡œê·¸ ìˆ˜ì§‘'
          ],
          answer: 1,
          explanation: 'SchedulerëŠ” DAG íŒŒì¼ì„ íŒŒì‹±í•˜ê³ , ìŠ¤ì¼€ì¤„ì— ë”°ë¼ íƒœìŠ¤í¬ë¥¼ ì‹¤í–‰ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤.'
        },
        {
          question: 'CeleryExecutorì˜ ì¥ì ì€?',
          options: [
            'ì„¤ì •ì´ ê°„ë‹¨í•¨',
            'ë¶„ì‚° ì‹¤í–‰ìœ¼ë¡œ í™•ì¥ì„± í™•ë³´',
            'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ìŒ',
            'ë””ë²„ê¹… ìš©ì´'
          ],
          answer: 1,
          explanation: 'CeleryExecutorëŠ” ì—¬ëŸ¬ Worker ë…¸ë“œì—ì„œ íƒœìŠ¤í¬ë¥¼ ë¶„ì‚° ì‹¤í–‰í•˜ì—¬ ëŒ€ê·œëª¨ ì›Œí¬ë¡œë“œë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        },
        {
          question: 'íƒœìŠ¤í¬ ê°„ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì€?',
          options: [
            'Variables',
            'Connections',
            'XCom',
            'Pools'
          ],
          answer: 2,
          explanation: 'XCom(Cross-Communication)ì€ íƒœìŠ¤í¬ ê°„ì— ì‘ì€ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.'
        }
      ]
    }
  },
  {
    id: 'w7d1t6',
    title: 'ë„ì „ê³¼ì œ: ê°„ë‹¨í•œ ETL DAG',
    type: 'challenge',
    duration: 35,
    content: {
      instructions: `
# ë„ì „ê³¼ì œ: ê°„ë‹¨í•œ ETL DAG ì‘ì„±

## ì‹œë‚˜ë¦¬ì˜¤
ë‚ ì”¨ APIì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê°„ë‹¨í•œ ETL íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤.

## ìš”êµ¬ì‚¬í•­

### 1. DAG ì„¤ì •
- dag_id: 'weather_etl'
- schedule: ë§¤ ì‹œê°„ ì •ê° (0 * * * *)
- start_date: ì–´ì œ
- catchup: False

### 2. Tasks
1. **check_api**: API ìƒíƒœ í™•ì¸ (BashOperator + curl)
2. **extract**: ë‚ ì”¨ ë°ì´í„° ì¶”ì¶œ (PythonOperator)
3. **transform**: ë°ì´í„° ë³€í™˜ (PythonOperator)
4. **load**: íŒŒì¼ë¡œ ì €ì¥ (PythonOperator)
5. **notify**: ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥ (BashOperator)

### 3. ì˜ì¡´ì„±
\`\`\`
check_api >> extract >> transform >> load >> notify
\`\`\`

### 4. ì¶”ê°€ ìš”êµ¬ì‚¬í•­
- XComìœ¼ë¡œ ë°ì´í„° ì „ë‹¬
- ì ì ˆí•œ ì—ëŸ¬ ì²˜ë¦¬
- ë¡œê¹… ì¶”ê°€

## íŒíŠ¸
- ì‹¤ì œ API ëŒ€ì‹  ë”ë¯¸ ë°ì´í„° ì‚¬ìš© ê°€ëŠ¥
- context['ti'].xcom_push/pull ì‚¬ìš©
      `,
      starterCode: `
# dags/weather_etl.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import json
import logging

logger = logging.getLogger(__name__)

default_args = {
    'owner': 'data_team',
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
}

# TODO: DAG ì •ì˜


# TODO: extract í•¨ìˆ˜ - ë‚ ì”¨ ë°ì´í„° ì¶”ì¶œ (ë”ë¯¸ ë°ì´í„° ì‚¬ìš©)
def extract_weather(**context):
    pass


# TODO: transform í•¨ìˆ˜ - ë°ì´í„° ë³€í™˜
def transform_weather(**context):
    pass


# TODO: load í•¨ìˆ˜ - íŒŒì¼ ì €ì¥
def load_weather(**context):
    pass


# TODO: Task ì •ì˜ ë° ì˜ì¡´ì„± ì„¤ì •
`,
      requirements: [
        'DAG ê¸°ë³¸ ì„¤ì • ì™„ë£Œ',
        '5ê°œ Task êµ¬í˜„ (check_api, extract, transform, load, notify)',
        'XComìœ¼ë¡œ íƒœìŠ¤í¬ ê°„ ë°ì´í„° ì „ë‹¬',
        'ì ì ˆí•œ ë¡œê¹… ì¶”ê°€'
      ],
      evaluationCriteria: [
        'DAG ì •ìƒ íŒŒì‹± (airflow dags test)',
        'ëª¨ë“  íƒœìŠ¤í¬ ì„±ê³µì  ì‹¤í–‰',
        'XCom ë°ì´í„° ì „ë‹¬ í™•ì¸',
        'ì½”ë“œ ê°€ë…ì„± ë° ì£¼ì„'
      ],
      hints: [
        'context["ti"].xcom_push(key, value)ë¡œ ë°ì´í„° ì €ì¥',
        'context["ti"].xcom_pull(task_ids, key)ë¡œ ë°ì´í„° ì¡°íšŒ',
        'BashOperatorì˜ bash_commandì— curl ì‚¬ìš©',
        'json.dumps/loadsë¡œ ì§ë ¬í™”'
      ]
    }
  }
]
