import { Task } from '../../types'

export const day4Tasks: Task[] = [
  {
    id: 'w8d4t1',
    title: 'í†µí•© DAG & ì˜ì¡´ì„± ê´€ë¦¬',
    type: 'video',
    duration: 25,
    content: {
      videoUrl: '',
      transcript: `# í†µí•© DAG & ì˜ì¡´ì„± ê´€ë¦¬

## ğŸ¯ ì˜¤ëŠ˜ì˜ ëª©í‘œ

ê°œë³„ DAGë“¤ì„ í•˜ë‚˜ì˜ í†µí•© íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì—°ê²°í•˜ê³ , ëª¨ë‹ˆí„°ë§/ì•Œë¦¼ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ“Š ì „ì²´ íŒŒì´í”„ë¼ì¸ íë¦„

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Daily Pipeline (02:00 UTC)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚   Extract   â”‚  02:00 ~ 03:00                                  â”‚
â”‚  â”‚  All Sourcesâ”‚  (PostgreSQL, S3, API)                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼ Dataset Trigger                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚  Transform  â”‚  03:00 ~ 04:30                                  â”‚
â”‚  â”‚   Staging   â”‚  (Raw â†’ Staging)                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚  Transform  â”‚  04:30 ~ 05:30                                  â”‚
â”‚  â”‚  Warehouse  â”‚  (Staging â†’ DW)                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚  Aggregate  â”‚  05:30 ~ 06:00                                  â”‚
â”‚  â”‚   Marts     â”‚  (DW â†’ Data Marts)                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚   Quality   â”‚  06:00 ~ 06:15                                  â”‚
â”‚  â”‚   Checks    â”‚  (Great Expectations)                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â”‚         â”‚                                                        â”‚
â”‚         â–¼                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚   Notify    â”‚  06:15                                          â”‚
â”‚  â”‚  Complete   â”‚  (Slack, Email)                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## ğŸ”§ DAG ì˜ì¡´ì„± íŒ¨í„´

### 1. Dataset ê¸°ë°˜ íŠ¸ë¦¬ê±° (Airflow 2.4+)

\`\`\`python
from airflow import Dataset

# ë°ì´í„°ì…‹ ì •ì˜
raw_users = Dataset("s3://data-lake/raw/users")
stg_users = Dataset("s3://data-lake/staging/stg_users")

# Producer DAG
@dag(...)
def extract_users():
    @task(outlets=[raw_users])  # ì´ ë°ì´í„°ì…‹ì„ ìƒì„±
    def extract():
        ...

# Consumer DAG
@dag(schedule=[raw_users])  # ì´ ë°ì´í„°ì…‹ì´ ì—…ë°ì´íŠ¸ë˜ë©´ ì‹¤í–‰
def transform_users():
    ...
\`\`\`

### 2. TriggerDagRunOperator

\`\`\`python
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

trigger_transform = TriggerDagRunOperator(
    task_id='trigger_transform',
    trigger_dag_id='transform_warehouse',
    wait_for_completion=True,
    poke_interval=60,
)
\`\`\`

### 3. ExternalTaskSensor

\`\`\`python
from airflow.sensors.external_task import ExternalTaskSensor

wait_for_extract = ExternalTaskSensor(
    task_id='wait_for_extract',
    external_dag_id='extract_all_sources',
    external_task_id='send_summary',
    mode='reschedule',
    timeout=7200,
)
\`\`\`

## ğŸ“ˆ SLA (Service Level Agreement) ì„¤ì •

\`\`\`python
@dag(
    dag_id='daily_pipeline',
    sla_miss_callback=notify_sla_miss,
)
def daily_pipeline():

    @task(sla=timedelta(hours=1))  # 1ì‹œê°„ ë‚´ ì™„ë£Œ
    def extract():
        ...

    @task(sla=timedelta(hours=2))  # 2ì‹œê°„ ë‚´ ì™„ë£Œ
    def transform():
        ...
\`\`\`

## ğŸ”” ì•Œë¦¼ ì‹œìŠ¤í…œ

### ì•Œë¦¼ ì±„ë„
- **Slack**: ì‹¤ì‹œê°„ ì•Œë¦¼ (ì‹¤íŒ¨, ê²½ê³ , ì™„ë£Œ)
- **Email**: ì¼ì¼ ìš”ì•½ ë¦¬í¬íŠ¸
- **PagerDuty**: í¬ë¦¬í‹°ì»¬ ì¥ì•  (On-call)

### ì•Œë¦¼ ê·œì¹™
| ìƒí™© | Slack | Email | PagerDuty |
|------|-------|-------|-----------|
| Task ì‹¤íŒ¨ | âœ… | âœ… | âŒ |
| DAG ì‹¤íŒ¨ | âœ… | âœ… | âœ… |
| SLA ë¯¸ìŠ¤ | âœ… | âœ… | âœ… |
| ì™„ë£Œ | âœ… | âŒ | âŒ |
| ë°ì´í„° í’ˆì§ˆ ê²½ê³  | âœ… | âœ… | âŒ |

## ğŸ“ ì˜¤ëŠ˜ êµ¬í˜„í•  íŒŒì¼

\`\`\`
dags/
â”œâ”€â”€ dag_daily_pipeline.py     # í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
â”œâ”€â”€ dag_data_quality.py       # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ notifications.py      # ì•Œë¦¼ ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ metrics.py            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
\`\`\`

ì, ì´ì œ êµ¬í˜„ì„ ì‹œì‘í•©ì‹œë‹¤! ğŸš€
`,
      objectives: [
        'ì—¬ëŸ¬ DAGë¥¼ í†µí•© íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ë°©ë²•ì„ ìµíŒë‹¤',
        'SLA ì„¤ì • ë° ì•Œë¦¼ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•œë‹¤',
        'ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ë¥¼ íŒŒì´í”„ë¼ì¸ì— í†µí•©í•œë‹¤'
      ],
      keyPoints: [
        'Dataset ê¸°ë°˜ íŠ¸ë¦¬ê±°ë¡œ ëŠìŠ¨í•œ ê²°í•© êµ¬í˜„',
        'SLAë¡œ íŒŒì´í”„ë¼ì¸ ì§€ì—° ê°ì§€',
        'ë‹¤ì–‘í•œ ì±„ë„ë¡œ ìƒí™©ë³„ ì•Œë¦¼'
      ]
    }
  },
  {
    id: 'w8d4t2',
    title: 'í†µí•© Daily Pipeline DAG êµ¬í˜„',
    type: 'code',
    duration: 40,
    content: {
      instructions: `## í†µí•© Daily Pipeline DAG êµ¬í˜„

ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í•˜ë‚˜ì˜ DAGë¡œ í†µí•©í•˜ì—¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **dag_daily_pipeline.py**
   - TriggerDagRunOperatorë¡œ í•˜ìœ„ DAG í˜¸ì¶œ
   - SLA ì„¤ì •
   - ì‹¤íŒ¨/ì„±ê³µ ì½œë°±
`,
      starterCode: `# dags/dag_daily_pipeline.py

from datetime import datetime, timedelta
from airflow.decorators import dag, task

@dag(
    dag_id='daily_pipeline',
    schedule='0 2 * * *',
    start_date=datetime(2024, 1, 1),
)
def daily_pipeline():
    """Master DAG that orchestrates the entire daily pipeline."""
    # TODO: êµ¬í˜„
    pass

dag = daily_pipeline()
`,
      solutionCode: `# dags/dag_daily_pipeline.py

from datetime import datetime, timedelta
from typing import Dict, Any

from airflow.decorators import dag, task
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.operators.empty import EmptyOperator
from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
from airflow.utils.trigger_rule import TriggerRule

default_args = {
    'owner': 'data-team',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email': ['data-team@company.com'],
    'email_on_failure': True,
}


def notify_pipeline_start(context):
    """Notify pipeline start."""
    execution_date = context['logical_date']
    message = f"""
:rocket: *Daily Pipeline Started*
:calendar: Date: {execution_date.strftime('%Y-%m-%d')}
:clock1: Time: {datetime.utcnow().strftime('%H:%M:%S')} UTC
    """

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception:
        pass


def notify_pipeline_success(context):
    """Notify pipeline completion."""
    execution_date = context['logical_date']
    duration = context.get('dag_run').get_duration() or 0

    message = f"""
:white_check_mark: *Daily Pipeline Completed*
:calendar: Date: {execution_date.strftime('%Y-%m-%d')}
:stopwatch: Duration: {duration // 60:.0f} minutes
    """

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception:
        pass


def notify_pipeline_failure(context):
    """Notify pipeline failure."""
    execution_date = context['logical_date']
    task_id = context['task_instance'].task_id
    exception = context.get('exception', 'Unknown error')

    message = f"""
:red_circle: *Daily Pipeline FAILED*
:calendar: Date: {execution_date.strftime('%Y-%m-%d')}
:x: Failed Task: {task_id}
:warning: Error: {str(exception)[:200]}

<https://airflow.company.com/dags/daily_pipeline|View DAG>
    """

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception:
        pass


def notify_sla_miss(dag, task_list, blocking_task_list, slas, blocking_tis):
    """Notify SLA miss."""
    message = f"""
:warning: *SLA Miss Alert*
:calendar: DAG: {dag.dag_id}
:clock1: Missed Tasks: {', '.join([t.task_id for t in task_list])}
:rotating_light: This requires immediate attention!
    """

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception:
        pass


@dag(
    dag_id='daily_pipeline',
    description='Master DAG that orchestrates the entire daily ETL pipeline',
    schedule='0 2 * * *',  # ë§¤ì¼ 02:00 UTC
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    on_success_callback=notify_pipeline_success,
    on_failure_callback=notify_pipeline_failure,
    sla_miss_callback=notify_sla_miss,
    tags=['master', 'daily', 'e2e-pipeline'],
    dagrun_timeout=timedelta(hours=6),
    max_active_runs=1,  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
)
def daily_pipeline():
    """Master DAG that orchestrates the entire daily pipeline."""

    # Start marker
    start = EmptyOperator(
        task_id='start',
        on_execute_callback=notify_pipeline_start,
    )

    # 1. Extract all sources
    extract = TriggerDagRunOperator(
        task_id='trigger_extraction',
        trigger_dag_id='extract_all_sources',
        wait_for_completion=True,
        poke_interval=60,
        allowed_states=['success'],
        failed_states=['failed'],
        execution_date="{{ ds }}",
        reset_dag_run=True,
        sla=timedelta(hours=1),
    )

    # 2. Transform to warehouse
    transform = TriggerDagRunOperator(
        task_id='trigger_transformation',
        trigger_dag_id='transform_warehouse',
        wait_for_completion=True,
        poke_interval=60,
        allowed_states=['success'],
        failed_states=['failed'],
        execution_date="{{ ds }}",
        reset_dag_run=True,
        sla=timedelta(hours=3),
    )

    # 3. Data quality checks
    quality_check = TriggerDagRunOperator(
        task_id='trigger_quality_check',
        trigger_dag_id='data_quality_check',
        wait_for_completion=True,
        poke_interval=30,
        allowed_states=['success'],
        failed_states=['failed'],
        execution_date="{{ ds }}",
        reset_dag_run=True,
        sla=timedelta(hours=4),
    )

    # 4. Aggregate to data marts
    aggregate = TriggerDagRunOperator(
        task_id='trigger_aggregation',
        trigger_dag_id='aggregate_marts',
        wait_for_completion=True,
        poke_interval=60,
        allowed_states=['success'],
        failed_states=['failed'],
        execution_date="{{ ds }}",
        reset_dag_run=True,
        sla=timedelta(hours=5),
    )

    # End marker
    end = EmptyOperator(
        task_id='end',
        trigger_rule=TriggerRule.ALL_SUCCESS,
    )

    # Handle partial failure - still send report
    @task(trigger_rule=TriggerRule.ALL_DONE)
    def generate_daily_report(**context):
        """Generate and send daily pipeline report."""
        from airflow.models import DagRun
        import json

        execution_date = context['logical_date']
        dag_run = context['dag_run']

        # Collect status of all triggered DAGs
        triggered_dags = [
            'extract_all_sources',
            'transform_warehouse',
            'data_quality_check',
            'aggregate_marts',
        ]

        report = {
            'date': execution_date.strftime('%Y-%m-%d'),
            'master_dag_status': dag_run.state,
            'duration_minutes': (dag_run.get_duration() or 0) // 60,
            'triggered_dags': {}
        }

        for dag_id in triggered_dags:
            runs = DagRun.find(
                dag_id=dag_id,
                execution_date=execution_date,
            )
            if runs:
                run = runs[0]
                report['triggered_dags'][dag_id] = {
                    'state': run.state,
                    'duration': (run.get_duration() or 0) // 60,
                }
            else:
                report['triggered_dags'][dag_id] = {'state': 'not_run'}

        # Build report message
        dag_statuses = '\\n'.join([
            f"â€¢ {dag_id}: {info.get('state', 'unknown')} ({info.get('duration', 0):.0f} min)"
            for dag_id, info in report['triggered_dags'].items()
        ])

        message = f"""
:bar_chart: *Daily Pipeline Report*
:calendar: Date: {report['date']}
:stopwatch: Total Duration: {report['duration_minutes']} minutes

*DAG Status:*
{dag_statuses}

*Overall:* {':white_check_mark:' if report['master_dag_status'] == 'success' else ':warning:'} {report['master_dag_status'].upper()}
        """

        try:
            hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
            hook.send(text=message)
        except Exception as e:
            print(f"Failed to send report: {e}")

        return report

    report = generate_daily_report()

    # DAG flow
    start >> extract >> transform >> quality_check >> aggregate >> end
    [end, aggregate] >> report  # Report runs after end OR after aggregate


dag = daily_pipeline()


# dags/utils/notifications.py

from typing import Optional, Dict, Any, List
from datetime import datetime

from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook


class NotificationManager:
    """Centralized notification management."""

    def __init__(self, slack_conn_id: str = 'slack_webhook'):
        self.slack_conn_id = slack_conn_id

    def send_slack(
        self,
        message: str,
        channel: Optional[str] = None,
        attachments: Optional[List[Dict]] = None,
    ) -> bool:
        """Send Slack notification."""
        try:
            hook = SlackWebhookHook(slack_webhook_conn_id=self.slack_conn_id)
            hook.send(
                text=message,
                channel=channel,
                attachments=attachments,
            )
            return True
        except Exception as e:
            print(f"Slack notification failed: {e}")
            return False

    def format_error(
        self,
        dag_id: str,
        task_id: str,
        execution_date: datetime,
        error: str,
    ) -> str:
        """Format error message for notifications."""
        return f"""
:red_circle: *Task Failed*
â€¢ DAG: {dag_id}
â€¢ Task: {task_id}
â€¢ Date: {execution_date.strftime('%Y-%m-%d %H:%M')}
â€¢ Error: {error[:500]}
        """

    def format_success(
        self,
        dag_id: str,
        execution_date: datetime,
        metrics: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Format success message."""
        metrics_str = ""
        if metrics:
            metrics_str = "\\n".join([
                f"â€¢ {k}: {v:,}" if isinstance(v, (int, float)) else f"â€¢ {k}: {v}"
                for k, v in metrics.items()
            ])
            metrics_str = f"\\n*Metrics:*\\n{metrics_str}"

        return f"""
:white_check_mark: *Pipeline Complete*
â€¢ DAG: {dag_id}
â€¢ Date: {execution_date.strftime('%Y-%m-%d')}{metrics_str}
        """

    def format_sla_miss(
        self,
        dag_id: str,
        task_id: str,
        expected_time: str,
        actual_time: str,
    ) -> str:
        """Format SLA miss message."""
        return f"""
:warning: *SLA Violation*
â€¢ DAG: {dag_id}
â€¢ Task: {task_id}
â€¢ Expected: {expected_time}
â€¢ Actual: {actual_time}
â€¢ Action Required: Check task performance
        """


# dags/utils/metrics.py

from typing import Dict, Any, Optional
from datetime import datetime
import json

from airflow.models import Variable
import boto3


class MetricsCollector:
    """Collect and store pipeline metrics."""

    def __init__(
        self,
        namespace: str = "E2EPipeline",
        use_cloudwatch: bool = False,
    ):
        self.namespace = namespace
        self.use_cloudwatch = use_cloudwatch
        self._metrics_buffer = []

        if use_cloudwatch:
            self.cloudwatch = boto3.client('cloudwatch')

    def record_metric(
        self,
        name: str,
        value: float,
        unit: str = "Count",
        dimensions: Optional[Dict[str, str]] = None,
    ):
        """Record a single metric."""
        metric = {
            "name": name,
            "value": value,
            "unit": unit,
            "dimensions": dimensions or {},
            "timestamp": datetime.utcnow().isoformat(),
        }

        self._metrics_buffer.append(metric)

        if self.use_cloudwatch:
            self._send_to_cloudwatch(metric)

    def record_duration(
        self,
        task_name: str,
        duration_seconds: float,
        dimensions: Optional[Dict[str, str]] = None,
    ):
        """Record task duration."""
        self.record_metric(
            name=f"{task_name}_duration",
            value=duration_seconds,
            unit="Seconds",
            dimensions=dimensions,
        )

    def record_row_count(
        self,
        table_name: str,
        count: int,
        dimensions: Optional[Dict[str, str]] = None,
    ):
        """Record processed row count."""
        self.record_metric(
            name=f"{table_name}_rows",
            value=count,
            unit="Count",
            dimensions=dimensions,
        )

    def _send_to_cloudwatch(self, metric: Dict[str, Any]):
        """Send metric to CloudWatch."""
        try:
            dimensions = [
                {"Name": k, "Value": v}
                for k, v in metric["dimensions"].items()
            ]

            self.cloudwatch.put_metric_data(
                Namespace=self.namespace,
                MetricData=[
                    {
                        "MetricName": metric["name"],
                        "Value": metric["value"],
                        "Unit": metric["unit"],
                        "Dimensions": dimensions,
                    }
                ]
            )
        except Exception as e:
            print(f"Failed to send metric to CloudWatch: {e}")

    def get_buffer(self) -> list:
        """Get all buffered metrics."""
        return self._metrics_buffer

    def flush_to_s3(self, bucket: str, prefix: str):
        """Flush metrics buffer to S3."""
        if not self._metrics_buffer:
            return

        timestamp = datetime.utcnow().strftime("%Y-%m-%dT%H-%M-%S")
        key = f"{prefix}/{timestamp}/metrics.json"

        s3 = boto3.client('s3')
        s3.put_object(
            Bucket=bucket,
            Key=key,
            Body=json.dumps(self._metrics_buffer),
            ContentType='application/json',
        )

        self._metrics_buffer = []
`,
      hints: [
        "TriggerDagRunOperatorì˜ wait_for_completion=Trueë¡œ ë™ê¸° ì‹¤í–‰í•©ë‹ˆë‹¤",
        "trigger_rule=TriggerRule.ALL_DONEì€ ìƒìœ„ Task ì„±ê³µ/ì‹¤íŒ¨ì™€ ë¬´ê´€í•˜ê²Œ ì‹¤í–‰í•©ë‹ˆë‹¤",
        "max_active_runs=1ë¡œ ë™ì‹œ ì‹¤í–‰ì„ ë°©ì§€í•©ë‹ˆë‹¤",
        "sla íŒŒë¼ë¯¸í„°ë¡œ Task ì™„ë£Œ ì‹œê°„ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w8d4t3',
    title: 'ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ DAG êµ¬í˜„',
    type: 'code',
    duration: 35,
    content: {
      instructions: `## ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ DAG êµ¬í˜„

Great Expectations ë˜ëŠ” ì»¤ìŠ¤í…€ ê²€ì¦ìœ¼ë¡œ ë°ì´í„° í’ˆì§ˆì„ ê²€ì‚¬í•˜ëŠ” DAGë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **dag_data_quality.py**
   - í…Œì´ë¸”ë³„ í’ˆì§ˆ ê·œì¹™ ì •ì˜
   - ê²€ì‚¬ ê²°ê³¼ ë¦¬í¬íŒ…
   - ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì•Œë¦¼
`,
      starterCode: `# dags/dag_data_quality.py

from datetime import datetime, timedelta
from airflow.decorators import dag, task

@dag(
    dag_id='data_quality_check',
    schedule=None,
    start_date=datetime(2024, 1, 1),
)
def data_quality_check():
    """Run data quality checks on warehouse tables."""
    # TODO: êµ¬í˜„
    pass

dag = data_quality_check()
`,
      solutionCode: `# dags/dag_data_quality.py

from datetime import datetime, timedelta
from typing import Dict, Any, List

from airflow.decorators import dag, task, task_group
from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
from airflow.exceptions import AirflowException

default_args = {
    'owner': 'data-team',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}


# Quality check configurations
QUALITY_CHECKS = {
    'dim_user': {
        'not_null': ['user_id', 'email'],
        'unique': ['user_id'],
        'value_range': {
            'lifetime_value': {'min': 0, 'max': 1000000},
        },
        'pattern': {
            'email': r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$',
        },
        'freshness': {'column': 'updated_at', 'max_hours': 48},
    },
    'fact_events': {
        'not_null': ['event_id', 'event_timestamp'],
        'unique': ['event_id'],
        'referential_integrity': {
            'user_sk': {'table': 'dim_user', 'column': 'user_sk', 'allow_unknown': True},
        },
        'value_set': {
            'event_category': ['engagement', 'conversion', 'other'],
        },
    },
    'fact_payments': {
        'not_null': ['payment_id', 'amount_usd'],
        'unique': ['payment_id'],
        'value_range': {
            'amount_usd': {'min': 0, 'max': 100000},
        },
        'business_rule': {
            'successful_has_amount': "is_successful = true AND amount_usd > 0",
        },
    },
}


def notify_quality_failure(context):
    """Notify on quality check failure."""
    task_id = context['task_instance'].task_id
    exception = context.get('exception', 'Unknown error')

    message = f"""
:warning: *Data Quality Check Failed*
â€¢ Check: {task_id}
â€¢ Error: {str(exception)[:300]}
â€¢ Action: Review data quality and fix issues
    """

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception:
        pass


@dag(
    dag_id='data_quality_check',
    description='Run data quality checks on warehouse tables',
    schedule=None,  # Triggered by master DAG
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    on_failure_callback=notify_quality_failure,
    tags=['quality', 'validation', 'e2e-pipeline'],
    dagrun_timeout=timedelta(hours=1),
)
def data_quality_check():
    """Run data quality checks on warehouse tables."""

    @task
    def check_table(table_name: str, checks: Dict[str, Any], **context) -> Dict[str, Any]:
        """Run quality checks on a single table."""
        import sys
        sys.path.insert(0, '/opt/airflow/src')

        from pyspark.sql import SparkSession
        from pyspark.sql import functions as F
        from e2e_pipeline.config import settings

        spark = SparkSession.builder \\
            .appName(f"quality_check_{table_name}") \\
            .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \\
            .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \\
            .getOrCreate()

        results = {
            'table': table_name,
            'timestamp': datetime.utcnow().isoformat(),
            'checks': [],
            'passed': True,
            'total_rows': 0,
        }

        try:
            # Read table
            table_path = f"s3a://{settings.s3_bucket}/warehouse/{table_name}"
            df = spark.read.format("delta").load(table_path)
            results['total_rows'] = df.count()

            # 1. Not Null Checks
            if 'not_null' in checks:
                for column in checks['not_null']:
                    null_count = df.filter(F.col(column).isNull()).count()
                    passed = null_count == 0

                    results['checks'].append({
                        'type': 'not_null',
                        'column': column,
                        'passed': passed,
                        'null_count': null_count,
                        'null_percentage': round(null_count / results['total_rows'] * 100, 2) if results['total_rows'] > 0 else 0,
                    })

                    if not passed:
                        results['passed'] = False

            # 2. Unique Checks
            if 'unique' in checks:
                for column in checks['unique']:
                    total = df.count()
                    distinct = df.select(column).distinct().count()
                    duplicates = total - distinct
                    passed = duplicates == 0

                    results['checks'].append({
                        'type': 'unique',
                        'column': column,
                        'passed': passed,
                        'duplicate_count': duplicates,
                    })

                    if not passed:
                        results['passed'] = False

            # 3. Value Range Checks
            if 'value_range' in checks:
                for column, range_spec in checks['value_range'].items():
                    min_val = range_spec.get('min')
                    max_val = range_spec.get('max')

                    violations = df.filter(
                        (F.col(column) < min_val) | (F.col(column) > max_val)
                    ).count()

                    passed = violations == 0

                    results['checks'].append({
                        'type': 'value_range',
                        'column': column,
                        'passed': passed,
                        'expected_min': min_val,
                        'expected_max': max_val,
                        'violations': violations,
                    })

                    if not passed:
                        results['passed'] = False

            # 4. Value Set Checks
            if 'value_set' in checks:
                for column, allowed_values in checks['value_set'].items():
                    violations = df.filter(
                        ~F.col(column).isin(allowed_values)
                    ).count()

                    passed = violations == 0

                    results['checks'].append({
                        'type': 'value_set',
                        'column': column,
                        'passed': passed,
                        'allowed_values': allowed_values,
                        'violations': violations,
                    })

                    if not passed:
                        results['passed'] = False

            # 5. Freshness Check
            if 'freshness' in checks:
                freshness_spec = checks['freshness']
                column = freshness_spec['column']
                max_hours = freshness_spec['max_hours']

                max_timestamp = df.agg(F.max(column)).collect()[0][0]

                if max_timestamp:
                    from datetime import timezone
                    now = datetime.now(timezone.utc)
                    max_ts = max_timestamp.replace(tzinfo=timezone.utc) if max_timestamp.tzinfo is None else max_timestamp
                    hours_old = (now - max_ts).total_seconds() / 3600
                    passed = hours_old <= max_hours
                else:
                    hours_old = float('inf')
                    passed = False

                results['checks'].append({
                    'type': 'freshness',
                    'column': column,
                    'passed': passed,
                    'max_hours_allowed': max_hours,
                    'hours_old': round(hours_old, 2),
                })

                if not passed:
                    results['passed'] = False

            return results

        finally:
            spark.stop()

    @task
    def aggregate_results(check_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Aggregate quality check results."""
        total_checks = sum(len(r['checks']) for r in check_results)
        passed_checks = sum(
            sum(1 for c in r['checks'] if c['passed'])
            for r in check_results
        )
        failed_checks = total_checks - passed_checks

        tables_passed = sum(1 for r in check_results if r['passed'])
        tables_failed = len(check_results) - tables_passed

        return {
            'timestamp': datetime.utcnow().isoformat(),
            'summary': {
                'total_tables': len(check_results),
                'tables_passed': tables_passed,
                'tables_failed': tables_failed,
                'total_checks': total_checks,
                'checks_passed': passed_checks,
                'checks_failed': failed_checks,
                'success_rate': round(passed_checks / total_checks * 100, 2) if total_checks > 0 else 0,
            },
            'details': check_results,
            'overall_passed': failed_checks == 0,
        }

    @task
    def send_quality_report(aggregated: Dict[str, Any], **context):
        """Send quality check report."""
        summary = aggregated['summary']
        passed = aggregated['overall_passed']

        status_emoji = ':white_check_mark:' if passed else ':x:'

        # Build failed checks detail
        failed_details = []
        for table_result in aggregated['details']:
            if not table_result['passed']:
                failed_checks = [c for c in table_result['checks'] if not c['passed']]
                for check in failed_checks[:3]:  # Limit to 3 per table
                    failed_details.append(
                        f"â€¢ {table_result['table']}.{check.get('column', 'N/A')}: {check['type']} failed"
                    )

        failed_str = '\\n'.join(failed_details[:10]) if failed_details else 'None'

        message = f"""
{status_emoji} *Data Quality Report*
:calendar: Date: {context['logical_date'].strftime('%Y-%m-%d')}

*Summary:*
â€¢ Tables: {summary['tables_passed']}/{summary['total_tables']} passed
â€¢ Checks: {summary['checks_passed']}/{summary['total_checks']} passed
â€¢ Success Rate: {summary['success_rate']}%

*Failed Checks:*
{failed_str}

{'*All checks passed!*' if passed else '*Action Required: Fix data quality issues*'}
        """

        try:
            hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
            hook.send(text=message)
        except Exception as e:
            print(f"Failed to send report: {e}")

        # Fail the task if quality checks failed (optional)
        if not passed and summary['success_rate'] < 90:  # Threshold
            raise AirflowException(
                f"Data quality below threshold: {summary['success_rate']}%"
            )

        return aggregated

    # Run checks for each table
    check_results = [
        check_table(table_name, checks)
        for table_name, checks in QUALITY_CHECKS.items()
    ]

    # Aggregate and report
    aggregated = aggregate_results(check_results)
    send_quality_report(aggregated)


dag = data_quality_check()
`,
      hints: [
        "í’ˆì§ˆ ê·œì¹™ì„ ì„¤ì • ë”•ì…”ë„ˆë¦¬ë¡œ ë¶„ë¦¬í•˜ë©´ ê´€ë¦¬ê°€ ì‰½ìŠµë‹ˆë‹¤",
        "success_rate ì„ê³„ê°’ìœ¼ë¡œ ì „ì²´ ì‹¤íŒ¨ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤",
        "ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ ì—¬ëŸ¬ í…Œì´ë¸” ê²€ì‚¬ë¥¼ ë³‘ë ¬í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        "AirflowExceptionì„ ë°œìƒì‹œì¼œ DAGë¥¼ ì‹¤íŒ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w8d4t4',
    title: 'Day 4 ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í€´ì¦ˆ',
    type: 'quiz',
    duration: 10,
    content: {
      questions: [
        {
          question: 'TriggerDagRunOperatorì˜ wait_for_completion=True ì„¤ì •ì˜ íš¨ê³¼ëŠ”?',
          options: [
            'íŠ¸ë¦¬ê±°ëœ DAGê°€ ì™„ë£Œë  ë•Œê¹Œì§€ í˜„ì¬ Taskê°€ ëŒ€ê¸°í•œë‹¤',
            'íŠ¸ë¦¬ê±°ëœ DAGë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰í•œë‹¤',
            'íŠ¸ë¦¬ê±°ëœ DAGê°€ ì‹¤íŒ¨í•´ë„ í˜„ì¬ DAGëŠ” ê³„ì† ì§„í–‰í•œë‹¤',
            'íŠ¸ë¦¬ê±°ëœ DAGì˜ ê²°ê³¼ë¥¼ XComìœ¼ë¡œ ë°˜í™˜í•œë‹¤'
          ],
          answer: 0,
          explanation: 'wait_for_completion=Trueë¡œ ì„¤ì •í•˜ë©´ TriggerDagRunOperatorê°€ íŠ¸ë¦¬ê±°ëœ DAGê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ DAG ê°„ ìˆœì°¨ ì‹¤í–‰ì„ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        },
        {
          question: 'SLA(Service Level Agreement) Miss Callbackì´ í˜¸ì¶œë˜ëŠ” ì‹œì ì€?',
          options: [
            'Taskê°€ ì‹¤íŒ¨í–ˆì„ ë•Œ',
            'Taskê°€ ì„¤ì •ëœ ì‹œê°„ ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ë•Œ',
            'DAG ì „ì²´ê°€ ì™„ë£Œë˜ì—ˆì„ ë•Œ',
            'Taskê°€ ì¬ì‹œë„í•  ë•Œ'
          ],
          answer: 1,
          explanation: 'SLAëŠ” Taskê°€ ì˜ˆì •ëœ ì‹œê°„(execution_date + sla) ë‚´ì— ì™„ë£Œë˜ì§€ ì•Šìœ¼ë©´ sla_miss_callbackì„ í˜¸ì¶œí•©ë‹ˆë‹¤. Task ì‹¤íŒ¨ì™€ëŠ” ë³„ê°œë¡œ, ì§€ì—°ì„ ê°ì§€í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.'
        },
        {
          question: 'max_active_runs=1 ì„¤ì •ì˜ ëª©ì ì€?',
          options: [
            'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ',
            'ë™ì‹œì— ì—¬ëŸ¬ DAG Runì´ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ë°©ì§€',
            'Task ë³‘ë ¬ ì‹¤í–‰ ìˆ˜ ì œí•œ',
            'Airflow Worker ìˆ˜ ì œí•œ'
          ],
          answer: 1,
          explanation: 'max_active_runs=1ì€ ë™ì¼ DAGì˜ ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ê°€ ë™ì‹œì— ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. ë°ì´í„° íŒŒì´í”„ë¼ì¸ì—ì„œ ë‚ ì§œë³„ ì²˜ë¦¬ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰ë˜ì–´ì•¼ í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.'
        },
        {
          question: 'ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ì—ì„œ Referential Integrity ê²€ì‚¬ì˜ ëª©ì ì€?',
          options: [
            'ì»¬ëŸ¼ ê°’ì´ NULLì´ ì•„ë‹Œì§€ í™•ì¸',
            'Fact í…Œì´ë¸”ì˜ FKê°€ Dimension í…Œì´ë¸”ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸',
            'ê°’ì´ í—ˆìš©ëœ ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸',
            'ë°ì´í„°ê°€ ìµœì‹ ì¸ì§€ í™•ì¸'
          ],
          answer: 1,
          explanation: 'Referential Integrity ê²€ì‚¬ëŠ” Fact í…Œì´ë¸”ì˜ ì™¸ë˜í‚¤(FK)ê°€ ì°¸ì¡°í•˜ëŠ” Dimension í…Œì´ë¸”ì— í•´ë‹¹ ë ˆì½”ë“œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ë°ì´í„° ì •í•©ì„±ì„ ë³´ì¥í•˜ëŠ” ì¤‘ìš”í•œ ê²€ì‚¬ì…ë‹ˆë‹¤.'
        }
      ]
    }
  }
]
