// Day 1: RAG ì•„í‚¤í…ì²˜ ì‹¬í™”
// íƒ€ê²Ÿ: ì¤‘ê¸‰ ê°œë°œì (Python, API ì‚¬ìš© ê²½í—˜ ìˆìŒ)

import type { Day } from '../../types'
import {
  createVideoTask,
  createReadingTask,
  createCodeTask,
  createQuizTask,
  createChallengeTask,
} from './types'

export const day1RagArchitecture: Day = {
  slug: 'rag-architecture',
  title: 'RAG ì•„í‚¤í…ì²˜ ì‹¬í™”',
  totalDuration: 280,
  tasks: [
    // ============================================
    // Task 1: RAGì˜ ì—­ì‚¬ì™€ ë°œì „
    // ============================================
    createVideoTask('w5d1-rag-intro', 'RAGì˜ íƒ„ìƒê³¼ ì§„í™”', 35, {
      introduction: `
## RAGì˜ íƒ„ìƒ ë°°ê²½

2020ë…„ Facebook AI Research(í˜„ Meta AI)ê°€ ë°œí‘œí•œ ë…¼ë¬¸
**"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"**ì—ì„œ RAGê°€ ì²˜ìŒ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.

### ë…¼ë¬¸ì´ í•´ê²°í•˜ë ¤ í–ˆë˜ ë¬¸ì œ

\`\`\`
2020ë…„ ë‹¹ì‹œ LLMì˜ í•œê³„:

1. Parametric Memoryë§Œ ì˜ì¡´
   - ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ì €ì¥ëœ ì§€ì‹ë§Œ ì‚¬ìš©
   - í•™ìŠµ í›„ ì§€ì‹ ì—…ë°ì´íŠ¸ ë¶ˆê°€ëŠ¥
   - ìˆ˜ì‹­ì–µ íŒŒë¼ë¯¸í„°ì—ë„ "ëª¨ë“  ê²ƒ"ì„ ê¸°ì–µí•  ìˆ˜ ì—†ìŒ

2. í™˜ê°(Hallucination) ë¬¸ì œ
   - ëª¨ë¥´ëŠ” ê²ƒë„ ìì‹ ìˆê²Œ ëŒ€ë‹µ
   - ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ì •ë³´ ìƒì„±
   - ì¶œì²˜ ì¶”ì  ë¶ˆê°€ëŠ¥

3. ì§€ì‹ ì—…ë°ì´íŠ¸ì˜ ì–´ë ¤ì›€
   - ìƒˆ ì •ë³´ ë°˜ì˜ = ì „ì²´ ì¬í•™ìŠµ í•„ìš”
   - ë¹„ìš©: ìˆ˜ì²œë§Œ ë‹¬ëŸ¬
   - ì‹œê°„: ìˆ˜ì£¼~ìˆ˜ê°œì›”
\`\`\`

### RAGì˜ í•µì‹¬ ì•„ì´ë””ì–´

**ê¸°ì¡´ LLM (Parametric Memory Only):**

| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
|-----------|------|
| **ì…ë ¥** | ì§ˆë¬¸ |
| **ì²˜ë¦¬** | LLM íŒŒë¼ë¯¸í„° (í•™ìŠµ ì‹œì ì˜ ì§€ì‹ë§Œ ì €ì¥) |
| **ì¶œë ¥** | ë‹µë³€ (íŒŒë¼ë¯¸í„°ì—ì„œ "ì¶”ë¡ ") |

> í•œê³„: í•™ìŠµ í›„ ì§€ì‹ ì—…ë°ì´íŠ¸ ë¶ˆê°€, ì¶œì²˜ ì¶”ì  ë¶ˆê°€

**RAG (Parametric + Non-parametric Memory):**

| ë‹¨ê³„ | êµ¬ì„± ìš”ì†Œ | ì„¤ëª… |
|------|-----------|------|
| 1 | **External Knowledge Base** | ë¬¸ì„œ, DB (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ê°€ëŠ¥!) |
| 2 | **ê²€ìƒ‰ (Retrieval)** | ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰ |
| 3 | **LLM íŒŒë¼ë¯¸í„°** | ì§ˆë¬¸ + ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì…ë ¥ |
| 4 | **ì¶œë ¥** | ë‹µë³€ ("ê·¼ê±° ê¸°ë°˜" ìƒì„±) |

> ì¥ì : ì‹¤ì‹œê°„ ì§€ì‹ ì—…ë°ì´íŠ¸, ì¶œì²˜ ì¶”ì  ê°€ëŠ¥, í™˜ê° ê°ì†Œ

---

## 2020ë…„ë¶€í„° 2025ë…„ê¹Œì§€ì˜ RAG ì§„í™”

RAGëŠ” 3ë‹¨ê³„ë¡œ ì§„í™”í•´ ì™”ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” í° ê·¸ë¦¼ë§Œ íŒŒì•…í•˜ê³ , ê° ê¸°ìˆ ì˜ ìƒì„¸ êµ¬í˜„ì€ ì´í›„ í•™ìŠµì—ì„œ ë‹¤ë£¹ë‹ˆë‹¤.

| Phase | ì‹œê¸° | í•µì‹¬ íŠ¹ì§• | ìƒì„¸ í•™ìŠµ |
|-------|------|----------|-----------|
| **Naive RAG** | 2020-2021 | Query â†’ Retrieve â†’ Generate | Day 1 Task 3 |
| **Advanced RAG** | 2022-2023 | Hybrid Search, Re-ranking, Query Rewriting | Day 3 (ì²­í‚¹ & ê²€ìƒ‰) |
| **Agentic RAG** | 2024-2025 | Self-RAG, CRAG, GraphRAG | Day 4 (LangChain), Week 6 (GraphRAG) |

> **í•™ìŠµ ë¡œë“œë§µ**: ì´ë²ˆ ì£¼(Week 5)ì—ì„œ Naive â†’ Advanced RAGë¥¼ ë§ˆìŠ¤í„°í•˜ê³ , Week 6ì—ì„œ GraphRAGë¥¼ ì‹¬í™” í•™ìŠµí•©ë‹ˆë‹¤.

---

## ì™œ ì§€ê¸ˆ RAGê°€ í•µì‹¬ ê¸°ìˆ ì¸ê°€?

### 1. LLMì˜ í•œê³„ëŠ” êµ¬ì¡°ì 

| í•œê³„ | ì›ì¸ | RAG í•´ê²°ì±… |
|------|------|-----------|
| í™˜ê° | í™•ë¥ ì  ìƒì„± | ë¬¸ì„œ ê¸°ë°˜ ìƒì„±ìœ¼ë¡œ ê·¼ê±° ì œê³µ |
| ì§€ì‹ ì»·ì˜¤í”„ | í•™ìŠµ ì‹œì  ê³ ì • | ì‹¤ì‹œê°„ ë¬¸ì„œ ì—…ë°ì´íŠ¸ |
| ë„ë©”ì¸ ë¶€ì¡± | ì¼ë°˜ í•™ìŠµ ë°ì´í„° | ì „ë¬¸ ë¬¸ì„œ ì¶”ê°€ |
| ë¹„ìš© | Fine-tuning ê³ ë¹„ìš© | ë¬¸ì„œë§Œ ì¶”ê°€í•˜ë©´ ë¨ |

### 2. ì‹¤ì œ ê¸°ì—… ì ìš© ì‚¬ë¡€

| ì„œë¹„ìŠ¤ | RAG í™œìš© | ì‚¬ìš©ì ê²½í—˜ |
|--------|---------|-------------|
| **Notion AI** | ì‚¬ìš©ì ë…¸íŠ¸ ê²€ìƒ‰ | "ë‚´ ë…¸íŠ¸ì—ì„œ í”„ë¡œì íŠ¸ ë§ˆê°ì¼ ì°¾ì•„ì¤˜" |
| **GitHub Copilot** | ë ˆí¬ì§€í† ë¦¬ ì¸ë±ì‹± | "ì´ í”„ë¡œì íŠ¸ì—ì„œ ì¸ì¦ì€ ì–´ë–»ê²Œ ì²˜ë¦¬í•´?" |
| **Perplexity AI** | ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ | ì¶œì²˜ì™€ í•¨ê»˜ ìµœì‹  ì •ë³´ ì œê³µ |
| **Cursor IDE** | ì½”ë“œë² ì´ìŠ¤ RAG | í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì½”ë“œ ìˆ˜ì • |

### 3. FDEê°€ RAGë¥¼ ì•Œì•„ì•¼ í•˜ëŠ” ì´ìœ 

> **FDEì˜ í•µì‹¬ ì—­í• **: "ê³ ê°ì˜ ë°ì´í„°ë¥¼ AIë¡œ ê°€ì¹˜í™”"

| ê³ ê°ì‚¬ê°€ ê°€ì§„ ê²ƒ | RAG ì—°ê²° í›„ ê°€ëŠ¥í•œ ê²ƒ |
|-----------------|---------------------|
| ì‚¬ë‚´ ë¬¸ì„œ (ìœ„í‚¤, ë§¤ë‰´ì–¼) | ìì—°ì–´ Q&A ì‹œìŠ¤í…œ |
| ê³ ê° ë°ì´í„° (FAQ, í‹°ì¼“) | ìë™ ê³ ê° ì‘ëŒ€ |
| ë„ë©”ì¸ ì§€ì‹ (ê·œì •, ì „ë¬¸ ìš©ì–´) | ì „ë¬¸ê°€ ìˆ˜ì¤€ AI ì–´ì‹œìŠ¤í„´íŠ¸ |

**ê²°ë¡ : FDE = RAG ì „ë¬¸ê°€** - RAGëŠ” ê³ ê° ë°ì´í„°ë¥¼ LLMì— ì—°ê²°í•˜ëŠ” í•µì‹¬ ë¸Œë¦¿ì§€ì…ë‹ˆë‹¤.
      `,
      keyPoints: [
        'RAGëŠ” 2020ë…„ Meta AI ë…¼ë¬¸ì—ì„œ íƒ„ìƒ - LLMì˜ ì§€ì‹ í•œê³„ë¥¼ ì™¸ë¶€ ë¬¸ì„œë¡œ ë³´ì™„',
        'RAG ì§„í™”: Naive(Day1) â†’ Advanced(Day3) â†’ Agentic(Day4, Week6)',
        'ì‹¤ì œ ì ìš©: Notion AI, GitHub Copilot, Perplexity, Cursor',
        'FDE = RAG ì „ë¬¸ê°€: ê³ ê° ë°ì´í„°ë¥¼ AIë¡œ ê°€ì¹˜í™”í•˜ëŠ” í•µì‹¬ ê¸°ìˆ ',
      ],
      practiceGoal: 'RAGì˜ í° ê·¸ë¦¼ì„ ì´í•´í•˜ê³ , Week 5-6 í•™ìŠµ ë¡œë“œë§µì„ íŒŒì•…í•œë‹¤',
    }),

    // ============================================
    // Task 2: RAG vs Fine-tuning vs Prompt Engineering
    // ============================================
    createReadingTask('w5d1-rag-comparison', 'RAG vs Fine-tuning: ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí•˜ë‚˜', 40, {
      introduction: `
## ì„¸ ê°€ì§€ LLM ì»¤ìŠ¤í„°ë§ˆì´ì§• ë°©ë²•

LLMì„ íŠ¹ì • ë„ë©”ì¸/ìš©ë„ì— ë§ê²Œ ì¡°ì •í•˜ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.
ê°ê°ì˜ ì‘ë™ ì›ë¦¬, ì¥ë‹¨ì , ì í•©í•œ ìƒí™©ì„ ê¹Šì´ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.

---

## 1. Prompt Engineering

### ì‘ë™ ì›ë¦¬

\`\`\`
ëª¨ë¸ íŒŒë¼ë¯¸í„°: ë³€ê²½ ì—†ìŒ
ì…ë ¥ í”„ë¡¬í”„íŠ¸ì— ì§€ì‹œì‚¬í•­/ì˜ˆì‹œë¥¼ ì¶”ê°€

System Prompt:
"ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
1. ìˆ«ìëŠ” í•­ìƒ ì‰¼í‘œë¡œ êµ¬ë¶„
2. íˆ¬ì ì¡°ì–¸ì€ ë©´ì±… ì¡°í•­ê³¼ í•¨ê»˜
3. í•œêµ­ì–´ë¡œ ë‹µë³€..."

User: "ì‚¼ì„±ì „ì ì£¼ê°€ ì „ë§ì€?"
\`\`\`

### ì¥ë‹¨ì 

| ì¥ì  | ë‹¨ì  |
|------|------|
| ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ | ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ |
| ë¹„ìš© ì—†ìŒ | ë³µì¡í•œ ì§€ì‹ ì „ë‹¬ ì–´ë ¤ì›€ |
| ì‹¤í—˜/ë°˜ë³µ ë¹ ë¦„ | ì¼ê´€ì„± ìœ ì§€ ì–´ë ¤ì›€ |
| ëª¨ë¸ ë³€ê²½ ì‰¬ì›€ | í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ìˆ  í•„ìš” |

### ì í•©í•œ ìƒí™©

\`\`\`
âœ… ì¶œë ¥ í˜•ì‹/ìŠ¤íƒ€ì¼ ì¡°ì •
âœ… ê°„ë‹¨í•œ ê·œì¹™ ì ìš©
âœ… Few-shot ì˜ˆì‹œë¡œ ì¶©ë¶„í•œ ê²½ìš°
âœ… ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

âŒ ëŒ€ëŸ‰ì˜ ë„ë©”ì¸ ì§€ì‹ í•„ìš”
âŒ ìµœì‹  ì •ë³´ í•„ìš”
âŒ ë³µì¡í•œ ì „ë¬¸ ì§€ì‹
\`\`\`

---

## 2. Fine-tuning

### ì‘ë™ ì›ë¦¬

\`\`\`
ëª¨ë¸ íŒŒë¼ë¯¸í„°: ì‹¤ì œë¡œ ìˆ˜ì •ë¨
ë„ë©”ì¸ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ

í•™ìŠµ ë°ì´í„° ì˜ˆì‹œ (JSONL):
{"prompt": "í™˜ìê°€ ë‘í†µì„ í˜¸ì†Œí•©ë‹ˆë‹¤", "completion": "ë‘í†µì˜ ìœ„ì¹˜, ê°•ë„, ì§€ì†ì‹œê°„ì„..."}
{"prompt": "í˜ˆì••ì´ 180/110ì…ë‹ˆë‹¤", "completion": "ê³ í˜ˆì•• ì‘ê¸‰ ìƒí™©ìœ¼ë¡œ..."}

í•™ìŠµ í›„:
ëª¨ë¸ì´ ì˜ë£Œ ë„ë©”ì¸ì˜ "ì–´íˆ¬", "ìš©ì–´", "ì¶”ë¡  íŒ¨í„´"ì„ í•™ìŠµ
\`\`\`

### ì¥ë‹¨ì 

| ì¥ì  | ë‹¨ì  |
|------|------|
| ê¹Šì€ ë„ë©”ì¸ ì´í•´ | í•™ìŠµ ë¹„ìš© ë†’ìŒ ($1K~$100K+) |
| ì¼ê´€ëœ ìŠ¤íƒ€ì¼/ì–´íˆ¬ | í•™ìŠµ ì‹œê°„ í•„ìš” (ì‹œê°„~ì¼) |
| ë³µì¡í•œ ì¶”ë¡  ê°€ëŠ¥ | í•™ìŠµ ë°ì´í„° ì¤€ë¹„ í•„ìš” |
| ì¶”ë¡  ì†ë„ ë™ì¼ | í™˜ê° ì—¬ì „íˆ ë°œìƒ |
| | ì§€ì‹ ì—…ë°ì´íŠ¸ = ì¬í•™ìŠµ |

### ì í•©í•œ ìƒí™©

\`\`\`
âœ… íŠ¹ì • ì–´íˆ¬/ìŠ¤íƒ€ì¼ í•„ìˆ˜ (ê³ ê°ì‚¬ ë¸Œëœë“œ)
âœ… ë³µì¡í•œ ë„ë©”ì¸ ì¶”ë¡  (ë²•ë¥ , ì˜ë£Œ)
âœ… ê³ ì •ëœ ì§€ì‹ ë² ì´ìŠ¤
âœ… ëŒ€ëŸ‰ ë°˜ë³µ ì‚¬ìš©

âŒ ìì£¼ ë³€ê²½ë˜ëŠ” ì •ë³´
âŒ ì¶œì²˜ ì¶”ì  í•„ìš”
âŒ ì˜ˆì‚° ì œí•œ
\`\`\`

### Fine-tuning ìœ í˜•

\`\`\`
1. Full Fine-tuning
   - ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ì •
   - ê°€ì¥ ë¹„ìŒˆ, ê°€ì¥ ê°•ë ¥
   - ëŒ€ê·œëª¨ í•™ìŠµ ë°ì´í„° í•„ìš”

2. LoRA (Low-Rank Adaptation)
   - ì¼ë¶€ ë ˆì´ì–´ë§Œ ìˆ˜ì •
   - ë¹„ìš© 90% ì ˆê°
   - í’ˆì§ˆì€ 80-90% ìˆ˜ì¤€

3. QLoRA (Quantized LoRA)
   - ì–‘ìí™” + LoRA
   - ë” ì €ë ´, ë¡œì»¬ GPU ê°€ëŠ¥
   - ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì— ì¸ê¸°
\`\`\`

---

## 3. RAG (Retrieval-Augmented Generation)

### ì‘ë™ ì›ë¦¬

\`\`\`
ëª¨ë¸ íŒŒë¼ë¯¸í„°: ë³€ê²½ ì—†ìŒ
ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…

1. ë¬¸ì„œ ì¸ë±ì‹± (ì˜¤í”„ë¼ì¸)
   ë¬¸ì„œ â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ë²¡í„° DB ì €ì¥

2. ì¿¼ë¦¬ ì²˜ë¦¬ (ì˜¨ë¼ì¸)
   ì§ˆë¬¸ â†’ ì„ë² ë”© â†’ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ â†’
   í”„ë¡¬í”„íŠ¸ = ì‹œìŠ¤í…œ ì§€ì‹œ + ê²€ìƒ‰ ê²°ê³¼ + ì§ˆë¬¸ â†’
   LLM ë‹µë³€ ìƒì„±
\`\`\`

### ì¥ë‹¨ì 

| ì¥ì  | ë‹¨ì  |
|------|------|
| ì‹¤ì‹œê°„ ì§€ì‹ ì—…ë°ì´íŠ¸ | ê²€ìƒ‰ í’ˆì§ˆì— ì˜ì¡´ |
| ì¶œì²˜ ì¶”ì  ê°€ëŠ¥ | ì§€ì—°ì‹œê°„ ì¦ê°€ |
| í™˜ê° ê°ì†Œ | ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ |
| ë¹„ìš© íš¨ìœ¨ì  | ì²­í‚¹/ê²€ìƒ‰ ìµœì í™” í•„ìš” |
| ë¯¼ê° ë°ì´í„° ì œì–´ | ì„ë² ë”© ë¹„ìš© ë°œìƒ |

### ì í•©í•œ ìƒí™©

\`\`\`
âœ… ìì£¼ ë³€ê²½ë˜ëŠ” ì •ë³´
âœ… ì¶œì²˜ í‘œì‹œ í•„ìˆ˜ (ë²•ë¥ , ì˜ë£Œ, ê¸ˆìœµ)
âœ… ì‚¬ë‚´ ë¬¸ì„œ ê¸°ë°˜ Q&A
âœ… ìµœì‹  ì •ë³´ ë°˜ì˜ í•„ìš”

âŒ íŠ¹ì • ì–´íˆ¬ í•„ìˆ˜
âŒ ë³µì¡í•œ ë„ë©”ì¸ ì¶”ë¡ 
âŒ ì§€ì—°ì‹œê°„ ê·¹ë„ë¡œ ë¯¼ê°
\`\`\`

---

## ê²°ì • í”„ë ˆì„ì›Œí¬

### ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨í•˜ê¸°

\`\`\`
Q1: ì§€ì‹ì´ ìì£¼ ë³€ê²½ë˜ë‚˜ìš”?
    ì˜ˆ â†’ RAG ê³ ë ¤
    ì•„ë‹ˆì˜¤ â†’ Q2ë¡œ

Q2: ì¶œì²˜ ì¶”ì ì´ í•„ìš”í•œê°€ìš”?
    ì˜ˆ â†’ RAG ê³ ë ¤
    ì•„ë‹ˆì˜¤ â†’ Q3ë¡œ

Q3: íŠ¹ì • ìŠ¤íƒ€ì¼/ì–´íˆ¬ê°€ í•„ìˆ˜ì¸ê°€ìš”?
    ì˜ˆ â†’ Fine-tuning ê³ ë ¤
    ì•„ë‹ˆì˜¤ â†’ Q4ë¡œ

Q4: ë³µì¡í•œ ë„ë©”ì¸ ì¶”ë¡ ì´ í•„ìš”í•œê°€ìš”?
    ì˜ˆ â†’ Fine-tuning + RAG ì¡°í•© ê³ ë ¤
    ì•„ë‹ˆì˜¤ â†’ Prompt Engineeringìœ¼ë¡œ ì‹œì‘
\`\`\`

### ì‹¤ì „ ì¡°í•© ì „ëµ

\`\`\`
ë ˆë²¨ 1: Prompt Engineering Only
- ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…
- ê°„ë‹¨í•œ ìš©ë„
- ë¹„ìš©: $0

ë ˆë²¨ 2: RAG
- ì‚¬ë‚´ ë¬¸ì„œ Q&A
- ì§€ì‹ ê¸°ë°˜ ì±—ë´‡
- ë¹„ìš©: ì„ë² ë”© ë¹„ìš©ë§Œ

ë ˆë²¨ 3: Fine-tuning + RAG
- ë„ë©”ì¸ ì–´íˆ¬ + ìµœì‹  ì§€ì‹
- ë²•ë¥ /ì˜ë£Œ/ê¸ˆìœµ ì „ë¬¸ ì„œë¹„ìŠ¤
- ë¹„ìš©: Fine-tuning + ì„ë² ë”©

ë ˆë²¨ 4: Fine-tuning + RAG + Agents
- ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°
- ë‹¤ë‹¨ê³„ ì¶”ë¡ 
- ì—”í„°í”„ë¼ì´ì¦ˆ ì†”ë£¨ì…˜
\`\`\`

---

## ì‹¤ì œ ì‚¬ë¡€: ë²•ë¥  AI ì„œë¹„ìŠ¤

\`\`\`
ìš”êµ¬ì‚¬í•­:
- ë²•ë¥  ìš©ì–´ ì •í™•í•œ ì‚¬ìš©
- ìµœì‹  íŒë¡€ ë°˜ì˜
- ì¶œì²˜ (ì¡°í•­, íŒë¡€ë²ˆí˜¸) í•„ìˆ˜
- ë²•ë¥  ë¬¸ì„œ ìŠ¤íƒ€ì¼ì˜ ë‹µë³€

ì†”ë£¨ì…˜:
1. Fine-tuning
   - ë²•ë¥  ë¬¸ì„œ ìŠ¤íƒ€ì¼ í•™ìŠµ
   - ë²•ë¥  ì¶”ë¡  íŒ¨í„´ í•™ìŠµ

2. RAG
   - ë²•ë ¹ DB ì¸ë±ì‹±
   - íŒë¡€ DB ì¸ë±ì‹±
   - ì‹¤ì‹œê°„ ë²•ë¥  ë‰´ìŠ¤ ì—°ë™

ê²°ê³¼:
Fine-tuned ëª¨ë¸ì´ RAGë¡œ ê²€ìƒ‰ëœ ë²•ë ¹/íŒë¡€ë¥¼
ë²•ë¥  ë¬¸ì„œ ìŠ¤íƒ€ì¼ë¡œ ì •ë¦¬í•˜ì—¬ ë‹µë³€
\`\`\`
      `,
      keyPoints: [
        'Prompt Engineering: ì¦‰ì‹œ ì ìš©, ë¹„ìš© ì—†ìŒ, ë³µì¡í•œ ì§€ì‹ ì „ë‹¬ í•œê³„',
        'Fine-tuning: ê¹Šì€ ë„ë©”ì¸ ì´í•´, ìŠ¤íƒ€ì¼ í•™ìŠµ, ì§€ì‹ ì—…ë°ì´íŠ¸ ì–´ë ¤ì›€',
        'RAG: ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸, ì¶œì²˜ ì¶”ì , ê²€ìƒ‰ í’ˆì§ˆì— ì˜ì¡´',
        'ì‹¤ì „ì—ì„œëŠ” ì¡°í•© ì „ëµ (Fine-tuning + RAG)ì´ ìµœì ì¸ ê²½ìš° ë§ìŒ',
      ],
      practiceGoal: 'ì„¸ ê°€ì§€ ë°©ë²•ì˜ ì‘ë™ ì›ë¦¬ì™€ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì´í•´í•˜ê³ , ìƒí™©ì— ë§ëŠ” ì„ íƒ ê¸°ì¤€ì„ ì„¸ìš´ë‹¤',
    }),

    // ============================================
    // Task 3: í”„ë¡œë•ì…˜ RAG ì•„í‚¤í…ì²˜ íŒ¨í„´
    // ============================================
    createReadingTask('w5d1-production-patterns', 'í”„ë¡œë•ì…˜ RAG ì•„í‚¤í…ì²˜ ì„¤ê³„', 45, {
      introduction: `
## í”„ë¡œë•ì…˜ RAGì˜ í˜„ì‹¤

ê°œë°œ í™˜ê²½ì˜ RAGì™€ í”„ë¡œë•ì…˜ RAGëŠ” ë‹¤ë¦…ë‹ˆë‹¤.

\`\`\`
ê°œë°œ í™˜ê²½:
- ë¬¸ì„œ 100ê°œ
- ë™ì‹œ ì‚¬ìš©ì 1ëª…
- ì§€ì—°ì‹œê°„ ë¬´ê´€
- ë¹„ìš© ë¬´ê´€

í”„ë¡œë•ì…˜:
- ë¬¸ì„œ 100ë§Œê°œ+
- ë™ì‹œ ì‚¬ìš©ì 1000ëª…+
- ì‘ë‹µ 2ì´ˆ ì´ë‚´
- ì›” $10K ì´ë‚´
\`\`\`

---

## í”„ë¡œë•ì…˜ RAG ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡°

| Layer | Component | Description |
|-------|-----------|-------------|
| **1. Entry** | Load Balancer | íŠ¸ë˜í”½ ë¶„ì‚° |
| **2. Gateway** | API Gateway | Rate Limiting, Auth, Caching |
| **3. Core** | RAG Orchestrator | Query Router â†’ Retrieval â†’ Rerank â†’ Generate |
| **4. Data** | Vector DB (Pinecone, Redis) | ë²¡í„° ê²€ìƒ‰ |
| **4. Data** | LLM API | OpenAI / Anthropic / Self-hosted |
| **5. Storage** | Document Store | S3 / PostgreSQL |

**ë°ì´í„° íë¦„:**
1. **Client** â†’ Load Balancer â†’ API Gateway
2. **API Gateway** â†’ RAG Orchestrator (ì¿¼ë¦¬ ë¶„ì„)
3. **Retrieval Service** â†’ Vector DB (ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰)
4. **Rerank Service** â†’ ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬
5. **Generate Service** â†’ LLM API (ë‹µë³€ ìƒì„±)
6. **Response** â†’ Client

---

## í•µì‹¬ ì„¤ê³„ ì›ì¹™

### 1. ê²€ìƒ‰ê³¼ ìƒì„±ì˜ ë¶„ë¦¬

\`\`\`python
# Bad: ëª¨ë†€ë¦¬ì‹
def answer(query):
    docs = vectorstore.search(query)  # ê²€ìƒ‰
    response = llm.generate(docs, query)  # ìƒì„±
    return response

# Good: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
class RetrievalService:
    def search(self, query: str, filters: dict) -> List[Document]:
        # ê²€ìƒ‰ ë¡œì§ë§Œ ë‹´ë‹¹
        pass

class GenerationService:
    def generate(self, context: str, query: str) -> str:
        # ìƒì„± ë¡œì§ë§Œ ë‹´ë‹¹
        pass

class RAGOrchestrator:
    def answer(self, query: str):
        docs = self.retrieval.search(query)
        reranked = self.reranker.rerank(docs, query)
        response = self.generation.generate(reranked, query)
        return response
\`\`\`

**ì´ì :**
- ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§ (ê²€ìƒ‰ íŠ¸ë˜í”½ â‰  ìƒì„± íŠ¸ë˜í”½)
- ë…ë¦½ì  ì—…ë°ì´íŠ¸ (ê²€ìƒ‰ ë¡œì§ë§Œ ë³€ê²½ ê°€ëŠ¥)
- ì¥ì•  ê²©ë¦¬ (ìƒì„± ì„œë¹„ìŠ¤ ì£½ì–´ë„ ê²€ìƒ‰ ê°€ëŠ¥)

---

### 2. ìºì‹± ì „ëµ

\`\`\`python
# ë‹¤ì¸µ ìºì‹± êµ¬ì¡°

class MultiLayerCache:
    def __init__(self):
        self.l1_cache = {}  # ì¸ë©”ëª¨ë¦¬ (10ì´ˆ)
        self.l2_cache = Redis()  # Redis (5ë¶„)
        self.l3_cache = PostgreSQL()  # DB (1ì¼)

    def get(self, query: str):
        # L1 ì²´í¬
        if query in self.l1_cache:
            return self.l1_cache[query]

        # L2 ì²´í¬
        cached = self.l2_cache.get(self.hash(query))
        if cached:
            self.l1_cache[query] = cached
            return cached

        # L3 ì²´í¬ (ì •í™•í•œ ì¿¼ë¦¬ë§Œ)
        cached = self.l3_cache.get_exact(query)
        if cached:
            self.l2_cache.set(self.hash(query), cached)
            self.l1_cache[query] = cached
            return cached

        return None

# ìºì‹± ëŒ€ìƒ
# 1. ì„ë² ë”© ê²°ê³¼ (ì¿¼ë¦¬ ì„ë² ë”©)
# 2. ê²€ìƒ‰ ê²°ê³¼ (ë™ì¼ ì¿¼ë¦¬)
# 3. ìµœì¢… ë‹µë³€ (ë™ì¼ ì¿¼ë¦¬)
\`\`\`

**ë¹„ìš© ì ˆê° íš¨ê³¼:**
\`\`\`
ìºì‹œ íˆíŠ¸ìœ¨ 50% ê°€ì •:
- LLM API í˜¸ì¶œ 50% ê°ì†Œ
- ê²€ìƒ‰ ë¹„ìš© 50% ê°ì†Œ
- ì‘ë‹µ ì‹œê°„ 90% ê°ì†Œ (ìºì‹œ íˆíŠ¸ ì‹œ)
\`\`\`

---

### 3. ë¹„ë™ê¸° ì²˜ë¦¬

\`\`\`python
import asyncio

async def answer_query(query: str):
    # ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—…ë“¤
    embedding_task = asyncio.create_task(
        embed_query(query)
    )

    # ì„ë² ë”© ì™„ë£Œ ëŒ€ê¸°
    query_embedding = await embedding_task

    # ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ ê²€ìƒ‰
    search_tasks = [
        asyncio.create_task(search_vectordb(query_embedding)),
        asyncio.create_task(search_keyword_index(query)),
        asyncio.create_task(search_knowledge_graph(query)),
    ]

    results = await asyncio.gather(*search_tasks)

    # ê²°ê³¼ ë³‘í•© ë° Rerank
    merged = merge_results(results)
    reranked = await rerank(merged, query)

    # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
    async for chunk in generate_stream(reranked, query):
        yield chunk

# ì‘ë‹µ ì‹œê°„ ë¹„êµ
# ë™ê¸°: ì„ë² ë”©(0.3s) + ê²€ìƒ‰(0.5s) + ìƒì„±(2s) = 2.8s
# ë¹„ë™ê¸°: ì„ë² ë”©(0.3s) + ê²€ìƒ‰(0.5s) + ìƒì„±(2s) = 2.8s
#         (but ê²€ìƒ‰ì´ ë³‘ë ¬ì´ë©´) = 0.3 + 0.2 + 2 = 2.5s
# ìŠ¤íŠ¸ë¦¬ë°: ì²« í† í° 0.8s í›„ ì‹¤ì‹œê°„ ì¶œë ¥
\`\`\`

---

### 4. ëª¨ë‹ˆí„°ë§ & ê´€ì°°ì„±

\`\`\`python
from dataclasses import dataclass
from datetime import datetime
import logging

@dataclass
class RAGMetrics:
    query_id: str
    timestamp: datetime

    # ê²€ìƒ‰ ë©”íŠ¸ë¦­
    retrieval_latency_ms: float
    num_docs_retrieved: int
    top_similarity_score: float

    # ìƒì„± ë©”íŠ¸ë¦­
    generation_latency_ms: float
    input_tokens: int
    output_tokens: int

    # í’ˆì§ˆ ë©”íŠ¸ë¦­
    user_feedback: Optional[int]  # 1-5
    was_helpful: Optional[bool]

class RAGObserver:
    def __init__(self):
        self.logger = logging.getLogger("rag")
        self.metrics_client = PrometheusClient()

    def log_query(self, metrics: RAGMetrics):
        # ë¡œê¹…
        self.logger.info(f"Query {metrics.query_id}: "
                        f"retrieval={metrics.retrieval_latency_ms}ms, "
                        f"generation={metrics.generation_latency_ms}ms")

        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        self.metrics_client.histogram(
            "rag_retrieval_latency",
            metrics.retrieval_latency_ms
        )
        self.metrics_client.histogram(
            "rag_generation_latency",
            metrics.generation_latency_ms
        )

        # ì•Œë¦¼ ì¡°ê±´ ì²´í¬
        if metrics.retrieval_latency_ms > 1000:
            self.alert("Slow retrieval detected")

        if metrics.top_similarity_score < 0.5:
            self.alert("Low retrieval quality")
\`\`\`

**í•µì‹¬ ëª¨ë‹ˆí„°ë§ ì§€í‘œ:**
\`\`\`
1. ì§€ì—°ì‹œê°„
   - P50, P95, P99 ì‘ë‹µ ì‹œê°„
   - ê²€ìƒ‰ vs ìƒì„± ì‹œê°„ ë¶„í•´

2. í’ˆì§ˆ
   - í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜
   - ì‚¬ìš©ì í”¼ë“œë°± ì ìˆ˜
   - í™˜ê° ê°ì§€ìœ¨

3. ë¹„ìš©
   - í† í° ì‚¬ìš©ëŸ‰ ì¶”ì´
   - ì„ë² ë”© API í˜¸ì¶œ ìˆ˜
   - ìºì‹œ íˆíŠ¸ìœ¨

4. ì‹œìŠ¤í…œ
   - ë²¡í„° DB ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
   - ì¸ë±ìŠ¤ í¬ê¸° ì¦ê°€ìœ¨
   - ì—ëŸ¬ìœ¨
\`\`\`

---

## ê·œëª¨ë³„ ì•„í‚¤í…ì²˜ ê¶Œì¥

### ì†Œê·œëª¨ (ë¬¸ì„œ <10K, ì‚¬ìš©ì <100)

\`\`\`
ìŠ¤íƒ:
- Chroma (ë¡œì»¬ ë²¡í„° DB)
- OpenAI API
- ë‹¨ì¼ ì„œë²„

ë¹„ìš©: ~$50/ì›”
\`\`\`

### ì¤‘ê·œëª¨ (ë¬¸ì„œ <100K, ì‚¬ìš©ì <1K)

\`\`\`
ìŠ¤íƒ:
- Pinecone / Weaviate (ê´€ë¦¬í˜• ë²¡í„° DB)
- OpenAI API + ìºì‹±
- Kubernetes 2-3 ë…¸ë“œ

ë¹„ìš©: ~$500/ì›”
\`\`\`

### ëŒ€ê·œëª¨ (ë¬¸ì„œ >1M, ì‚¬ìš©ì >10K)

\`\`\`
ìŠ¤íƒ:
- Milvus / Qdrant (ì…€í”„ í˜¸ìŠ¤íŒ…)
- ì˜¤í”ˆì†ŒìŠ¤ LLM (vLLM) + OpenAI í´ë°±
- Kubernetes í´ëŸ¬ìŠ¤í„° + Auto-scaling

ë¹„ìš©: $5,000+/ì›”
\`\`\`
      `,
      keyPoints: [
        'ê²€ìƒ‰/ìƒì„± ì„œë¹„ìŠ¤ ë¶„ë¦¬ë¡œ ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§ê³¼ ì¥ì•  ê²©ë¦¬',
        'ë‹¤ì¸µ ìºì‹± (ì¸ë©”ëª¨ë¦¬ â†’ Redis â†’ DB)ìœ¼ë¡œ ë¹„ìš© 50%+ ì ˆê° ê°€ëŠ¥',
        'ë¹„ë™ê¸° ì²˜ë¦¬ì™€ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²´ê° ì‘ë‹µ ì‹œê°„ ê°œì„ ',
        'ì§€ì—°ì‹œê°„, í’ˆì§ˆ, ë¹„ìš©, ì‹œìŠ¤í…œ 4ê°€ì§€ ì¶•ì˜ ëª¨ë‹ˆí„°ë§ í•„ìˆ˜',
      ],
      practiceGoal: 'í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™ê³¼ ê·œëª¨ë³„ ê¶Œì¥ ìŠ¤íƒì„ ì´í•´í•œë‹¤',
    }),

    // ============================================
    // Task 4: RAG íŒŒì´í”„ë¼ì¸ ì§ì ‘ êµ¬í˜„
    // ============================================
    createCodeTask('w5d1-rag-implementation', 'í”„ë¡œë•ì…˜ê¸‰ RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„', 60, {
      introduction: `
## ì™œ ë°°ìš°ëŠ”ê°€?

**ë¬¸ì œ**: RAGë¥¼ ì²˜ìŒ êµ¬í˜„í•˜ë©´ í”íˆ "ì‘ë™ì€ í•˜ëŠ”ë°... í”„ë¡œë•ì…˜ì—ì„œ ì“¸ ìˆ˜ ìˆë‚˜?" í•˜ëŠ” ì˜ë¬¸ì´ ìƒê¹ë‹ˆë‹¤.
- ì—ëŸ¬ê°€ ë‚˜ë©´ ì‹œìŠ¤í…œì´ ë©ˆì¶˜ë‹¤
- ëŠë¦° ì‘ë‹µ ì‹œê°„ (10ì´ˆ+)
- ë¹„ìš© ê´€ë¦¬ ë¶ˆê°€ëŠ¥
- ë””ë²„ê¹…ì´ ì–´ë µë‹¤

**í•´ê²°**: í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ RAGëŠ” **ì•ˆì •ì„±, ì„±ëŠ¥, ê´€ì¸¡ì„±**ì„ ëª¨ë‘ ê°–ì¶°ì•¼ í•©ë‹ˆë‹¤.

---

## ë¹„ìœ : RAG ì‹œìŠ¤í…œ = ë„ì„œê´€ ìë™í™”

\`\`\`
í”„ë¡œí† íƒ€ì… RAG = ì‘ì€ ì„œì 
- ì£¼ì¸ì´ ì§ì ‘ ì±… ì°¾ì•„ì¤Œ (ëŠë¦¼)
- ì±… ìœ„ì¹˜ë¥¼ ê¸°ì–µ (ìºì‹± ì—†ìŒ)
- ë¬¸ ë‹«ìœ¼ë©´ ë (ì—ëŸ¬ ì²˜ë¦¬ ì—†ìŒ)

í”„ë¡œë•ì…˜ RAG = ëŒ€í˜• ë„ì„œê´€
- ìë™í™” ì‹œìŠ¤í…œ (ë¹ ë¦„)
- ìºì‹±: ìì£¼ ì°¾ëŠ” ì±…ì€ ë°”ë¡œ êº¼ëƒ„
- ë°±ì—…: ë©”ì¸ ì‹œìŠ¤í…œ ê³ ì¥ ì‹œ ì„œë¸Œ ì‹œìŠ¤í…œ
- ë¡œê·¸: ëˆ„ê°€ ë¬´ìŠ¨ ì±…ì„ ì–¸ì œ ì°¾ì•˜ëŠ”ì§€ ê¸°ë¡
\`\`\`

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

\`\`\`
rag_system/
â”œâ”€â”€ config.py          # ì„¤ì •
â”œâ”€â”€ embeddings.py      # ì„ë² ë”© ì„œë¹„ìŠ¤
â”œâ”€â”€ retrieval.py       # ê²€ìƒ‰ ì„œë¹„ìŠ¤
â”œâ”€â”€ generation.py      # ìƒì„± ì„œë¹„ìŠ¤
â”œâ”€â”€ rag_pipeline.py    # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
â”œâ”€â”€ cache.py           # ìºì‹±
â””â”€â”€ metrics.py         # ëª¨ë‹ˆí„°ë§
\`\`\`

---

## í•µì‹¬ êµ¬í˜„ (ê°„ì†Œí™”)

\`\`\`python
# ğŸ“Œ Step 1: ì„¤ì • ê´€ë¦¬
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    openai_api_key: str
    embedding_model: str = "text-embedding-3-small"
    retrieval_top_k: int = 5
    llm_model: str = "gpt-4o-mini"
    cache_ttl_seconds: int = 300

    class Config:
        env_file = ".env"

# ğŸ“Œ Step 2: ìºì‹±ì´ ìˆëŠ” ì„ë² ë”© ì„œë¹„ìŠ¤
class EmbeddingService:
    def __init__(self):
        self.client = OpenAI(api_key=Settings().openai_api_key)
        self._cache = {}

    def embed_query(self, text: str) -> list[float]:
        cache_key = hashlib.md5(text.encode()).hexdigest()
        if cache_key in self._cache:
            return self._cache[cache_key]  # ğŸ’° ìºì‹œ íˆíŠ¸ = ë¹„ìš© 0

        embedding = self.client.embeddings.create(
            input=text,
            model="text-embedding-3-small"
        ).data[0].embedding

        self._cache[cache_key] = embedding
        return embedding

# ğŸ“Œ Step 3: ì¬ì‹œë„ê°€ ìˆëŠ” ê²€ìƒ‰ ì„œë¹„ìŠ¤
class RetrievalService:
    def search(self, query: str, top_k: int = 5) -> list[Document]:
        try:
            query_embedding = self.embedding_service.embed_query(query)
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k
            )
            return self._format_results(results)
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []  # ğŸ›¡ï¸ ì—ëŸ¬ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì‹œìŠ¤í…œ ì¤‘ë‹¨ ë°©ì§€)

# ğŸ“Œ Step 4: RAG íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
class RAGPipeline:
    def query(self, question: str) -> RAGResponse:
        start_time = time.time()

        # 1ï¸âƒ£ ê²€ìƒ‰
        documents = self.retrieval.search(question)
        if not documents:
            return RAGResponse(
                answer="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                sources=[],
                metrics={"retrieval_time_ms": 0}
            )

        # 2ï¸âƒ£ ë‹µë³€ ìƒì„±
        answer = self.generation.generate(question, documents)

        # 3ï¸âƒ£ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        total_time = time.time() - start_time
        return RAGResponse(
            answer=answer,
            sources=[self._extract_source(d) for d in documents],
            metrics={"total_time_ms": total_time * 1000}
        )
\`\`\`

---

## ì „ì²´ ì½”ë“œ (ìƒì„¸)

### 1. ì„¤ì • (config.py)

\`\`\`python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    # API Keys
    openai_api_key: str

    # Embedding
    embedding_model: str = "text-embedding-3-small"
    embedding_dimensions: int = 1536

    # Retrieval
    retrieval_top_k: int = 5
    similarity_threshold: float = 0.7

    # Generation
    llm_model: str = "gpt-4o-mini"
    llm_temperature: float = 0
    max_tokens: int = 1000

    # Cache
    cache_ttl_seconds: int = 300

    class Config:
        env_file = ".env"

@lru_cache()
def get_settings():
    return Settings()
\`\`\`

---

## 2. ì„ë² ë”© ì„œë¹„ìŠ¤ (embeddings.py)

\`\`\`python
from openai import OpenAI
import hashlib
from typing import List, Optional
from config import get_settings

class EmbeddingService:
    def __init__(self):
        self.settings = get_settings()
        self.client = OpenAI(api_key=self.settings.openai_api_key)
        self._cache = {}  # ê°„ë‹¨í•œ ì¸ë©”ëª¨ë¦¬ ìºì‹œ

    def _cache_key(self, text: str) -> str:
        return hashlib.md5(text.encode()).hexdigest()

    def embed_query(self, text: str) -> List[float]:
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”© (ìºì‹± ì ìš©)"""
        cache_key = self._cache_key(text)

        if cache_key in self._cache:
            return self._cache[cache_key]

        response = self.client.embeddings.create(
            input=text,
            model=self.settings.embedding_model
        )
        embedding = response.data[0].embedding

        self._cache[cache_key] = embedding
        return embedding

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """ë‹¤ì¤‘ ë¬¸ì„œ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)"""
        # ìºì‹œ í™•ì¸
        results = [None] * len(texts)
        texts_to_embed = []
        indices_to_embed = []

        for i, text in enumerate(texts):
            cache_key = self._cache_key(text)
            if cache_key in self._cache:
                results[i] = self._cache[cache_key]
            else:
                texts_to_embed.append(text)
                indices_to_embed.append(i)

        # ìºì‹œ ë¯¸ìŠ¤ëœ ê²ƒë§Œ ì„ë² ë”©
        if texts_to_embed:
            response = self.client.embeddings.create(
                input=texts_to_embed,
                model=self.settings.embedding_model
            )

            for j, embedding_data in enumerate(response.data):
                idx = indices_to_embed[j]
                embedding = embedding_data.embedding
                results[idx] = embedding

                # ìºì‹œ ì €ì¥
                cache_key = self._cache_key(texts_to_embed[j])
                self._cache[cache_key] = embedding

        return results
\`\`\`

---

## 3. ê²€ìƒ‰ ì„œë¹„ìŠ¤ (retrieval.py)

\`\`\`python
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import numpy as np
from chromadb import PersistentClient
from embeddings import EmbeddingService
from config import get_settings

@dataclass
class RetrievedDocument:
    content: str
    metadata: Dict[str, Any]
    score: float

class RetrievalService:
    def __init__(self, collection_name: str = "documents"):
        self.settings = get_settings()
        self.embedding_service = EmbeddingService()

        # Chroma í´ë¼ì´ì–¸íŠ¸
        self.chroma = PersistentClient(path="./chroma_db")
        self.collection = self.chroma.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

    def add_documents(
        self,
        documents: List[str],
        metadatas: Optional[List[Dict]] = None,
        ids: Optional[List[str]] = None
    ):
        """ë¬¸ì„œ ì¶”ê°€"""
        if ids is None:
            ids = [f"doc_{i}" for i in range(len(documents))]

        if metadatas is None:
            metadatas = [{}] * len(documents)

        # ë°°ì¹˜ ì„ë² ë”©
        embeddings = self.embedding_service.embed_documents(documents)

        self.collection.add(
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )

        return len(documents)

    def search(
        self,
        query: str,
        top_k: Optional[int] = None,
        filter_metadata: Optional[Dict] = None
    ) -> List[RetrievedDocument]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        if top_k is None:
            top_k = self.settings.retrieval_top_k

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_service.embed_query(query)

        # ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=filter_metadata,
            include=["documents", "metadatas", "distances"]
        )

        # ê²°ê³¼ ë³€í™˜
        documents = []
        for i in range(len(results["ids"][0])):
            # ChromaëŠ” distance ë°˜í™˜, similarityë¡œ ë³€í™˜
            distance = results["distances"][0][i]
            similarity = 1 - distance  # cosine distance to similarity

            # ì„ê³„ê°’ í•„í„°ë§
            if similarity >= self.settings.similarity_threshold:
                documents.append(RetrievedDocument(
                    content=results["documents"][0][i],
                    metadata=results["metadatas"][0][i],
                    score=similarity
                ))

        return documents

    def hybrid_search(
        self,
        query: str,
        top_k: int = 5,
        vector_weight: float = 0.7
    ) -> List[RetrievedDocument]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)"""
        # ë²¡í„° ê²€ìƒ‰
        vector_results = self.search(query, top_k=top_k * 2)

        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ì¶”ê°€
        query_terms = set(query.lower().split())

        for doc in vector_results:
            doc_terms = set(doc.content.lower().split())
            keyword_score = len(query_terms & doc_terms) / len(query_terms) if query_terms else 0

            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
            doc.score = (
                vector_weight * doc.score +
                (1 - vector_weight) * keyword_score
            )

        # ì¬ì •ë ¬
        vector_results.sort(key=lambda x: x.score, reverse=True)
        return vector_results[:top_k]
\`\`\`

---

## 4. ìƒì„± ì„œë¹„ìŠ¤ (generation.py)

\`\`\`python
from openai import OpenAI
from typing import List, Generator
from retrieval import RetrievedDocument
from config import get_settings

class GenerationService:
    def __init__(self):
        self.settings = get_settings()
        self.client = OpenAI(api_key=self.settings.openai_api_key)

        self.system_prompt = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
3. ë‹µë³€ì— ì‚¬ìš©í•œ ì •ë³´ì˜ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”.
4. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ê·¸ ì ì„ ëª…ì‹œí•˜ì„¸ìš”."""

    def _format_context(self, documents: List[RetrievedDocument]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§¤íŒ…"""
        context_parts = []

        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get("source", "Unknown")
            context_parts.append(
                f"[ë¬¸ì„œ {i}] (ì¶œì²˜: {source}, ê´€ë ¨ë„: {doc.score:.2f})\\n"
                f"{doc.content}"
            )

        return "\\n\\n---\\n\\n".join(context_parts)

    def generate(
        self,
        query: str,
        documents: List[RetrievedDocument]
    ) -> str:
        """ë‹µë³€ ìƒì„±"""
        context = self._format_context(documents)

        response = self.client.chat.completions.create(
            model=self.settings.llm_model,
            temperature=self.settings.llm_temperature,
            max_tokens=self.settings.max_tokens,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": f"""ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""}
            ]
        )

        return response.choices[0].message.content

    def generate_stream(
        self,
        query: str,
        documents: List[RetrievedDocument]
    ) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±"""
        context = self._format_context(documents)

        stream = self.client.chat.completions.create(
            model=self.settings.llm_model,
            temperature=self.settings.llm_temperature,
            max_tokens=self.settings.max_tokens,
            stream=True,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": f"""ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""}
            ]
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
\`\`\`

---

## 5. RAG ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (rag_pipeline.py)

\`\`\`python
from typing import List, Dict, Any, Optional, Generator
from dataclasses import dataclass
from datetime import datetime
import time

from retrieval import RetrievalService, RetrievedDocument
from generation import GenerationService
from config import get_settings

@dataclass
class RAGResponse:
    answer: str
    sources: List[Dict[str, Any]]
    metrics: Dict[str, Any]

class RAGPipeline:
    def __init__(self, collection_name: str = "documents"):
        self.settings = get_settings()
        self.retrieval = RetrievalService(collection_name)
        self.generation = GenerationService()

    def index_documents(
        self,
        documents: List[str],
        metadatas: Optional[List[Dict]] = None
    ) -> int:
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        return self.retrieval.add_documents(documents, metadatas)

    def query(
        self,
        question: str,
        use_hybrid: bool = False,
        top_k: Optional[int] = None
    ) -> RAGResponse:
        """RAG ì¿¼ë¦¬ ì‹¤í–‰"""
        start_time = time.time()

        # 1. ê²€ìƒ‰
        retrieval_start = time.time()
        if use_hybrid:
            documents = self.retrieval.hybrid_search(question, top_k or 5)
        else:
            documents = self.retrieval.search(question, top_k)
        retrieval_time = time.time() - retrieval_start

        # 2. ê²€ìƒ‰ ê²°ê³¼ ì—†ìœ¼ë©´ ì¡°ê¸° ë°˜í™˜
        if not documents:
            return RAGResponse(
                answer="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                sources=[],
                metrics={
                    "retrieval_time_ms": retrieval_time * 1000,
                    "generation_time_ms": 0,
                    "total_time_ms": (time.time() - start_time) * 1000,
                    "num_documents": 0
                }
            )

        # 3. ë‹µë³€ ìƒì„±
        generation_start = time.time()
        answer = self.generation.generate(question, documents)
        generation_time = time.time() - generation_start

        # 4. ê²°ê³¼ ì¡°í•©
        total_time = time.time() - start_time

        return RAGResponse(
            answer=answer,
            sources=[
                {
                    "content": doc.content[:200] + "...",
                    "metadata": doc.metadata,
                    "score": doc.score
                }
                for doc in documents
            ],
            metrics={
                "retrieval_time_ms": retrieval_time * 1000,
                "generation_time_ms": generation_time * 1000,
                "total_time_ms": total_time * 1000,
                "num_documents": len(documents),
                "avg_similarity": sum(d.score for d in documents) / len(documents)
            }
        )

    def query_stream(
        self,
        question: str,
        use_hybrid: bool = False
    ) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° RAG ì¿¼ë¦¬"""
        # ê²€ìƒ‰
        if use_hybrid:
            documents = self.retrieval.hybrid_search(question)
        else:
            documents = self.retrieval.search(question)

        if not documents:
            yield "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return

        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        for chunk in self.generation.generate_stream(question, documents):
            yield chunk


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì´ˆê¸°í™”
    rag = RAGPipeline()

    # ë¬¸ì„œ ì¸ë±ì‹±
    documents = [
        "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìì…ë‹ˆë‹¤.",
        "RAGëŠ” 2020ë…„ Facebook AI Researchì—ì„œ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "RAGëŠ” ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ë‹µë³€ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.",
        "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        "ChromaëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.",
    ]
    metadatas = [
        {"source": "rag_intro.md", "section": "ì •ì˜"},
        {"source": "rag_intro.md", "section": "ì—­ì‚¬"},
        {"source": "rag_intro.md", "section": "ì¥ì "},
        {"source": "vectordb.md", "section": "ê°œë…"},
        {"source": "vectordb.md", "section": "ë„êµ¬"},
    ]

    rag.index_documents(documents, metadatas)
    print("ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")

    # ì¿¼ë¦¬
    response = rag.query("RAGê°€ ë­ì•¼?")
    print(f"\\në‹µë³€: {response.answer}")
    print(f"\\nì¶œì²˜: {response.sources}")
    print(f"\\në©”íŠ¸ë¦­: {response.metrics}")

    # ìŠ¤íŠ¸ë¦¬ë°
    print("\\nìŠ¤íŠ¸ë¦¬ë° ë‹µë³€: ", end="")
    for chunk in rag.query_stream("RAGì˜ ì¥ì ì€?"):
        print(chunk, end="", flush=True)
    print()
\`\`\`

## âš ï¸ Common Pitfalls (ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜)

### 1. [ì„ë² ë”© ë¶ˆì¼ì¹˜] ë¬¸ì„œì™€ ì¿¼ë¦¬ì— ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
**ì¦ìƒ**: ê²€ìƒ‰ ê²°ê³¼ê°€ ì´ìƒí•˜ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œë§Œ ë°˜í™˜
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ëª¨ë¸ ë¶ˆì¼ì¹˜
doc_embedding = embed_with_ada002(doc)      # text-embedding-ada-002
query_embedding = embed_with_3_small(query)  # text-embedding-3-small ğŸ’¥
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: ë‹¤ë¥¸ ëª¨ë¸ì€ ë‹¤ë¥¸ ë²¡í„° ê³µê°„ â†’ ìœ ì‚¬ë„ ê³„ì‚° ë¬´ì˜ë¯¸
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - ë™ì¼ ëª¨ë¸ ì‚¬ìš©
embedding_model = "text-embedding-3-small"
doc_embedding = embed(doc, model=embedding_model)
query_embedding = embed(query, model=embedding_model)
\`\`\`
**ê¸°ì–µí•  ì **: ë¬¸ì„œ ì¸ë±ì‹±ê³¼ ì¿¼ë¦¬ì— í•­ìƒ ë™ì¼í•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©

### 2. [ì»¨í…ìŠ¤íŠ¸ ì˜¤ë²„í”Œë¡œìš°] ê²€ìƒ‰ ê²°ê³¼ ë„ˆë¬´ ë§ì´ ë„£ê¸°
**ì¦ìƒ**: í† í° ì œí•œ ì—ëŸ¬, ë¹„ìš© í­ë°œ, ë‹µë³€ í’ˆì§ˆ ì €í•˜
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
documents = retrieval.search(query, top_k=20)  # 20ê°œ ë¬¸ì„œ
context = "\\n".join([d.content for d in documents])  # ğŸ’¥ 100K+ í† í°
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: LLM ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì´ˆê³¼, ë¶ˆí•„ìš”í•œ ë¹„ìš©
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - top_k ì œí•œ + í† í° ì¹´ìš´íŒ…
MAX_CONTEXT_TOKENS = 4000
documents = retrieval.search(query, top_k=5)
context = ""
for doc in documents:
    if count_tokens(context + doc.content) < MAX_CONTEXT_TOKENS:
        context += doc.content + "\\n---\\n"
\`\`\`
**ê¸°ì–µí•  ì **: top_këŠ” 3-7ê°œ, ì»¨í…ìŠ¤íŠ¸ëŠ” ì „ì²´ ìœˆë„ìš°ì˜ 50% ì´í•˜

### 3. [ìºì‹œ ë¬´íš¨í™”] ë¬¸ì„œ ì—…ë°ì´íŠ¸ í›„ ìºì‹œ ì•ˆ ì§€ì›€
**ì¦ìƒ**: ìƒˆ ë¬¸ì„œë¥¼ ì¶”ê°€í–ˆëŠ”ë° ê²€ìƒ‰ ê²°ê³¼ì— ì•ˆ ë‚˜ì˜´
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ìºì‹œ ë¬´íš¨í™” ëˆ„ë½
def add_document(doc):
    embed_and_store(doc)  # ìƒˆ ë¬¸ì„œ ì €ì¥
    # ìºì‹œëŠ” ê·¸ëŒ€ë¡œ â†’ ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ë°˜í™˜ë¨
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: ìºì‹œëœ ê²€ìƒ‰ ê²°ê³¼ê°€ ìƒˆ ë¬¸ì„œë¥¼ ë°˜ì˜í•˜ì§€ ì•ŠìŒ
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - ë¬¸ì„œ ë³€ê²½ ì‹œ ìºì‹œ ë¬´íš¨í™”
def add_document(doc):
    embed_and_store(doc)
    cache.clear()  # ë˜ëŠ” ê´€ë ¨ ìºì‹œë§Œ ì‚­ì œ
    # ë˜ëŠ” TTL ê¸°ë°˜ ìë™ ë§Œë£Œ
\`\`\`
**ê¸°ì–µí•  ì **: ë¬¸ì„œ ì¶”ê°€/ìˆ˜ì •/ì‚­ì œ ì‹œ ë°˜ë“œì‹œ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”
      `,
      keyPoints: [
        'ğŸ—ï¸ ì„¤ì •, ì„ë² ë”©, ê²€ìƒ‰, ìƒì„±ì„ ë¶„ë¦¬ëœ ì„œë¹„ìŠ¤ë¡œ êµ¬í˜„',
        'ğŸ’° ì„ë² ë”©/ê²€ìƒ‰ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ë¹„ìš©ê³¼ ì§€ì—°ì‹œê°„ ì ˆê°',
        'ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ) êµ¬í˜„',
        'ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê¸°ë°˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë‚´ì¥',
      ],
      practiceGoal: 'í”„ë¡œë•ì…˜ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ RAG íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ êµ¬í˜„í•œë‹¤',
    }),

    // ============================================
    // Task 5: Day 1 í€´ì¦ˆ
    // ============================================
    createQuizTask('w5d1-quiz', 'Day 1 ì´í•´ë„ ì ê²€', 20, {
      introduction: '',
      questions: [
        {
          id: 'w5d1-q1',
          question: 'RAGê°€ ìµœì´ˆë¡œ ë°œí‘œëœ ì—°ë„ì™€ ê¸°ê´€ì€?',
          options: [
            '2019ë…„ Google',
            '2020ë…„ Facebook AI Research',
            '2021ë…„ OpenAI',
            '2022ë…„ Anthropic'
          ],
          correctAnswer: 1,
          explanation: 'RAGëŠ” 2020ë…„ Facebook AI Research(í˜„ Meta AI)ì—ì„œ "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" ë…¼ë¬¸ìœ¼ë¡œ ì²˜ìŒ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.',
        },
        {
          id: 'w5d1-q2',
          question: 'Fine-tuning ëŒ€ë¹„ RAGì˜ ê°€ì¥ í° ì¥ì ì€?',
          options: [
            'ë” ë¹ ë¥¸ ì‘ë‹µ ì†ë„',
            'ë” ì €ë ´í•œ GPU ë¹„ìš©',
            'ì‹¤ì‹œê°„ ì§€ì‹ ì—…ë°ì´íŠ¸ì™€ ì¶œì²˜ ì¶”ì ',
            'ë” ë†’ì€ ë‹µë³€ í’ˆì§ˆ'
          ],
          correctAnswer: 2,
          explanation: 'RAGëŠ” ë¬¸ì„œë§Œ ì—…ë°ì´íŠ¸í•˜ë©´ ì¦‰ì‹œ ìƒˆ ì§€ì‹ì´ ë°˜ì˜ë˜ê³ , ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ì„œë¥¼ ì¶œì²˜ë¡œ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Fine-tuningì€ ì¬í•™ìŠµì´ í•„ìš”í•˜ê³  ì¶œì²˜ ì¶”ì ì´ ì–´ë µìŠµë‹ˆë‹¤.',
        },
        {
          id: 'w5d1-q3',
          question: 'í”„ë¡œë•ì…˜ RAGì—ì„œ ìºì‹±ì˜ ì£¼ìš” ëŒ€ìƒì´ ì•„ë‹Œ ê²ƒì€?',
          options: [
            'ì¿¼ë¦¬ ì„ë² ë”© ê²°ê³¼',
            'ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡',
            'ëª¨ë¸ íŒŒë¼ë¯¸í„°',
            'LLM ìµœì¢… ë‹µë³€'
          ],
          correctAnswer: 2,
          explanation: 'ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ìºì‹± ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤. ì¿¼ë¦¬ ì„ë² ë”©, ê²€ìƒ‰ ê²°ê³¼, ìµœì¢… ë‹µë³€ì€ ë™ì¼ ì¿¼ë¦¬ ë°˜ë³µ ì‹œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ìºì‹± íš¨ê³¼ê°€ í½ë‹ˆë‹¤.',
        },
        {
          id: 'w5d1-q4',
          question: '2024-2025 ìµœì‹  RAG íŠ¸ë Œë“œê°€ ì•„ë‹Œ ê²ƒì€?',
          options: [
            'Self-RAG (ìê°€ ê²€ì¦)',
            'Corrective RAG (ìë™ ìˆ˜ì •)',
            'Naive RAG (ë‹¨ìˆœ ê²€ìƒ‰-ìƒì„±)',
            'Agentic RAG (ì—ì´ì „íŠ¸ ê¸°ë°˜)'
          ],
          correctAnswer: 2,
          explanation: 'Naive RAGëŠ” 2020-2021ë…„ì˜ ì´ˆê¸° íŒ¨í„´ì…ë‹ˆë‹¤. ìµœì‹  íŠ¸ë Œë“œëŠ” Self-RAG, CRAG, Agentic RAG, GraphRAG ë“± ë” ì •êµí•œ ê²€ì¦/ì¶”ë¡  ë©”ì»¤ë‹ˆì¦˜ì„ í¬í•¨í•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d1-q5',
          question: 'í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì´ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë‚˜ì€ ì´ìœ ëŠ”?',
          options: [
            'í•­ìƒ ë” ë¹ ë¥´ê¸° ë•Œë¬¸ì—',
            'í‚¤ì›Œë“œ ì •í™• ë§¤ì¹­ê³¼ ì˜ë¯¸ ê²€ìƒ‰ì˜ ì¥ì ì„ ê²°í•©í•˜ê¸° ë•Œë¬¸ì—',
            'ë²¡í„° DBê°€ í•„ìš” ì—†ê¸° ë•Œë¬¸ì—',
            'ì„ë² ë”© ë¹„ìš©ì´ ì—†ê¸° ë•Œë¬¸ì—'
          ],
          correctAnswer: 1,
          explanation: 'ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì— ê°•í•˜ì§€ë§Œ ì •í™•í•œ í‚¤ì›Œë“œë¥¼ ë†“ì¹  ìˆ˜ ìˆê³ , í‚¤ì›Œë“œ ê²€ìƒ‰ì€ ê·¸ ë°˜ëŒ€ì…ë‹ˆë‹¤. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì€ ë‘ ë°©ì‹ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ ë” ë†’ì€ ê²€ìƒ‰ í’ˆì§ˆì„ ë‹¬ì„±í•©ë‹ˆë‹¤.',
        },
      ],
      keyPoints: [
        'RAG ì—­ì‚¬: 2020ë…„ Meta AI ë…¼ë¬¸ì—ì„œ ì‹œì‘',
        'RAG vs Fine-tuning: ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸, ì¶œì²˜ ì¶”ì ì´ RAGì˜ í•µì‹¬ ì¥ì ',
        'í”„ë¡œë•ì…˜ ìµœì í™”: ìºì‹±, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ëª¨ë‹ˆí„°ë§',
        'ìµœì‹  íŠ¸ë Œë“œ: Self-RAG, CRAG, Agentic RAG',
      ],
      practiceGoal: 'Day 1 í•™ìŠµ ë‚´ìš©ì˜ í•µì‹¬ ê°œë…ì„ ì •í™•íˆ ì´í•´í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤',
    }),
  ],

  // ============================================
  // Day 1 Challenge
  // ============================================
  challenge: createChallengeTask('w5d1-challenge', 'Challenge: RAG ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ ì‘ì„±', 60, {
    introduction: `
## ê³¼ì œ ëª©í‘œ

ê°€ìƒì˜ ê³ ê°ì‚¬ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ **RAG ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
ì´ ê³¼ì œëŠ” FDEë¡œì„œ ê³ ê°ì—ê²Œ ì†”ë£¨ì…˜ì„ ì œì•ˆí•˜ëŠ” ì‹¤ì „ ê²½í—˜ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

---

## ì‹œë‚˜ë¦¬ì˜¤

**ê³ ê°ì‚¬: ë¯¸ë˜ë²•ë¥ ì‚¬ë¬´ì†Œ**

\`\`\`
ì—…ì¢…: ë²•ë¥  ì„œë¹„ìŠ¤
ê·œëª¨: ë³€í˜¸ì‚¬ 50ëª…, ì§ì› 100ëª…
í˜„ì¬ ìƒí™©:
- 10ë…„ê°„ ì¶•ì ëœ íŒë¡€ ë¶„ì„ ìë£Œ 50,000ê±´
- ë‚´ë¶€ ë²•ë¥  ì˜ê²¬ì„œ 10,000ê±´
- ë²•ë ¹ DB ì—°ë™ í•„ìš” (êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°)

ìš”êµ¬ì‚¬í•­:
1. ë³€í˜¸ì‚¬ë“¤ì´ ìì—°ì–´ë¡œ íŒë¡€/ë²•ë ¹ ê²€ìƒ‰
2. ê´€ë ¨ íŒë¡€ + ë²•ë ¹ + ë‚´ë¶€ ì˜ê²¬ì„œë¥¼ ì¢…í•©í•œ ë‹µë³€
3. ì¶œì²˜ (íŒë¡€ë²ˆí˜¸, ë²•ë ¹ ì¡°í•­, ì˜ê²¬ì„œ ë²ˆí˜¸) í•„ìˆ˜ í‘œì‹œ
4. ì‘ë‹µ ì‹œê°„ 5ì´ˆ ì´ë‚´
5. ì›” ì˜ˆì‚° $3,000 ì´ë‚´
\`\`\`

---

## ì œì¶œë¬¼ (Markdown í˜•ì‹)

### 1. ê¸°ìˆ  ì„ íƒ ë° ê·¼ê±° (30%)

\`\`\`
ë‹¤ìŒ í•­ëª©ì— ëŒ€í•œ ì„ íƒê³¼ ì´ìœ ë¥¼ ì‘ì„±:
- Prompt Engineering / Fine-tuning / RAG ì¤‘ ì„ íƒ ë° ì´ìœ 
- ì„ë² ë”© ëª¨ë¸ ì„ íƒ
- ë²¡í„° DB ì„ íƒ
- LLM ì„ íƒ
\`\`\`

### 2. ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ (30%)

\`\`\`
ASCII ì•„íŠ¸ ë˜ëŠ” ì„¤ëª…ìœ¼ë¡œ ë‹¤ìŒ í¬í•¨:
- ë°ì´í„° íë¦„ (ë¬¸ì„œ ìˆ˜ì§‘ â†’ ì¸ë±ì‹± â†’ ê²€ìƒ‰ â†’ ìƒì„±)
- ì»´í¬ë„ŒíŠ¸ êµ¬ì„±
- ìºì‹± ì „ëµ
- ì—ëŸ¬ ì²˜ë¦¬
\`\`\`

### 3. ë¹„ìš© ì¶”ì • (20%)

\`\`\`
ì›”ê°„ ì˜ˆìƒ ë¹„ìš© ì‚°ì¶œ:
- ì„ë² ë”© ë¹„ìš© (ë¬¸ì„œ ê°œìˆ˜ Ã— í† í° Ã— ë‹¨ê°€)
- LLM ë¹„ìš© (ì˜ˆìƒ ì¿¼ë¦¬ ìˆ˜ Ã— í† í° Ã— ë‹¨ê°€)
- ì¸í”„ë¼ ë¹„ìš© (ë²¡í„° DB, ì„œë²„)
\`\`\`

### 4. ë¦¬ìŠ¤í¬ ë° ì™„í™” ë°©ì•ˆ (20%)

\`\`\`
ìµœì†Œ 3ê°€ì§€ ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘ ë°©ì•ˆ:
ì˜ˆ: ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜ â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Re-ranking
\`\`\`

---

## í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ê¸°ì¤€ |
|------|------|------|
| ê¸°ìˆ  ì„ íƒ ê·¼ê±° | 30% | ìš”êµ¬ì‚¬í•­ê³¼ì˜ ì í•©ì„±, ë…¼ë¦¬ì  ê·¼ê±° |
| ì•„í‚¤í…ì²˜ ì™„ì„±ë„ | 30% | í”„ë¡œë•ì…˜ ë ˆë²¨ì˜ ê³ ë ¤ì‚¬í•­ í¬í•¨ |
| ë¹„ìš© í˜„ì‹¤ì„± | 20% | ì‹¤ì œ ê°€ê²© ê¸°ë°˜, ì˜ˆì‚° ë‚´ |
| ë¦¬ìŠ¤í¬ ë¶„ì„ | 20% | ì‹¤ì§ˆì  ë¦¬ìŠ¤í¬ ì‹ë³„ ë° ëŒ€ì‘ |

---

## ì°¸ê³  ê°€ê²© (2024ë…„ ê¸°ì¤€)

\`\`\`
OpenAI:
- text-embedding-3-small: $0.02 / 1M í† í°
- text-embedding-3-large: $0.13 / 1M í† í°
- gpt-4o-mini: $0.15 / 1M ì…ë ¥, $0.60 / 1M ì¶œë ¥
- gpt-4o: $2.50 / 1M ì…ë ¥, $10 / 1M ì¶œë ¥

ë²¡í„° DB:
- Pinecone: Free tier (100K ë²¡í„°) + $70/ì›” (1M ë²¡í„°)
- Weaviate Cloud: $25/ì›”~
- Chroma: ë¬´ë£Œ (ì…€í”„ í˜¸ìŠ¤íŒ…)

ì„œë²„:
- AWS EC2 t3.medium: ~$30/ì›”
- AWS EC2 m5.large: ~$70/ì›”
\`\`\`
    `,
    hints: [
`# ============================================
# RAG ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ Pseudocode
# ============================================

# 1. ê¸°ìˆ  ì„ íƒ
SECTION "ê¸°ìˆ  ì„ íƒ ë° ê·¼ê±°":

    ì ‘ê·¼ ë°©ì‹:
        ì„ íƒ: RAG
        ì´ìœ :
            - íŒë¡€/ë²•ë ¹ì€ ìì£¼ ì—…ë°ì´íŠ¸ë¨ â†’ Fine-tuning ë¶€ì í•©
            - ì¶œì²˜ í‘œì‹œ í•„ìˆ˜ â†’ RAGì˜ í•µì‹¬ ì¥ì 
            - ê¸°ì¡´ ë¬¸ì„œ í™œìš© â†’ ì¶”ê°€ í•™ìŠµ ë°ì´í„° ìƒì„± ë¶ˆí•„ìš”

    ì„ë² ë”© ëª¨ë¸:
        ì„ íƒ: text-embedding-3-small
        ì´ìœ :
            - ë¹„ìš© íš¨ìœ¨ ($0.02/1M)
            - 60K ë¬¸ì„œ Ã— í‰ê·  1000 í† í° = 60M í† í°
            - ì¼íšŒì„± ë¹„ìš©: $1.2

    ë²¡í„° DB:
        ì„ íƒ: Pinecone
        ì´ìœ :
            - ê´€ë¦¬í˜• â†’ ìš´ì˜ ë¶€ë‹´ ê°ì†Œ
            - 60K ë²¡í„° â†’ Free tierë¡œ ê°€ëŠ¥
            - í•„í„°ë§, ë©”íƒ€ë°ì´í„° ì§€ì›

    LLM:
        ì„ íƒ: gpt-4o-mini
        ì´ìœ :
            - ë¹„ìš© íš¨ìœ¨ ($0.15/1M ì…ë ¥)
            - ë²•ë¥  í…ìŠ¤íŠ¸ ì´í•´ë ¥ ì¶©ë¶„
            - ì‘ë‹µ ì†ë„ ë¹ ë¦„

# 2. ì•„í‚¤í…ì²˜
SECTION "ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨":

    [ë°ì´í„° ì†ŒìŠ¤]
        â”œâ”€â”€ íŒë¡€ ë¶„ì„ ìë£Œ (50Kê±´)
        â”œâ”€â”€ ë‚´ë¶€ ì˜ê²¬ì„œ (10Kê±´)
        â””â”€â”€ êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° API
             â”‚
             â–¼
    [ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸]
        â”œâ”€â”€ PDF/HWP í…ìŠ¤íŠ¸ ì¶”ì¶œ
        â”œâ”€â”€ ì²­í‚¹ (1000ì, 100ì ì˜¤ë²„ë©)
        â””â”€â”€ ì„ë² ë”© (text-embedding-3-small)
             â”‚
             â–¼
    [ë²¡í„° ì €ì¥ì†Œ: Pinecone]
        â””â”€â”€ ë©”íƒ€ë°ì´í„°: source, type, date, case_number
             â”‚
             â–¼
    [ê²€ìƒ‰ ì„œë¹„ìŠ¤]
        â”œâ”€â”€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° 70% + í‚¤ì›Œë“œ 30%)
        â””â”€â”€ Re-ranking (Cohere)
             â”‚
             â–¼
    [ìƒì„± ì„œë¹„ìŠ¤]
        â”œâ”€â”€ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ì¶œì²˜ í¬í•¨)
        â”œâ”€â”€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ë²•ë¥  ì „ìš©)
        â””â”€â”€ LLM ìƒì„± (gpt-4o-mini)
             â”‚
             â–¼
    [ìºì‹± ë ˆì´ì–´: Redis]
        â””â”€â”€ ì¿¼ë¦¬ ê²°ê³¼ ìºì‹± (TTL: 1ì‹œê°„)
             â”‚
             â–¼
    [API Gateway]
        â””â”€â”€ Rate limiting, Auth

# 3. ë¹„ìš© ì¶”ì •
SECTION "ì›”ê°„ ë¹„ìš© ì¶”ì •":

    ê°€ì •:
        - ì›”ê°„ ì¿¼ë¦¬ ìˆ˜: 10,000ê±´
        - í‰ê·  ê²€ìƒ‰ ê²°ê³¼: 5ê°œ ë¬¸ì„œ
        - í‰ê·  ì…ë ¥ í† í°: 2,000 (ì»¨í…ìŠ¤íŠ¸ + ì§ˆë¬¸)
        - í‰ê·  ì¶œë ¥ í† í°: 500

    ì„ë² ë”© (ì¼íšŒì„±):
        - 60K ë¬¸ì„œ Ã— 1,000 í† í° = 60M í† í°
        - ë¹„ìš©: 60M Ã— $0.02/1M = $1.2

    LLM (ì›”ê°„):
        - ì…ë ¥: 10,000 Ã— 2,000 = 20M í† í°
        - ì¶œë ¥: 10,000 Ã— 500 = 5M í† í°
        - ë¹„ìš©: (20M Ã— $0.15/1M) + (5M Ã— $0.60/1M)
               = $3 + $3 = $6/ì›”

    ì¸í”„ë¼:
        - Pinecone: Free tier ($0)
        - EC2 t3.medium: $30/ì›”
        - Redis (ElastiCache): $15/ì›”

    ì´ ì›”ê°„ ë¹„ìš©: ~$51/ì›” (ì˜ˆì‚° $3,000ì˜ 1.7%)

# 4. ë¦¬ìŠ¤í¬ ë° ì™„í™”
SECTION "ë¦¬ìŠ¤í¬ ë¶„ì„":

    ë¦¬ìŠ¤í¬ 1: ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜
        ì›ì¸: ë²•ë¥  ìš©ì–´ì˜ ë‹¤ì–‘í•œ í‘œí˜„
        ì™„í™”:
            - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì ìš©
            - ë²•ë¥  ë™ì˜ì–´ ì‚¬ì „ êµ¬ì¶•
            - Re-ranking ì ìš©

    ë¦¬ìŠ¤í¬ 2: í™˜ê° (ì˜ëª»ëœ íŒë¡€ ì¸ìš©)
        ì›ì¸: LLMì˜ ë³¸ì§ˆì  í•œê³„
        ì™„í™”:
            - í”„ë¡¬í”„íŠ¸ì— "ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©" ê°•ì¡°
            - ì¶œì²˜ ê²€ì¦ í›„ì²˜ë¦¬ ë¡œì§
            - ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘

    ë¦¬ìŠ¤í¬ 3: ì‘ë‹µ ì§€ì—°
        ì›ì¸: ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬
        ì™„í™”:
            - Redis ìºì‹± (ë°˜ë³µ ì§ˆë¬¸)
            - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            - ë¹„ë™ê¸° ì²˜ë¦¬

    ë¦¬ìŠ¤í¬ 4: ë²•ë ¹ ì—…ë°ì´íŠ¸ ëˆ„ë½
        ì›ì¸: ìˆ˜ë™ ì—…ë°ì´íŠ¸ ì˜ì¡´
        ì™„í™”:
            - êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° API ì—°ë™
            - ì¼ì¼ ë°°ì¹˜ ì—…ë°ì´íŠ¸
            - ë³€ê²½ ì•Œë¦¼ ì‹œìŠ¤í…œ`
    ],
    keyPoints: [
      'ê³ ê° ìš”êµ¬ì‚¬í•­ ë¶„ì„ â†’ ê¸°ìˆ  ì„ íƒ ê·¼ê±° ë„ì¶œ',
      'í”„ë¡œë•ì…˜ ì•„í‚¤í…ì²˜ ì„¤ê³„ (ìºì‹±, ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)',
      'í˜„ì‹¤ì ì¸ ë¹„ìš© ì¶”ì • ë° ì˜ˆì‚° ì¤€ìˆ˜',
      'ë¦¬ìŠ¤í¬ ì‹ë³„ ë° ì„ ì œì  ì™„í™” ë°©ì•ˆ',
    ],
    practiceGoal: 'FDEë¡œì„œ ê³ ê°ì—ê²Œ RAG ì†”ë£¨ì…˜ì„ ì œì•ˆí•˜ëŠ” ì„¤ê³„ì„œë¥¼ ì‘ì„±í•œë‹¤',
  }),
}
