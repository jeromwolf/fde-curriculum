// Day 6: ë¡œì»¬ LLMê³¼ RAG í†µí•© (sLLM)

import type { Day } from '../../types'
import {
  createVideoTask,
  createReadingTask,
  createCodeTask,
  createQuizTask,
  createChallengeTask,
} from './types'

export const day6SllmRag: Day = {
  slug: 'sllm-rag',
  title: 'ë¡œì»¬ LLMê³¼ RAG í†µí•© (sLLM)',
  totalDuration: 300,
  tasks: [
    // ========================================
    // Task 1: sLLM ê°œìš”ì™€ ì„ íƒ ê°€ì´ë“œ (40ë¶„)
    // ========================================
    createVideoTask('w5d6-sllm-overview', 'sLLM ê°œìš”ì™€ ì„ íƒ ê°€ì´ë“œ', 40, {
      introduction: `
## sLLMì´ë€?

**sLLM (Small Language Model)** ë˜ëŠ” **Local LLM**ì€ ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ê²½ëŸ‰í™”ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.

> "GPT-4ì˜ 90% ì„±ëŠ¥ì„ 10%ì˜ ë¹„ìš©ìœ¼ë¡œ"
> â€” 2024ë…„ sLLM í˜ëª…ì˜ í•µì‹¬ ë©”ì‹œì§€

## ì™œ sLLMì¸ê°€?

| ê´€ì  | Cloud LLM (GPT-4, Claude) | sLLM (Llama, Mistral) |
|------|---------------------------|----------------------|
| **ë¹„ìš©** | í† í°ë‹¹ ê³¼ê¸ˆ ($15-60/1M) | ì „ê¸°ë£Œë§Œ (ê±°ì˜ ë¬´ë£Œ) |
| **ë°ì´í„° ë³´ì•ˆ** | ì™¸ë¶€ ì„œë²„ ì „ì†¡ | ì™„ì „ ë¡œì»¬ ì²˜ë¦¬ |
| **ë ˆì´í„´ì‹œ** | ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ (200-500ms) | ë¡œì»¬ ì²˜ë¦¬ (50-100ms) |
| **ì»¤ìŠ¤í„°ë§ˆì´ì§•** | ì œí•œì  (í”„ë¡¬í”„íŠ¸ë§Œ) | íŒŒì¸íŠœë‹ ê°€ëŠ¥ |
| **ì˜¤í”„ë¼ì¸** | ë¶ˆê°€ëŠ¥ | ì™„ì „ ì§€ì› |
| **ì„±ëŠ¥** | ìµœê³  ìˆ˜ì¤€ | 80-95% ìˆ˜ì¤€ |

## 2024-2025 ì£¼ìš” sLLM ë¹„êµ

### ë²”ìš© ëª¨ë¸ (General Purpose)

| ëª¨ë¸ | í¬ê¸° | ì¶œì‹œ | MMLU | í•œêµ­ì–´ | ë¼ì´ì„ ìŠ¤ |
|------|------|------|------|--------|----------|
| **Llama 3.2** | 1B, 3B | 2024.09 | 66.6 | â­â­ | Llama 3.2 License |
| **Llama 3.1** | 8B, 70B | 2024.07 | 73.0 | â­â­â­ | Llama 3.1 License |
| **Mistral 7B** | 7B | 2023.09 | 62.5 | â­â­ | Apache 2.0 |
| **Mixtral 8x7B** | 46.7B* | 2023.12 | 70.6 | â­â­â­ | Apache 2.0 |
| **Qwen2.5** | 0.5B-72B | 2024.09 | 74.2 | â­â­â­â­ | Qwen License |
| **Phi-3.5** | 3.8B | 2024.08 | 69.0 | â­â­ | MIT |
| **Gemma 2** | 2B, 9B, 27B | 2024.06 | 71.3 | â­â­â­ | Gemma License |

> *Mixtralì€ MoE(Mixture of Experts) êµ¬ì¡°ë¡œ ì‹¤ì œ í™œì„± íŒŒë¼ë¯¸í„°ëŠ” 12.9B

### ì½”ë”© íŠ¹í™” ëª¨ë¸

| ëª¨ë¸ | í¬ê¸° | HumanEval | íŠ¹ì§• |
|------|------|-----------|------|
| **CodeLlama** | 7B, 13B, 34B | 53.7% | Metaì˜ ì½”ë“œ íŠ¹í™” |
| **DeepSeek Coder** | 1.3B-33B | 65.2% | ìµœê³  ìˆ˜ì¤€ ì½”ë“œ ìƒì„± |
| **StarCoder2** | 3B, 7B, 15B | 46.3% | BigCode í”„ë¡œì íŠ¸ |
| **Qwen2.5-Coder** | 1.5B-32B | 61.6% | ì½”ë“œ+ìì—°ì–´ ê²¸ìš© |

### í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸

| ëª¨ë¸ | í¬ê¸° | í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ | íŠ¹ì§• |
|------|------|-----------------|------|
| **SOLAR** | 10.7B | KoBEST ìµœìƒìœ„ | ì—…ìŠ¤í…Œì´ì§€ ê°œë°œ |
| **KULLM3** | 8B | ìš°ìˆ˜ | ê³ ë ¤ëŒ€í•™êµ ê°œë°œ |
| **Polyglot-Ko** | 1.3B-12.8B | ì–‘í˜¸ | EleutherAI í•œêµ­ì–´ |
| **KoAlpaca** | 5.8B, 12.8B | ì–‘í˜¸ | Polyglot ê¸°ë°˜ |

## ëª¨ë¸ ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬

<svg viewBox="0 0 900 370" xmlns="http://www.w3.org/2000/svg" style="max-width: 100%; height: auto; background: #1e293b; border-radius: 12px;">
  <defs>
    <linearGradient id="blueGrad1" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#3b82f6"/>
      <stop offset="100%" style="stop-color:#2563eb"/>
    </linearGradient>
    <linearGradient id="purpleGrad1" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#8b5cf6"/>
      <stop offset="100%" style="stop-color:#7c3aed"/>
    </linearGradient>
  </defs>
  <rect x="350" y="15" width="200" height="50" rx="10" fill="url(#blueGrad1)"/>
  <text x="450" y="47" text-anchor="middle" fill="white" font-size="15" font-weight="bold">ì–´ë–¤ ìš©ë„ì¸ê°€ìš”?</text>
  <path d="M 400 65 Q 400 85 150 108" stroke="#64748b" stroke-width="2" fill="none"/>
  <path d="M 450 65 L 450 108" stroke="#64748b" stroke-width="2"/>
  <path d="M 500 65 Q 500 85 750 108" stroke="#64748b" stroke-width="2" fill="none"/>
  <rect x="70" y="108" width="160" height="44" rx="8" fill="url(#purpleGrad1)"/>
  <text x="150" y="136" text-anchor="middle" fill="white" font-size="14" font-weight="bold">ë²”ìš© ì±—ë´‡</text>
  <rect x="370" y="108" width="160" height="44" rx="8" fill="url(#purpleGrad1)"/>
  <text x="450" y="136" text-anchor="middle" fill="white" font-size="14" font-weight="bold">ì½”ë“œ ìƒì„±</text>
  <rect x="670" y="108" width="160" height="44" rx="8" fill="url(#purpleGrad1)"/>
  <text x="750" y="136" text-anchor="middle" fill="white" font-size="14" font-weight="bold">í•œêµ­ì–´</text>
  <path d="M 110 152 L 75 200" stroke="#64748b" stroke-width="2"/>
  <path d="M 190 152 L 220 200" stroke="#64748b" stroke-width="2"/>
  <path d="M 410 152 L 375 200" stroke="#64748b" stroke-width="2"/>
  <path d="M 490 152 L 520 200" stroke="#64748b" stroke-width="2"/>
  <path d="M 710 152 L 675 200" stroke="#64748b" stroke-width="2"/>
  <path d="M 790 152 L 820 200" stroke="#64748b" stroke-width="2"/>
  <rect x="20" y="200" width="110" height="38" rx="6" fill="#f59e0b"/>
  <text x="75" y="225" text-anchor="middle" fill="white" font-size="12" font-weight="600">GPU ì—†ìŒ</text>
  <rect x="165" y="200" width="110" height="38" rx="6" fill="#10b981"/>
  <text x="220" y="225" text-anchor="middle" fill="white" font-size="12" font-weight="600">GPU ìˆìŒ</text>
  <rect x="320" y="200" width="110" height="38" rx="6" fill="#f59e0b"/>
  <text x="375" y="225" text-anchor="middle" fill="white" font-size="12" font-weight="600">GPU ì—†ìŒ</text>
  <rect x="465" y="200" width="110" height="38" rx="6" fill="#10b981"/>
  <text x="520" y="225" text-anchor="middle" fill="white" font-size="12" font-weight="600">GPU ìˆìŒ</text>
  <rect x="620" y="200" width="110" height="38" rx="6" fill="#ec4899"/>
  <text x="675" y="225" text-anchor="middle" fill="white" font-size="12" font-weight="600">ìµœê³ ì„±ëŠ¥</text>
  <rect x="765" y="200" width="110" height="38" rx="6" fill="#06b6d4"/>
  <text x="820" y="225" text-anchor="middle" fill="white" font-size="12" font-weight="600">ê²½ëŸ‰</text>
  <line x1="75" y1="238" x2="75" y2="285" stroke="#64748b" stroke-width="2"/>
  <line x1="220" y1="238" x2="220" y2="285" stroke="#64748b" stroke-width="2"/>
  <line x1="375" y1="238" x2="375" y2="285" stroke="#64748b" stroke-width="2"/>
  <line x1="520" y1="238" x2="520" y2="285" stroke="#64748b" stroke-width="2"/>
  <line x1="675" y1="238" x2="675" y2="285" stroke="#64748b" stroke-width="2"/>
  <line x1="820" y1="238" x2="820" y2="285" stroke="#64748b" stroke-width="2"/>
  <rect x="20" y="285" width="110" height="60" rx="8" fill="#1e3a5f" stroke="#3b82f6" stroke-width="2"/>
  <text x="75" y="310" text-anchor="middle" fill="white" font-size="13" font-weight="bold">Phi-3.5</text>
  <text x="75" y="330" text-anchor="middle" fill="#93c5fd" font-size="11">3.8B</text>
  <rect x="165" y="285" width="110" height="60" rx="8" fill="#1e3a5f" stroke="#3b82f6" stroke-width="2"/>
  <text x="220" y="310" text-anchor="middle" fill="white" font-size="13" font-weight="bold">Llama 3.1</text>
  <text x="220" y="330" text-anchor="middle" fill="#93c5fd" font-size="11">8B / 70B</text>
  <rect x="320" y="285" width="110" height="60" rx="8" fill="#1e3a5f" stroke="#3b82f6" stroke-width="2"/>
  <text x="375" y="310" text-anchor="middle" fill="white" font-size="13" font-weight="bold">StarCoder2</text>
  <text x="375" y="330" text-anchor="middle" fill="#93c5fd" font-size="11">3B</text>
  <rect x="465" y="285" width="110" height="60" rx="8" fill="#1e3a5f" stroke="#3b82f6" stroke-width="2"/>
  <text x="520" y="310" text-anchor="middle" fill="white" font-size="13" font-weight="bold">DeepSeek</text>
  <text x="520" y="330" text-anchor="middle" fill="#93c5fd" font-size="11">Coder</text>
  <rect x="620" y="285" width="110" height="60" rx="8" fill="#1e3a5f" stroke="#3b82f6" stroke-width="2"/>
  <text x="675" y="310" text-anchor="middle" fill="white" font-size="13" font-weight="bold">SOLAR</text>
  <text x="675" y="330" text-anchor="middle" fill="#93c5fd" font-size="11">10.7B</text>
  <rect x="765" y="285" width="110" height="60" rx="8" fill="#1e3a5f" stroke="#3b82f6" stroke-width="2"/>
  <text x="820" y="310" text-anchor="middle" fill="white" font-size="13" font-weight="bold">Polyglot</text>
  <text x="820" y="330" text-anchor="middle" fill="#93c5fd" font-size="11">5.8B</text>
</svg>

## FDE ê´€ì : sLLMì´ í•„ìš”í•œ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤

| ì‹œë‚˜ë¦¬ì˜¤ | ì™œ sLLM? | ì¶”ì²œ ëª¨ë¸ |
|----------|----------|-----------|
| **ê¸ˆìœµ ë¬¸ì„œ RAG** | ë¯¼ê° ë°ì´í„° ì™¸ë¶€ ì „ì†¡ ë¶ˆê°€ | Llama 3.1 8B |
| **ì‚¬ë‚´ ì½”ë“œ ë¶„ì„** | ì†ŒìŠ¤ì½”ë“œ ë³´ì•ˆ | DeepSeek Coder |
| **ì˜¤í”„ë¼ì¸ í™˜ê²½** | ì¸í„°ë„· ì—†ëŠ” í™˜ê²½ (ê³µì¥, ì„ ë°•) | Phi-3.5 (ê²½ëŸ‰) |
| **ë¹„ìš© ìµœì í™”** | ì›” ìˆ˜ì²œ ë‹¬ëŸ¬ API ë¹„ìš© ì ˆê° | Mistral 7B |
| **ë¹ ë¥¸ ì‘ë‹µ** | ì‹¤ì‹œê°„ ëŒ€ì‘ í•„ìš” | Llama 3.2 3B |

> **ì´ë²ˆ Dayì˜ í•™ìŠµ ë¡œë“œë§µ**: Task 1ì—ì„œ ëª¨ë¸ ì´í•´ â†’ Task 2-3ì—ì„œ ì„¤ì¹˜ â†’ Task 4ì—ì„œ RAG í†µí•© â†’ Task 5ì—ì„œ í”„ë¡œë•ì…˜ ë°°í¬
`,
      keyPoints: [
        'sLLMì€ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ëŸ‰ LLM',
        'Llama 3.1, Mistral, Qwenì´ 2024ë…„ ì£¼ìš” ëª¨ë¸',
        'ìš©ë„(ë²”ìš©/ì½”ë“œ/í•œêµ­ì–´)ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ',
        'ë³´ì•ˆ, ë¹„ìš©, ë ˆì´í„´ì‹œê°€ sLLM ì„ íƒì˜ í•µì‹¬ ì´ìœ ',
      ],
    }),

    // ========================================
    // Task 2: ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ê³¼ GPU ì´í•´ (50ë¶„)
    // ========================================
    createReadingTask('w5d6-system-requirements', 'ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ê³¼ GPU ì´í•´', 50, {
      introduction: `
## LLM ì‹¤í–‰ì„ ìœ„í•œ í•˜ë“œì›¨ì–´ ì´í•´

sLLMì„ ë¡œì»¬ì—ì„œ ì‹¤í–‰í•˜ë ¤ë©´ **GPU VRAM**ì´ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤.

### ì™œ VRAMì´ ì¤‘ìš”í•œê°€?

<svg viewBox="0 0 600 420" xmlns="http://www.w3.org/2000/svg" style="max-width: 100%; height: auto; background: #1e293b; border-radius: 12px;">
  <defs>
    <linearGradient id="memBlueGrad" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#3b82f6"/>
      <stop offset="100%" style="stop-color:#2563eb"/>
    </linearGradient>
    <linearGradient id="memGreenGrad" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#22c55e"/>
      <stop offset="100%" style="stop-color:#16a34a"/>
    </linearGradient>
    <linearGradient id="memPurpleGrad" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#8b5cf6"/>
      <stop offset="100%" style="stop-color:#7c3aed"/>
    </linearGradient>
  </defs>
  <text x="300" y="35" text-anchor="middle" fill="#f1f5f9" font-size="18" font-weight="bold">LLM ë©”ëª¨ë¦¬ êµ¬ì¡°</text>
  <rect x="50" y="60" width="500" height="110" rx="8" fill="url(#memBlueGrad)" opacity="0.9"/>
  <text x="300" y="90" text-anchor="middle" fill="#ffffff" font-size="14" font-weight="bold">Model Weights (ê³ ì •)</text>
  <text x="80" y="115" fill="#e2e8f0" font-size="12">â€¢ 7B ëª¨ë¸ = ~14GB (FP16)</text>
  <text x="80" y="135" fill="#e2e8f0" font-size="12">â€¢ 13B ëª¨ë¸ = ~26GB (FP16)</text>
  <text x="80" y="155" fill="#e2e8f0" font-size="12">â€¢ 70B ëª¨ë¸ = ~140GB (FP16)</text>
  <text x="300" y="190" text-anchor="middle" fill="#94a3b8" font-size="24" font-weight="bold">+</text>
  <rect x="50" y="205" width="500" height="100" rx="8" fill="url(#memGreenGrad)" opacity="0.9"/>
  <text x="300" y="235" text-anchor="middle" fill="#ffffff" font-size="14" font-weight="bold">KV Cache (ë™ì )</text>
  <text x="80" y="260" fill="#e2e8f0" font-size="12">â€¢ Context ê¸¸ì´ì— ë¹„ë¡€</text>
  <text x="80" y="280" fill="#e2e8f0" font-size="12">â€¢ ë°°ì¹˜ í¬ê¸°ì— ë¹„ë¡€  â†’  ì•½ 1-4GB ì¶”ê°€</text>
  <text x="300" y="330" text-anchor="middle" fill="#94a3b8" font-size="24" font-weight="bold">=</text>
  <rect x="50" y="345" width="500" height="60" rx="8" fill="url(#memPurpleGrad)" opacity="0.9"/>
  <text x="300" y="382" text-anchor="middle" fill="#ffffff" font-size="14" font-weight="bold">ì´ í•„ìš” VRAM: 7B ëª¨ë¸ ì‹¤í–‰ = ì•½ 16-18GB</text>
</svg>

### VRAM ê³„ì‚° ê³µì‹

\`\`\`
í•„ìš” VRAM (FP16) = íŒŒë¼ë¯¸í„° ìˆ˜ Ã— 2 bytes + 2~4GB (KV Cache, ì˜¤ë²„í—¤ë“œ)

ì˜ˆì‹œ:
- 7B ëª¨ë¸: 7 Ã— 10^9 Ã— 2 = 14GB + 2GB = ~16GB
- 13B ëª¨ë¸: 13 Ã— 10^9 Ã— 2 = 26GB + 2GB = ~28GB
- 70B ëª¨ë¸: 70 Ã— 10^9 Ã— 2 = 140GB + 4GB = ~144GB
\`\`\`

## ì–‘ìí™”(Quantization)ì˜ ë§ˆë²•

ì–‘ìí™”ëŠ” ëª¨ë¸ ê°€ì¤‘ì¹˜ì˜ ì •ë°€ë„ë¥¼ ë‚®ì¶° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

| ì–‘ìí™” ìˆ˜ì¤€ | ë©”ëª¨ë¦¬ ë°°ìˆ˜ | í’ˆì§ˆ ì†ì‹¤ | 7B ëª¨ë¸ VRAM |
|-------------|-------------|-----------|--------------|
| **FP16** (ê¸°ë³¸) | 2 bytes | ì—†ìŒ | 14GB |
| **INT8** | 1 byte | ìµœì†Œ (~1%) | 7GB |
| **INT4** (Q4) | 0.5 bytes | ì†Œí­ (~3%) | 3.5GB |
| **GGUF Q4_K_M** | ~0.5 bytes | ì†Œí­ (~2%) | 4GB |
| **GGUF Q2_K** | ~0.3 bytes | ìƒë‹¹ (~10%) | 2GB |

> **ì‹¤ë¬´ ê¶Œì¥**: Q4_K_Mì´ í’ˆì§ˆ/í¬ê¸° ê· í˜•ì´ ê°€ì¥ ì¢‹ìŒ

### GGUF í¬ë§· ì´í•´

**GGUF (GPT-Generated Unified Format)**ëŠ” llama.cppì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–‘ìí™” í¬ë§·ì…ë‹ˆë‹¤.

\`\`\`
GGUF íŒŒì¼ëª… ê·œì¹™:
[ëª¨ë¸ëª…]-[í¬ê¸°]-[ì–‘ìí™”].gguf

ì˜ˆì‹œ:
- llama-3.1-8b-instruct-q4_k_m.gguf
  â””â”€ Llama 3.1 8B, Q4_K_M ì–‘ìí™”

- mistral-7b-instruct-v0.3-q8_0.gguf
  â””â”€ Mistral 7B v0.3, INT8 ì–‘ìí™”
\`\`\`

| GGUF ì–‘ìí™” | ì„¤ëª… | ê¶Œì¥ ìš©ë„ |
|-------------|------|-----------|
| **Q2_K** | 2bit, ê·¹ë‹¨ì  ì••ì¶• | í…ŒìŠ¤íŠ¸ìš© |
| **Q4_0** | 4bit, ê¸°ë³¸ | ê°€ë²¼ìš´ ì‹¤í—˜ |
| **Q4_K_M** | 4bit, ê°œì„ ë¨ | **í”„ë¡œë•ì…˜ ê¶Œì¥** |
| **Q5_K_M** | 5bit, í’ˆì§ˆ ìš°ì„  | í’ˆì§ˆ ì¤‘ìš” ì‹œ |
| **Q6_K** | 6bit, ê³ í’ˆì§ˆ | VRAM ì—¬ìœ  ì‹œ |
| **Q8_0** | 8bit, ìµœê³  í’ˆì§ˆ | ìµœëŒ€ í’ˆì§ˆ í•„ìš” |
| **F16** | 16bit, ë¬´ì†ì‹¤ | íŒŒì¸íŠœë‹ìš© |

## í•˜ë“œì›¨ì–´ë³„ ê¶Œì¥ ëª¨ë¸

### NVIDIA GPU

| GPU | VRAM | ê¶Œì¥ ëª¨ë¸ | ë¹„ê³  |
|-----|------|-----------|------|
| **RTX 3060** | 12GB | Llama 3.2 3B, Phi-3.5 | Q4 ì–‘ìí™” í•„ìˆ˜ |
| **RTX 3080** | 10GB | Mistral 7B Q4 | ë°°ì¹˜ 1ê°œ |
| **RTX 3090** | 24GB | Llama 3.1 8B Q8 | ì—¬ìœ  ìˆìŒ |
| **RTX 4070** | 12GB | Llama 3.2 3B, Qwen2.5 7B Q4 | íš¨ìœ¨ì  |
| **RTX 4080** | 16GB | Llama 3.1 8B Q4 | ê¶Œì¥ |
| **RTX 4090** | 24GB | Llama 3.1 8B Q8, 13B Q4 | ìµœì  |
| **A100** | 40/80GB | 70B ëª¨ë¸ Q4-Q8 | ì„œë²„ìš© |
| **H100** | 80GB | 70B ëª¨ë¸ F16 | ìµœê³  ì„±ëŠ¥ |

### Apple Silicon

| ì¹© | í†µí•© ë©”ëª¨ë¦¬ | ê¶Œì¥ ëª¨ë¸ | ë¹„ê³  |
|----|------------|-----------|------|
| **M1** | 8-16GB | Phi-3.5 3.8B Q4 | ì œí•œì  |
| **M1 Pro/Max** | 16-64GB | Llama 3.1 8B Q4 | ê´œì°®ìŒ |
| **M2** | 8-24GB | Mistral 7B Q4 | ì–‘í˜¸ |
| **M2 Pro/Max** | 16-96GB | Llama 3.1 8B Q8 | ì¢‹ìŒ |
| **M3 Pro/Max** | 18-128GB | 70B Q4 ê°€ëŠ¥ | í›Œë¥­í•¨ |
| **M4 Pro/Max** | 24-128GB | 70B Q8 ê°€ëŠ¥ | ìµœê³  |

> Apple Siliconì€ CPUì™€ GPUê°€ í†µí•© ë©”ëª¨ë¦¬ë¥¼ ê³µìœ í•˜ë¯€ë¡œ VRAMì´ ì•„ë‹Œ ì „ì²´ RAMì´ ì¤‘ìš”

### CPU Only (GPU ì—†ìŒ)

| ì„¤ì • | RAM | ê¶Œì¥ ëª¨ë¸ | ì„±ëŠ¥ |
|------|-----|-----------|------|
| **ìµœì†Œ** | 8GB | Phi-3 Mini Q4 | ë§¤ìš° ëŠë¦¼ |
| **ê¶Œì¥** | 16GB | Llama 3.2 1B Q4 | ëŠë¦¼ |
| **ìµœì ** | 32GB | Llama 3.2 3B Q4 | ì‚¬ìš© ê°€ëŠ¥ |

> CPU ì „ìš©ì€ ì¶”ë¡  ì†ë„ê°€ 10-50x ëŠë¦¼. ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ ê¶Œì¥

## í´ë¼ìš°ë“œ GPU ì˜µì…˜

ë¡œì»¬ GPUê°€ ì—†ë‹¤ë©´ í´ë¼ìš°ë“œ GPUë¥¼ ê³ ë ¤í•˜ì„¸ìš”.

| ì„œë¹„ìŠ¤ | GPU | ì‹œê°„ë‹¹ ë¹„ìš© | íŠ¹ì§• |
|--------|-----|-------------|------|
| **RunPod** | RTX 4090 | $0.44 | ì €ë ´, ê°„í¸ |
| **Vast.ai** | RTX 4090 | $0.30-0.50 | ìµœì €ê°€ |
| **Lambda Labs** | A100 | $1.10 | ì•ˆì •ì  |
| **AWS (g5.xlarge)** | A10G | $1.00 | ê¸°ì—…ìš© |
| **GCP (a2-highgpu)** | A100 | $3.67 | ì—”í„°í”„ë¼ì´ì¦ˆ |
| **Google Colab Pro** | T4/A100 | $10/ì›” | í•™ìŠµìš© |

## ì‹¤ìŠµ í™˜ê²½ ì²´í¬ë¦¬ìŠ¤íŠ¸

\`\`\`bash
# 1. GPU í™•ì¸ (NVIDIA)
nvidia-smi

# ì¶œë ¥ ì˜ˆì‹œ:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 535.104.05   Driver Version: 535.104.05   CUDA Version: 12.2    |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |===============================+======================+======================|
# |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
# | 30%   35C    P8    15W / 350W |    500MiB / 24564MiB |      0%      Default |
# +-------------------------------+----------------------+----------------------+

# 2. GPU í™•ì¸ (Apple Silicon)
system_profiler SPDisplaysDataType

# 3. RAM í™•ì¸ (Linux/Mac)
free -h  # Linux
sysctl hw.memsize  # Mac

# 4. ì €ì¥ê³µê°„ í™•ì¸
df -h

# ëª¨ë¸ ì €ì¥ ê³µê°„ í•„ìš”:
# - 7B Q4: ~4GB
# - 7B Q8: ~7GB
# - 13B Q4: ~8GB
# - 70B Q4: ~40GB
\`\`\`
`,
      keyPoints: [
        'VRAM = íŒŒë¼ë¯¸í„° Ã— 2 (FP16) + KV Cache',
        'Q4_K_M ì–‘ìí™”ê°€ í’ˆì§ˆ/í¬ê¸° ìµœì  ê· í˜•',
        'RTX 4080/4090ì´ ë¡œì»¬ LLM ìµœì  GPU',
        'Apple Siliconì€ í†µí•© ë©”ëª¨ë¦¬ë¡œ ìœ ë¦¬',
      ],
    }),

    // ========================================
    // Task 3: Ollama ì„¤ì¹˜ì™€ ëª¨ë¸ ê´€ë¦¬ (45ë¶„)
    // ========================================
    createCodeTask('w5d6-ollama-setup', 'Ollama ì„¤ì¹˜ì™€ ëª¨ë¸ ê´€ë¦¬', 45, {
      introduction: `
## ğŸ¯ ì™œ Ollamaê°€ í•„ìš”í•œê°€?

### ë¬¸ì œ ìƒí™©
sLLMì„ ì§ì ‘ ì„¤ì¹˜í•˜ë ¤ë©´:
- ğŸ”§ PyTorch, CUDA, cuDNN ë²„ì „ ë§ì¶”ê¸°
- ğŸ“¦ ëª¨ë¸ íŒŒì¼ ì°¾ê³  ë‹¤ìš´ë¡œë“œí•˜ê¸°
- âš™ï¸ ì–‘ìí™”, ë©”ëª¨ë¦¬ ì„¤ì • ì§ì ‘ ì¡°ì •í•˜ê¸°
- ğŸ˜“ "ì™œ ì•ˆ ë˜ì§€?" ì—ëŸ¬ì™€ ì‹¸ìš°ê¸°...

### í•´ê²°ì±…: Ollama
> ğŸ³ **ë¹„ìœ **: ë¼ë©´ ë“ì´ê¸° vs ìš”ë¦¬ ì¬ë£Œ ì‚¬ê¸°
>
> ì§ì ‘ ì„¤ì¹˜ = ë°€ê°€ë£¨ë¶€í„° ë©´ ë½‘ê¸°
> Ollama = ë¼ë©´ ë´‰ì§€ ëœ¯ê³  ë¬¼ ë“ì´ê¸°

**Ollama**ëŠ” ë¡œì»¬ LLMì„ í•œ ì¤„ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

| ì§ì ‘ ì„¤ì¹˜ | Ollama ì‚¬ìš© |
|----------|-------------|
| PyTorch ì„¤ì¹˜ + CUDA ì„¤ì • | \`brew install ollama\` |
| HuggingFaceì—ì„œ ëª¨ë¸ ê²€ìƒ‰ | \`ollama pull llama3.1\` |
| ì–‘ìí™” ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ | ìë™ ìµœì í™” |
| API ì„œë²„ ì§ì ‘ êµ¬ì¶• | \`ollama serve\` â†’ ë°”ë¡œ ì‚¬ìš©

## ì„¤ì¹˜ ê°€ì´ë“œ

### macOS

\`\`\`bash
# ë°©ë²• 1: ê³µì‹ ì¸ìŠ¤í†¨ëŸ¬ (ê¶Œì¥)
# https://ollama.ai/download ì—ì„œ ë‹¤ìš´ë¡œë“œ
# â†’ ì•± ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ì„œë²„ ì‹œì‘ë¨

# ë°©ë²• 2: Homebrew
brew install ollama

# ì„¤ì¹˜ í™•ì¸
ollama --version
# ollama version is 0.4.x
\`\`\`

**ì¤‘ìš”: Ollama ì„œë²„ ì‹¤í–‰**

\`\`\`bash
# Homebrewë¡œ ì„¤ì¹˜í•œ ê²½ìš° ì„œë²„ë¥¼ ì§ì ‘ ì‹œì‘í•´ì•¼ í•¨
ollama serve

# ì¶œë ¥:
# Couldn't find '~/.ollama/id_ed25519'. Generating new private key.
# time=... level=INFO msg="starting llama server" ...
# time=... level=INFO msg="llama runner started" ...

# ì„œë²„ê°€ ì‹¤í–‰ë˜ë©´ ìƒˆ í„°ë¯¸ë„ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ì‹¤í–‰
# (ì„œë²„ëŠ” ê³„ì† ì‹¤í–‰ ìƒíƒœ ìœ ì§€)
\`\`\`

> **Tip**: ê³µì‹ ì¸ìŠ¤í†¨ëŸ¬(.dmg)ë¡œ ì„¤ì¹˜í•˜ë©´ ë©”ë‰´ë°”ì— Ollama ì•„ì´ì½˜ì´ ìƒê¸°ê³  ìë™ìœ¼ë¡œ ì„œë²„ê°€ ì‹¤í–‰ë©ë‹ˆë‹¤. Homebrew ì„¤ì¹˜ëŠ” \`ollama serve\`ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

### Linux

\`\`\`bash
# ê³µì‹ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
curl -fsSL https://ollama.com/install.sh | sh

# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start ollama
sudo systemctl enable ollama

# ì„¤ì¹˜ í™•ì¸
ollama --version
\`\`\`

### Windows

\`\`\`powershell
# ë°©ë²• 1: ê³µì‹ ì¸ìŠ¤í†¨ëŸ¬
# https://ollama.ai/download ì—ì„œ OllamaSetup.exe ë‹¤ìš´ë¡œë“œ

# ë°©ë²• 2: winget
winget install Ollama.Ollama

# ì„¤ì¹˜ í™•ì¸
ollama --version
\`\`\`

### Docker

\`\`\`bash
# CPU ì „ìš©
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# NVIDIA GPU ì§€ì›
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
\`\`\`

## ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê´€ë¦¬

### ê¸°ë³¸ ëª…ë ¹ì–´

\`\`\`bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.2        # Llama 3.2 3B (ê¸°ë³¸)
ollama pull llama3.1        # Llama 3.1 8B
ollama pull mistral         # Mistral 7B
ollama pull qwen2.5         # Qwen 2.5 7B
ollama pull phi3.5          # Phi-3.5 3.8B
ollama pull codellama       # CodeLlama 7B
ollama pull deepseek-coder  # DeepSeek Coder

# íŠ¹ì • í¬ê¸° ì§€ì •
ollama pull llama3.1:70b    # 70B ë²„ì „
ollama pull qwen2.5:0.5b    # 0.5B ê²½ëŸ‰ ë²„ì „

# ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡
ollama list

# ì¶œë ¥ ì˜ˆì‹œ:
# NAME              ID              SIZE    MODIFIED
# llama3.2:latest   a80c4f17acd5    2.0 GB  2 days ago
# llama3.1:latest   42182419e950    4.7 GB  1 day ago
# mistral:latest    f974a74358d6    4.1 GB  3 days ago

# ëª¨ë¸ ì‚­ì œ
ollama rm llama3.2

# ëª¨ë¸ ì •ë³´ í™•ì¸
ollama show llama3.1
\`\`\`

### ëª¨ë¸ ì‹¤í–‰

\`\`\`bash
# ëŒ€í™”í˜• ëª¨ë“œ
ollama run llama3.1

# ì¶œë ¥:
# >>> ì•ˆë…•í•˜ì„¸ìš”!
# ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?

# ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
ollama run llama3.1 "RAGë€ ë¬´ì—‡ì¸ê°€ìš”? í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜
ollama run llama3.1 --system "ë‹¹ì‹ ì€ í•œêµ­ì–´ ê¸°ìˆ  ë¬¸ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
\`\`\`

## Ollama REST API

OllamaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ \`http://localhost:11434\`ì—ì„œ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### API ì—”ë“œí¬ì¸íŠ¸

\`\`\`bash
# 1. í…ìŠ¤íŠ¸ ìƒì„±
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1",
  "prompt": "RAGì˜ í•µì‹¬ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
  "stream": false
}'

# ì‘ë‹µ:
# {
#   "model": "llama3.1",
#   "response": "RAG(Retrieval-Augmented Generation)ëŠ”...",
#   "done": true,
#   "total_duration": 5432123456,
#   "load_duration": 123456789,
#   "prompt_eval_count": 12,
#   "eval_count": 156
# }

# 2. ì±„íŒ… API (OpenAI í˜¸í™˜)
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.1",
  "messages": [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
    {"role": "user", "content": "RAGë€?"}
  ],
  "stream": false
}'

# 3. ì„ë² ë”© ìƒì„±
curl http://localhost:11434/api/embeddings -d '{
  "model": "llama3.1",
  "prompt": "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤."
}'

# 4. ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
curl http://localhost:11434/api/tags

# 5. ëª¨ë¸ ì •ë³´ ì¡°íšŒ
curl http://localhost:11434/api/show -d '{"name": "llama3.1"}'
\`\`\`

### OpenAI í˜¸í™˜ API

OllamaëŠ” OpenAI SDKì™€ í˜¸í™˜ë˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ë„ ì œê³µí•©ë‹ˆë‹¤.

\`\`\`python
from openai import OpenAI

# Ollamaë¥¼ OpenAI í´ë¼ì´ì–¸íŠ¸ë¡œ ì‚¬ìš©
client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # ì•„ë¬´ ê°’ì´ë‚˜ OK
)

# ì±„íŒ… ì™„ì„±
response = client.chat.completions.create(
    model="llama3.1",
    messages=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ RAG ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "RAGì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."}
    ],
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].message.content)
\`\`\`

## Pythonì—ì„œ Ollama ì‚¬ìš©

### ollama íŒ¨í‚¤ì§€

\`\`\`python
# ì„¤ì¹˜
# pip install ollama

import ollama

# 1. ê¸°ë³¸ ìƒì„±
response = ollama.generate(
    model='llama3.1',
    prompt='RAGë€ ë¬´ì—‡ì¸ê°€ìš”?'
)
print(response['response'])

# 2. ì±„íŒ…
response = ollama.chat(
    model='llama3.1',
    messages=[
        {'role': 'system', 'content': 'ë‹¹ì‹ ì€ AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.'},
        {'role': 'user', 'content': 'sLLMì˜ ì¥ì ì€?'}
    ]
)
print(response['message']['content'])

# 3. ìŠ¤íŠ¸ë¦¬ë°
stream = ollama.chat(
    model='llama3.1',
    messages=[{'role': 'user', 'content': 'Pythonì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.'}],
    stream=True
)

for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)

# 4. ì„ë² ë”©
embeddings = ollama.embeddings(
    model='llama3.1',
    prompt='RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤.'
)
print(f"ì„ë² ë”© ì°¨ì›: {len(embeddings['embedding'])}")
\`\`\`

## ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„± (Modelfile)

OllamaëŠ” \`Modelfile\`ì„ í†µí•´ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

\`\`\`dockerfile
# Modelfile
FROM llama3.1

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM """
ë‹¹ì‹ ì€ FDE(Forward Deployed Engineer) êµìœ¡ì„ ìœ„í•œ AI íŠœí„°ì…ë‹ˆë‹¤.
í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ê³ , ì‹¤ë¬´ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
ì½”ë“œ ì˜ˆì œë¥¼ í¬í•¨í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""

# íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096
PARAMETER stop "<|eot_id|>"

# í…œí”Œë¦¿ ì»¤ìŠ¤í„°ë§ˆì´ì§• (ì„ íƒì‚¬í•­)
TEMPLATE """
{{ if .System }}<|start_header_id|>system<|end_header_id|>
{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>
{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>
{{ .Response }}<|eot_id|>
"""
\`\`\`

\`\`\`bash
# ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±
ollama create fde-tutor -f Modelfile

# ëª¨ë¸ ì‚¬ìš©
ollama run fde-tutor "RAG íŒŒì´í”„ë¼ì¸ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."

# ëª¨ë¸ ë°°í¬ (Ollama Hubì— í‘¸ì‹œ)
ollama push yourusername/fde-tutor
\`\`\`

## ì„±ëŠ¥ ìµœì í™” íŒ

\`\`\`bash
# 1. GPU ë©”ëª¨ë¦¬ í• ë‹¹ í™•ì¸
OLLAMA_GPU_LAYERS=35 ollama serve  # 35ê°œ ë ˆì´ì–´ë¥¼ GPUì— ë¡œë“œ

# 2. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì •
# Modelfileì—ì„œ:
PARAMETER num_ctx 8192  # ê¸°ë³¸ 4096 â†’ 8192

# 3. ë°°ì¹˜ í¬ê¸° ì¡°ì •
PARAMETER num_batch 512  # ê¸°ë³¸ 512

# 4. ì—¬ëŸ¬ ëª¨ë¸ ë™ì‹œ ë¡œë“œ ë°©ì§€
# í•œ ë²ˆì— í•˜ë‚˜ì˜ ëª¨ë¸ë§Œ í™œì„±í™”

# 5. ëª¨ë¸ ìºì‹œ ìœ„ì¹˜ ë³€ê²½ (ì €ì¥ê³µê°„ ë¶€ì¡± ì‹œ)
OLLAMA_MODELS=/path/to/models ollama serve
\`\`\`

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

| ë¬¸ì œ | ì›ì¸ | í•´ê²°ì±… |
|------|------|--------|
| "Out of memory" | VRAM ë¶€ì¡± | ë” ì‘ì€ ëª¨ë¸ ë˜ëŠ” Q4 ì–‘ìí™” |
| ëŠë¦° ì‘ë‹µ | GPU ë¯¸ì‚¬ìš© | \`nvidia-smi\`ë¡œ GPU í™•ì¸ |
| ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ | ì €ì¥ê³µê°„ ë¶€ì¡± | \`df -h\`ë¡œ í™•ì¸ í›„ ì •ë¦¬ |
| API ì—°ê²° ì‹¤íŒ¨ | Ollama ë¯¸ì‹¤í–‰ | \`ollama serve\` ì‹¤í–‰ |
| CUDA ì˜¤ë¥˜ | ë“œë¼ì´ë²„ ë¶ˆì¼ì¹˜ | CUDA ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸ |
`,
      codeExample: `# ========================================
# Ollama í•µì‹¬ ì‚¬ìš©ë²• (3ë‹¨ê³„)
# ========================================
import ollama

# ğŸ“Œ Step 1: ê¸°ë³¸ í…ìŠ¤íŠ¸ ìƒì„±
response = ollama.generate(
    model='llama3.1',
    prompt='RAGë€ ë¬´ì—‡ì¸ê°€ìš”?'
)
print(response['response'])

# ğŸ“Œ Step 2: ì±„íŒ… í˜•ì‹ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨)
response = ollama.chat(
    model='llama3.1',
    messages=[
        {'role': 'system', 'content': 'í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.'},
        {'role': 'user', 'content': 'sLLMì˜ ì¥ì ì€?'}
    ]
)
print(response['message']['content'])

# ğŸ“Œ Step 3: OpenAI SDKë¡œ Ollama ì‚¬ìš© (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # ì•„ë¬´ ê°’ì´ë‚˜ OK
)

response = client.chat.completions.create(
    model="llama3.1",
    messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”!"}]
)
print(response.choices[0].message.content)

---

## ğŸ’¥ Common Pitfalls (ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜)

### 1. [ì„œë²„ ìƒíƒœ] Ollama ì„œë²„ ë¯¸ì‹¤í–‰ ìƒíƒœì—ì„œ API í˜¸ì¶œ

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: ì„œë²„ í™•ì¸ ì—†ì´ ë°”ë¡œ í˜¸ì¶œ
client = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")
response = client.chat.completions.create(...)  # ğŸ”´ ConnectionRefusedError

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ì„œë²„ ìƒíƒœ í™•ì¸ í›„ í˜¸ì¶œ
import requests

def check_ollama_running():
    try:
        requests.get("http://localhost:11434/api/tags", timeout=2)
        return True
    except:
        return False

if not check_ollama_running():
    print("ollama serve ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•˜ì„¸ìš”!")
\`\`\`

**ê¸°ì–µí•  ì **: OllamaëŠ” í´ë¼ì´ì–¸íŠ¸/ì„œë²„ êµ¬ì¡°. ì„œë²„ ì‹¤í–‰ ìƒíƒœ í•­ìƒ í™•ì¸.

---

### 2. [ë©”ëª¨ë¦¬ ë¶€ì¡±] GPU VRAM ì´ˆê³¼

\`\`\`bash
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: VRAM í™•ì¸ ì—†ì´ í° ëª¨ë¸ ì‹¤í–‰
ollama run llama3.1:70b  # ğŸ”´ 8GB GPUì—ì„œ OOM ì—ëŸ¬

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ëª¨ë¸ í¬ê¸° í™•ì¸ í›„ ì„ íƒ
# 8GB VRAM â†’ 7B ëª¨ë¸ (Q4 ì–‘ìí™”)
# 16GB VRAM â†’ 13B ëª¨ë¸ (Q4 ì–‘ìí™”)
# 24GB+ VRAM â†’ 70B ëª¨ë¸ (Q4 ì–‘ìí™”)

ollama run llama3.1:8b-instruct-q4_0  # ì•½ 5GB VRAM ì‚¬ìš©
\`\`\`

**ê¸°ì–µí•  ì **: ëª¨ë¸ í¬ê¸°(B) Ã— 0.6 â‰ˆ í•„ìš” VRAM(GB) (Q4 ì–‘ìí™” ê¸°ì¤€).

---

### 3. [API í˜¸í™˜ì„±] OpenAI SDK ë²„ì „ ë¶ˆì¼ì¹˜

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: êµ¬ë²„ì „ OpenAI SDK ë¬¸ë²•
import openai
openai.api_base = "http://localhost:11434/v1"  # ğŸ”´ 1.0+ ë²„ì „ì—ì„œ ì—ëŸ¬!

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: OpenAI SDK 1.0+ ë¬¸ë²•
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11434/v1",  # âœ… 1.0+ ë¬¸ë²•
    api_key="ollama"
)
\`\`\`

**ê¸°ì–µí•  ì **: OpenAI SDK 1.0+ ë²„ì „ì—ì„œëŠ” \`OpenAI(base_url=...)\` í˜•ì‹ ì‚¬ìš©.
`,
      keyPoints: [
        'ğŸ’¡ Ollama = "Docker for LLMs" (í•œ ì¤„ë¡œ ì„¤ì¹˜/ì‹¤í–‰)',
        'âš¡ ollama pull â†’ ë‹¤ìš´ë¡œë“œ, ollama run â†’ ì‹¤í–‰',
        'ğŸ”„ OpenAI í˜¸í™˜ APIë¡œ ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ì—†ì´ ì „í™˜',
        'ğŸ› ï¸ Modelfileë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê³ ì • ê°€ëŠ¥',
      ],
    }),

    // ========================================
    // Task 4: sLLM + RAG í†µí•© êµ¬í˜„ (50ë¶„)
    // ========================================
    createCodeTask('w5d6-sllm-rag-integration', 'sLLM + RAG í†µí•© êµ¬í˜„', 50, {
      introduction: `
## ğŸ¯ ì´ë²ˆ íƒœìŠ¤í¬ì—ì„œ ë°°ìš°ëŠ” ê²ƒ

**ëª©í‘œ**: Cloud LLM ëŒ€ì‹  ë¡œì»¬ sLLMì„ ì‚¬ìš©í•˜ëŠ” RAG ì‹œìŠ¤í…œ êµ¬ì¶•

---

## ğŸ’¡ ì™œ sLLM + RAGì¸ê°€?

### í˜„ì‹¤ ë¬¸ì œ
> "ìš°ë¦¬ íšŒì‚¬ì—ì„œ RAGë¥¼ êµ¬ì¶•í–ˆëŠ”ë°, GPT-4 API ë¹„ìš©ì´ í•œ ë‹¬ì— 500ë§Œì›ì´ ë‚˜ì™”ì–´ìš”..."

ì´ëŸ° ìƒí™©, ì‹¤ì œë¡œ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤.

### í•´ê²°ì±…: sLLM
**sLLM(Small Language Model)**ì„ ì‚¬ìš©í•˜ë©´:
- API ë¹„ìš©: **$0** (ì „ê¸°ë£Œë§Œ)
- ë°ì´í„° ë³´ì•ˆ: ì™¸ë¶€ ì „ì†¡ **ì—†ìŒ**
- ì‘ë‹µ ì†ë„: ë„¤íŠ¸ì›Œí¬ ì§€ì—° **ì—†ìŒ**

> ğŸ“Œ **í•µì‹¬ í¬ì¸íŠ¸**
>
> sLLMì€ "90%ì˜ ì„±ëŠ¥ì„ 10%ì˜ ë¹„ìš©ìœ¼ë¡œ" ì œê³µí•©ë‹ˆë‹¤.
> ëª¨ë“  ì‘ì—…ì— GPT-4ê°€ í•„ìš”í•œ ê±´ ì•„ë‹™ë‹ˆë‹¤!

---

## ğŸ• ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°: í”¼ì ë°°ë‹¬ vs í™ˆë©”ì´ë“œ

| Cloud LLM (GPT-4) | sLLM (Llama) |
|-------------------|--------------|
| í”¼ì ë°°ë‹¬ ì£¼ë¬¸ | ì§‘ì—ì„œ ì§ì ‘ ë§Œë“¤ê¸° |
| ë§¤ë²ˆ ë°°ë‹¬ë¹„ ë°œìƒ | ì¬ë£Œë¹„ë§Œ (í•œ ë²ˆë§Œ) |
| ê¸°ë‹¤ë ¤ì•¼ í•¨ | ë°”ë¡œ ë¨¹ì„ ìˆ˜ ìˆìŒ |
| ì „ë¬¸ ì…°í”„ í’ˆì§ˆ | 80-90% ìˆ˜ì¤€ |
| ë©”ë‰´ ì„ íƒë§Œ ê°€ëŠ¥ | ì›í•˜ëŠ” ëŒ€ë¡œ ì»¤ìŠ¤í…€ |

**ì–¸ì œ ë­˜ ì“¸ê¹Œ?**
- ì¤‘ìš”í•œ ì†ë‹˜ ëŒ€ì ‘ â†’ ë°°ë‹¬ (Cloud LLM)
- í˜¼ì ì €ë… â†’ í™ˆë©”ì´ë“œ (sLLM) âœ…

---

## ğŸ”„ ì•„í‚¤í…ì²˜ ë¹„êµ

<svg viewBox="0 0 800 480" xmlns="http://www.w3.org/2000/svg" style="max-width: 100%; height: auto; background: #1e293b; border-radius: 12px;">
  <defs>
    <linearGradient id="cloudGrad2" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#f59e0b"/>
      <stop offset="100%" style="stop-color:#d97706"/>
    </linearGradient>
    <linearGradient id="localGrad2" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#22c55e"/>
      <stop offset="100%" style="stop-color:#16a34a"/>
    </linearGradient>
    <linearGradient id="stepGrad2" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#3b82f6"/>
      <stop offset="100%" style="stop-color:#2563eb"/>
    </linearGradient>
  </defs>
  <text x="200" y="35" text-anchor="middle" fill="#fbbf24" font-size="16" font-weight="bold">Cloud RAG (ê¸°ì¡´)</text>
  <text x="600" y="35" text-anchor="middle" fill="#4ade80" font-size="16" font-weight="bold">sLLM RAG (ì˜¤ëŠ˜)</text>
  <rect x="80" y="55" width="240" height="40" rx="6" fill="url(#stepGrad2)"/>
  <text x="200" y="82" text-anchor="middle" fill="white" font-size="14" font-weight="bold">Query</text>
  <text x="200" y="108" text-anchor="middle" fill="#64748b" font-size="16">â†“</text>
  <rect x="80" y="120" width="240" height="55" rx="6" fill="url(#cloudGrad2)"/>
  <text x="200" y="143" text-anchor="middle" fill="white" font-size="13" font-weight="bold">Embedding</text>
  <text x="200" y="163" text-anchor="middle" fill="#fef3c7" font-size="11">(OpenAI API) â† ë¹„ìš© ë°œìƒ</text>
  <text x="200" y="188" text-anchor="middle" fill="#64748b" font-size="16">â†“</text>
  <rect x="80" y="200" width="240" height="55" rx="6" fill="url(#stepGrad2)"/>
  <text x="200" y="223" text-anchor="middle" fill="white" font-size="13" font-weight="bold">Vector Search</text>
  <text x="200" y="243" text-anchor="middle" fill="#bfdbfe" font-size="11">(Chroma)</text>
  <text x="200" y="268" text-anchor="middle" fill="#64748b" font-size="16">â†“</text>
  <rect x="80" y="280" width="240" height="55" rx="6" fill="url(#cloudGrad2)"/>
  <text x="200" y="303" text-anchor="middle" fill="white" font-size="13" font-weight="bold">LLM Generation</text>
  <text x="200" y="323" text-anchor="middle" fill="#fef3c7" font-size="11">(GPT-4 API) â† ë¹„ìš© ë°œìƒ</text>
  <text x="200" y="348" text-anchor="middle" fill="#64748b" font-size="16">â†“</text>
  <rect x="80" y="360" width="240" height="40" rx="6" fill="url(#stepGrad2)"/>
  <text x="200" y="387" text-anchor="middle" fill="white" font-size="14" font-weight="bold">Response</text>
  <text x="200" y="430" text-anchor="middle" fill="#fbbf24" font-size="13" font-weight="bold">ë¹„ìš©: $15-60/1M tokens</text>
  <rect x="480" y="55" width="240" height="40" rx="6" fill="url(#stepGrad2)"/>
  <text x="600" y="82" text-anchor="middle" fill="white" font-size="14" font-weight="bold">Query</text>
  <text x="600" y="108" text-anchor="middle" fill="#64748b" font-size="16">â†“</text>
  <rect x="480" y="120" width="240" height="55" rx="6" fill="url(#localGrad2)"/>
  <text x="600" y="143" text-anchor="middle" fill="white" font-size="13" font-weight="bold">Embedding</text>
  <text x="600" y="163" text-anchor="middle" fill="#dcfce7" font-size="11">(Ollama Local) â† ë¬´ë£Œ</text>
  <text x="600" y="188" text-anchor="middle" fill="#64748b" font-size="16">â†“</text>
  <rect x="480" y="200" width="240" height="55" rx="6" fill="url(#stepGrad2)"/>
  <text x="600" y="223" text-anchor="middle" fill="white" font-size="13" font-weight="bold">Vector Search</text>
  <text x="600" y="243" text-anchor="middle" fill="#bfdbfe" font-size="11">(Chroma)</text>
  <text x="600" y="268" text-anchor="middle" fill="#64748b" font-size="16">â†“</text>
  <rect x="480" y="280" width="240" height="55" rx="6" fill="url(#localGrad2)"/>
  <text x="600" y="303" text-anchor="middle" fill="white" font-size="13" font-weight="bold">LLM Generation</text>
  <text x="600" y="323" text-anchor="middle" fill="#dcfce7" font-size="11">(Ollama Local) â† ë¬´ë£Œ</text>
  <text x="600" y="348" text-anchor="middle" fill="#64748b" font-size="16">â†“</text>
  <rect x="480" y="360" width="240" height="40" rx="6" fill="url(#stepGrad2)"/>
  <text x="600" y="387" text-anchor="middle" fill="white" font-size="14" font-weight="bold">Response</text>
  <text x="600" y="430" text-anchor="middle" fill="#4ade80" font-size="13" font-weight="bold">ë¹„ìš©: ì „ê¸°ë£Œ (~$0.1/ì‹œê°„)</text>
  <line x1="400" y1="50" x2="400" y2="440" stroke="#475569" stroke-width="1" stroke-dasharray="5,5"/>
</svg>

---

## ğŸ› ï¸ ì‹¤ìŠµ: sLLM RAG ì‹œìŠ¤í…œ ë§Œë“¤ê¸°

ì´ì œ ì‹¤ì œë¡œ êµ¬í˜„í•´ë´…ì‹œë‹¤. **4ë‹¨ê³„**ë¡œ ë‚˜ëˆ ì„œ ì§„í–‰í•©ë‹ˆë‹¤.

---

### Step 1: ì¬ë£Œ ì¤€ë¹„ (Import)

**ë¬´ì—‡ì„ í• ê¹Œìš”?**
í•„ìš”í•œ ë„êµ¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. ìš”ë¦¬ë¡œ ì¹˜ë©´ ì¬ë£Œì™€ ì¡°ë¦¬ë„êµ¬ë¥¼ êº¼ë‚´ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.

\`\`\`python
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
\`\`\`

> ğŸ” **ê° ë„êµ¬ì˜ ì—­í• **
> - \`Ollama\`: ë¡œì»¬ LLMê³¼ ëŒ€í™” (ìš”ë¦¬ì‚¬)
> - \`OllamaEmbeddings\`: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ì¬ë£Œ ì†ì§ˆ)
> - \`Chroma\`: ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ê²€ìƒ‰ (ëƒ‰ì¥ê³ )

---

### Step 2: LLM ì—°ê²°í•˜ê¸°

**ë¬´ì—‡ì„ í• ê¹Œìš”?**
ë¡œì»¬ì—ì„œ ëŒì•„ê°€ëŠ” Ollama ì„œë²„ì™€ ì—°ê²°í•©ë‹ˆë‹¤.

\`\`\`python
# LLM (ë‹µë³€ ìƒì„±ìš©)
llm = Ollama(model="llama3.2")

# í…ŒìŠ¤íŠ¸: ì˜ ì—°ê²°ëë‚˜?
print(llm.invoke("ì•ˆë…•í•˜ì„¸ìš”!"))
\`\`\`

> âš ï¸ **ì£¼ì˜**
> ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ë¨¼ì € \`ollama serve\`ê°€ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤!
> (Task 3ì—ì„œ ë°°ìš´ ë‚´ìš©)

**ì™œ llama3.2ì¸ê°€ìš”?**
- ê°€ë³ê³  ë¹ ë¦„ (3B íŒŒë¼ë¯¸í„°)
- í•œêµ­ì–´ ê¸°ë³¸ ì§€ì›
- ë©”ëª¨ë¦¬ 4GBë©´ ì¶©ë¶„

---

### Step 3: ë¬¸ì„œ ì €ì¥í•˜ê¸°

**ë¬´ì—‡ì„ í• ê¹Œìš”?**
ìš°ë¦¬ì˜ ë¬¸ì„œë¥¼ LLMì´ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ë²¡í„°ë¡œ ë³€í™˜í•´ì„œ ì €ì¥í•©ë‹ˆë‹¤.

\`\`\`python
# ì„ë² ë”© ëª¨ë¸ (í…ìŠ¤íŠ¸ â†’ ë²¡í„°)
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# ì˜ˆì‹œ ë¬¸ì„œë“¤
documents = [
    "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìì…ë‹ˆë‹¤.",
    "ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.",
    "sLLMì€ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.",
]

# ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embeddings,
)
\`\`\`

> ğŸ’¡ **ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚¬ë‚˜ìš”?**
>
> 1. ê° ë¬¸ì„œê°€ ìˆ«ì ë°°ì—´(ë²¡í„°)ë¡œ ë³€í™˜ë¨
> 2. ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë¬¸ì„œëŠ” ë¹„ìŠ·í•œ ë²¡í„°ê°’ì„ ê°€ì§
> 3. ë‚˜ì¤‘ì— ì§ˆë¬¸í•˜ë©´ ë¹„ìŠ·í•œ ë²¡í„°ë¥¼ ì°¾ì•„ì„œ ë°˜í™˜

---

### Step 4: ì§ˆë¬¸í•˜ê¸° (RAG ì‹¤í–‰!)

**ë¬´ì—‡ì„ í• ê¹Œìš”?**
ì‚¬ìš©ì ì§ˆë¬¸ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ â†’ LLMì´ ë‹µë³€ ìƒì„±

\`\`\`python
# ì§ˆë¬¸
query = "RAGê°€ ë­ì•¼?"

# 1) ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
docs = vectorstore.similarity_search(query, k=2)
context = "\\n".join([d.page_content for d in docs])

# 2) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {query}
ë‹µë³€:"""

# 3) LLMì—ê²Œ ë‹µë³€ ìš”ì²­
answer = llm.invoke(prompt)
print(answer)
\`\`\`

> âœ… **ì¶•í•˜í•©ë‹ˆë‹¤!**
>
> ë°©ê¸ˆ **ì™„ì „ ë¬´ë£Œ** RAG ì‹œìŠ¤í…œì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
> - ì„ë² ë”©: Ollama (ë¬´ë£Œ)
> - LLM: Ollama (ë¬´ë£Œ)
> - ë²¡í„°DB: Chroma (ë¬´ë£Œ)

---

## ğŸ“ ì „ì²´ ì½”ë“œ (ë³µì‚¬ìš©)

ìœ„ 4ë‹¨ê³„ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹œ ì™„ì„± ì½”ë“œì…ë‹ˆë‹¤:

\`\`\`python
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma

# 1. LLMê³¼ ì„ë² ë”© ì„¤ì •
llm = Ollama(model="llama3.2")
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# 2. ë¬¸ì„œ ì €ì¥
documents = [
    "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìì…ë‹ˆë‹¤.",
    "ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.",
    "sLLMì€ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.",
]
vectorstore = Chroma.from_texts(texts=documents, embedding=embeddings)

# 3. ì§ˆë¬¸í•˜ê¸°
query = "RAGê°€ ë­ì•¼?"
docs = vectorstore.similarity_search(query, k=2)
context = "\\n".join([d.page_content for d in docs])

prompt = f"""ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {query}
ë‹µë³€:"""

answer = llm.invoke(prompt)
print(answer)
\`\`\`

---

## ğŸš€ ë” ë‚˜ì•„ê°€ê¸°: ì„±ëŠ¥ ìµœì í™”

ê¸°ë³¸ êµ¬í˜„ì´ ëë‚¬ìœ¼ë‹ˆ, ì´ì œ ìµœì í™” ë°©ë²•ì„ ì•Œì•„ë´…ì‹œë‹¤.

### ì„ë² ë”© ìºì‹± (ë°˜ë³µ í˜¸ì¶œ ë°©ì§€)

**ë¬¸ì œ**: ê°™ì€ ë¬¸ì„œë¥¼ ì—¬ëŸ¬ ë²ˆ ì„ë² ë”©í•˜ë©´ ì‹œê°„ ë‚­ë¹„

\`\`\`python
# ìºì‹œê°€ ì—†ìœ¼ë©´
# "ì•ˆë…•í•˜ì„¸ìš”" â†’ ì„ë² ë”© (0.5ì´ˆ)
# "ì•ˆë…•í•˜ì„¸ìš”" â†’ ë˜ ì„ë² ë”© (0.5ì´ˆ) ğŸ˜¢

# ìºì‹œê°€ ìˆìœ¼ë©´
# "ì•ˆë…•í•˜ì„¸ìš”" â†’ ì„ë² ë”© (0.5ì´ˆ)
# "ì•ˆë…•í•˜ì„¸ìš”" â†’ ìºì‹œì—ì„œ ë°”ë¡œ ë°˜í™˜ (0.001ì´ˆ) ğŸ˜Š
\`\`\`

**í•´ê²°ì±…**: ê°„ë‹¨í•œ ìºì‹± ì¶”ê°€

\`\`\`python
import hashlib
import json
from pathlib import Path

class CachedEmbeddings:
    def __init__(self):
        self.cache_dir = Path(".cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.embeddings = OllamaEmbeddings(model="nomic-embed-text")

    def embed(self, text):
        # ìºì‹œ í‚¤ ìƒì„±
        key = hashlib.md5(text.encode()).hexdigest()
        cache_file = self.cache_dir / f"{key}.json"

        # ìºì‹œì— ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        if cache_file.exists():
            return json.loads(cache_file.read_text())

        # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± í›„ ì €ì¥
        vector = self.embeddings.embed_query(text)
        cache_file.write_text(json.dumps(vector))
        return vector
\`\`\`

> ğŸ’¡ **íš¨ê³¼**
> - ì²« ì‹¤í–‰: ê¸°ì¡´ê³¼ ë™ì¼
> - ë‘ ë²ˆì§¸ ì‹¤í–‰ë¶€í„°: **100ë°° ì´ìƒ ë¹¨ë¼ì§!**

---

## ğŸ“‹ ì •ë¦¬: ì˜¤ëŠ˜ ë°°ìš´ ê²ƒ

| ë‹¨ê³„ | ë‚´ìš© | í•µì‹¬ ì½”ë“œ |
|------|------|----------|
| 1 | Import | \`from langchain_community.llms import Ollama\` |
| 2 | LLM ì—°ê²° | \`llm = Ollama(model="llama3.2")\` |
| 3 | ë¬¸ì„œ ì €ì¥ | \`Chroma.from_texts(...)\` |
| 4 | RAG ì‹¤í–‰ | \`similarity_search() â†’ llm.invoke()\` |

> ğŸ‰ **í•µì‹¬ ë©”ì‹œì§€**
>
> sLLM + RAG = **ë¬´ë£Œ + ë¹ ë¦„ + ì•ˆì „**
>
> Cloud LLMì´ í•„ìš” ì—†ëŠ” ê²½ìš°ê°€ ìƒê°ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤!
`,
      codeExample: `# sLLM + RAG ê¸°ë³¸ êµ¬í˜„
# ì´ ì½”ë“œë¥¼ ë³µì‚¬í•´ì„œ ì‹¤í–‰í•´ë³´ì„¸ìš”!

from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma

# 1. LLMê³¼ ì„ë² ë”© ì„¤ì •
llm = Ollama(model="llama3.2")
embeddings = OllamaEmbeddings(model="nomic-embed-text")

# 2. ë¬¸ì„œ ì €ì¥
documents = [
    "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìì…ë‹ˆë‹¤.",
    "ë²¡í„° ê²€ìƒ‰ì€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.",
    "sLLMì€ ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ì†Œí˜• ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.",
]
vectorstore = Chroma.from_texts(texts=documents, embedding=embeddings)

# 3. RAG ì‹¤í–‰
query = "RAGê°€ ë­ì•¼?"
docs = vectorstore.similarity_search(query, k=2)
context = "\\n".join([d.page_content for d in docs])

prompt = f"""ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {query}
ë‹µë³€:"""

answer = llm.invoke(prompt)
print(answer)
`,
      keyPoints: [
        'sLLM + RAG = ë¹„ìš© $0 + ë¹ ë¥¸ ì†ë„ + ë°ì´í„° ë³´ì•ˆ',
        '4ë‹¨ê³„: Import â†’ LLMì—°ê²° â†’ ë¬¸ì„œì €ì¥ â†’ RAGì‹¤í–‰',
        'ëª¨ë“  ì‘ì—…ì— GPT-4ê°€ í•„ìš”í•œ ê±´ ì•„ë‹ˆë‹¤',
        'ì„ë² ë”© ìºì‹±ìœ¼ë¡œ ë°˜ë³µ ì‘ì—… 100ë°° ë¹¨ë¼ì§',
      ],
    }),

    // ========================================
    // Task 5: í”„ë¡œë•ì…˜ ë°°í¬ì™€ ìµœì í™” (45ë¶„)
    // ========================================
    createVideoTask('w5d6-production-deployment', 'í”„ë¡œë•ì…˜ ë°°í¬ì™€ ìµœì í™”', 45, {
      introduction: `
## í”„ë¡œë•ì…˜ sLLM ë°°í¬

ë¡œì»¬ ê°œë°œì—ì„œ í”„ë¡œë•ì…˜ìœ¼ë¡œ ë„˜ì–´ê°ˆ ë•Œ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## Docker ì»¨í…Œì´ë„ˆí™”

### Ollama + RAG ì„œë¹„ìŠ¤ Dockerfile

\`\`\`dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \\
    curl \\
    && rm -rf /var/lib/apt/lists/*

# Ollama ì„¤ì¹˜
RUN curl -fsSL https://ollama.com/install.sh | sh

# Python ì˜ì¡´ì„±
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY . .

# Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸
COPY start.sh /start.sh
RUN chmod +x /start.sh

EXPOSE 8000 11434

CMD ["/start.sh"]
\`\`\`

\`\`\`bash
# start.sh
#!/bin/bash

# Ollama ì„œë²„ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
ollama serve &

# Ollamaê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
sleep 5

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
ollama pull llama3.1

# FastAPI ì„œë²„ ì‹œì‘
uvicorn main:app --host 0.0.0.0 --port 8000
\`\`\`

### Docker Compose (GPU ì§€ì›)

\`\`\`yaml
# docker-compose.yml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  rag-api:
    build: .
    ports:
      - "8000:8000"
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      - ./data:/app/data
      - chroma_data:/app/chroma_db

volumes:
  ollama_data:
  chroma_data:
\`\`\`

## FastAPI í”„ë¡œë•ì…˜ ì„œë²„

\`\`\`python
# main.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
import asyncio
from contextlib import asynccontextmanager

from rag_system import SLLMRagSystem, RAGConfig

# ========================================
# ì•± ì„¤ì •
# ========================================

rag_system: Optional[SLLMRagSystem] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    global rag_system

    # ì‹œì‘ ì‹œ: RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    config = RAGConfig(
        llm_model="llama3.1",
        embedding_model="llama3.1"
    )
    rag_system = SLLMRagSystem(config)

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    rag_system.load_existing()

    print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    yield

    # ì¢…ë£Œ ì‹œ: ì •ë¦¬ ì‘ì—…
    print("RAG ì‹œìŠ¤í…œ ì¢…ë£Œ")

app = FastAPI(
    title="sLLM RAG API",
    description="ë¡œì»¬ LLM ê¸°ë°˜ RAG ì„œë¹„ìŠ¤",
    version="1.0.0",
    lifespan=lifespan
)

# ========================================
# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
# ========================================

class QueryRequest(BaseModel):
    question: str
    num_results: int = 4

class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    model: str

class DocumentRequest(BaseModel):
    documents: List[str]
    metadatas: Optional[List[dict]] = None

class StatsResponse(BaseModel):
    status: str
    model: str
    num_documents: int

# ========================================
# API ì—”ë“œí¬ì¸íŠ¸
# ========================================

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "model": rag_system.config.llm_model if rag_system else "not_loaded"}

@app.get("/stats", response_model=StatsResponse)
async def get_stats():
    """ì‹œìŠ¤í…œ í†µê³„"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œ ë¯¸ì´ˆê¸°í™”")

    stats = rag_system.get_stats()
    return StatsResponse(
        status=stats["status"],
        model=stats["model"],
        num_documents=stats.get("num_documents", 0)
    )

@app.post("/documents")
async def add_documents(request: DocumentRequest):
    """ë¬¸ì„œ ì¶”ê°€"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œ ë¯¸ì´ˆê¸°í™”")

    num_chunks = rag_system.add_documents(
        request.documents,
        request.metadatas
    )

    return {"message": f"{num_chunks}ê°œ ì²­í¬ ì¶”ê°€ë¨"}

@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """ì§ˆë¬¸ ì‘ë‹µ"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œ ë¯¸ì´ˆê¸°í™”")

    try:
        result = rag_system.query(request.question)
        return QueryResponse(
            answer=result["answer"],
            sources=result["sources"],
            model=rag_system.config.llm_model
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/query/stream")
async def query_stream(request: QueryRequest):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    if not rag_system:
        raise HTTPException(status_code=503, detail="RAG ì‹œìŠ¤í…œ ë¯¸ì´ˆê¸°í™”")

    async def generate():
        for chunk in rag_system.query_stream(request.question):
            yield chunk

    return StreamingResponse(
        generate(),
        media_type="text/plain"
    )
\`\`\`

## vLLMì„ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ ì„œë¹™

Ollamaë³´ë‹¤ ë†’ì€ ì²˜ë¦¬ëŸ‰ì´ í•„ìš”í•œ ê²½ìš° **vLLM**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

### vLLM ì„¤ì¹˜ ë° ì‹¤í–‰

\`\`\`bash
# ì„¤ì¹˜
pip install vllm

# ì„œë²„ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server \\
    --model meta-llama/Meta-Llama-3.1-8B-Instruct \\
    --port 8000 \\
    --tensor-parallel-size 1

# ë˜ëŠ” ì–‘ìí™”ëœ ëª¨ë¸
python -m vllm.entrypoints.openai.api_server \\
    --model TheBloke/Llama-2-7B-Chat-AWQ \\
    --quantization awq \\
    --port 8000
\`\`\`

### vLLM vs Ollama ë¹„êµ

| íŠ¹ì§• | Ollama | vLLM |
|------|--------|------|
| **ì„¤ì¹˜ ë‚œì´ë„** | ë§¤ìš° ì‰¬ì›€ | ë³´í†µ |
| **ì²˜ë¦¬ëŸ‰** | ë³´í†µ | ë†’ìŒ (2-5x) |
| **ë°°ì¹˜ ì²˜ë¦¬** | ì œí•œì  | ìš°ìˆ˜ |
| **ë©”ëª¨ë¦¬ íš¨ìœ¨** | ë³´í†µ | ìš°ìˆ˜ (PagedAttention) |
| **ìš©ë„** | ê°œë°œ/ì†Œê·œëª¨ | í”„ë¡œë•ì…˜/ëŒ€ê·œëª¨ |

## ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### Prometheus + Grafana ë©”íŠ¸ë¦­

\`\`\`python
# metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# ë©”íŠ¸ë¦­ ì •ì˜
QUERY_COUNT = Counter(
    'rag_queries_total',
    'Total RAG queries',
    ['status']
)

QUERY_LATENCY = Histogram(
    'rag_query_latency_seconds',
    'RAG query latency',
    buckets=[0.1, 0.5, 1, 2, 5, 10, 30]
)

TOKENS_GENERATED = Counter(
    'rag_tokens_generated_total',
    'Total tokens generated'
)

ACTIVE_REQUESTS = Gauge(
    'rag_active_requests',
    'Currently processing requests'
)

MODEL_LOADED = Gauge(
    'rag_model_loaded',
    'Model load status',
    ['model']
)

# ë©”íŠ¸ë¦­ ë°ì½”ë ˆì´í„°
def track_query(func):
    async def wrapper(*args, **kwargs):
        ACTIVE_REQUESTS.inc()
        start = time.time()

        try:
            result = await func(*args, **kwargs)
            QUERY_COUNT.labels(status='success').inc()
            return result
        except Exception as e:
            QUERY_COUNT.labels(status='error').inc()
            raise
        finally:
            QUERY_LATENCY.observe(time.time() - start)
            ACTIVE_REQUESTS.dec()

    return wrapper

# Prometheus ì„œë²„ ì‹œì‘ (ë³„ë„ í¬íŠ¸)
def start_metrics_server(port: int = 9090):
    start_http_server(port)
    print(f"Metrics server started on port {port}")
\`\`\`

## ë¹„ìš© ë¹„êµ ì‹¤í—˜

### API vs sLLM ë¹„ìš© ê³„ì‚°ê¸°

\`\`\`python
from dataclasses import dataclass
from typing import Literal

@dataclass
class CostEstimate:
    """ë¹„ìš© ì¶”ì •"""
    provider: str
    input_tokens: int
    output_tokens: int
    queries_per_day: int

    def calculate_monthly_cost(self) -> dict:
        # í† í°ë‹¹ ë¹„ìš© ($ per 1M tokens)
        COSTS = {
            "gpt-4o": {"input": 5.0, "output": 15.0},
            "gpt-4o-mini": {"input": 0.15, "output": 0.6},
            "claude-3-5-sonnet": {"input": 3.0, "output": 15.0},
            "claude-3-5-haiku": {"input": 1.0, "output": 5.0},
            "local-llama": {"input": 0, "output": 0},  # ì „ê¸°ë£Œë§Œ
        }

        if self.provider not in COSTS:
            raise ValueError(f"Unknown provider: {self.provider}")

        cost = COSTS[self.provider]

        # ì¼ì¼ ë¹„ìš©
        daily_input_cost = (self.input_tokens * self.queries_per_day / 1_000_000) * cost["input"]
        daily_output_cost = (self.output_tokens * self.queries_per_day / 1_000_000) * cost["output"]
        daily_total = daily_input_cost + daily_output_cost

        # ì›”ê°„ ë¹„ìš© (30ì¼)
        monthly_total = daily_total * 30

        return {
            "provider": self.provider,
            "daily_cost": round(daily_total, 2),
            "monthly_cost": round(monthly_total, 2),
            "yearly_cost": round(monthly_total * 12, 2)
        }

# ì‚¬ìš© ì˜ˆì‹œ
def compare_costs():
    """ë¹„ìš© ë¹„êµ"""

    # ì‹œë‚˜ë¦¬ì˜¤: ì¼ì¼ 1000 ì¿¼ë¦¬, í‰ê·  500 input + 200 output tokens
    scenarios = [
        CostEstimate("gpt-4o", 500, 200, 1000),
        CostEstimate("gpt-4o-mini", 500, 200, 1000),
        CostEstimate("claude-3-5-sonnet", 500, 200, 1000),
        CostEstimate("local-llama", 500, 200, 1000),
    ]

    print("=== ì›”ê°„ ë¹„ìš© ë¹„êµ (1000 ì¿¼ë¦¬/ì¼) ===\\n")
    print(f"{'Provider':<20} {'Daily':<10} {'Monthly':<12} {'Yearly':<12}")
    print("-" * 54)

    for scenario in scenarios:
        cost = scenario.calculate_monthly_cost()
        print(f"{cost['provider']:<20} \${cost['daily_cost']:<9} \${cost['monthly_cost']:<11} \${cost['yearly_cost']:<11}")

    # ë¡œì»¬ ìš´ì˜ ë¹„ìš© ì¶”ê°€
    print("\\n* local-llama: ì „ê¸°ë£Œ ì•½ $10-30/ì›” (GPU 24ì‹œê°„ ê°€ë™ ê¸°ì¤€)")
    print("* GPU ì„œë²„ ë Œíƒˆ: RTX 4090 ì•½ $300-500/ì›”")

if __name__ == "__main__":
    compare_costs()
\`\`\`

## ì¶œë ¥ ì˜ˆì‹œ

\`\`\`
=== ì›”ê°„ ë¹„ìš© ë¹„êµ (1000 ì¿¼ë¦¬/ì¼) ===

Provider             Daily      Monthly      Yearly
------------------------------------------------------
gpt-4o               $6.0       $180.0       $2160.0
gpt-4o-mini          $0.16      $4.8         $57.6
claude-3-5-sonnet    $4.5       $135.0       $1620.0
local-llama          $0.0       $0.0         $0.0

* local-llama: ì „ê¸°ë£Œ ì•½ $10-30/ì›” (GPU 24ì‹œê°„ ê°€ë™ ê¸°ì¤€)
* GPU ì„œë²„ ë Œíƒˆ: RTX 4090 ì•½ $300-500/ì›”
\`\`\`

> **ê²°ë¡ **: ì¼ì¼ 1000 ì¿¼ë¦¬ ì´ìƒì´ë©´ sLLMì´ ê²½ì œì . ê·¸ ì´í•˜ë©´ gpt-4o-mini ì¶”ì²œ.
`,
      keyPoints: [
        'Docker + Composeë¡œ í”„ë¡œë•ì…˜ ë°°í¬',
        'FastAPIë¡œ REST API ì„œë¹„ìŠ¤í™”',
        'vLLMì€ Ollamaë³´ë‹¤ 2-5x ë†’ì€ ì²˜ë¦¬ëŸ‰',
        'ì¼ì¼ 1000+ ì¿¼ë¦¬ì‹œ sLLMì´ ê²½ì œì ',
      ],
    }),

    // ========================================
    // Task 6: Day 6 ì¢…í•© í€´ì¦ˆ (20ë¶„)
    // ========================================
    createQuizTask('w5d6-quiz', 'Day 6 ì¢…í•© í€´ì¦ˆ', 20, {
      introduction: `
## Day 6 í•™ìŠµ ë‚´ìš© ì ê²€

sLLM ê°œìš”, ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­, Ollama, RAG í†µí•©, í”„ë¡œë•ì…˜ ë°°í¬ì— ëŒ€í•œ ì´í•´ë„ë¥¼ ì ê²€í•©ë‹ˆë‹¤.
`,
      questions: [
        {
          id: 'w5d6-q1',
          question: '7B íŒŒë¼ë¯¸í„° LLMì„ FP16ìœ¼ë¡œ ë¡œë“œí•  ë•Œ í•„ìš”í•œ VRAMì€ ì•½ ì–¼ë§ˆì¸ê°€ìš”?',
          options: [
            '7GB',
            '14GB',
            '28GB',
            '56GB'
          ],
          correctAnswer: 1,
          explanation: 'FP16ì€ íŒŒë¼ë¯¸í„°ë‹¹ 2ë°”ì´íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 7B Ã— 2 bytes = 14GB. ì—¬ê¸°ì— KV Cache ë“±ì„ ë”í•˜ë©´ ì‹¤ì œë¡œëŠ” 16-18GB ì •ë„ í•„ìš”í•©ë‹ˆë‹¤.'
        },
        {
          id: 'w5d6-q2',
          question: 'Q4_K_M ì–‘ìí™”ì˜ ì¥ì ìœ¼ë¡œ ê°€ì¥ ì ì ˆí•œ ê²ƒì€?',
          options: [
            'ë¬´ì†ì‹¤ ì••ì¶•ìœ¼ë¡œ í’ˆì§ˆ ì €í•˜ê°€ ì „í˜€ ì—†ë‹¤',
            'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì•½ 75% ì¤„ì´ë©´ì„œ í’ˆì§ˆ ì†ì‹¤ì€ 2-3%ì— ë¶ˆê³¼í•˜ë‹¤',
            'ê°€ì¥ ë¹ ë¥¸ ì¶”ë¡  ì†ë„ë¥¼ ì œê³µí•œë‹¤',
            'íŒŒì¸íŠœë‹ì— ê°€ì¥ ì í•©í•˜ë‹¤'
          ],
          correctAnswer: 1,
          explanation: 'Q4_K_Mì€ 4bit ì–‘ìí™”ë¡œ FP16 ëŒ€ë¹„ ì•½ 75% ë©”ëª¨ë¦¬ ì ˆê°(14GB â†’ ~4GB)ì„ ë‹¬ì„±í•˜ë©´ì„œ í’ˆì§ˆ ì†ì‹¤ì€ 2-3% ìˆ˜ì¤€ì…ë‹ˆë‹¤. í”„ë¡œë•ì…˜ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì–‘ìí™” ë ˆë²¨ì…ë‹ˆë‹¤.'
        },
        {
          id: 'w5d6-q3',
          question: 'Ollamaì˜ íŠ¹ì§•ì´ ì•„ë‹Œ ê²ƒì€?',
          options: [
            'OpenAI í˜¸í™˜ API ì œê³µ',
            'Modelfileë¡œ ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„± ê°€ëŠ¥',
            'ëª¨ë¸ íŒŒì¸íŠœë‹ ê¸°ëŠ¥ ë‚´ì¥',
            'ìë™ ì–‘ìí™” ë° ìµœì í™”'
          ],
          correctAnswer: 2,
          explanation: 'OllamaëŠ” ëª¨ë¸ ì‹¤í–‰ê³¼ ì„œë¹™ì— íŠ¹í™”ëœ ë„êµ¬ì…ë‹ˆë‹¤. íŒŒì¸íŠœë‹ì€ ë³„ë„ì˜ ë„êµ¬(Axolotl, LLaMA-Factory ë“±)ê°€ í•„ìš”í•©ë‹ˆë‹¤. OllamaëŠ” ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‹¤í–‰í•˜ëŠ” ë° ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.'
        },
        {
          id: 'w5d6-q4',
          question: 'LangChainì—ì„œ Ollamaë¥¼ ì‚¬ìš©í•  ë•Œ ì˜¬ë°”ë¥¸ ì„í¬íŠ¸ëŠ”?',
          options: [
            'from langchain.llms import Ollama',
            'from langchain_community.llms import Ollama',
            'from langchain_ollama import Ollama',
            'from ollama.langchain import LLM'
          ],
          correctAnswer: 1,
          explanation: 'LangChain 0.1.x ì´í›„ë¡œ ì»¤ë®¤ë‹ˆí‹° í†µí•©(community integrations)ì€ langchain_community íŒ¨í‚¤ì§€ì—ì„œ ì„í¬íŠ¸í•©ë‹ˆë‹¤. langchain_ollamaë„ ì¡´ì¬í•˜ì§€ë§Œ langchain_community.llms.Ollamaê°€ ë” ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.'
        },
        {
          id: 'w5d6-q5',
          question: 'sLLM ë„ì…ì´ ì í•©í•˜ì§€ ì•Šì€ ìƒí™©ì€?',
          options: [
            'ê¸ˆìœµì‚¬ì—ì„œ ê³ ê° ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” RAG ì‹œìŠ¤í…œ',
            'í•˜ë£¨ 100ê±´ ë¯¸ë§Œì˜ ê°„í—ì  ì¿¼ë¦¬ê°€ ë°œìƒí•˜ëŠ” ë‚´ë¶€ ë„êµ¬',
            'ì¸í„°ë„·ì´ ì°¨ë‹¨ëœ ë³´ì•ˆ ì‹œì„¤ì˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ',
            'ì›” ìˆ˜ì²œ ë‹¬ëŸ¬ì˜ API ë¹„ìš©ì´ ë°œìƒí•˜ëŠ” ì„œë¹„ìŠ¤'
          ],
          correctAnswer: 1,
          explanation: 'í•˜ë£¨ 100ê±´ ë¯¸ë§Œì˜ ì¿¼ë¦¬ë¼ë©´ gpt-4o-mini ê¸°ì¤€ ì›” $5 ë¯¸ë§Œì˜ ë¹„ìš©ì´ ë°œìƒí•©ë‹ˆë‹¤. GPU ì„œë²„ ìš´ì˜ ë¹„ìš©(ì „ê¸°ë£Œ, ê´€ë¦¬)ì´ ë” ë†’ì„ ìˆ˜ ìˆì–´ Cloud APIê°€ ë” ê²½ì œì ì…ë‹ˆë‹¤. ë°˜ë©´ ê¸ˆìœµì‚¬(ë³´ì•ˆ), ì˜¤í”„ë¼ì¸ í™˜ê²½, ëŒ€ëŸ‰ ì¿¼ë¦¬ëŠ” sLLMì´ ì í•©í•©ë‹ˆë‹¤.'
        },
        {
          id: 'w5d6-q6',
          question: 'vLLMì˜ PagedAttentionì´ ì œê³µí•˜ëŠ” ì£¼ìš” ì´ì ì€?',
          options: [
            'ëª¨ë¸ í•™ìŠµ ì†ë„ í–¥ìƒ',
            'KV Cache ë©”ëª¨ë¦¬ íš¨ìœ¨í™”ë¡œ ì²˜ë¦¬ëŸ‰ ì¦ê°€',
            'ëª¨ë¸ ì••ì¶•ë¥  í–¥ìƒ',
            'ë„¤íŠ¸ì›Œí¬ ì§€ì—° ê°ì†Œ'
          ],
          correctAnswer: 1,
          explanation: 'PagedAttentionì€ KV Cacheë¥¼ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ë‹¨í¸í™”ë¥¼ ì¤„ì´ê³ , ë™ì ìœ¼ë¡œ í• ë‹¹/í•´ì œí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°™ì€ GPU ë©”ëª¨ë¦¬ë¡œ ë” ë§ì€ ë™ì‹œ ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ ì²˜ë¦¬ëŸ‰ì´ 2-5ë°° í–¥ìƒë©ë‹ˆë‹¤.'
        },
        {
          id: 'w5d6-q7',
          question: 'Apple Silicon M3 Max (128GB)ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°€ì¥ í° ëª¨ë¸ì€?',
          options: [
            'Llama 3.1 8B Q8',
            'Llama 3.1 13B Q4',
            'Llama 3.1 70B Q4',
            'Llama 3.1 70B Q8'
          ],
          correctAnswer: 2,
          explanation: '70B ëª¨ë¸ì€ Q4 ì–‘ìí™”ì‹œ ì•½ 40GBê°€ í•„ìš”í•©ë‹ˆë‹¤. M3 Max 128GBëŠ” í†µí•© ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ 70B Q4 ëª¨ë¸ì„ ì¶©ë¶„íˆ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Q8ì€ ì•½ 70GBê°€ í•„ìš”í•˜ì—¬ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì™€ í•¨ê»˜ ì‚¬ìš©ì‹œ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        },
        {
          id: 'w5d6-q8',
          question: 'ë‹¤ìŒ ì¤‘ í•œêµ­ì–´ ì„±ëŠ¥ì´ ê°€ì¥ ìš°ìˆ˜í•œ ëª¨ë¸ì€?',
          options: [
            'Phi-3.5',
            'Mistral 7B',
            'SOLAR 10.7B',
            'CodeLlama 7B'
          ],
          correctAnswer: 2,
          explanation: 'SOLARëŠ” ì—…ìŠ¤í…Œì´ì§€(Upstage)ì—ì„œ ê°œë°œí•œ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ë¡œ, KoBEST ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìµœìƒìœ„ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Phi-3.5, Mistral, CodeLlamaëŠ” ì˜ì–´ ì¤‘ì‹¬ìœ¼ë¡œ í•™ìŠµë˜ì–´ í•œêµ­ì–´ ì„±ëŠ¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ë–¨ì–´ì§‘ë‹ˆë‹¤.'
        },
      ],
      keyPoints: [
        'VRAM ê³„ì‚°: íŒŒë¼ë¯¸í„° Ã— 2(FP16) + ì˜¤ë²„í—¤ë“œ',
        'Q4_K_Mì´ í”„ë¡œë•ì…˜ ìµœì  ì–‘ìí™”',
        'OllamaëŠ” ì‹¤í–‰/ì„œë¹™, íŒŒì¸íŠœë‹ì€ ë³„ë„ ë„êµ¬',
        'ì¼ì¼ ì¿¼ë¦¬ ìˆ˜ì— ë”°ë¼ API vs sLLM ì„ íƒ',
      ],
    }),

    // ========================================
    // Challenge: í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ (50ë¶„)
    // ========================================
    createChallengeTask('w5d6-challenge', 'í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•', 50, {
      introduction: `
## ğŸ¯ Challenge: Cloud + sLLM í•˜ì´ë¸Œë¦¬ë“œ RAG

### ì™œ í•˜ì´ë¸Œë¦¬ë“œì¸ê°€?

> ğŸš— **ë¹„ìœ **: ì¶œí‡´ê·¼ vs ì¥ê±°ë¦¬ ì—¬í–‰
>
> - **ì¼ìƒ ì¶œí‡´ê·¼** = ìì „ê±°/ì§€í•˜ì²  (ì €ë ´, ë¹ ë¦„) â†’ **sLLM**
> - **ì¥ê±°ë¦¬ ì—¬í–‰** = ë¹„í–‰ê¸° (ë¹„ì‹¸ì§€ë§Œ í•„ìš”) â†’ **Cloud LLM**
> - **ì¤‘ìš” í™”ë¬¼** = ì§ì ‘ ìš´ì „ (ì™¸ë¶€ ìœ„íƒ ë¶ˆê°€) â†’ **sLLM only**

ì‹¤ë¬´ì—ì„œëŠ” ëª¨ë“  ì¿¼ë¦¬ì— GPT-4ë¥¼ ì“°ë©´ ë¹„ìš©ì´ í­ë°œí•˜ê³ ,
ëª¨ë“  ì¿¼ë¦¬ì— sLLMë§Œ ì“°ë©´ ë³µì¡í•œ ë¶„ì„ì—ì„œ í’ˆì§ˆì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.

**í•´ê²°ì±…**: ì¿¼ë¦¬ ìœ í˜•ì— ë”°ë¼ ìµœì ì˜ ëª¨ë¸ì„ ìë™ ì„ íƒ!

### ì‹œë‚˜ë¦¬ì˜¤

ë‹¹ì‹ ì€ FDEë¡œì„œ ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ê°€ì§„ RAG ì‹œìŠ¤í…œì„ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤:

1. **ê¸°ë³¸ ì¿¼ë¦¬**: sLLM (ë¹„ìš© ì ˆê°)
2. **ë³µì¡í•œ ì¿¼ë¦¬**: Cloud LLM (ë†’ì€ í’ˆì§ˆ)
3. **ë¯¼ê° ë°ì´í„°**: sLLMë§Œ ì‚¬ìš© (ë³´ì•ˆ)

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

\`\`\`python
class HybridRAGSystem:
    """í•˜ì´ë¸Œë¦¬ë“œ RAG ì‹œìŠ¤í…œ"""

    def __init__(self):
        # TODO: sLLM (Ollama) ì´ˆê¸°í™”
        # TODO: Cloud LLM (OpenAI ë˜ëŠ” Anthropic) ì´ˆê¸°í™”
        # TODO: ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        pass

    def classify_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜

        Returns:
            - "simple": ë‹¨ìˆœ ê²€ìƒ‰/ìš”ì•½ â†’ sLLM
            - "complex": ë¶„ì„/ì¶”ë¡ /ìƒì„± â†’ Cloud LLM
            - "sensitive": ë¯¼ê° ë°ì´í„° ê´€ë ¨ â†’ sLLM only
        """
        # TODO: êµ¬í˜„
        pass

    def route_query(self, query: str, force_local: bool = False) -> dict:
        """ì¿¼ë¦¬ ë¼ìš°íŒ… ë° ì‹¤í–‰

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            force_local: Trueë©´ í•­ìƒ sLLM ì‚¬ìš©

        Returns:
            {
                "answer": str,
                "model_used": str,
                "reason": str,
                "cost": float
            }
        """
        # TODO: êµ¬í˜„
        pass
\`\`\`

### í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  |
|------|------|
| ì¿¼ë¦¬ ë¶„ë¥˜ ì •í™•ë„ | 30ì  |
| ë¼ìš°íŒ… ë¡œì§ êµ¬í˜„ | 30ì  |
| ë¹„ìš© ì¶”ì  ê¸°ëŠ¥ | 20ì  |
| ì—ëŸ¬ í•¸ë“¤ë§ | 10ì  |
| ì½”ë“œ í’ˆì§ˆ | 10ì  |

### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤

\`\`\`python
# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
test_queries = [
    # Simple â†’ sLLM
    ("RAGë€ ë¬´ì—‡ì¸ê°€ìš”?", "simple"),
    ("ì´ ë¬¸ì„œë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”", "simple"),

    # Complex â†’ Cloud
    ("ì´ ì„¸ ë¬¸ì„œì˜ ê³µí†µì ê³¼ ì°¨ì´ì ì„ ë¶„ì„í•˜ê³ , ê°ê°ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•´ì£¼ì„¸ìš”", "complex"),
    ("ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë¶„ê¸° ë§¤ì¶œì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”", "complex"),

    # Sensitive â†’ sLLM only
    ("ê³ ê° ê¹€ì² ìˆ˜ì˜ ê³„ì¢Œ ì”ì•¡ì€?", "sensitive"),
    ("í™˜ì ID 12345ì˜ ì§„ë‹¨ ê²°ê³¼", "sensitive"),
]
\`\`\`

### ë³´ë„ˆìŠ¤ ê³¼ì œ

1. **Fallback ë©”ì»¤ë‹ˆì¦˜**: Cloud LLM ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ sLLMìœ¼ë¡œ ì „í™˜
2. **ë¹„ìš© ëŒ€ì‹œë³´ë“œ**: ì¼ë³„/ì£¼ë³„ ë¹„ìš© ì¶”ì  ë° ì‹œê°í™”
3. **A/B í…ŒìŠ¤íŠ¸**: ê°™ì€ ì¿¼ë¦¬ë¥¼ ì–‘ìª½ìœ¼ë¡œ ë³´ë‚´ í’ˆì§ˆ ë¹„êµ
`,
      hints: [
        `**[Step 1] ê¸°ë³¸ êµ¬ì¡° ì„¤ì •**
\`\`\`python
from langchain_community.llms import Ollama
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

class HybridRAGSystem:
    def __init__(self):
        # sLLM (ë¡œì»¬)
        self.local_llm = Ollama(model="llama3.2")

        # Cloud LLM (API í‚¤ í•„ìš”)
        self.cloud_llm = ChatOpenAI(model="gpt-4o-mini")

        # ë²¡í„° ìŠ¤í† ì–´
        self.embeddings = OllamaEmbeddings(model="nomic-embed-text")
        self.vectorstore = Chroma(embedding_function=self.embeddings)

        # ë¹„ìš© ì¶”ì 
        self.cost_log = []
\`\`\``,

        `**[Step 2] ì¿¼ë¦¬ ë¶„ë¥˜ ë¡œì§**
\`\`\`python
def classify_query(self, query: str) -> str:
    query_lower = query.lower()

    # ë¯¼ê° ë°ì´í„° í‚¤ì›Œë“œ ì²´í¬ (ìµœìš°ì„ )
    sensitive_keywords = ["ê³ ê°", "í™˜ì", "ê³„ì¢Œ", "ë¹„ë°€ë²ˆí˜¸", "ì£¼ë¯¼ë²ˆí˜¸", "ê°œì¸ì •ë³´"]
    if any(kw in query_lower for kw in sensitive_keywords):
        return "sensitive"

    # ë³µì¡í•œ ì¿¼ë¦¬ í‚¤ì›Œë“œ ì²´í¬
    complex_keywords = ["ë¶„ì„", "ë¹„êµ", "ì˜ˆì¸¡", "ì¶”ë¡ ", "ì¥ë‹¨ì ", "ì „ëµ"]
    if any(kw in query_lower for kw in complex_keywords):
        return "complex"

    # ê¸°ë³¸: ë‹¨ìˆœ ì¿¼ë¦¬
    return "simple"
\`\`\``,

        `**[Step 3] ë¼ìš°íŒ… ë¡œì§ êµ¬í˜„**
\`\`\`python
def route_query(self, query: str, force_local: bool = False) -> dict:
    # 1. ì¿¼ë¦¬ ë¶„ë¥˜
    query_type = self.classify_query(query)

    # 2. ë¯¼ê° ë°ì´í„° ë˜ëŠ” force_localì´ë©´ sLLMë§Œ ì‚¬ìš©
    if query_type == "sensitive" or force_local:
        llm = self.local_llm
        model_name = "llama3.2 (local)"
        reason = "ë¯¼ê° ë°ì´í„° - ë¡œì»¬ ì²˜ë¦¬" if query_type == "sensitive" else "ê°•ì œ ë¡œì»¬ ëª¨ë“œ"
        cost = 0.0

    # 3. ë³µì¡í•œ ì¿¼ë¦¬ëŠ” Cloud LLM
    elif query_type == "complex":
        llm = self.cloud_llm
        model_name = "gpt-4o-mini (cloud)"
        reason = "ë³µì¡í•œ ë¶„ì„ í•„ìš”"
        cost = 0.00015  # ì˜ˆìƒ ë¹„ìš© (ì…ë ¥ í† í° ê¸°ì¤€)

    # 4. ë‹¨ìˆœ ì¿¼ë¦¬ëŠ” sLLM
    else:
        llm = self.local_llm
        model_name = "llama3.2 (local)"
        reason = "ë‹¨ìˆœ ì¿¼ë¦¬ - ë¹„ìš© ì ˆê°"
        cost = 0.0

    # 5. RAG ì‹¤í–‰
    docs = self.vectorstore.similarity_search(query, k=3)
    context = "\\n".join([d.page_content for d in docs])

    prompt = f"""Context: {context}

Question: {query}

Answer:"""

    answer = llm.invoke(prompt)

    # 6. ë¹„ìš© ê¸°ë¡
    self.cost_log.append({"query": query, "cost": cost, "model": model_name})

    return {
        "answer": answer,
        "model_used": model_name,
        "reason": reason,
        "cost": cost
    }
\`\`\``,

        `**[Step 4] Fallback êµ¬í˜„ (ë³´ë„ˆìŠ¤)**
\`\`\`python
def route_query_with_fallback(self, query: str) -> dict:
    query_type = self.classify_query(query)

    # ë³µì¡í•œ ì¿¼ë¦¬: Cloud ë¨¼ì € ì‹œë„
    if query_type == "complex":
        try:
            return self._call_cloud_llm(query)
        except Exception as e:
            print(f"Cloud LLM ì‹¤íŒ¨: {e}, sLLMìœ¼ë¡œ ì „í™˜")
            return self._call_local_llm(query, reason="Cloud ì‹¤íŒ¨ - Fallback")

    # ë‚˜ë¨¸ì§€ëŠ” ë¡œì»¬
    return self._call_local_llm(query)
\`\`\``,

        `**[Step 5] í…ŒìŠ¤íŠ¸ ì‹¤í–‰**
\`\`\`python
# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
rag = HybridRAGSystem()

# ë¬¸ì„œ ì¶”ê°€ (ì˜ˆì‹œ)
rag.vectorstore.add_texts([
    "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìì…ë‹ˆë‹¤.",
    "ë²¡í„° ê²€ìƒ‰ì€ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.",
    "ê³ ê° ê¹€ì² ìˆ˜ì˜ ê³„ì¢Œë²ˆí˜¸ëŠ” 123-456-789ì…ë‹ˆë‹¤.",  # ë¯¼ê° ë°ì´í„°
])

# í…ŒìŠ¤íŠ¸
queries = [
    "RAGë€ ë¬´ì—‡ì¸ê°€ìš”?",  # simple â†’ sLLM
    "ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì˜ ì¥ë‹¨ì ì„ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”",  # complex â†’ Cloud
    "ê³ ê° ê¹€ì² ìˆ˜ì˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",  # sensitive â†’ sLLM only
]

for q in queries:
    result = rag.route_query(q)
    print(f"Q: {q}")
    print(f"Model: {result['model_used']}")
    print(f"Reason: {result['reason']}")
    print("---")
\`\`\``,
      ],
      keyPoints: [
        'ì‹¤ë¬´ì—ì„œëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ì´ ìµœì ',
        'ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ë¼ìš°íŒ…ì´ í•µì‹¬',
        'ë¯¼ê° ë°ì´í„°ëŠ” í•­ìƒ ë¡œì»¬ ì²˜ë¦¬',
        'ë¹„ìš© ì¶”ì ìœ¼ë¡œ ROI ê²€ì¦',
      ],
    }),
  ],
}
