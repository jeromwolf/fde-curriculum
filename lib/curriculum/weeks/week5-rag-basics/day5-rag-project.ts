// Day 5: í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ êµ¬ì¶• - Week 5 ìµœì¢… í”„ë¡œì íŠ¸

import type { Day } from '../../types'
import {
  createVideoTask,
  createReadingTask,
  createCodeTask,
  createQuizTask,
  createChallengeTask,
} from './types'

export const day5RagProject: Day = {
  slug: 'rag-project',
  title: 'í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ êµ¬ì¶•',
  totalDuration: 300,
  tasks: [
    // ========================================
    // Task 1: í”„ë¡œë•ì…˜ RAG ì•„í‚¤í…ì²˜ ì„¤ê³„ (40ë¶„)
    // ========================================
    createVideoTask('w5d5-prod-architecture', 'í”„ë¡œë•ì…˜ RAG ì•„í‚¤í…ì²˜ ì„¤ê³„', 40, {
      introduction: `
## Week 5 ìµœì¢… í”„ë¡œì íŠ¸ ê°œìš”

Day 1-4ì—ì„œ í•™ìŠµí•œ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ **í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ RAG ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

> "The difference between a demo and production is about 10x the effort."
> â€” ì‹¤ì œ AI ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ ìì£¼ ë“¤ë¦¬ëŠ” ë§

## ë°ëª¨ vs í”„ë¡œë•ì…˜ RAG

| ì¸¡ë©´ | ë°ëª¨ RAG | í”„ë¡œë•ì…˜ RAG |
|------|----------|--------------|
| **ë¬¸ì„œ** | 1ê°œ PDF | ìˆ˜ì²œ ê°œ ë¬¸ì„œ, ë‹¤ì–‘í•œ í¬ë§· |
| **ì‚¬ìš©ì** | ê°œë°œì 1ëª… | ìˆ˜ë°±/ìˆ˜ì²œ ë™ì‹œ ì‚¬ìš©ì |
| **ì‘ë‹µì‹œê°„** | 10ì´ˆë„ OK | < 3ì´ˆ ëª©í‘œ |
| **ì •í™•ë„** | "ëŒ€ì¶© ë§ìœ¼ë©´ OK" | 99%+ ì •í™•ë„ í•„ìš” |
| **ì—ëŸ¬** | ì—ëŸ¬ ë¬´ì‹œ | ìš°ì•„í•œ ì—ëŸ¬ ì²˜ë¦¬ |
| **ëª¨ë‹ˆí„°ë§** | printë¬¸ | ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¶”ì  |
| **ë¹„ìš©** | ì‹ ê²½ ì•ˆ ì”€ | í† í°ë‹¹ ë¹„ìš© ìµœì í™” |

## í”„ë¡œë•ì…˜ RAG ì•„í‚¤í…ì²˜

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Production RAG Architecture                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data Ingestion Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â”‚  [Sources]           [Processors]          [Chunking]        [Indexing]  â”‚   â”‚
â”‚  â”‚  â”œâ”€ PDF              â”œâ”€ OCR               â”œâ”€ Semantic        â”œâ”€ Vector   â”‚   â”‚
â”‚  â”‚  â”œâ”€ Word             â”œâ”€ Table Extract     â”œâ”€ Recursive       â”œâ”€ Keyword  â”‚   â”‚
â”‚  â”‚  â”œâ”€ HTML             â”œâ”€ Image Caption     â”œâ”€ Markdown        â”œâ”€ Hybrid   â”‚   â”‚
â”‚  â”‚  â”œâ”€ Notion           â”œâ”€ Code Parse        â”œâ”€ Code            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
â”‚  â”‚  â””â”€ Confluence       â””â”€ Metadata          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                       â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Vector Database Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚   â”‚   Primary   â”‚    â”‚   Backup    â”‚    â”‚      Metadata Store         â”‚  â”‚   â”‚
â”‚  â”‚   â”‚  (Pinecone) â”‚    â”‚  (Chroma)   â”‚    â”‚      (PostgreSQL)           â”‚  â”‚   â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                       â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Retrieval Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â”‚  [Query Analysis]  â†’  [Multi-Query]  â†’  [Hybrid Search]  â†’  [Re-ranking] â”‚   â”‚
â”‚  â”‚  â”œâ”€ Intent         â”œâ”€ Expansion      â”œâ”€ Vector + BM25    â”œâ”€ Cross-Enc   â”‚   â”‚
â”‚  â”‚  â”œâ”€ Entity         â”œâ”€ Translation    â”œâ”€ Filter           â”œâ”€ MMR         â”‚   â”‚
â”‚  â”‚  â””â”€ Language       â””â”€ Decomposition  â””â”€ Fusion           â””â”€ Diversity   â”‚   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                       â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Generation Pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â”‚  [Context Assembly]  â†’  [Prompt Engineering]  â†’  [LLM Call]  â†’  [Output] â”‚   â”‚
â”‚  â”‚  â”œâ”€ Compression       â”œâ”€ System Prompt         â”œâ”€ Streaming   â”œâ”€ Source  â”‚   â”‚
â”‚  â”‚  â”œâ”€ Ordering          â”œâ”€ Few-shot              â”œâ”€ Fallback    â”œâ”€ Cache   â”‚   â”‚
â”‚  â”‚  â””â”€ Deduplication     â””â”€ Format Control        â””â”€ Retry       â””â”€ Log     â”‚   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                       â†“                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Monitoring & Observability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚LangSmithâ”‚  â”‚  Logs   â”‚  â”‚ Metrics â”‚  â”‚ Alerts  â”‚  â”‚ User Feedback   â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                                                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## í•µì‹¬ ì„¤ê³„ ì›ì¹™ (Production-Ready 5ì›ì¹™)

### 1. Fail-Safe (ì•ˆì „í•œ ì‹¤íŒ¨)
\`\`\`python
class FailSafeRAG:
    """ì–´ë–¤ ìƒí™©ì—ì„œë„ ì•ˆì „í•˜ê²Œ ë™ì‘í•˜ëŠ” RAG"""

    def __init__(self):
        self.primary_llm = ChatOpenAI(model="gpt-4o-mini")
        self.fallback_llm = ChatAnthropic(model="claude-3-haiku")
        self.cache = RedisCache()

    async def answer(self, question: str) -> RAGResponse:
        try:
            # 1ì°¨: ìºì‹œ í™•ì¸
            cached = await self.cache.get(question)
            if cached:
                return RAGResponse(answer=cached, source="cache")

            # 2ì°¨: Primary LLM
            try:
                answer = await self._rag_with_llm(question, self.primary_llm)
                await self.cache.set(question, answer)
                return RAGResponse(answer=answer, source="primary")
            except RateLimitError:
                # 3ì°¨: Fallback LLM
                answer = await self._rag_with_llm(question, self.fallback_llm)
                return RAGResponse(answer=answer, source="fallback")

        except Exception as e:
            # 4ì°¨: ì•ˆì „í•œ ê¸°ë³¸ ì‘ë‹µ
            logger.error(f"RAG failed: {e}")
            return RAGResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                source="error_fallback"
            )
\`\`\`

### 2. Observable (ê´€ì°° ê°€ëŠ¥)
\`\`\`python
from langsmith import traceable
from datadog import statsd
import structlog

logger = structlog.get_logger()

class ObservableRAG:
    @traceable(name="rag_pipeline")  # LangSmith ì¶”ì 
    async def answer(self, question: str) -> RAGResponse:
        start_time = time.time()

        # ê²€ìƒ‰ ë‹¨ê³„ ì¶”ì 
        with logger.contextualize(step="retrieval"):
            docs = await self._retrieve(question)
            statsd.histogram("rag.retrieval.doc_count", len(docs))

        # ìƒì„± ë‹¨ê³„ ì¶”ì 
        with logger.contextualize(step="generation"):
            answer = await self._generate(question, docs)
            statsd.histogram("rag.generation.token_count", count_tokens(answer))

        # ì „ì²´ ì§€ì—°ì‹œê°„
        latency = time.time() - start_time
        statsd.histogram("rag.latency_seconds", latency)

        # êµ¬ì¡°í™”ëœ ë¡œê·¸
        logger.info(
            "rag_completed",
            question=question[:100],
            doc_count=len(docs),
            latency=latency
        )

        return RAGResponse(answer=answer, latency=latency)
\`\`\`

### 3. Scalable (í™•ì¥ ê°€ëŠ¥)
\`\`\`python
from asyncio import Semaphore
from functools import lru_cache

class ScalableRAG:
    def __init__(self, max_concurrent: int = 10):
        self.semaphore = Semaphore(max_concurrent)  # ë™ì‹œ ìš”ì²­ ì œí•œ
        self.embedding_cache = lru_cache(maxsize=10000)

    async def batch_answer(self, questions: list[str]) -> list[RAGResponse]:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”"""

        # 1. ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬ (API í˜¸ì¶œ ìµœì†Œí™”)
        unique_questions = list(set(questions))
        embeddings = await self.embed_batch(unique_questions)

        # 2. ë³‘ë ¬ ê²€ìƒ‰ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œì„± ì œì–´)
        async def retrieve_one(q: str, emb: list[float]):
            async with self.semaphore:
                return await self._retrieve(q, emb)

        docs_list = await asyncio.gather(*[
            retrieve_one(q, emb) for q, emb in zip(unique_questions, embeddings)
        ])

        # 3. ë‹µë³€ ìƒì„±
        return [self._generate(q, docs) for q, docs in zip(questions, docs_list)]
\`\`\`

### 4. Cost-Efficient (ë¹„ìš© íš¨ìœ¨ì )
\`\`\`python
class CostEfficientRAG:
    def __init__(self):
        self.router = QueryRouter()  # ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜

        # ë¹„ìš©/ì„±ëŠ¥ í‹°ì–´
        self.tiers = {
            "simple": ChatOpenAI(model="gpt-4o-mini"),      # $0.15/1M
            "complex": ChatOpenAI(model="gpt-4o"),          # $2.50/1M
            "expert": ChatAnthropic(model="claude-opus-4")  # $15/1M
        }

    async def answer(self, question: str) -> RAGResponse:
        # ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ
        tier = self.router.classify(question)
        llm = self.tiers[tier]

        # ë¹„ìš© ì¶”ì 
        with CostTracker() as tracker:
            answer = await self._generate(question, llm)

        logger.info(f"Query tier: {tier}, Cost: \${tracker.total_cost:.6f}")
        return answer

# ê²°ê³¼:
# - Simple ì¿¼ë¦¬ (80%): $0.15/1M tokens
# - Complex ì¿¼ë¦¬ (18%): $2.50/1M tokens
# - Expert ì¿¼ë¦¬ (2%): $15/1M tokens
# í‰ê·  ë¹„ìš©: 80% * 0.15 + 18% * 2.50 + 2% * 15 = $0.87/1M (vs $2.50 ë‹¨ì¼ ëª¨ë¸)
\`\`\`

### 5. Secure (ë³´ì•ˆ)
\`\`\`python
from pydantic import BaseModel, field_validator
import re

class SecureRAG:
    # í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ë°©ì§€ íŒ¨í„´
    INJECTION_PATTERNS = [
        r"ignore previous instructions",
        r"forget your instructions",
        r"you are now",
        r"\\[system\\]",
    ]

    def sanitize_input(self, text: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ ê²€ì¦"""
        for pattern in self.INJECTION_PATTERNS:
            if re.search(pattern, text.lower()):
                raise ValueError("Potential injection detected")
        return text.strip()[:2000]  # ê¸¸ì´ ì œí•œ

    def filter_output(self, answer: str) -> str:
        """ë¯¼ê° ì •ë³´ í•„í„°ë§"""
        # ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
        answer = re.sub(r'\\b\\d{6}-\\d{7}\\b', '[ì£¼ë¯¼ë²ˆí˜¸]', answer)
        answer = re.sub(r'\\b\\d{3}-\\d{4}-\\d{4}\\b', '[ì „í™”ë²ˆí˜¸]', answer)
        return answer

    async def answer(self, question: str) -> RAGResponse:
        # ì…ë ¥ ê²€ì¦
        safe_question = self.sanitize_input(question)

        # RAG ì‹¤í–‰
        answer = await self._rag(safe_question)

        # ì¶œë ¥ í•„í„°ë§
        safe_answer = self.filter_output(answer)

        return RAGResponse(answer=safe_answer)
\`\`\`

## ê¸°ìˆ  ìŠ¤íƒ ê²°ì • ë§¤íŠ¸ë¦­ìŠ¤

í”„ë¡œì íŠ¸ ê·œëª¨ì— ë”°ë¥¸ ê¸°ìˆ  ìŠ¤íƒ ê°€ì´ë“œ:

| ê·œëª¨ | ë²¡í„°DB | LLM | í”„ë ˆì„ì›Œí¬ | ì¸í”„ë¼ |
|------|--------|-----|-----------|--------|
| **PoC** | Chroma (ë¡œì»¬) | GPT-4o-mini | LangChain | ë¡œì»¬/Colab |
| **MVP** | Pinecone Free | GPT-4o-mini + Claude Haiku | LangChain | Vercel/Railway |
| **Scale-up** | Pinecone Standard | GPT-4o + Fallback | LangGraph | AWS ECS/GCP Cloud Run |
| **Enterprise** | pgvector + Pinecone | Fine-tuned + GPT-4 | Custom | Kubernetes |
      `,
      keyPoints: [
        'í”„ë¡œë•ì…˜ RAGëŠ” ë°ëª¨ì˜ 10ë°° ë³µì¡ë„',
        '5ì›ì¹™: Fail-Safe, Observable, Scalable, Cost-Efficient, Secure',
        'ê·œëª¨ì— ë§ëŠ” ê¸°ìˆ  ìŠ¤íƒ ì„ íƒ',
      ],
      practiceGoal: 'í”„ë¡œë•ì…˜ ìˆ˜ì¤€ RAG ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜ì™€ ì„¤ê³„ ì›ì¹™ì„ ì´í•´í•œë‹¤',
    }),

    // ========================================
    // Task 2: ë©€í‹° í¬ë§· ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (50ë¶„)
    // ========================================
    createCodeTask('w5d5-document-pipeline', 'ë©€í‹° í¬ë§· ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸', 50, {
      introduction: `
## ì™œ ë°°ìš°ëŠ”ê°€?

**ë¬¸ì œ**: RAG íŠœí† ë¦¬ì–¼ì€ í•­ìƒ "PDF í•˜ë‚˜ë§Œ" ë‹¤ë£¨ëŠ”ë°, ì‹¤ì œ ê¸°ì—…ì€ PDF, Word, Excel, HTML, ì½”ë“œ ë“± ìˆ˜ì‹­ ê°€ì§€ í¬ë§·ì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤.
- ê° í¬ë§·ë§ˆë‹¤ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”
- í…Œì´ë¸”, ì´ë¯¸ì§€ ëˆ„ë½
- ë©”íƒ€ë°ì´í„° ì†ì‹¤
- í†µí•© ì²˜ë¦¬ ì–´ë ¤ì›€

**í•´ê²°**: ëª¨ë“  í¬ë§·ì„ í‘œì¤€í™”ëœ ProcessedDocumentë¡œ ë³€í™˜í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

---

## ë¹„ìœ : ë¬¸ì„œ ì²˜ë¦¬ = ë²ˆì—­ê¸°

\`\`\`
ë¬¸ì œ:
- PDF ë§í•˜ëŠ” ì‚¬ëŒ
- Word ë§í•˜ëŠ” ì‚¬ëŒ
- HTML ë§í•˜ëŠ” ì‚¬ëŒ
â†’ ì„œë¡œ ëª» ì•Œì•„ë“£ìŒ!

í•´ê²°:
ëª¨ë“  ì–¸ì–´(í¬ë§·) â†’ ì˜ì–´(ProcessedDocument) ë²ˆì—­
â†’ í†µí•© ì²˜ë¦¬ ê°€ëŠ¥!

ProcessedDocument = {
  content: "ì¶”ì¶œëœ í…ìŠ¤íŠ¸"
  metadata: {"source": "file.pdf", "page": 1}
  tables: [...]  # í…Œì´ë¸” ë°ì´í„°
}
\`\`\`

---

## í•µì‹¬ êµ¬í˜„ (ê°„ì†Œí™”)

\`\`\`python
# ğŸ“Œ Step 1: í†µí•© ë¬¸ì„œ ë¡œë”
class UniversalDocumentLoader:
    def __init__(self):
        self.loaders = {
            ".pdf": PDFLoader(),
            ".docx": DocxLoader(),
            ".html": HTMLLoader(),
            ".md": MarkdownLoader(),
            ".py": CodeLoader()
        }

    def load(self, path: str) -> ProcessedDocument:
        ext = Path(path).suffix.lower()
        loader = self.loaders.get(ext)
        if not loader:
            raise ValueError(f"Unsupported format: {ext}")
        return loader.load(path)

# ğŸ“Œ Step 2: ì‚¬ìš©
loader = UniversalDocumentLoader()

# PDF ì²˜ë¦¬
pdf_doc = loader.load("report.pdf")
print(f"Text: {pdf_doc.content[:100]}")
print(f"Tables: {len(pdf_doc.tables)}")

# Word ì²˜ë¦¬
word_doc = loader.load("proposal.docx")
print(f"Text: {word_doc.content[:100]}")

# ğŸ“Œ Step 3: ë°°ì¹˜ ì²˜ë¦¬
import os

docs = []
for file in os.listdir("./documents"):
    try:
        doc = loader.load(f"./documents/{file}")
        docs.append(doc)
    except ValueError:
        print(f"Skipped unsupported file: {file}")

print(f"Loaded {len(docs)} documents")

# ğŸ“Œ Step 4: ì²­í‚¹ & ë²¡í„°í™”
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(chunk_size=500)
all_chunks = []

for doc in docs:
    chunks = splitter.split_text(doc.content)
    all_chunks.extend(chunks)

print(f"Total chunks: {len(all_chunks)}")
\`\`\`

---

## ì‹¤ë¬´ ë¬¸ì„œ í¬ë§·ì˜ ë‹¤ì–‘ì„±

ì‹¤ì œ ê¸°ì—… í™˜ê²½ì—ì„œëŠ” PDFë§Œ ìˆëŠ” ê²Œ ì•„ë‹™ë‹ˆë‹¤:

\`\`\`
ğŸ“ ì‹¤ì œ ê¸°ì—… ë¬¸ì„œ í˜„í™©
â”œâ”€â”€ ğŸ“„ PDF (40%) - ê³„ì•½ì„œ, ë§¤ë‰´ì–¼, ë³´ê³ ì„œ
â”œâ”€â”€ ğŸ“ Word/Docx (25%) - ì œì•ˆì„œ, ê¸°íšì„œ
â”œâ”€â”€ ğŸ“Š Excel/CSV (15%) - ë°ì´í„°, ë¦¬í¬íŠ¸
â”œâ”€â”€ ğŸŒ HTML/Web (10%) - ì‚¬ë‚´ ìœ„í‚¤, í¬íƒˆ
â”œâ”€â”€ ğŸ““ Notion/Confluence (5%) - ê¸°ìˆ  ë¬¸ì„œ
â””â”€â”€ ğŸ’» Code (5%) - README, ì£¼ì„
\`\`\`

---

## ì „ì²´ ì½”ë“œ (ìƒì„¸)

### í†µí•© ë¬¸ì„œ ë¡œë” êµ¬í˜„

\`\`\`python
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional
from dataclasses import dataclass
from enum import Enum
import mimetypes

class DocumentType(Enum):
    PDF = "pdf"
    DOCX = "docx"
    XLSX = "xlsx"
    HTML = "html"
    MARKDOWN = "markdown"
    CODE = "code"
    TEXT = "text"
    UNKNOWN = "unknown"

@dataclass
class ProcessedDocument:
    """ì²˜ë¦¬ëœ ë¬¸ì„œì˜ í‘œì¤€ í˜•ì‹"""
    content: str                    # ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    metadata: dict                  # ë©”íƒ€ë°ì´í„°
    source_type: DocumentType       # ì›ë³¸ í˜•ì‹
    chunks: list[str] = None        # ì²­í‚¹ ê²°ê³¼ (ì„ íƒì )
    tables: list[dict] = None       # í…Œì´ë¸” ë°ì´í„° (ì„ íƒì )
    images: list[dict] = None       # ì´ë¯¸ì§€ ì„¤ëª… (ì„ íƒì )

class BaseDocumentLoader(ABC):
    """ë¬¸ì„œ ë¡œë” ë² ì´ìŠ¤ í´ë˜ìŠ¤"""

    @abstractmethod
    def load(self, path: str) -> ProcessedDocument:
        pass

    @abstractmethod
    def supports(self, path: str) -> bool:
        pass

class PDFLoader(BaseDocumentLoader):
    """PDF ë¬¸ì„œ ë¡œë” - í…Œì´ë¸”, ì´ë¯¸ì§€ í¬í•¨"""

    def __init__(self, ocr_enabled: bool = False):
        self.ocr_enabled = ocr_enabled

    def supports(self, path: str) -> bool:
        return path.lower().endswith('.pdf')

    def load(self, path: str) -> ProcessedDocument:
        import fitz  # PyMuPDF
        from tabula import read_pdf

        doc = fitz.open(path)
        full_text = []
        tables = []
        images = []

        for page_num, page in enumerate(doc):
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = page.get_text("text")

            # OCR í•„ìš” ì‹œ (ìŠ¤ìº” PDF)
            if not text.strip() and self.ocr_enabled:
                text = self._ocr_page(page)

            full_text.append(f"[Page {page_num + 1}]\\n{text}")

            # ì´ë¯¸ì§€ ì¶”ì¶œ (ì„ íƒì )
            for img in page.get_images():
                img_data = self._extract_image(doc, img)
                if img_data:
                    images.append({
                        "page": page_num + 1,
                        "data": img_data
                    })

        # í…Œì´ë¸” ì¶”ì¶œ (tabula-py)
        try:
            df_list = read_pdf(path, pages='all')
            for i, df in enumerate(df_list):
                if not df.empty:
                    tables.append({
                        "index": i,
                        "data": df.to_dict('records'),
                        "markdown": df.to_markdown()
                    })
        except Exception:
            pass  # í…Œì´ë¸” ì—†ëŠ” PDF

        return ProcessedDocument(
            content="\\n\\n".join(full_text),
            metadata={
                "source": path,
                "page_count": len(doc),
                "table_count": len(tables),
                "image_count": len(images)
            },
            source_type=DocumentType.PDF,
            tables=tables,
            images=images
        )

    def _ocr_page(self, page) -> str:
        """OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        import pytesseract
        from PIL import Image
        import io

        pix = page.get_pixmap()
        img = Image.open(io.BytesIO(pix.tobytes()))
        return pytesseract.image_to_string(img, lang='kor+eng')

class DocxLoader(BaseDocumentLoader):
    """Word ë¬¸ì„œ ë¡œë”"""

    def supports(self, path: str) -> bool:
        return path.lower().endswith(('.docx', '.doc'))

    def load(self, path: str) -> ProcessedDocument:
        from docx import Document
        from docx.table import Table

        doc = Document(path)
        paragraphs = []
        tables = []

        for element in doc.element.body:
            if element.tag.endswith('p'):  # ë‹¨ë½
                for para in doc.paragraphs:
                    if para._element == element:
                        paragraphs.append(para.text)
                        break
            elif element.tag.endswith('tbl'):  # í…Œì´ë¸”
                for table in doc.tables:
                    if table._tbl == element:
                        table_data = []
                        for row in table.rows:
                            table_data.append([cell.text for cell in row.cells])
                        tables.append({
                            "data": table_data,
                            "markdown": self._table_to_markdown(table_data)
                        })
                        break

        return ProcessedDocument(
            content="\\n\\n".join(paragraphs),
            metadata={"source": path, "table_count": len(tables)},
            source_type=DocumentType.DOCX,
            tables=tables
        )

    def _table_to_markdown(self, data: list[list[str]]) -> str:
        if not data:
            return ""
        headers = data[0]
        md = "| " + " | ".join(headers) + " |\\n"
        md += "| " + " | ".join(["---"] * len(headers)) + " |\\n"
        for row in data[1:]:
            md += "| " + " | ".join(row) + " |\\n"
        return md

class HTMLLoader(BaseDocumentLoader):
    """HTML/ì›¹ í˜ì´ì§€ ë¡œë”"""

    def supports(self, path: str) -> bool:
        return path.lower().endswith(('.html', '.htm')) or path.startswith('http')

    def load(self, path: str) -> ProcessedDocument:
        from bs4 import BeautifulSoup
        import requests

        if path.startswith('http'):
            response = requests.get(path, timeout=10)
            html = response.text
        else:
            with open(path, 'r', encoding='utf-8') as f:
                html = f.read()

        soup = BeautifulSoup(html, 'html.parser')

        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
            tag.decompose()

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text(separator='\\n', strip=True)

        # ì œëª© ì¶”ì¶œ
        title = soup.find('title')
        title_text = title.string if title else "Unknown"

        return ProcessedDocument(
            content=text,
            metadata={"source": path, "title": title_text},
            source_type=DocumentType.HTML
        )

class CodeLoader(BaseDocumentLoader):
    """ì†ŒìŠ¤ ì½”ë“œ ë¡œë” - êµ¬ì¡° ë³´ì¡´"""

    CODE_EXTENSIONS = {
        '.py': 'python', '.js': 'javascript', '.ts': 'typescript',
        '.java': 'java', '.go': 'go', '.rs': 'rust', '.cpp': 'cpp'
    }

    def supports(self, path: str) -> bool:
        return Path(path).suffix in self.CODE_EXTENSIONS

    def load(self, path: str) -> ProcessedDocument:
        import ast

        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()

        ext = Path(path).suffix
        language = self.CODE_EXTENSIONS.get(ext, 'text')

        # Pythonì¸ ê²½ìš° êµ¬ì¡° ë¶„ì„
        structure = None
        if ext == '.py':
            try:
                tree = ast.parse(content)
                structure = self._analyze_python(tree)
            except SyntaxError:
                pass

        return ProcessedDocument(
            content=content,
            metadata={
                "source": path,
                "language": language,
                "structure": structure,
                "line_count": len(content.split('\\n'))
            },
            source_type=DocumentType.CODE
        )

    def _analyze_python(self, tree) -> dict:
        """Python AST ë¶„ì„"""
        import ast

        classes = []
        functions = []
        imports = []

        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                methods = [n.name for n in node.body if isinstance(n, ast.FunctionDef)]
                classes.append({"name": node.name, "methods": methods})
            elif isinstance(node, ast.FunctionDef):
                if node.col_offset == 0:  # ìµœìƒìœ„ í•¨ìˆ˜ë§Œ
                    functions.append(node.name)
            elif isinstance(node, (ast.Import, ast.ImportFrom)):
                imports.append(ast.unparse(node))

        return {"classes": classes, "functions": functions, "imports": imports}
\`\`\`

## í†µí•© ë¬¸ì„œ í”„ë¡œì„¸ì„œ

\`\`\`python
class UnifiedDocumentProcessor:
    """í†µí•© ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, ocr_enabled: bool = False):
        self.loaders = [
            PDFLoader(ocr_enabled=ocr_enabled),
            DocxLoader(),
            HTMLLoader(),
            CodeLoader(),
        ]

    def process(self, path: str) -> ProcessedDocument:
        """íŒŒì¼ í˜•ì‹ì— ë§ëŠ” ë¡œë”ë¡œ ìë™ ì²˜ë¦¬"""
        for loader in self.loaders:
            if loader.supports(path):
                return loader.load(path)

        # ê¸°ë³¸: í…ìŠ¤íŠ¸ë¡œ ì½ê¸°
        with open(path, 'r', encoding='utf-8') as f:
            return ProcessedDocument(
                content=f.read(),
                metadata={"source": path},
                source_type=DocumentType.TEXT
            )

    def process_directory(
        self,
        directory: str,
        patterns: list[str] = None,
        recursive: bool = True
    ) -> list[ProcessedDocument]:
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬"""
        from pathlib import Path
        import fnmatch

        directory = Path(directory)
        patterns = patterns or ['*']
        docs = []

        # íŒŒì¼ ê²€ìƒ‰
        if recursive:
            files = list(directory.rglob('*'))
        else:
            files = list(directory.glob('*'))

        # íŒ¨í„´ í•„í„°ë§
        matched_files = []
        for f in files:
            if f.is_file():
                for pattern in patterns:
                    if fnmatch.fnmatch(f.name, pattern):
                        matched_files.append(f)
                        break

        # ë³‘ë ¬ ì²˜ë¦¬
        from concurrent.futures import ThreadPoolExecutor

        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(
                lambda f: self._safe_process(f),
                matched_files
            ))

        return [doc for doc in results if doc is not None]

    def _safe_process(self, path: Path) -> Optional[ProcessedDocument]:
        """ì•ˆì „í•œ ë¬¸ì„œ ì²˜ë¦¬ (ì—ëŸ¬ ì‹œ None)"""
        try:
            return self.process(str(path))
        except Exception as e:
            print(f"Error processing {path}: {e}")
            return None
\`\`\`

## ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ with ë©”íƒ€ë°ì´í„°

\`\`\`python
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

class SmartChunker:
    """ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹"""

    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def chunk(self, doc: ProcessedDocument) -> list[Document]:
        """ë¬¸ì„œ íƒ€ì…ì— ë§ëŠ” ì²­í‚¹ ì „ëµ ì ìš©"""

        if doc.source_type == DocumentType.CODE:
            return self._chunk_code(doc)
        elif doc.source_type == DocumentType.MARKDOWN:
            return self._chunk_markdown(doc)
        else:
            return self._chunk_default(doc)

    def _chunk_default(self, doc: ProcessedDocument) -> list[Document]:
        """ê¸°ë³¸ ì¬ê·€ì  ì²­í‚¹"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\\n\\n", "\\n", ". ", " ", ""]
        )

        texts = splitter.split_text(doc.content)

        # í…Œì´ë¸” í…ìŠ¤íŠ¸ ì¶”ê°€
        if doc.tables:
            for table in doc.tables:
                texts.append(f"[Table Data]\\n{table['markdown']}")

        return [
            Document(
                page_content=text,
                metadata={
                    **doc.metadata,
                    "chunk_index": i,
                    "source_type": doc.source_type.value
                }
            )
            for i, text in enumerate(texts)
        ]

    def _chunk_code(self, doc: ProcessedDocument) -> list[Document]:
        """ì½”ë“œìš© ì²­í‚¹ - í•¨ìˆ˜/í´ë˜ìŠ¤ ë‹¨ìœ„"""
        from langchain.text_splitter import PythonCodeTextSplitter

        if doc.metadata.get("language") == "python":
            splitter = PythonCodeTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
        else:
            splitter = RecursiveCharacterTextSplitter.from_language(
                language=doc.metadata.get("language", "python"),
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )

        texts = splitter.split_text(doc.content)

        return [
            Document(
                page_content=text,
                metadata={
                    **doc.metadata,
                    "chunk_index": i,
                    "source_type": "code"
                }
            )
            for i, text in enumerate(texts)
        ]

    def _chunk_markdown(self, doc: ProcessedDocument) -> list[Document]:
        """ë§ˆí¬ë‹¤ìš´ìš© ì²­í‚¹ - í—¤ë” êµ¬ì¡° ë³´ì¡´"""
        from langchain.text_splitter import MarkdownHeaderTextSplitter

        headers_to_split_on = [
            ("#", "h1"),
            ("##", "h2"),
            ("###", "h3"),
        ]

        splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on
        )

        splits = splitter.split_text(doc.content)

        return [
            Document(
                page_content=split.page_content,
                metadata={
                    **doc.metadata,
                    **split.metadata,  # í—¤ë” ì •ë³´ í¬í•¨
                    "chunk_index": i,
                    "source_type": "markdown"
                }
            )
            for i, split in enumerate(splits)
        ]
\`\`\`

## ì „ì²´ Ingestion íŒŒì´í”„ë¼ì¸

\`\`\`python
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

class IngestionPipeline:
    """ë¬¸ì„œ ìˆ˜ì§‘ â†’ ì²˜ë¦¬ â†’ ì €ì¥ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        persist_directory: str = "./chroma_db",
        embedding_model: str = "text-embedding-3-small"
    ):
        self.processor = UnifiedDocumentProcessor(ocr_enabled=True)
        self.chunker = SmartChunker()
        self.embeddings = OpenAIEmbeddings(model=embedding_model)
        self.persist_directory = persist_directory

    def ingest(self, source: str) -> dict:
        """ë‹¨ì¼ íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ìˆ˜ì§‘"""
        from pathlib import Path

        path = Path(source)

        if path.is_file():
            docs = [self.processor.process(str(path))]
        else:
            docs = self.processor.process_directory(str(path))

        # ì²­í‚¹
        all_chunks = []
        for doc in docs:
            chunks = self.chunker.chunk(doc)
            all_chunks.extend(chunks)

        # ë²¡í„° ì €ì¥ì†Œì— ì €ì¥
        vectorstore = Chroma.from_documents(
            documents=all_chunks,
            embedding=self.embeddings,
            persist_directory=self.persist_directory
        )

        return {
            "total_documents": len(docs),
            "total_chunks": len(all_chunks),
            "persist_directory": self.persist_directory
        }

    def ingest_incremental(self, source: str) -> dict:
        """ì¦ë¶„ ìˆ˜ì§‘ - ë³€ê²½ëœ íŒŒì¼ë§Œ"""
        import hashlib
        import json

        # ê¸°ì¡´ í•´ì‹œ ë¡œë“œ
        hash_file = Path(self.persist_directory) / "file_hashes.json"
        if hash_file.exists():
            with open(hash_file) as f:
                existing_hashes = json.load(f)
        else:
            existing_hashes = {}

        # ë³€ê²½ëœ íŒŒì¼ í™•ì¸
        path = Path(source)
        files = list(path.rglob('*')) if path.is_dir() else [path]

        changed_files = []
        new_hashes = {}

        for f in files:
            if f.is_file():
                file_hash = hashlib.md5(f.read_bytes()).hexdigest()
                new_hashes[str(f)] = file_hash

                if existing_hashes.get(str(f)) != file_hash:
                    changed_files.append(str(f))

        if not changed_files:
            return {"message": "No changes detected", "files_updated": 0}

        # ë³€ê²½ëœ íŒŒì¼ë§Œ ì²˜ë¦¬
        result = self.ingest(changed_files[0] if len(changed_files) == 1 else source)

        # í•´ì‹œ ì €ì¥
        existing_hashes.update(new_hashes)
        with open(hash_file, 'w') as f:
            json.dump(existing_hashes, f)

        return {**result, "files_updated": len(changed_files)}

# ì‚¬ìš© ì˜ˆì‹œ
pipeline = IngestionPipeline()

# ì „ì²´ ë””ë ‰í† ë¦¬ ìˆ˜ì§‘
result = pipeline.ingest("./documents")
print(f"ìˆ˜ì§‘ ì™„ë£Œ: {result['total_documents']}ê°œ ë¬¸ì„œ, {result['total_chunks']}ê°œ ì²­í¬")

# ì¦ë¶„ ìˆ˜ì§‘ (ë³€ê²½ëœ íŒŒì¼ë§Œ)
result = pipeline.ingest_incremental("./documents")
print(f"ì—…ë°ì´íŠ¸: {result.get('files_updated', 0)}ê°œ íŒŒì¼")
\`\`\`

---

## ğŸ’¥ Common Pitfalls (ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜)

### 1. [ì¸ì½”ë”©] PDF/Word íŒŒì¼ ì¸ì½”ë”© ë¬¸ì œ

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: ì¸ì½”ë”© ì²˜ë¦¬ ì—†ì´ ì½ê¸°
text = Path(file_path).read_text()  # ğŸ”´ í•œê¸€ PDFì—ì„œ ê¹¨ì§

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë°”ì´ë„ˆë¦¬ë¡œ ì²˜ë¦¬
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader(file_path)  # âœ… ë°”ì´ë„ˆë¦¬ ì²˜ë¦¬
docs = loader.load()
\`\`\`

**ê¸°ì–µí•  ì **: PDF/WordëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ ì•„ë‹˜. ì „ìš© ë¡œë” ì‚¬ìš© í•„ìˆ˜.

---

### 2. [ë©”ëª¨ë¦¬] ëŒ€ìš©ëŸ‰ íŒŒì¼ ì¼ê´„ ë¡œë“œ

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: ëª¨ë“  íŒŒì¼ í•œ ë²ˆì— ë©”ëª¨ë¦¬ì— ë¡œë“œ
all_docs = []
for path in glob.glob("*.pdf"):
    loader = PyPDFLoader(path)
    all_docs.extend(loader.load())  # ğŸ”´ 1000ê°œ PDF â†’ ë©”ëª¨ë¦¬ ë¶€ì¡±

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ìŠ¤íŠ¸ë¦¬ë° + ë°°ì¹˜ ì²˜ë¦¬
for batch in chunked(file_paths, size=50):
    docs = []
    for path in batch:
        loader = PyPDFLoader(path)
        docs.extend(loader.load())
    vectorstore.add_documents(docs)  # ë°°ì¹˜ë³„ë¡œ ì €ì¥
    docs = []  # ë©”ëª¨ë¦¬ í•´ì œ
\`\`\`

**ê¸°ì–µí•  ì **: ëŒ€ìš©ëŸ‰ ìˆ˜ì§‘ ì‹œ ë°°ì¹˜ ì²˜ë¦¬ + ëª…ì‹œì  ë©”ëª¨ë¦¬ í•´ì œ.

---

### 3. [ì¦ë¶„ ìˆ˜ì§‘] íŒŒì¼ í•´ì‹œë§Œìœ¼ë¡œ ë³€ê²½ ê°ì§€

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: íŒŒì¼ í•´ì‹œë§Œ í™•ì¸
def is_changed(file_path):
    current_hash = hashlib.md5(Path(file_path).read_bytes()).hexdigest()
    return current_hash != self.cache.get(file_path)
# ë¬¸ì œ: ì‚­ì œëœ íŒŒì¼ ê°ì§€ ëª»í•¨, ë©”íƒ€ë°ì´í„° ë³€ê²½ ë¬´ì‹œ

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: í•´ì‹œ + mtime + ì‚­ì œ ê°ì§€
def detect_changes(self, directory):
    current_files = set(glob.glob(f"{directory}/**/*", recursive=True))
    cached_files = set(self.cache.keys())

    new_files = current_files - cached_files      # ì‹ ê·œ
    deleted_files = cached_files - current_files  # ì‚­ì œë¨
    modified_files = [f for f in current_files & cached_files
                      if self._is_modified(f)]    # ìˆ˜ì •ë¨
    return new_files, deleted_files, modified_files
\`\`\`

**ê¸°ì–µí•  ì **: ì‹ ê·œ/ì‚­ì œ/ìˆ˜ì • ì„¸ ê°€ì§€ ìƒíƒœ ëª¨ë‘ ê°ì§€í•´ì•¼ ì™„ì „í•œ ì¦ë¶„ ìˆ˜ì§‘.
      `,
      keyPoints: [
        'ğŸ“„ PDF, Word, HTML, Code ë“± ë‹¤ì–‘í•œ í¬ë§· ì²˜ë¦¬',
        'âœ‚ï¸ ë¬¸ì„œ íƒ€ì…ë³„ ìµœì í™”ëœ ì²­í‚¹ ì „ëµ',
        'ğŸ”„ ì¦ë¶„ ìˆ˜ì§‘ìœ¼ë¡œ íš¨ìœ¨ì  ì—…ë°ì´íŠ¸',
      ],
      practiceGoal: 'ì‹¤ë¬´ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¤ì–‘í•œ ë¬¸ì„œ í¬ë§·ì„ ì²˜ë¦¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤',
    }),

    // ========================================
    // Task 3: ê³ ê¸‰ RAG ì²´ì¸ êµ¬í˜„ (45ë¶„)
    // ========================================
    createCodeTask('w5d5-advanced-rag-chain', 'ê³ ê¸‰ RAG ì²´ì¸ êµ¬í˜„', 45, {
      introduction: `
## í”„ë¡œë•ì…˜ê¸‰ RAG ì²´ì¸

Day 4ì—ì„œ ë°°ìš´ ë‚´ìš©ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ ìˆ˜ì¤€ìœ¼ë¡œ ë°œì „ì‹œí‚µë‹ˆë‹¤.

\`\`\`python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from pydantic import BaseModel, Field
from typing import Optional, Literal
from dataclasses import dataclass
from enum import Enum
import time

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AnswerQuality(str, Enum):
    HIGH = "high"           # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ë‹µë³€ ê°€ëŠ¥
    MEDIUM = "medium"       # ì¶”ë¡  í•„ìš”
    LOW = "low"             # ë¶ˆí™•ì‹¤
    NOT_FOUND = "not_found" # ì •ë³´ ì—†ìŒ

class RAGResponse(BaseModel):
    """êµ¬ì¡°í™”ëœ RAG ì‘ë‹µ"""
    answer: str = Field(description="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€")
    confidence: float = Field(ge=0, le=1, description="ì‹ ë¢°ë„ (0-1)")
    quality: AnswerQuality = Field(description="ë‹µë³€ í’ˆì§ˆ")
    sources: list[dict] = Field(description="ì°¸ì¡° ë¬¸ì„œ ì •ë³´")
    reasoning: Optional[str] = Field(default=None, description="ì¶”ë¡  ê³¼ì •")
    follow_up_questions: list[str] = Field(default=[], description="í›„ì† ì§ˆë¬¸ ì œì•ˆ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì¿¼ë¦¬ ë¶„ì„ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class QueryAnalysis(BaseModel):
    """ì¿¼ë¦¬ ë¶„ì„ ê²°ê³¼"""
    intent: Literal["factual", "analytical", "comparison", "procedural"] = Field(
        description="ì§ˆë¬¸ ì˜ë„"
    )
    entities: list[str] = Field(description="ì¶”ì¶œëœ ì—”í‹°í‹°")
    keywords: list[str] = Field(description="ê²€ìƒ‰ í‚¤ì›Œë“œ")
    requires_multi_hop: bool = Field(description="ë©€í‹°í™‰ ì¶”ë¡  í•„ìš” ì—¬ë¶€")
    language: str = Field(description="ì§ˆë¬¸ ì–¸ì–´")

class QueryAnalyzer:
    """LLM ê¸°ë°˜ ì¿¼ë¦¬ ë¶„ì„"""

    def __init__(self, llm: ChatOpenAI):
        self.llm = llm.with_structured_output(QueryAnalysis)
        self.prompt = ChatPromptTemplate.from_template("""
        Analyze the following question and extract:
        1. Intent: factual (ë‹¨ìˆœ ì‚¬ì‹¤), analytical (ë¶„ì„), comparison (ë¹„êµ), procedural (ë°©ë²•)
        2. Entities: ì£¼ìš” ê°œì²´ (íšŒì‚¬ëª…, ì œí’ˆëª…, ê¸°ìˆ ëª… ë“±)
        3. Keywords: ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œ
        4. Multi-hop: ì—¬ëŸ¬ ë¬¸ì„œ ì¡°í•©ì´ í•„ìš”í•œì§€
        5. Language: ì§ˆë¬¸ ì–¸ì–´

        Question: {question}
        """)

    async def analyze(self, question: str) -> QueryAnalysis:
        chain = self.prompt | self.llm
        return await chain.ainvoke({"question": question})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì ì‘í˜• ê²€ìƒ‰ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AdaptiveRetriever:
    """ì¿¼ë¦¬ ë¶„ì„ì— ë”°ë¥¸ ì ì‘í˜• ê²€ìƒ‰"""

    def __init__(self, vectorstore: Chroma, llm: ChatOpenAI):
        self.vectorstore = vectorstore
        self.llm = llm

        # ê¸°ë³¸ ê²€ìƒ‰ê¸°ë“¤
        self.similarity_retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}
        )

        self.mmr_retriever = vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 5, "fetch_k": 20, "lambda_mult": 0.7}
        )

        # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ê²€ìƒ‰ê¸°
        compressor = LLMChainExtractor.from_llm(llm)
        self.compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=self.similarity_retriever
        )

    async def retrieve(self, question: str, analysis: QueryAnalysis) -> list:
        """ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìµœì  ê²€ìƒ‰ ì „ëµ ì„ íƒ"""

        # ë¹„êµ ì§ˆë¬¸: MMRë¡œ ë‹¤ì–‘í•œ ê´€ì  í™•ë³´
        if analysis.intent == "comparison":
            docs = await self.mmr_retriever.ainvoke(question)

        # ë¶„ì„ ì§ˆë¬¸: ì••ì¶•ìœ¼ë¡œ ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        elif analysis.intent == "analytical":
            docs = await self.compression_retriever.ainvoke(question)

        # ë©€í‹°í™‰ í•„ìš”: ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        elif analysis.requires_multi_hop:
            docs = await self.vectorstore.asimilarity_search(question, k=10)

        # ê¸°ë³¸: ìœ ì‚¬ë„ ê²€ìƒ‰
        else:
            docs = await self.similarity_retriever.ainvoke(question)

        return docs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. í”„ë¡œë•ì…˜ RAG ì²´ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ProductionRAGChain:
    """í”„ë¡œë•ì…˜ ìˆ˜ì¤€ RAG ì²´ì¸"""

    def __init__(
        self,
        vectorstore: Chroma,
        model: str = "gpt-4o-mini",
        temperature: float = 0
    ):
        self.llm = ChatOpenAI(model=model, temperature=temperature)
        self.analyzer = QueryAnalyzer(self.llm)
        self.retriever = AdaptiveRetriever(vectorstore, self.llm)

        self.generation_prompt = ChatPromptTemplate.from_template("""
You are an AI assistant answering questions based on provided documents.

## Instructions
1. Answer ONLY based on the context provided
2. If information is not in context, say "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
3. Cite sources with [Source: filename, page X]
4. Be concise but complete
5. For comparison questions, use tables or bullet points
6. Suggest 2-3 follow-up questions

## Context
{context}

## Question
{question}

## Query Analysis
- Intent: {intent}
- Key entities: {entities}

## Answer (in Korean)
""")

        # êµ¬ì¡°í™”ëœ ì¶œë ¥ìš© LLM
        self.structured_llm = self.llm.with_structured_output(RAGResponse)

    async def ainvoke(self, question: str) -> RAGResponse:
        """ë¹„ë™ê¸° RAG ì‹¤í–‰"""
        start_time = time.time()

        # 1. ì¿¼ë¦¬ ë¶„ì„
        analysis = await self.analyzer.analyze(question)

        # 2. ì ì‘í˜• ê²€ìƒ‰
        docs = await self.retriever.retrieve(question, analysis)

        if not docs:
            return RAGResponse(
                answer="ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                confidence=0.0,
                quality=AnswerQuality.NOT_FOUND,
                sources=[],
                follow_up_questions=[]
            )

        # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._format_context(docs)

        # 4. ë‹µë³€ ìƒì„±
        prompt_input = {
            "context": context,
            "question": question,
            "intent": analysis.intent,
            "entities": ", ".join(analysis.entities)
        }

        chain = self.generation_prompt | self.structured_llm
        response = await chain.ainvoke(prompt_input)

        # 5. ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
        response.sources = [
            {
                "source": doc.metadata.get("source", "Unknown"),
                "page": doc.metadata.get("page", "N/A"),
                "chunk_index": doc.metadata.get("chunk_index", 0)
            }
            for doc in docs
        ]

        # 6. ì‹ ë¢°ë„ ê³„ì‚°
        response.confidence = self._calculate_confidence(response, docs)

        return response

    def _format_context(self, docs: list) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        formatted = []
        for i, doc in enumerate(docs):
            source = doc.metadata.get("source", "Unknown")
            page = doc.metadata.get("page", "N/A")
            formatted.append(
                f"[Document {i+1}] [Source: {source}, Page: {page}]\\n"
                f"{doc.page_content}\\n"
            )
        return "\\n---\\n".join(formatted)

    def _calculate_confidence(self, response: RAGResponse, docs: list) -> float:
        """ì‹ ë¢°ë„ ê³„ì‚°"""
        # ê¸°ë³¸ ì‹ ë¢°ë„ (ë‹µë³€ í’ˆì§ˆ ê¸°ë°˜)
        quality_scores = {
            AnswerQuality.HIGH: 0.9,
            AnswerQuality.MEDIUM: 0.7,
            AnswerQuality.LOW: 0.4,
            AnswerQuality.NOT_FOUND: 0.1
        }

        base_confidence = quality_scores.get(response.quality, 0.5)

        # ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ì¡°ì • (ë” ë§ì€ ë¬¸ì„œ = ë” ë†’ì€ ì‹ ë¢°ë„)
        doc_factor = min(len(docs) / 5, 1.0)  # ìµœëŒ€ 5ê°œê¹Œì§€ ê³ ë ¤

        return base_confidence * 0.7 + doc_factor * 0.3

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ìŠ¤íŠ¸ë¦¬ë° RAG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class StreamingRAGChain:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ RAG"""

    def __init__(self, vectorstore: Chroma, model: str = "gpt-4o-mini"):
        self.llm = ChatOpenAI(model=model, streaming=True)
        self.retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        self.prompt = ChatPromptTemplate.from_template("""
Context: {context}
Question: {question}
Answer (cite sources with [Source: filename]):""")

    async def astream(self, question: str):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""

        # 1. ë¬¸ì„œ ê²€ìƒ‰ (ìŠ¤íŠ¸ë¦¬ë° ì „)
        docs = await self.retriever.ainvoke(question)
        context = "\\n\\n".join(d.page_content for d in docs)

        # 2. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        chain = self.prompt | self.llm | StrOutputParser()

        async for chunk in chain.astream({
            "context": context,
            "question": question
        }):
            yield chunk

        # 3. ì†ŒìŠ¤ ì •ë³´ (ë§ˆì§€ë§‰ì—)
        yield "\\n\\n---\\n**ì°¸ì¡° ë¬¸ì„œ:**\\n"
        for doc in docs:
            source = doc.metadata.get("source", "Unknown")
            yield f"- {source}\\n"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ìš© ì˜ˆì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main():
    # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vectorstore = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embeddings
    )

    # 1. êµ¬ì¡°í™”ëœ RAG
    rag = ProductionRAGChain(vectorstore)
    response = await rag.ainvoke("ì‚¼ì„±ì „ìì˜ 2024ë…„ ë°˜ë„ì²´ ì „ëµì€?")

    print(f"ë‹µë³€: {response.answer}")
    print(f"ì‹ ë¢°ë„: {response.confidence:.2f}")
    print(f"í’ˆì§ˆ: {response.quality}")
    print(f"ì†ŒìŠ¤: {response.sources}")
    print(f"í›„ì† ì§ˆë¬¸: {response.follow_up_questions}")

    # 2. ìŠ¤íŠ¸ë¦¬ë° RAG
    streaming_rag = StreamingRAGChain(vectorstore)

    print("\\nìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ:")
    async for chunk in streaming_rag.astream("AI ë°˜ë„ì²´ ì‹œì¥ ì „ë§ì€?"):
        print(chunk, end="", flush=True)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
\`\`\`
      `,
      keyPoints: [
        'ì¿¼ë¦¬ ë¶„ì„ â†’ ì ì‘í˜• ê²€ìƒ‰ â†’ êµ¬ì¡°í™”ëœ ì‘ë‹µ',
        'Pydanticìœ¼ë¡œ íƒ€ì… ì•ˆì „í•œ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ',
        'ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‚¬ìš©ì ê²½í—˜ ê°œì„ ',
      ],
      practiceGoal: 'ì¿¼ë¦¬ ë¶„ì„, ì ì‘í˜• ê²€ìƒ‰, êµ¬ì¡°í™”ëœ ì‘ë‹µì„ ê°–ì¶˜ í”„ë¡œë•ì…˜ê¸‰ RAG ì²´ì¸ì„ êµ¬í˜„í•œë‹¤',
    }),

    // ========================================
    // Task 4: ì—ëŸ¬ í•¸ë“¤ë§ & ëª¨ë‹ˆí„°ë§ (40ë¶„)
    // ========================================
    createCodeTask('w5d5-error-monitoring', 'ì—ëŸ¬ í•¸ë“¤ë§ & ëª¨ë‹ˆí„°ë§', 40, {
      introduction: `
## í”„ë¡œë•ì…˜ RAGì˜ í˜„ì‹¤

**ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤:**

\`\`\`
ğŸ“Š ì‹¤ì œ RAG ì„œë¹„ìŠ¤ ì¥ì•  í†µê³„ (ê°€ìƒ)
â”œâ”€â”€ API Rate Limit: 35%
â”œâ”€â”€ Timeout: 25%
â”œâ”€â”€ Empty Results: 20%
â”œâ”€â”€ Hallucination: 12%
â”œâ”€â”€ Invalid Input: 5%
â””â”€â”€ Unknown: 3%
\`\`\`

## ê°•ê±´í•œ ì—ëŸ¬ í•¸ë“¤ë§

\`\`\`python
from enum import Enum
from typing import TypeVar, Generic, Union
from dataclasses import dataclass
import asyncio
import time
from functools import wraps

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì—ëŸ¬ íƒ€ì… ì •ì˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class RAGErrorType(Enum):
    RATE_LIMIT = "rate_limit"
    TIMEOUT = "timeout"
    EMPTY_CONTEXT = "empty_context"
    GENERATION_FAILED = "generation_failed"
    INVALID_INPUT = "invalid_input"
    VECTOR_DB_ERROR = "vector_db_error"
    EMBEDDING_ERROR = "embedding_error"
    UNKNOWN = "unknown"

@dataclass
class RAGError:
    """êµ¬ì¡°í™”ëœ ì—ëŸ¬ ì •ë³´"""
    type: RAGErrorType
    message: str
    recoverable: bool
    retry_after: float = 0  # ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„
    details: dict = None

T = TypeVar('T')

@dataclass
class Result(Generic[T]):
    """ì„±ê³µ/ì‹¤íŒ¨ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” Result íƒ€ì…"""
    success: bool
    value: T = None
    error: RAGError = None

    @classmethod
    def ok(cls, value: T) -> 'Result[T]':
        return cls(success=True, value=value)

    @classmethod
    def fail(cls, error: RAGError) -> 'Result[T]':
        return cls(success=False, error=error)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def retry_with_exponential_backoff(
    max_retries: int = 3,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    exponential_base: float = 2.0,
    retryable_errors: tuple = (Exception,)
):
    """ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""

    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            delay = base_delay
            last_error = None

            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)

                except retryable_errors as e:
                    last_error = e

                    if attempt == max_retries:
                        raise

                    # Rate limit ì—ëŸ¬ ì‹œ Retry-After í—¤ë” ì‚¬ìš©
                    if hasattr(e, 'retry_after'):
                        delay = e.retry_after

                    print(f"Attempt {attempt + 1} failed: {e}")
                    print(f"Retrying in {delay:.2f} seconds...")

                    await asyncio.sleep(delay)
                    delay = min(delay * exponential_base, max_delay)

            raise last_error

        return wrapper
    return decorator

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Fallback íŒ¨í„´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class FallbackRAG:
    """ë‹¤ì¤‘ í´ë°±ì„ ì§€ì›í•˜ëŠ” RAG"""

    def __init__(self):
        from langchain_openai import ChatOpenAI
        from langchain_anthropic import ChatAnthropic

        self.llms = [
            ("openai_primary", ChatOpenAI(model="gpt-4o-mini")),
            ("openai_backup", ChatOpenAI(model="gpt-3.5-turbo")),
            ("anthropic", ChatAnthropic(model="claude-3-haiku-20240307")),
        ]

        self.cached_responses = {}

    async def generate(self, prompt: str) -> Result[str]:
        """ìˆœì°¨ì  í´ë°± ì‹œë„"""

        # 1. ìºì‹œ í™•ì¸
        cache_key = hash(prompt)
        if cache_key in self.cached_responses:
            return Result.ok(self.cached_responses[cache_key])

        errors = []

        # 2. ìˆœì°¨ì  ì‹œë„
        for name, llm in self.llms:
            try:
                response = await llm.ainvoke(prompt)
                answer = response.content

                # ìºì‹œ ì €ì¥
                self.cached_responses[cache_key] = answer

                return Result.ok(answer)

            except Exception as e:
                error = RAGError(
                    type=self._classify_error(e),
                    message=str(e),
                    recoverable=True,
                    details={"provider": name}
                )
                errors.append(error)
                continue

        # 3. ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        return Result.fail(RAGError(
            type=RAGErrorType.GENERATION_FAILED,
            message="All LLM providers failed",
            recoverable=False,
            details={"errors": [e.message for e in errors]}
        ))

    def _classify_error(self, e: Exception) -> RAGErrorType:
        """ì—ëŸ¬ ë¶„ë¥˜"""
        error_str = str(e).lower()

        if "rate" in error_str or "429" in error_str:
            return RAGErrorType.RATE_LIMIT
        elif "timeout" in error_str or "timed out" in error_str:
            return RAGErrorType.TIMEOUT
        else:
            return RAGErrorType.UNKNOWN

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class CircuitBreaker:
    """ì„œí‚· ë¸Œë ˆì´ì»¤ - ì—°ì† ì‹¤íŒ¨ ì‹œ ì°¨ë‹¨"""

    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: float = 60.0,
        half_open_requests: int = 1
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.half_open_requests = half_open_requests

        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half_open

    async def call(self, func, *args, **kwargs):
        """ì„œí‚· ë¸Œë ˆì´ì»¤ë¥¼ í†µí•œ í•¨ìˆ˜ í˜¸ì¶œ"""

        # Open ìƒíƒœ: ë°”ë¡œ ì‹¤íŒ¨
        if self.state == "open":
            if self._should_attempt_reset():
                self.state = "half_open"
            else:
                raise CircuitBreakerOpenError(
                    f"Circuit breaker is open. Retry after {self._time_until_reset():.1f}s"
                )

        try:
            result = await func(*args, **kwargs)

            # ì„±ê³µ ì‹œ ë¦¬ì…‹
            self._on_success()
            return result

        except Exception as e:
            self._on_failure()
            raise

    def _on_success(self):
        """ì„±ê³µ ì‹œ ìƒíƒœ ë¦¬ì…‹"""
        self.failure_count = 0
        self.state = "closed"

    def _on_failure(self):
        """ì‹¤íŒ¨ ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            self.state = "open"

    def _should_attempt_reset(self) -> bool:
        """ë³µêµ¬ ì‹œë„ ì—¬ë¶€"""
        if self.last_failure_time is None:
            return True
        return time.time() - self.last_failure_time >= self.recovery_timeout

    def _time_until_reset(self) -> float:
        """ë³µêµ¬ê¹Œì§€ ë‚¨ì€ ì‹œê°„"""
        if self.last_failure_time is None:
            return 0
        return max(0, self.recovery_timeout - (time.time() - self.last_failure_time))

class CircuitBreakerOpenError(Exception):
    pass

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ëª¨ë‹ˆí„°ë§ & ë¡œê¹…
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import structlog
from dataclasses import dataclass, field
from datetime import datetime
from collections import deque

# êµ¬ì¡°í™”ëœ ë¡œê±° ì„¤ì •
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)

logger = structlog.get_logger()

@dataclass
class RAGMetrics:
    """RAG íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­"""
    request_id: str
    question: str
    start_time: datetime = field(default_factory=datetime.now)

    # íƒ€ì´ë°
    embedding_time: float = 0
    retrieval_time: float = 0
    generation_time: float = 0
    total_time: float = 0

    # ê²°ê³¼
    doc_count: int = 0
    token_count: int = 0
    success: bool = True
    error_type: str = None

    def to_dict(self) -> dict:
        return {
            "request_id": self.request_id,
            "question_preview": self.question[:50],
            "embedding_time_ms": self.embedding_time * 1000,
            "retrieval_time_ms": self.retrieval_time * 1000,
            "generation_time_ms": self.generation_time * 1000,
            "total_time_ms": self.total_time * 1000,
            "doc_count": self.doc_count,
            "token_count": self.token_count,
            "success": self.success,
            "error_type": self.error_type
        }

class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""

    def __init__(self, window_size: int = 1000):
        self.metrics_buffer = deque(maxlen=window_size)

    def record(self, metrics: RAGMetrics):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        self.metrics_buffer.append(metrics)

        # êµ¬ì¡°í™”ëœ ë¡œê·¸
        logger.info(
            "rag_request_completed",
            **metrics.to_dict()
        )

    def get_stats(self) -> dict:
        """í†µê³„ ê³„ì‚°"""
        if not self.metrics_buffer:
            return {}

        total = len(self.metrics_buffer)
        success_count = sum(1 for m in self.metrics_buffer if m.success)

        avg_latency = sum(m.total_time for m in self.metrics_buffer) / total
        p95_latency = sorted(m.total_time for m in self.metrics_buffer)[int(total * 0.95)]

        return {
            "total_requests": total,
            "success_rate": success_count / total,
            "avg_latency_ms": avg_latency * 1000,
            "p95_latency_ms": p95_latency * 1000,
            "avg_doc_count": sum(m.doc_count for m in self.metrics_buffer) / total
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. í†µí•©: ëª¨ë‹ˆí„°ë§ RAG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MonitoredRAG:
    """ëª¨ë‹ˆí„°ë§ì´ í†µí•©ëœ RAG"""

    def __init__(self, vectorstore, llm):
        self.vectorstore = vectorstore
        self.llm = llm
        self.metrics_collector = MetricsCollector()
        self.circuit_breaker = CircuitBreaker()

    async def answer(self, question: str) -> Result[str]:
        import uuid

        metrics = RAGMetrics(
            request_id=str(uuid.uuid4()),
            question=question
        )

        try:
            # ì„œí‚· ë¸Œë ˆì´ì»¤ ì ìš©
            answer = await self.circuit_breaker.call(
                self._answer_impl,
                question,
                metrics
            )

            metrics.total_time = (datetime.now() - metrics.start_time).total_seconds()
            self.metrics_collector.record(metrics)

            return Result.ok(answer)

        except CircuitBreakerOpenError as e:
            metrics.success = False
            metrics.error_type = "circuit_breaker_open"
            self.metrics_collector.record(metrics)

            return Result.fail(RAGError(
                type=RAGErrorType.UNKNOWN,
                message=str(e),
                recoverable=True,
                retry_after=self.circuit_breaker._time_until_reset()
            ))

        except Exception as e:
            metrics.success = False
            metrics.error_type = str(type(e).__name__)
            self.metrics_collector.record(metrics)
            raise

    @retry_with_exponential_backoff(max_retries=3)
    async def _answer_impl(self, question: str, metrics: RAGMetrics) -> str:
        # ê²€ìƒ‰
        start = time.time()
        docs = await self.vectorstore.asimilarity_search(question, k=5)
        metrics.retrieval_time = time.time() - start
        metrics.doc_count = len(docs)

        # ìƒì„±
        start = time.time()
        context = "\\n".join(d.page_content for d in docs)
        response = await self.llm.ainvoke(f"Context: {context}\\n\\nQ: {question}")
        metrics.generation_time = time.time() - start

        return response.content

# ì‚¬ìš© ì˜ˆì‹œ
async def demo():
    rag = MonitoredRAG(vectorstore, llm)

    # ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬
    questions = ["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3"]

    for q in questions:
        result = await rag.answer(q)
        if result.success:
            print(f"âœ… {result.value[:100]}...")
        else:
            print(f"âŒ Error: {result.error.message}")

    # í†µê³„ í™•ì¸
    stats = rag.metrics_collector.get_stats()
    print(f"\\nğŸ“Š ì„±ê³µë¥ : {stats['success_rate']:.1%}")
    print(f"ğŸ“Š í‰ê·  ì§€ì—°: {stats['avg_latency_ms']:.1f}ms")
    print(f"ğŸ“Š P95 ì§€ì—°: {stats['p95_latency_ms']:.1f}ms")
\`\`\`
      `,
      keyPoints: [
        'Result íƒ€ì…ìœ¼ë¡œ ì„±ê³µ/ì‹¤íŒ¨ ëª…ì‹œì  ì²˜ë¦¬',
        'ì§€ìˆ˜ ë°±ì˜¤í”„ + ì„œí‚· ë¸Œë ˆì´ì»¤ë¡œ ì¥ì•  ëŒ€ì‘',
        'êµ¬ì¡°í™”ëœ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§',
      ],
      practiceGoal: 'í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ì—ëŸ¬ í•¸ë“¤ë§ê³¼ ëª¨ë‹ˆí„°ë§ì„ êµ¬í˜„í•œë‹¤',
    }),

    // ========================================
    // Task 5: Streamlit í”„ë¡œë•ì…˜ UI (35ë¶„)
    // ========================================
    createCodeTask('w5d5-streamlit-prod', 'Streamlit í”„ë¡œë•ì…˜ UI', 35, {
      introduction: `
## í”„ë¡œë•ì…˜ê¸‰ Streamlit UI

\`\`\`python
# app.py - í”„ë¡œë•ì…˜ê¸‰ RAG ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜

import streamlit as st
import asyncio
from datetime import datetime
from pathlib import Path
import os

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. í˜ì´ì§€ ì„¤ì • & ìŠ¤íƒ€ì¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(
    page_title="ğŸ“š Enterprise Document Q&A",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2rem;
        font-weight: bold;
        color: #1a1a2e;
        margin-bottom: 1rem;
    }
    .source-card {
        background-color: #f8f9fa;
        border-radius: 8px;
        padding: 12px;
        margin: 8px 0;
        border-left: 4px solid #4CAF50;
    }
    .confidence-high { color: #4CAF50; font-weight: bold; }
    .confidence-medium { color: #FFC107; font-weight: bold; }
    .confidence-low { color: #F44336; font-weight: bold; }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 10px;
        padding: 20px;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def init_session_state():
    defaults = {
        "vectorstore": None,
        "messages": [],
        "processed_docs": [],
        "total_chunks": 0,
        "query_count": 0,
        "avg_latency": 0,
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

init_session_state()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì‚¬ì´ë“œë°” - ì„¤ì • & ë¬¸ì„œ ì—…ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    st.markdown("## âš™ï¸ Settings")

    # API Key (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì…ë ¥)
    api_key = os.getenv("OPENAI_API_KEY") or st.text_input(
        "OpenAI API Key",
        type="password",
        help="API í‚¤ëŠ” ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
    )

    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key

    st.divider()

    # RAG ì„¤ì •
    st.markdown("### ğŸ”§ RAG Configuration")

    chunk_size = st.slider("Chunk Size", 200, 2000, 1000, 100)
    chunk_overlap = st.slider("Chunk Overlap", 0, 500, 200, 50)
    k_results = st.slider("Top-K Results", 1, 10, 5)

    search_type = st.selectbox(
        "Search Type",
        ["similarity", "mmr"],
        help="MMR: ë‹¤ì–‘ì„± í™•ë³´ë¥¼ ìœ„í•œ ê²€ìƒ‰"
    )

    st.divider()

    # ë¬¸ì„œ ì—…ë¡œë“œ
    st.markdown("### ğŸ“ Document Upload")

    uploaded_files = st.file_uploader(
        "Upload Documents",
        type=["pdf", "docx", "txt", "md"],
        accept_multiple_files=True,
        help="PDF, Word, TXT, Markdown ì§€ì›"
    )

    if uploaded_files and api_key:
        if st.button("ğŸš€ Process Documents", use_container_width=True):
            with st.spinner("Processing documents..."):
                process_documents(uploaded_files, chunk_size, chunk_overlap)

    # ì²˜ë¦¬ëœ ë¬¸ì„œ í‘œì‹œ
    if st.session_state.processed_docs:
        st.markdown("### ğŸ“š Loaded Documents")
        for doc in st.session_state.processed_docs:
            st.markdown(f"- {doc['name']} ({doc['chunks']} chunks)")

        st.metric("Total Chunks", st.session_state.total_chunks)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_documents(files, chunk_size, chunk_overlap):
    from langchain_openai import OpenAIEmbeddings
    from langchain_chroma import Chroma
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.document_loaders import (
        PyPDFLoader, Docx2txtLoader, TextLoader
    )
    import tempfile

    all_docs = []
    processed_info = []

    for file in files:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.name).suffix) as tmp:
            tmp.write(file.getvalue())
            tmp_path = tmp.name

        # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ë¡œë” ì„ íƒ
        ext = Path(file.name).suffix.lower()
        if ext == ".pdf":
            loader = PyPDFLoader(tmp_path)
        elif ext == ".docx":
            loader = Docx2txtLoader(tmp_path)
        else:
            loader = TextLoader(tmp_path)

        docs = loader.load()

        # ì²­í‚¹
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        chunks = splitter.split_documents(docs)

        all_docs.extend(chunks)
        processed_info.append({
            "name": file.name,
            "chunks": len(chunks)
        })

        os.unlink(tmp_path)

    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    st.session_state.vectorstore = Chroma.from_documents(
        documents=all_docs,
        embedding=embeddings
    )

    st.session_state.processed_docs = processed_info
    st.session_state.total_chunks = len(all_docs)

    st.success(f"âœ… Processed {len(files)} files, {len(all_docs)} chunks")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ë©”ì¸ ì˜ì—­ - ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.markdown('<p class="main-header">ğŸ“š Enterprise Document Q&A</p>', unsafe_allow_html=True)

# ë©”íŠ¸ë¦­ í‘œì‹œ
if st.session_state.vectorstore:
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ğŸ“„ Documents", len(st.session_state.processed_docs))
    with col2:
        st.metric("ğŸ“¦ Chunks", st.session_state.total_chunks)
    with col3:
        st.metric("â“ Queries", st.session_state.query_count)
    with col4:
        st.metric("âš¡ Avg Latency", f"{st.session_state.avg_latency:.2f}s")

st.divider()

# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

        # ì†ŒìŠ¤ í‘œì‹œ (assistant ë©”ì‹œì§€)
        if msg["role"] == "assistant" and "sources" in msg:
            with st.expander("ğŸ“š View Sources"):
                for src in msg["sources"]:
                    st.markdown(f"""
                    <div class="source-card">
                        <strong>{src['source']}</strong><br>
                        Page {src.get('page', 'N/A')} |
                        Chunk {src.get('chunk_index', 0)}
                    </div>
                    """, unsafe_allow_html=True)

        # ì‹ ë¢°ë„ í‘œì‹œ
        if msg["role"] == "assistant" and "confidence" in msg:
            conf = msg["confidence"]
            conf_class = "high" if conf > 0.8 else "medium" if conf > 0.5 else "low"
            st.markdown(f"""
                <span class="confidence-{conf_class}">
                    Confidence: {conf:.0%}
                </span>
            """, unsafe_allow_html=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if st.session_state.vectorstore:
    if question := st.chat_input("Ask a question about your documents..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": question})

        with st.chat_message("user"):
            st.markdown(question)

        # RAG ì‹¤í–‰
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                start_time = datetime.now()
                answer, sources, confidence = run_rag(question, k_results, search_type)
                latency = (datetime.now() - start_time).total_seconds()

                # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
                placeholder = st.empty()
                full_response = ""
                for char in answer:
                    full_response += char
                    placeholder.markdown(full_response + "â–Œ")
                    import time
                    time.sleep(0.01)
                placeholder.markdown(full_response)

                # ì†ŒìŠ¤ í‘œì‹œ
                with st.expander("ğŸ“š View Sources"):
                    for src in sources:
                        st.markdown(f"- **{src['source']}**, Page {src.get('page', 'N/A')}")

                # ì‹ ë¢°ë„
                conf_class = "high" if confidence > 0.8 else "medium" if confidence > 0.5 else "low"
                st.markdown(f"<span class='confidence-{conf_class}'>Confidence: {confidence:.0%}</span>",
                           unsafe_allow_html=True)

        # ë©”ì‹œì§€ ì €ì¥
        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "sources": sources,
            "confidence": confidence
        })

        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        st.session_state.query_count += 1
        n = st.session_state.query_count
        st.session_state.avg_latency = (
            (st.session_state.avg_latency * (n - 1) + latency) / n
        )

else:
    st.info("ğŸ‘ˆ Upload documents in the sidebar to get started!")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. RAG ì‹¤í–‰ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_rag(question: str, k: int, search_type: str):
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables import RunnablePassthrough

    # ê²€ìƒ‰
    retriever = st.session_state.vectorstore.as_retriever(
        search_type=search_type,
        search_kwargs={"k": k}
    )

    docs = retriever.invoke(question)

    # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
    sources = [
        {
            "source": doc.metadata.get("source", "Unknown"),
            "page": doc.metadata.get("page"),
            "chunk_index": doc.metadata.get("chunk_index", 0)
        }
        for doc in docs
    ]

    # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "\\n\\n---\\n\\n".join(
        f"[Source: {doc.metadata.get('source', 'Unknown')}]\\n{doc.page_content}"
        for doc in docs
    )

    # í”„ë¡¬í”„íŠ¸
    prompt = ChatPromptTemplate.from_template("""
You are an expert assistant answering questions based on provided documents.

Instructions:
1. Answer based ONLY on the context provided
2. If unsure, say "I couldn't find this information in the documents"
3. Be concise and accurate
4. Use Korean for the response

Context:
{context}

Question: {question}

Answer:""")

    # ì²´ì¸
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    chain = prompt | llm | StrOutputParser()

    answer = chain.invoke({"context": context, "question": question})

    # ì‹ ë¢°ë„ ê³„ì‚° (ë‹¨ìˆœí™”)
    confidence = min(len(docs) / k, 1.0) * 0.8 + 0.2

    return answer, sources, confidence

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. í‘¸í„°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.divider()
st.markdown("""
<div style='text-align: center; color: #666; font-size: 0.8rem;'>
    Built with LangChain + Streamlit |
    <a href='https://github.com/your-repo'>GitHub</a>
</div>
""", unsafe_allow_html=True)
\`\`\`

## ì‹¤í–‰ ë°©ë²•

\`\`\`bash
# ì„¤ì¹˜
pip install streamlit langchain langchain-openai langchain-chroma chromadb pypdf python-docx

# ì‹¤í–‰
streamlit run app.py

# í™˜ê²½ë³€ìˆ˜ë¡œ API í‚¤ ì„¤ì • (ì„ íƒ)
OPENAI_API_KEY=sk-... streamlit run app.py
\`\`\`
      `,
      keyPoints: [
        'ì»¤ìŠ¤í…€ CSSë¡œ ì „ë¬¸ì ì¸ UI',
        'ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ',
        'ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ì™€ ì‹ ë¢°ë„ í‘œì‹œ',
      ],
      practiceGoal: 'í”„ë¡œë•ì…˜ê¸‰ RAG ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ Streamlitìœ¼ë¡œ êµ¬í˜„í•œë‹¤',
    }),

    // ========================================
    // Task 6: ì¢…í•© í€´ì¦ˆ (20ë¶„)
    // ========================================
    createQuizTask('w5d5-quiz', 'Day 5 ì¢…í•© í€´ì¦ˆ', 20, {
      introduction: `
## Day 5 í•µì‹¬ ê°œë… ì ê²€

í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œ êµ¬ì¶•ì— í•„ìš”í•œ í•µì‹¬ ê°œë…ì„ í™•ì¸í•©ë‹ˆë‹¤.
      `,
      questions: [
        {
          id: 'w5d5-q1',
          question: 'í”„ë¡œë•ì…˜ RAGì˜ 5ê°€ì§€ ì„¤ê³„ ì›ì¹™ ì¤‘ "ì—°ì† ì‹¤íŒ¨ ì‹œ ì¼ì • ì‹œê°„ ë™ì•ˆ ìš”ì²­ì„ ì°¨ë‹¨í•˜ì—¬ ì‹œìŠ¤í…œì„ ë³´í˜¸í•˜ëŠ” íŒ¨í„´"ì€?',
          options: [
            'Retry with Backoff',
            'Circuit Breaker',
            'Rate Limiter',
            'Load Balancer'
          ],
          correctAnswer: 1,
          explanation: 'Circuit BreakerëŠ” ì—°ì† ì‹¤íŒ¨ ì‹œ ì¼ì • ì‹œê°„ ë™ì•ˆ ìš”ì²­ì„ ì°¨ë‹¨(Open)í•˜ê³ , ë³µêµ¬ ì‹œê°„ í›„ Half-Open ìƒíƒœì—ì„œ í…ŒìŠ¤íŠ¸ ìš”ì²­ì„ ë³´ë‚´ ì‹œìŠ¤í…œì„ ë³´í˜¸í•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d5-q2',
          question: 'ë‹¤ìŒ ì¤‘ í”„ë¡œë•ì…˜ RAGì—ì„œ ë¹„ìš©ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì ì ˆí•˜ì§€ ì•Šì€ ê²ƒì€?',
          options: [
            'ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¥¸ ëª¨ë¸ í‹°ì–´ë§ (simple: mini, complex: GPT-4)',
            'ì„ë² ë”© ê²°ê³¼ ìºì‹±',
            'ëª¨ë“  ì¿¼ë¦¬ì— GPT-4o ì‚¬ìš©í•˜ì—¬ í’ˆì§ˆ ê·¹ëŒ€í™”',
            'ì ì ˆí•œ ì²­í¬ í¬ê¸°ë¡œ í† í° ì ˆì•½'
          ],
          correctAnswer: 2,
          explanation: 'ëª¨ë“  ì¿¼ë¦¬ì— ìµœê³ ê¸‰ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´ ë¹„ìš©ì´ ê¸‰ì¦í•©ë‹ˆë‹¤. ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” í‹°ì–´ë§ì´ ë¹„ìš© íš¨ìœ¨ì ì…ë‹ˆë‹¤.',
        },
        {
          id: 'w5d5-q3',
          question: 'RAG ì‹œìŠ¤í…œì—ì„œ í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ê³µê²©ì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì€?',
          options: [
            'ë” í° LLM ëª¨ë¸ ì‚¬ìš©',
            'ì…ë ¥ ê²€ì¦ ë° ìœ„í—˜ íŒ¨í„´ í•„í„°ë§',
            'ì²­í¬ í¬ê¸° ì¤„ì´ê¸°',
            'ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ëŠ˜ë¦¬ê¸°'
          ],
          correctAnswer: 1,
          explanation: 'í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ì€ "ignore previous instructions" ê°™ì€ ì•…ì˜ì  íŒ¨í„´ì„ íƒì§€í•˜ì—¬ í•„í„°ë§í•˜ê±°ë‚˜, ì…ë ¥ ê¸¸ì´ ì œí•œ, íŠ¹ìˆ˜ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ ë“±ìœ¼ë¡œ ë°©ì§€í•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d5-q4',
          question: 'Streamlitì—ì„œ RAG ì‘ë‹µì˜ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ êµ¬í˜„í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€?',
          options: [
            'st.cache_data ë°ì½”ë ˆì´í„°',
            'st.empty()ì™€ placeholder.markdown() ì¡°í•©',
            'st.session_state ì§ì ‘ ìˆ˜ì •',
            'st.rerun() ë°˜ë³µ í˜¸ì¶œ'
          ],
          correctAnswer: 1,
          explanation: 'st.empty()ë¡œ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ìƒì„±í•˜ê³ , ì‘ë‹µì˜ ê° ë¬¸ì/ì²­í¬ë¥¼ placeholder.markdown()ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ë©´ íƒ€ì´í•‘ íš¨ê³¼ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        },
        {
          id: 'w5d5-q5',
          question: 'ë©€í‹° í¬ë§· ë¬¸ì„œ ì²˜ë¦¬ì—ì„œ ìŠ¤ìº”ëœ PDF(ì´ë¯¸ì§€ PDF)ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¸°ìˆ ì€?',
          options: [
            'Text Splitter',
            'OCR (Optical Character Recognition)',
            'Embedding',
            'Re-ranking'
          ],
          correctAnswer: 1,
          explanation: 'OCRì€ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¸ì‹í•˜ëŠ” ê¸°ìˆ ë¡œ, ìŠ¤ìº”ëœ PDFë‚˜ ì‚¬ì§„ ì† í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. Pytesseract, EasyOCR ë“±ì„ í™œìš©í•©ë‹ˆë‹¤.',
        },
      ],
      keyPoints: [
        'Circuit Breaker = ì—°ì† ì‹¤íŒ¨ ì‹œ ì°¨ë‹¨',
        'ì¿¼ë¦¬ ë³µì¡ë„ë³„ ëª¨ë¸ í‹°ì–´ë§ìœ¼ë¡œ ë¹„ìš© ìµœì í™”',
        'ì…ë ¥ ê²€ì¦ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ë°©ì§€',
        'st.empty() + placeholderë¡œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼',
        'OCRë¡œ ìŠ¤ìº” PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ',
      ],
      practiceGoal: 'í”„ë¡œë•ì…˜ RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ê°œë…ì„ í™•ì¸í•œë‹¤',
    }),
  ],

  // ========================================
  // Challenge: ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì‹œìŠ¤í…œ êµ¬ì¶• (70ë¶„)
  // ========================================
  challenge: createChallengeTask('w5d5-challenge', 'Challenge: ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì‹œìŠ¤í…œ êµ¬ì¶•', 70, {
    introduction: `
## ğŸ† Week 5 ìµœì¢… í”„ë¡œì íŠ¸

Week 5ì—ì„œ í•™ìŠµí•œ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ **ì‹¤ì œ ì„œë¹„ìŠ¤ ìˆ˜ì¤€ì˜ RAG ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ“‹ ì‹œë‚˜ë¦¬ì˜¤

> **ìƒí™©**: ë‹¹ì‹ ì€ ìŠ¤íƒ€íŠ¸ì—…ì˜ AI ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
> íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ(ê¸°ìˆ  ë¬¸ì„œ, ì •ì±…, ê°€ì´ë“œë¼ì¸)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
> ì§ì›ë“¤ì´ ì§ˆë¬¸í•  ìˆ˜ ìˆëŠ” "ì‚¬ë‚´ AI ì–´ì‹œìŠ¤í„´íŠ¸"ë¥¼ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤.

## âœ… í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ (MVP)

### 1. ë©€í‹° í¬ë§· ë¬¸ì„œ ì§€ì›
- [ ] PDF, Word, Markdown íŒŒì¼ ì²˜ë¦¬
- [ ] ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì—…ë¡œë“œ
- [ ] ì²˜ë¦¬ ì§„í–‰ë¥  í‘œì‹œ

### 2. ê³ ê¸‰ ê²€ìƒ‰
- [ ] í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Vector + BM25) ë˜ëŠ” MMR
- [ ] ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¡°ì ˆ ê°€ëŠ¥

### 3. ëŒ€í™”í˜• RAG
- [ ] ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€
- [ ] ì¶œì²˜ì™€ ì‹ ë¢°ë„ í‘œì‹œ

### 4. í”„ë¡œë•ì…˜ í’ˆì§ˆ
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ (API ì‹¤íŒ¨, ë¹ˆ ê²°ê³¼ ë“±)
- [ ] ë¡œë”© ìƒíƒœ í‘œì‹œ
- [ ] ê¸°ë³¸ì ì¸ ì…ë ¥ ê²€ì¦

### 5. UI/UX
- [ ] ê¹”ë”í•œ Streamlit ì¸í„°í˜ì´ìŠ¤
- [ ] ëª¨ë°”ì¼ ë°˜ì‘í˜• (ê¸°ë³¸ Streamlit)

## ğŸŒŸ ë³´ë„ˆìŠ¤ ìš”êµ¬ì‚¬í•­ (ì„ íƒ)

### ë ˆë²¨ 1: í–¥ìƒëœ ê¸°ëŠ¥
- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (íƒ€ì´í•‘ íš¨ê³¼)
- [ ] í›„ì† ì§ˆë¬¸ ì œì•ˆ
- [ ] ëŒ€í™” ë‚´ì—­ ë‚´ë³´ë‚´ê¸° (TXT/JSON)

### ë ˆë²¨ 2: ê³ ê¸‰ ê¸°ëŠ¥
- [ ] Re-ranking ì ìš©
- [ ] ì¿¼ë¦¬ ë¶„ì„ ë° ì˜ë„ í‘œì‹œ
- [ ] ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ (ì¿¼ë¦¬ ìˆ˜, í‰ê·  ì§€ì—° ë“±)

### ë ˆë²¨ 3: ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥
- [ ] ë©€í‹° LLM í´ë°± (OpenAI â†’ Anthropic)
- [ ] ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ (ğŸ‘/ğŸ‘)

## ğŸ“Š í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ê¸°ì¤€ |
|------|------|------|
| í•„ìˆ˜ ê¸°ëŠ¥ ì™„ì„±ë„ | 50% | 5ê°œ í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ êµ¬í˜„ |
| ì½”ë“œ í’ˆì§ˆ | 20% | íƒ€ì… íŒíŠ¸, ì—ëŸ¬ í•¸ë“¤ë§, ëª¨ë“ˆí™” |
| UI/UX | 15% | ì‚¬ìš©ì„±, ë””ìì¸ |
| ë³´ë„ˆìŠ¤ ê¸°ëŠ¥ | 15% | ë ˆë²¨ë³„ ì¶”ê°€ ì ìˆ˜ |

## ğŸ“ ì œì¶œë¬¼

1. **GitHub ì €ì¥ì†Œ**
   - ì „ì²´ ì†ŒìŠ¤ ì½”ë“œ
   - README.md (ì„¤ì¹˜ ë°©ë²•, ìŠ¤í¬ë¦°ìƒ·)
   - requirements.txt

2. **ë°ëª¨ ì˜ìƒ** (ì„ íƒ)
   - 30ì´ˆ-2ë¶„ ì‚¬ìš© ì‹œì—°

3. **ë°°í¬ URL** (ì„ íƒ)
   - Streamlit Cloud ë˜ëŠ” ê¸°íƒ€ í”Œë«í¼

## ğŸ’¡ íŒíŠ¸

ì•„ë˜ ì˜ì‚¬ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ êµ¬í˜„í•˜ì„¸ìš”.
    `,
    hints: [
`# ============================================
# ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì‹œìŠ¤í…œ Pseudocode
# ============================================
#
# ì´ PseudocodeëŠ” Week 5 ì „ì²´ ë‚´ìš©ì„ ì¢…í•©í•©ë‹ˆë‹¤:
# - Day 1: RAG ì•„í‚¤í…ì²˜
# - Day 2: ì„ë² ë”© & ë²¡í„°DB
# - Day 3: ì²­í‚¹ & ê²€ìƒ‰ ìµœì í™”
# - Day 4: LangChain RAG
# - Day 5: í”„ë¡œë•ì…˜ íŒ¨í„´

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 1: í”„ë¡œì íŠ¸ êµ¬ì¡°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# enterprise-rag/
# â”œâ”€â”€ app.py                    # Streamlit ë©”ì¸
# â”œâ”€â”€ requirements.txt          # ì˜ì¡´ì„±
# â”œâ”€â”€ README.md                 # ë¬¸ì„œ
# â”‚
# â”œâ”€â”€ core/
# â”‚   â”œâ”€â”€ __init__.py
# â”‚   â”œâ”€â”€ document_processor.py # ë©€í‹°í¬ë§· ë¬¸ì„œ ì²˜ë¦¬
# â”‚   â”œâ”€â”€ chunker.py            # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
# â”‚   â”œâ”€â”€ retriever.py          # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
# â”‚   â””â”€â”€ rag_chain.py          # RAG ì²´ì¸
# â”‚
# â”œâ”€â”€ utils/
# â”‚   â”œâ”€â”€ __init__.py
# â”‚   â”œâ”€â”€ error_handler.py      # ì—ëŸ¬ í•¸ë“¤ë§
# â”‚   â”œâ”€â”€ metrics.py            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
# â”‚   â””â”€â”€ validators.py         # ì…ë ¥ ê²€ì¦
# â”‚
# â””â”€â”€ config/
#     â””â”€â”€ settings.py           # ì„¤ì • ê´€ë¦¬

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 2: ë¬¸ì„œ ì²˜ë¦¬ (core/document_processor.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

IMPORT PyPDFLoader, Docx2txtLoader
IMPORT tempfile, pathlib

CLASS DocumentProcessor:

    FUNCTION process_file(self, file) -> list[Document]:
        # 1. ì„ì‹œ íŒŒì¼ ì €ì¥
        WITH tempfile.NamedTemporaryFile(suffix=file.suffix) AS tmp:
            tmp.write(file.read())
            tmp_path = tmp.name

        # 2. íŒŒì¼ í˜•ì‹ë³„ ë¡œë” ì„ íƒ
        ext = pathlib.Path(file.name).suffix.lower()

        IF ext == ".pdf":
            loader = PyPDFLoader(tmp_path)
        ELIF ext == ".docx":
            loader = Docx2txtLoader(tmp_path)
        ELIF ext IN [".md", ".txt"]:
            loader = TextLoader(tmp_path)
        ELSE:
            RAISE ValueError(f"Unsupported format: {ext}")

        # 3. ë¬¸ì„œ ë¡œë“œ
        docs = loader.load()

        # 4. ë©”íƒ€ë°ì´í„° ì¶”ê°€
        FOR doc IN docs:
            doc.metadata["source_file"] = file.name
            doc.metadata["processed_at"] = datetime.now().isoformat()

        RETURN docs

    FUNCTION process_multiple(self, files, progress_callback) -> list[Document]:
        all_docs = []

        FOR i, file IN enumerate(files):
            docs = self.process_file(file)
            all_docs.extend(docs)

            # ì§„í–‰ë¥  ì½œë°±
            IF progress_callback:
                progress_callback((i + 1) / len(files))

        RETURN all_docs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 3: ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (core/chunker.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

IMPORT RecursiveCharacterTextSplitter
IMPORT MarkdownHeaderTextSplitter

CLASS SmartChunker:

    FUNCTION __init__(self, chunk_size=1000, overlap=200):
        self.chunk_size = chunk_size
        self.overlap = overlap

    FUNCTION chunk(self, docs: list[Document]) -> list[Document]:
        all_chunks = []

        FOR doc IN docs:
            # íŒŒì¼ í˜•ì‹ì— ë”°ë¥¸ ì²­í‚¹ ì „ëµ
            IF doc.metadata.get("source_file", "").endswith(".md"):
                chunks = self._chunk_markdown(doc)
            ELSE:
                chunks = self._chunk_default(doc)

            all_chunks.extend(chunks)

        RETURN all_chunks

    FUNCTION _chunk_default(self, doc) -> list[Document]:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.overlap,
            separators=["\\n\\n", "\\n", ". ", " ", ""]
        )
        RETURN splitter.split_documents([doc])

    FUNCTION _chunk_markdown(self, doc) -> list[Document]:
        # í—¤ë” ê¸°ë°˜ ë¶„í• 
        headers_to_split = [("#", "h1"), ("##", "h2"), ("###", "h3")]
        md_splitter = MarkdownHeaderTextSplitter(headers_to_split)

        splits = md_splitter.split_text(doc.page_content)

        # Document ê°ì²´ë¡œ ë³€í™˜
        RETURN [
            Document(
                page_content=split.page_content,
                metadata={**doc.metadata, **split.metadata}
            )
            FOR split IN splits
        ]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 4: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (core/retriever.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

FROM langchain.retrievers IMPORT EnsembleRetriever
FROM langchain_community.retrievers IMPORT BM25Retriever

CLASS HybridRetriever:

    FUNCTION __init__(self, vectorstore, k=5, use_mmr=False):
        self.vectorstore = vectorstore
        self.k = k
        self.use_mmr = use_mmr

    FUNCTION get_retriever(self, docs=None):
        # ë²¡í„° ê²€ìƒ‰ê¸°
        IF self.use_mmr:
            vector_retriever = self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={"k": self.k, "fetch_k": self.k * 4, "lambda_mult": 0.7}
            )
        ELSE:
            vector_retriever = self.vectorstore.as_retriever(
                search_kwargs={"k": self.k}
            )

        # BM25 ê²€ìƒ‰ê¸° (í•˜ì´ë¸Œë¦¬ë“œìš©)
        IF docs:
            bm25_retriever = BM25Retriever.from_documents(docs)
            bm25_retriever.k = self.k

            # ì•™ìƒë¸”
            ensemble = EnsembleRetriever(
                retrievers=[bm25_retriever, vector_retriever],
                weights=[0.4, 0.6]
            )
            RETURN ensemble

        RETURN vector_retriever

    FUNCTION retrieve(self, query: str, retriever) -> list[Document]:
        docs = retriever.invoke(query)

        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_docs = []
        FOR doc IN docs:
            content_hash = hash(doc.page_content[:100])
            IF content_hash NOT IN seen:
                seen.add(content_hash)
                unique_docs.append(doc)

        RETURN unique_docs[:self.k]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 5: RAG ì²´ì¸ (core/rag_chain.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

FROM langchain_openai IMPORT ChatOpenAI
FROM langchain.prompts IMPORT ChatPromptTemplate, MessagesPlaceholder
FROM langchain_core.runnables IMPORT RunnablePassthrough

CLASS RAGChain:

    FUNCTION __init__(self, model="gpt-4o-mini", temperature=0):
        self.llm = ChatOpenAI(model=model, temperature=temperature)

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì •ì§í•˜ê²Œ ë§í•˜ì„¸ìš”
3. ì¶œì²˜ë¥¼ [Source: íŒŒì¼ëª…] í˜•ì‹ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”
4. ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"""),

            MessagesPlaceholder("chat_history"),

            ("human", """ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}"""),
        ])

    FUNCTION build_chain(self, retriever):

        FUNCTION format_docs(docs):
            formatted = []
            FOR i, doc IN enumerate(docs):
                source = doc.metadata.get("source_file", "Unknown")
                formatted.append(f"[Document {i+1}] [Source: {source}]\\n{doc.page_content}")
            RETURN "\\n\\n---\\n\\n".join(formatted)

        chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough(),
                "chat_history": lambda x: x.get("chat_history", [])
            }
            | self.prompt
            | self.llm
        )

        RETURN chain

    FUNCTION invoke(self, question: str, retriever, chat_history=None) -> dict:
        chain = self.build_chain(retriever)

        # ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        docs = retriever.invoke(question)

        # ì‘ë‹µ ìƒì„±
        response = chain.invoke({
            "question": question,
            "chat_history": chat_history or []
        })

        RETURN {
            "answer": response.content,
            "sources": [
                {"source": doc.metadata.get("source_file", "Unknown"),
                 "page": doc.metadata.get("page", "N/A")}
                FOR doc IN docs
            ],
            "confidence": self._calculate_confidence(docs)
        }

    FUNCTION _calculate_confidence(self, docs) -> float:
        IF NOT docs:
            RETURN 0.1
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ ê¸°ë°˜ (ìµœëŒ€ 5ê°œ ê¸°ì¤€)
        RETURN min(len(docs) / 5, 1.0) * 0.8 + 0.2

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 6: ì—ëŸ¬ í•¸ë“¤ë§ (utils/error_handler.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

FROM functools IMPORT wraps
IMPORT asyncio

CLASS ErrorHandler:

    @staticmethod
    FUNCTION safe_execute(func):
        """ì—ëŸ¬ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
        @wraps(func)
        FUNCTION wrapper(*args, **kwargs):
            TRY:
                RETURN {"success": True, "result": func(*args, **kwargs)}
            EXCEPT RateLimitError AS e:
                RETURN {"success": False, "error": "API ìš”ì²­ í•œë„ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "retry_after": 60}
            EXCEPT TimeoutError AS e:
                RETURN {"success": False, "error": "ìš”ì²­ ì‹œê°„ ì´ˆê³¼. ì§ˆë¬¸ì„ ì§§ê²Œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}
            EXCEPT Exception AS e:
                RETURN {"success": False, "error": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}
        RETURN wrapper

    @staticmethod
    FUNCTION retry_with_backoff(max_retries=3, base_delay=1.0):
        """ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°"""
        FUNCTION decorator(func):
            @wraps(func)
            FUNCTION wrapper(*args, **kwargs):
                delay = base_delay
                FOR attempt IN range(max_retries + 1):
                    TRY:
                        RETURN func(*args, **kwargs)
                    EXCEPT Exception AS e:
                        IF attempt == max_retries:
                            RAISE
                        time.sleep(delay)
                        delay *= 2
            RETURN wrapper
        RETURN decorator

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 7: ì…ë ¥ ê²€ì¦ (utils/validators.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

IMPORT re

CLASS InputValidator:

    # ìœ„í—˜ íŒ¨í„´ (í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜ ë°©ì§€)
    DANGEROUS_PATTERNS = [
        r"ignore previous",
        r"forget your",
        r"you are now",
        r"\\[system\\]",
        r"<\\|",
    ]

    @classmethod
    FUNCTION validate_question(cls, question: str) -> tuple[bool, str]:
        # 1. ë¹ˆ ì…ë ¥
        IF NOT question OR NOT question.strip():
            RETURN False, "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

        # 2. ê¸¸ì´ ì œí•œ
        IF len(question) > 2000:
            RETURN False, "ì§ˆë¬¸ì€ 2000ì ì´í•˜ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”."

        # 3. ìœ„í—˜ íŒ¨í„´ ê²€ì‚¬
        FOR pattern IN cls.DANGEROUS_PATTERNS:
            IF re.search(pattern, question.lower()):
                RETURN False, "í—ˆìš©ë˜ì§€ ì•ŠëŠ” ì…ë ¥ì…ë‹ˆë‹¤."

        RETURN True, question.strip()

    @classmethod
    FUNCTION validate_file(cls, file) -> tuple[bool, str]:
        # 1. íŒŒì¼ í¬ê¸° (10MB ì œí•œ)
        IF file.size > 10 * 1024 * 1024:
            RETURN False, f"{file.name}: íŒŒì¼ í¬ê¸°ê°€ 10MBë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤."

        # 2. í™•ì¥ì í™•ì¸
        ext = Path(file.name).suffix.lower()
        IF ext NOT IN [".pdf", ".docx", ".doc", ".txt", ".md"]:
            RETURN False, f"{file.name}: ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤."

        RETURN True, ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 8: ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (utils/metrics.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

FROM dataclasses IMPORT dataclass, field
FROM collections IMPORT deque
FROM datetime IMPORT datetime

@dataclass
CLASS QueryMetrics:
    question: str
    latency: float
    doc_count: int
    success: bool
    timestamp: datetime = field(default_factory=datetime.now)

CLASS MetricsCollector:

    FUNCTION __init__(self, max_size=100):
        self.metrics = deque(maxlen=max_size)

    FUNCTION record(self, metrics: QueryMetrics):
        self.metrics.append(metrics)

    FUNCTION get_summary(self) -> dict:
        IF NOT self.metrics:
            RETURN {"queries": 0, "avg_latency": 0, "success_rate": 0}

        total = len(self.metrics)
        successes = sum(1 FOR m IN self.metrics IF m.success)
        avg_latency = sum(m.latency FOR m IN self.metrics) / total

        RETURN {
            "queries": total,
            "avg_latency": round(avg_latency, 2),
            "success_rate": round(successes / total * 100, 1)
        }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Part 9: Streamlit ì•± (app.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

IMPORT streamlit AS st
FROM core.document_processor IMPORT DocumentProcessor
FROM core.chunker IMPORT SmartChunker
FROM core.retriever IMPORT HybridRetriever
FROM core.rag_chain IMPORT RAGChain
FROM utils.error_handler IMPORT ErrorHandler
FROM utils.validators IMPORT InputValidator
FROM utils.metrics IMPORT MetricsCollector, QueryMetrics

# â”€â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€
st.set_page_config(
    page_title="ğŸ“š Enterprise RAG",
    layout="wide"
)

# â”€â”€â”€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” â”€â”€â”€
IF "vectorstore" NOT IN st.session_state:
    st.session_state.vectorstore = None
    st.session_state.chunks = []
    st.session_state.messages = []
    st.session_state.metrics = MetricsCollector()

# â”€â”€â”€ ì‚¬ì´ë“œë°” â”€â”€â”€
WITH st.sidebar:
    st.header("âš™ï¸ Settings")

    # API Key
    api_key = st.text_input("OpenAI API Key", type="password")
    IF api_key:
        os.environ["OPENAI_API_KEY"] = api_key

    # RAG ì„¤ì •
    chunk_size = st.slider("Chunk Size", 500, 2000, 1000)
    k_results = st.slider("Top-K", 3, 10, 5)
    use_mmr = st.checkbox("Use MMR", value=True)

    st.divider()

    # íŒŒì¼ ì—…ë¡œë“œ
    files = st.file_uploader(
        "Upload Documents",
        type=["pdf", "docx", "txt", "md"],
        accept_multiple_files=True
    )

    IF files AND api_key:
        IF st.button("ğŸš€ Process"):
            # íŒŒì¼ ê²€ì¦
            FOR file IN files:
                valid, error = InputValidator.validate_file(file)
                IF NOT valid:
                    st.error(error)
                    CONTINUE

            # ì²˜ë¦¬
            progress = st.progress(0)
            processor = DocumentProcessor()
            chunker = SmartChunker(chunk_size=chunk_size)

            docs = processor.process_multiple(files, lambda p: progress.progress(p))
            chunks = chunker.chunk(docs)

            # ë²¡í„° ì €ì¥ì†Œ
            embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            st.session_state.vectorstore = Chroma.from_documents(chunks, embeddings)
            st.session_state.chunks = chunks

            st.success(f"âœ… {len(files)} files, {len(chunks)} chunks")

    # ë©”íŠ¸ë¦­
    IF st.session_state.metrics.metrics:
        st.divider()
        st.header("ğŸ“Š Metrics")
        summary = st.session_state.metrics.get_summary()
        st.metric("Queries", summary["queries"])
        st.metric("Avg Latency", f"{summary['avg_latency']}s")
        st.metric("Success Rate", f"{summary['success_rate']}%")

# â”€â”€â”€ ë©”ì¸ ì˜ì—­ â”€â”€â”€
st.title("ğŸ“š Enterprise RAG")

# ëŒ€í™” íˆìŠ¤í† ë¦¬
FOR msg IN st.session_state.messages:
    WITH st.chat_message(msg["role"]):
        st.write(msg["content"])
        IF msg.get("sources"):
            WITH st.expander("ğŸ“š Sources"):
                FOR src IN msg["sources"]:
                    st.write(f"- {src['source']}")

# ì‚¬ìš©ì ì…ë ¥
IF st.session_state.vectorstore:
    IF question := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì…ë ¥ ê²€ì¦
        valid, validated = InputValidator.validate_question(question)
        IF NOT valid:
            st.error(validated)
        ELSE:
            # ì‚¬ìš©ì ë©”ì‹œì§€
            st.session_state.messages.append({"role": "user", "content": validated})
            WITH st.chat_message("user"):
                st.write(validated)

            # RAG ì‹¤í–‰
            WITH st.chat_message("assistant"):
                WITH st.spinner("Thinking..."):
                    start = datetime.now()

                    TRY:
                        retriever = HybridRetriever(
                            st.session_state.vectorstore,
                            k=k_results,
                            use_mmr=use_mmr
                        ).get_retriever(st.session_state.chunks)

                        rag = RAGChain()
                        result = rag.invoke(
                            validated,
                            retriever,
                            st.session_state.messages[-10:]  # ìµœê·¼ 10ê°œ ëŒ€í™”
                        )

                        latency = (datetime.now() - start).total_seconds()

                        # ì‘ë‹µ í‘œì‹œ
                        st.write(result["answer"])

                        WITH st.expander("ğŸ“š Sources"):
                            FOR src IN result["sources"]:
                                st.write(f"- {src['source']}, Page {src['page']}")

                        st.caption(f"Confidence: {result['confidence']:.0%} | Latency: {latency:.2f}s")

                        # ë©”ì‹œì§€ ì €ì¥
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": result["answer"],
                            "sources": result["sources"]
                        })

                        # ë©”íŠ¸ë¦­ ê¸°ë¡
                        st.session_state.metrics.record(QueryMetrics(
                            question=validated,
                            latency=latency,
                            doc_count=len(result["sources"]),
                            success=True
                        ))

                    EXCEPT Exception AS e:
                        st.error(f"ì˜¤ë¥˜: {str(e)}")
                        st.session_state.metrics.record(QueryMetrics(
                            question=validated,
                            latency=0,
                            doc_count=0,
                            success=False
                        ))

ELSE:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”!")

# â”€â”€â”€ í‘¸í„° â”€â”€â”€
st.divider()
st.caption("Built with LangChain + Streamlit | Week 5 Final Project")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# requirements.txt
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# streamlit>=1.28.0
# langchain>=0.1.0
# langchain-openai>=0.0.5
# langchain-chroma>=0.1.0
# chromadb>=0.4.0
# pypdf>=3.17.0
# python-docx>=1.0.0
# rank-bm25>=0.2.2
# python-dotenv>=1.0.0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#
# 1. ì„¤ì¹˜
#    pip install -r requirements.txt
#
# 2. ì‹¤í–‰
#    streamlit run app.py
#
# 3. ì ‘ì†
#    http://localhost:8501`
    ],
    keyPoints: [
      'Week 5 ì „ì²´ ë‚´ìš© ì¢…í•©: RAG ì•„í‚¤í…ì²˜ â†’ ì„ë² ë”© â†’ ì²­í‚¹ â†’ ê²€ìƒ‰ â†’ í”„ë¡œë•ì…˜',
      'ë©€í‹°í¬ë§·, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, ëŒ€í™”í˜• RAG, ì—ëŸ¬ í•¸ë“¤ë§',
      'ì‹¤ì œ ì„œë¹„ìŠ¤ ìˆ˜ì¤€ì˜ ì½”ë“œ í’ˆì§ˆ',
    ],
    practiceGoal: 'Week 5ì—ì„œ ë°°ìš´ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ RAG ì‹œìŠ¤í…œì„ ì™„ì„±í•œë‹¤',
  }),
}
