// Day 2: ì„ë² ë”© & ë²¡í„° DB (Chroma, Pinecone) - ì™„ì „ ë¦¬ë‰´ì–¼ ë²„ì „

import type { Day } from '../../types'
import {
  createVideoTask,
  createReadingTask,
  createCodeTask,
  createQuizTask,
  createChallengeTask,
} from './types'

export const day2EmbeddingsVectordb: Day = {
  slug: 'embeddings-vectordb',
  title: 'ì„ë² ë”© & ë²¡í„° DB',
  totalDuration: 300, // 5ì‹œê°„
  tasks: [
    // ========================================
    // Task 1: ì„ë² ë”©ì˜ ì—­ì‚¬ì™€ ì›ë¦¬ (40ë¶„)
    // ========================================
    createVideoTask('w5d2-embeddings-history', 'ì„ë² ë”©ì˜ ì—­ì‚¬ì™€ ìˆ˜í•™ì  ì›ë¦¬', 40, {
      videoUrl: 'https://www.youtube.com/watch?v=pBmj-HgUO1Y',
      introduction: `
## í•™ìŠµ ëª©í‘œ
- ì„ë² ë”© ê¸°ìˆ ì˜ ë°œì „ ì—­ì‚¬ë¥¼ ì´í•´í•œë‹¤
- ì„ë² ë”©ì˜ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- ì£¼ìš” ì„ë² ë”© ëª¨ë¸ë“¤ì˜ íŠ¹ì§•ì„ ë¹„êµí•œë‹¤

---

## ì„ë² ë”©ì˜ ì—­ì‚¬: Word2Vecì—ì„œ LLMê¹Œì§€

### 1ì„¸ëŒ€: í†µê³„ ê¸°ë°˜ (2003-2012)

**TF-IDF (Term Frequency-Inverse Document Frequency)**

> ğŸ’¡ **í•œ ì¤„ ìš”ì•½**: ë¬¸ì„œì—ì„œ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ "ë¹ˆë„"ì™€ "í¬ì†Œì„±"ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” í†µê³„ ê¸°ë²•
>
> TF(ë‹¨ì–´ ë¹ˆë„) Ã— IDF(ì—­ë¬¸ì„œ ë¹ˆë„) = í•´ë‹¹ ë¬¸ì„œì—ì„œ ê·¸ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ ì ìˆ˜í™”

\`\`\`python
# ì „í†µì ì¸ ë¬¸ì„œ í‘œí˜„ ë°©ì‹
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
documents = [
    "ê³ ì–‘ì´ê°€ ë°©ì—ì„œ ì”ë‹¤",
    "ê°•ì•„ì§€ê°€ ë§ˆë‹¹ì—ì„œ ë…¼ë‹¤",
    "ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ê°€ ì¹œêµ¬ë‹¤"
]

# í¬ì†Œ ë²¡í„° (ëŒ€ë¶€ë¶„ 0)
tfidf_matrix = vectorizer.fit_transform(documents)
print(f"ì°¨ì›: {tfidf_matrix.shape}")  # (3, 8) - ë‹¨ì–´ ìˆ˜ë§Œí¼
print(f"ë¹„ì˜ ìš”ì†Œ ë¹„ìœ¨: {tfidf_matrix.nnz / tfidf_matrix.size:.2%}")  # ~25%
\`\`\`

**í•œê³„ì :**
- ë‹¨ì–´ ìˆœì„œ ë¬´ì‹œ ("ê°œê°€ ì‚¬ëŒì„ ë¬¼ì—ˆë‹¤" vs "ì‚¬ëŒì´ ê°œë¥¼ ë¬¼ì—ˆë‹¤")
- ë™ì˜ì–´ ì¸ì‹ ë¶ˆê°€ ("ì°¨" vs "ìë™ì°¨")
- í¬ì†Œ ë²¡í„° â†’ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨

---

### 2ì„¸ëŒ€: ì‹ ê²½ë§ ê¸°ë°˜ Word Embeddings (2013-2017)

**Word2Vec (2013, Google)**

> ğŸ’¡ **í•œ ì¤„ ìš”ì•½**: ë‹¨ì–´ë¥¼ ê³ ì • ê¸¸ì´ì˜ ë°€ì§‘ ë²¡í„°(dense vector)ë¡œ ë³€í™˜í•˜ëŠ” ìµœì´ˆì˜ ì‹ ê²½ë§ ê¸°ë°˜ ì„ë² ë”©
>
> í•µì‹¬ ì›ë¦¬: "ë¹„ìŠ·í•œ ë¬¸ë§¥ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤" (ë¶„í¬ ê°€ì„¤)
> ê²°ê³¼: ë‹¨ì–´ ê°„ ì˜ë¯¸ ê´€ê³„ê°€ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í‘œí˜„ë¨ (King - Man + Woman â‰ˆ Queen)

Mikolov et al.ì˜ í˜ì‹ ì  ë…¼ë¬¸ "Efficient Estimation of Word Representations in Vector Space"

\`\`\`
í•µì‹¬ ì•„ì´ë””ì–´: ë¹„ìŠ·í•œ ë¬¸ë§¥ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤

"ë‚˜ëŠ” [ì»¤í”¼]ë¥¼ ë§ˆì‹ ë‹¤"
"ë‚˜ëŠ” [ì°¨]ë¥¼ ë§ˆì‹ ë‹¤"
â†’ "ì»¤í”¼"ì™€ "ì°¨"ëŠ” ìœ ì‚¬í•œ ë²¡í„°
\`\`\`

**ë‘ ê°€ì§€ í•™ìŠµ ë°©ì‹:**

\`\`\`
CBOW (Continuous Bag of Words):
  - ì£¼ë³€ ë‹¨ì–´ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ ì˜ˆì¸¡
  - [ë‚˜ëŠ”, ë¥¼, ë§ˆì‹ ë‹¤] â†’ [ì»¤í”¼] ì˜ˆì¸¡
  - ë¹ ë¥¸ í•™ìŠµ, ë¹ˆë²ˆí•œ ë‹¨ì–´ì— ìœ ë¦¬

Skip-gram:
  - ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ ì˜ˆì¸¡
  - [ì»¤í”¼] â†’ [ë‚˜ëŠ”, ë¥¼, ë§ˆì‹ ë‹¤] ì˜ˆì¸¡
  - í¬ê·€ ë‹¨ì–´, ì‘ì€ ë°ì´í„°ì…‹ì— ìœ ë¦¬
\`\`\`

**Word2Vecì˜ ë†€ë¼ìš´ ì„±ì§ˆ - ë²¡í„° ì—°ì‚°:**

\`\`\`python
# ìœ ëª…í•œ ì˜ˆì‹œ
vector("King") - vector("Man") + vector("Woman") â‰ˆ vector("Queen")

# ì‹¤ì œ ì½”ë“œ
from gensim.models import Word2Vec

# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
import gensim.downloader as api
model = api.load('word2vec-google-news-300')

# ë²¡í„° ì—°ì‚°
result = model.most_similar(
    positive=['king', 'woman'],
    negative=['man'],
    topn=1
)
print(result)  # [('queen', 0.7118)]
\`\`\`

**GloVe (2014, Stanford)**

Global Vectors for Word Representation - ì „ì—­ í†µê³„ ì •ë³´ í™œìš©

\`\`\`
Word2Vec: ë¡œì»¬ ë¬¸ë§¥ë§Œ í•™ìŠµ
GloVe: ì „ì²´ ì½”í¼ìŠ¤ì˜ ë™ì‹œ ì¶œí˜„ í†µê³„ í™œìš©
â†’ ë” ì•ˆì •ì ì¸ ì„ë² ë”©
\`\`\`

**FastText (2016, Facebook)**

ì„œë¸Œì›Œë“œ(subword) ê¸°ë°˜ - ë¯¸ë“±ë¡ ë‹¨ì–´ ì²˜ë¦¬ ê°€ëŠ¥

\`\`\`
"ìì—°ì–´ì²˜ë¦¬" â†’ ["ìì—°", "ì—°ì–´", "ì–´ì²˜", "ì²˜ë¦¬", ...]
â†’ í•™ìŠµì— ì—†ëŠ” ë‹¨ì–´ë„ ì„œë¸Œì›Œë“œ ì¡°í•©ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
\`\`\`

---

### 3ì„¸ëŒ€: Transformer ê¸°ë°˜ (2018-í˜„ì¬)

**BERT (2018, Google)**

> ğŸ’¡ **í•œ ì¤„ ìš”ì•½**: ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´í•˜ì—¬ ë™ì¼ ë‹¨ì–´ë„ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” í˜ì‹ ì  ëª¨ë¸
>
> Word2Vecê³¼ì˜ ì°¨ì´: "bank"ê°€ í•­ìƒ ê°™ì€ ë²¡í„° â†’ BERTëŠ” ë¬¸ë§¥ì— ë”°ë¼ "ì€í–‰" ë˜ëŠ” "ê°•ë‘‘"ìœ¼ë¡œ ë‹¤ë¥¸ ë²¡í„°
> í•µì‹¬ ê¸°ìˆ : Transformerì˜ Self-Attention ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë¬¸ì¥ ì „ì²´ì˜ ê´€ê³„ë¥¼ í•™ìŠµ

Bidirectional Encoder Representations from Transformers

\`\`\`
í•µì‹¬ í˜ì‹ : ì–‘ë°©í–¥ ë¬¸ë§¥ ì´í•´

Word2Vec/GloVe: "bank"ëŠ” í•­ìƒ ê°™ì€ ë²¡í„°
BERT: ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¥¸ ë²¡í„°
  - "I went to the bank to deposit money" â†’ ê¸ˆìœµ bank
  - "I sat on the river bank" â†’ ê°•ë‘‘ bank
\`\`\`

**Sentence-BERT (2019)**

ë¬¸ì¥ ë‹¨ìœ„ ì„ë² ë”©ì— ìµœì í™”

\`\`\`python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

sentences = [
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤",
    "The weather is nice today",  # ê°™ì€ ì˜ë¯¸, ë‹¤ë¥¸ ì–¸ì–´
    "ì£¼ì‹ ì‹œì¥ì´ í­ë½í–ˆë‹¤"
]

embeddings = model.encode(sentences)
# ì²« ë‘ ë¬¸ì¥ì€ ìœ ì‚¬í•œ ë²¡í„°, ì„¸ ë²ˆì§¸ëŠ” ë‹¤ë¦„
\`\`\`

**OpenAI Embeddings (2022-2024)**

\`\`\`
text-embedding-ada-002 (2022): 1536ì°¨ì›, $0.0001/1K tokens
text-embedding-3-small (2024): 1536ì°¨ì›, $0.00002/1K tokens (80% ì €ë ´!)
text-embedding-3-large (2024): 3072ì°¨ì›, $0.00013/1K tokens
\`\`\`

---

## ì„ë² ë”©ì˜ ìˆ˜í•™ì  ì›ë¦¬

### ë²¡í„° ê³µê°„ ëª¨ë¸ (Vector Space Model)

\`\`\`
Nì°¨ì› ê³µê°„ì—ì„œ ê° ì (ë²¡í„°)ì€ í…ìŠ¤íŠ¸ì˜ "ì˜ë¯¸"ë¥¼ í‘œí˜„

ì˜ˆ: 3ì°¨ì›ìœ¼ë¡œ ë‹¨ìˆœí™”
    ë™ë¬¼ ì¶•  ìŒì‹ ì¶•  ê¸°ìˆ  ì¶•
ê³ ì–‘ì´: [0.9,   0.1,   0.0]
ê°•ì•„ì§€: [0.8,   0.2,   0.0]
í”¼ì:   [0.0,   0.9,   0.1]
ì»´í“¨í„°: [0.0,   0.1,   0.9]

â†’ ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ëŠ” ê°€ê¹ê³ , í”¼ìì™€ëŠ” ë©€ë‹¤
\`\`\`

### ìœ ì‚¬ë„ ì¸¡ì • ë°©ë²•

**1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Cosine Similarity) - ê°€ì¥ ì¼ë°˜ì **

\`\`\`python
import numpy as np

def cosine_similarity(a, b):
    """
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‘ ë²¡í„°ì˜ ê°ë„
    - ë°©í–¥ë§Œ ê³ ë ¤, í¬ê¸° ë¬´ì‹œ
    - í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë¬´ê´€
    """
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# ê°’ ë²”ìœ„: -1 ~ 1
# 1: ì™„ì „íˆ ê°™ì€ ë°©í–¥ (ì˜ë¯¸ ë™ì¼)
# 0: ì§êµ (ê´€ë ¨ ì—†ìŒ)
# -1: ë°˜ëŒ€ ë°©í–¥ (ì˜ë¯¸ ë°˜ëŒ€)

vec_cat = np.array([0.9, 0.8, 0.1])
vec_dog = np.array([0.85, 0.75, 0.15])
vec_pizza = np.array([0.1, 0.2, 0.9])

print(f"ê³ ì–‘ì´-ê°•ì•„ì§€: {cosine_similarity(vec_cat, vec_dog):.3f}")  # ~0.99
print(f"ê³ ì–‘ì´-í”¼ì: {cosine_similarity(vec_cat, vec_pizza):.3f}")  # ~0.35
\`\`\`

**2. ìœ í´ë¦¬ë“œ ê±°ë¦¬ (Euclidean Distance)**

\`\`\`python
def euclidean_distance(a, b):
    """
    ìœ í´ë¦¬ë“œ ê±°ë¦¬ = ë‘ ì  ì‚¬ì´ì˜ ì§ì„  ê±°ë¦¬
    - í¬ê¸°ë„ ê³ ë ¤
    - ì‘ì„ìˆ˜ë¡ ìœ ì‚¬
    """
    return np.linalg.norm(a - b)

# ì£¼ì˜: ê±°ë¦¬ì´ë¯€ë¡œ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬!
print(f"ê³ ì–‘ì´-ê°•ì•„ì§€ ê±°ë¦¬: {euclidean_distance(vec_cat, vec_dog):.3f}")
\`\`\`

**3. ë‚´ì  (Dot Product)**

\`\`\`python
def dot_product(a, b):
    """
    ë‚´ì  = í¬ê¸° Ã— ë°©í–¥
    - ì •ê·œí™”ëœ ë²¡í„°ì—ì„œëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼
    - ë¹ ë¥¸ ê³„ì‚°
    """
    return np.dot(a, b)
\`\`\`

**ì–¸ì œ ì–´ë–¤ ì¸¡ì • ë°©ë²•ì„?**

| ë°©ë²• | ì‚¬ìš© ìƒí™© | íŠ¹ì§• |
|------|----------|------|
| **ì½”ì‚¬ì¸ ìœ ì‚¬ë„** | ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ê²€ìƒ‰ | ê¸¸ì´ ë¬´ê´€, ì˜ë¯¸ë§Œ ë¹„êµ |
| **ìœ í´ë¦¬ë“œ ê±°ë¦¬** | í´ëŸ¬ìŠ¤í„°ë§, ì´ìƒì¹˜ íƒì§€ | ì ˆëŒ€ì  ìœ„ì¹˜ ì¤‘ìš” |
| **ë‚´ì ** | ì¶”ì²œ ì‹œìŠ¤í…œ, ì •ê·œí™”ëœ ë²¡í„° | ë¹ ë¥¸ ê³„ì‚° |

---

## ì°¨ì›ì˜ ì €ì£¼ì™€ í•´ê²°ì±…

\`\`\`
ë¬¸ì œ: ê³ ì°¨ì›ì—ì„œëŠ” ëª¨ë“  ì ì´ ë¹„ìŠ·í•˜ê²Œ "ë©€ë¦¬" ë³´ì„

1536ì°¨ì›ì—ì„œ:
- ë¬´ì‘ìœ„ ë‘ ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ê±°ì˜ 0ì— ê°€ê¹Œì›€
- ìœ ì‚¬í•œ ë²¡í„°ë§Œ ë†’ì€ ìœ ì‚¬ë„ â†’ ì¢‹ì€ ë¶„ë³„ë ¥
\`\`\`

**ì°¨ì› ì¶•ì†Œ ê¸°ë²•:**

\`\`\`python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap

# PCA: ë¹ ë¥´ì§€ë§Œ ì„ í˜•ì 
pca = PCA(n_components=2)
reduced_pca = pca.fit_transform(embeddings)

# t-SNE: ì‹œê°í™”ì— ì¢‹ìŒ, ëŠë¦¼
tsne = TSNE(n_components=2)
reduced_tsne = tsne.fit_transform(embeddings)

# UMAP: t-SNEë³´ë‹¤ ë¹ ë¥´ê³  ì „ì—­ êµ¬ì¡° ë³´ì¡´
reducer = umap.UMAP(n_components=2)
reduced_umap = reducer.fit_transform(embeddings)
\`\`\`
      `,
      keyPoints: [
        'Word2Vec(2013) â†’ GloVe â†’ FastText â†’ BERT â†’ OpenAI ì„ë² ë”© ë°œì „ì‚¬',
        'ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë°©í–¥ ê¸°ë°˜, í…ìŠ¤íŠ¸ ê¸¸ì´ ë¬´ê´€',
        'ê³ ì°¨ì› ë²¡í„°ëŠ” ì˜ë¯¸ì  ë¶„ë³„ë ¥ì´ ë†’ìŒ',
      ],
      practiceGoal: 'ì„ë² ë”©ì˜ ì—­ì‚¬ì™€ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤',
      simulators: [
        {
          id: 'embedding-visualizer',
          title: 'Embedding Visualizer 3D',
          description: 'ë‹¨ì–´/ë¬¸ì¥ ì„ë² ë”©ì„ 3D ê³µê°„ì—ì„œ ì‹œê°í™”í•˜ê³  ìœ ì‚¬ë„ë¥¼ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”',
          url: '/simulators/embedding-visualizer'
        }
      ],
    }),

    // ========================================
    // Task 2: ì„ë² ë”© ëª¨ë¸ ë¹„êµ & ì„ íƒ ê°€ì´ë“œ (45ë¶„)
    // ========================================
    createReadingTask('w5d2-embedding-models', 'ì„ë² ë”© ëª¨ë¸ ë¹„êµ & ì„ íƒ ê°€ì´ë“œ', 45, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- ì£¼ìš” ì„ë² ë”© ëª¨ë¸ë“¤ì˜ íŠ¹ì„±ì„ ë¹„êµí•œë‹¤
- ìƒí™©ì— ë§ëŠ” ìµœì ì˜ ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•œë‹¤
- ë‹¤êµ­ì–´ ë° í•œêµ­ì–´ ì „ìš© ëª¨ë¸ì„ ì´í•´í•œë‹¤

---

## ì£¼ìš” ì„ë² ë”© ëª¨ë¸ ì¢…í•© ë¹„êµ

### OpenAI Embeddings (2024 ê¸°ì¤€)

| ëª¨ë¸ | ì°¨ì› | ê°€ê²©/1M í† í° | ìµœëŒ€ í† í° | íŠ¹ì§• |
|------|------|-------------|----------|------|
| **text-embedding-3-small** | 1536 | $0.02 | 8191 | ê°€ì„±ë¹„ ìµœê³ , ëŒ€ë¶€ë¶„ ì¶©ë¶„ |
| **text-embedding-3-large** | 3072 | $0.13 | 8191 | ìµœê³  í’ˆì§ˆ, ë³µì¡í•œ ë„ë©”ì¸ |
| text-embedding-ada-002 | 1536 | $0.10 | 8191 | ë ˆê±°ì‹œ, ì‚¬ìš© ì§€ì–‘ |

\`\`\`python
from openai import OpenAI
client = OpenAI()

# 3-small: ëŒ€ë¶€ë¶„ì˜ RAGì— ì¶©ë¶„
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ",
    encoding_format="float"
)

# ì°¨ì› ì¶•ì†Œ ì˜µì…˜ (ìƒˆë¡œìš´ ê¸°ëŠ¥!)
response_reduced = client.embeddings.create(
    model="text-embedding-3-large",
    input="ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ",
    dimensions=1024  # 3072 â†’ 1024ë¡œ ì¶•ì†Œ
)
# ì €ì¥ ê³µê°„ ì ˆì•½ + ì„±ëŠ¥ ìœ ì§€
\`\`\`

---

### ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸

**Sentence-Transformers (Hugging Face)**

| ëª¨ë¸ | ì°¨ì› | ì–¸ì–´ | ì†ë„ | í’ˆì§ˆ |
|------|------|------|------|------|
| all-MiniLM-L6-v2 | 384 | ì˜ì–´ | âš¡ï¸âš¡ï¸âš¡ï¸ | â­ï¸â­ï¸â­ï¸ |
| all-mpnet-base-v2 | 768 | ì˜ì–´ | âš¡ï¸âš¡ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ |
| paraphrase-multilingual-MiniLM-L12-v2 | 384 | 50+ | âš¡ï¸âš¡ï¸âš¡ï¸ | â­ï¸â­ï¸â­ï¸ |
| **multilingual-e5-large** | 1024 | 100+ | âš¡ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ |

\`\`\`python
from sentence_transformers import SentenceTransformer

# ì˜ì–´ ì „ìš© - ë¹ ë¥´ê³  ê°€ë²¼ì›€
model_en = SentenceTransformer('all-MiniLM-L6-v2')

# ë‹¤êµ­ì–´ - í•œêµ­ì–´ í¬í•¨
model_multi = SentenceTransformer('intfloat/multilingual-e5-large')

# ì„ë² ë”© ìƒì„±
texts = [
    "query: RAG ì‹œìŠ¤í…œì´ë€?",  # E5 ëª¨ë¸ì€ "query:" í”„ë¦¬í”½ìŠ¤ í•„ìš”
    "passage: RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤"  # ë¬¸ì„œëŠ” "passage:"
]
embeddings = model_multi.encode(texts)
\`\`\`

---

### í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸

| ëª¨ë¸ | ì°¨ì› | íŠ¹ì§• | ì¶”ì²œ ìƒí™© |
|------|------|------|----------|
| **KoSentenceT5** | 768 | í•œêµ­ì–´ íŠ¹í™” | ìˆœìˆ˜ í•œêµ­ì–´ ë¬¸ì„œ |
| **ko-sbert-nli** | 768 | í•œêµ­ì–´ NLI í•™ìŠµ | ì˜ë¯¸ì  ìœ ì‚¬ë„ |
| **KoSimCSE** | 768 | ëŒ€ì¡° í•™ìŠµ | ë¹„ì§€ë„ í•™ìŠµ |
| **multilingual-e5** | 1024 | 100ê°œ ì–¸ì–´ | ë‹¤êµ­ì–´ í˜¼í•© |

\`\`\`python
from sentence_transformers import SentenceTransformer

# í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
model_ko = SentenceTransformer('jhgan/ko-sroberta-multitask')

korean_texts = [
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆë‹¤",
    "AI ê¸°ìˆ ì´ ì§„ë³´í•˜ê³  ìˆë‹¤",
    "ì˜¤ëŠ˜ ì ì‹¬ì€ ê¹€ì¹˜ì°Œê°œ"
]

embeddings = model_ko.encode(korean_texts)
\`\`\`

---

## ì„ë² ë”© ëª¨ë¸ ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬

\`\`\`
ì‹œì‘
â”œâ”€â”€ ì˜ˆì‚°ì´ ìˆëŠ”ê°€?
â”‚   â”œâ”€â”€ ìˆë‹¤ â†’ OpenAI
â”‚   â”‚   â”œâ”€â”€ ì¼ë°˜ RAG â†’ text-embedding-3-small
â”‚   â”‚   â””â”€â”€ ê³ í’ˆì§ˆ í•„ìš” â†’ text-embedding-3-large
â”‚   â”‚
â”‚   â””â”€â”€ ì—†ë‹¤ (ë¬´ë£Œ) â†’ ì˜¤í”ˆì†ŒìŠ¤
â”‚       â”œâ”€â”€ ì˜ì–´ë§Œ? â†’ all-MiniLM-L6-v2 (ë¹ ë¦„)
â”‚       â”œâ”€â”€ ë‹¤êµ­ì–´? â†’ multilingual-e5-large
â”‚       â””â”€â”€ í•œêµ­ì–´? â†’ ko-sroberta-multitask
â”‚
â”œâ”€â”€ ë„ë©”ì¸ íŠ¹ìˆ˜ì„±ì´ ìˆëŠ”ê°€?
â”‚   â”œâ”€â”€ ë²•ë¥ /ì˜ë£Œ â†’ Fine-tuning ê³ ë ¤
â”‚   â”œâ”€â”€ ì½”ë“œ â†’ CodeBERT, text-embedding-3-large
â”‚   â””â”€â”€ ì¼ë°˜ â†’ ë²”ìš© ëª¨ë¸
â”‚
â””â”€â”€ ì§€ì—° ì‹œê°„ ìš”êµ¬ì‚¬í•­?
    â”œâ”€â”€ ì‹¤ì‹œê°„ (<100ms) â†’ ë¡œì»¬ ëª¨ë¸ + ìºì‹±
    â””â”€â”€ ë°°ì¹˜ ê°€ëŠ¥ â†’ OpenAI API
\`\`\`

---

## ë²¤ì¹˜ë§ˆí¬: MTEB (Massive Text Embedding Benchmark)

\`\`\`
MTEB: ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ í‘œì¤€

7ê°€ì§€ íƒœìŠ¤í¬:
1. Retrieval (ê²€ìƒ‰)
2. Semantic Textual Similarity (ì˜ë¯¸ì  ìœ ì‚¬ë„)
3. Clustering (í´ëŸ¬ìŠ¤í„°ë§)
4. Classification (ë¶„ë¥˜)
5. Reranking (ì¬ìˆœìœ„)
6. Pair Classification (ìŒ ë¶„ë¥˜)
7. Summarization (ìš”ì•½)

2024ë…„ ìƒìœ„ ëª¨ë¸ (ê²€ìƒ‰ íƒœìŠ¤í¬ ê¸°ì¤€):
1. text-embedding-3-large (OpenAI): 64.6
2. multilingual-e5-large-instruct: 62.4
3. text-embedding-3-small (OpenAI): 62.3
4. all-mpnet-base-v2: 57.8
\`\`\`

---

## í”„ë¡œë•ì…˜ ê³ ë ¤ì‚¬í•­

### 1. ë¹„ìš© ê³„ì‚°

\`\`\`python
def calculate_embedding_cost():
    """
    ì˜ˆì‹œ: 100ë§Œ ê°œ ë¬¸ì„œ (í‰ê·  500 í† í°)
    """
    documents = 1_000_000
    avg_tokens = 500
    total_tokens = documents * avg_tokens  # 500M í† í°

    costs = {
        "text-embedding-3-small": total_tokens / 1_000_000 * 0.02,
        "text-embedding-3-large": total_tokens / 1_000_000 * 0.13,
        "ada-002 (legacy)": total_tokens / 1_000_000 * 0.10,
        "ì˜¤í”ˆì†ŒìŠ¤": 0  # GPU ë¹„ìš© ë³„ë„
    }

    for model, cost in costs.items():
        print(f"{model}: \${cost:,.2f}")

# text-embedding-3-small: $10.00 â† ê°€ì„±ë¹„!
# text-embedding-3-large: $65.00
# ada-002 (legacy): $50.00
\`\`\`

### 2. ì§€ì—° ì‹œê°„ ë¹„êµ

| ë°©ì‹ | ì§€ì—° ì‹œê°„ | ì¥ë‹¨ì  |
|------|----------|--------|
| OpenAI API | 100-500ms | ê°„í¸, ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ |
| ë¡œì»¬ CPU | 50-200ms | GPU ë¶ˆí•„ìš”, ì¤‘ê°„ ì†ë„ |
| ë¡œì»¬ GPU | 5-20ms | ë¹ ë¦„, GPU í•„ìš” |
| ìºì‹± | <1ms | ê°€ì¥ ë¹ ë¦„, íˆíŠ¸ìœ¨ ì¤‘ìš” |

### 3. ì„ë² ë”© ìºì‹± ì „ëµ

\`\`\`python
import hashlib
import redis
import json
import numpy as np

class EmbeddingCache:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url)
        self.ttl = 86400 * 30  # 30ì¼

    def _hash_text(self, text: str, model: str) -> str:
        """í…ìŠ¤íŠ¸ì™€ ëª¨ë¸ ì¡°í•©ì˜ í•´ì‹œ"""
        content = f"{model}:{text}"
        return hashlib.sha256(content.encode()).hexdigest()

    def get(self, text: str, model: str) -> np.ndarray | None:
        """ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ"""
        key = self._hash_text(text, model)
        cached = self.redis.get(key)
        if cached:
            return np.array(json.loads(cached))
        return None

    def set(self, text: str, model: str, embedding: list):
        """ìºì‹œì— ì„ë² ë”© ì €ì¥"""
        key = self._hash_text(text, model)
        self.redis.setex(key, self.ttl, json.dumps(embedding))

# ì‚¬ìš© ì˜ˆì‹œ
cache = EmbeddingCache()

def get_embedding_with_cache(text: str, model: str = "text-embedding-3-small"):
    # 1. ìºì‹œ í™•ì¸
    cached = cache.get(text, model)
    if cached is not None:
        return cached

    # 2. API í˜¸ì¶œ
    response = client.embeddings.create(model=model, input=text)
    embedding = response.data[0].embedding

    # 3. ìºì‹œ ì €ì¥
    cache.set(text, model, embedding)

    return embedding
\`\`\`
      `,
      keyPoints: [
        'text-embedding-3-smallì´ ëŒ€ë¶€ë¶„ì˜ RAGì— ì¶©ë¶„ (ê°€ì„±ë¹„ ìµœê³ )',
        'í•œêµ­ì–´ëŠ” multilingual-e5 ë˜ëŠ” ko-sroberta ì¶”ì²œ',
        'ì„ë² ë”© ìºì‹±ìœ¼ë¡œ ë¹„ìš©ê³¼ ì§€ì—°ì‹œê°„ ìµœì í™”',
      ],
      practiceGoal: 'ìƒí™©ì— ë§ëŠ” ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 3: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‹¬ì¸µ ì´í•´ (45ë¶„)
    // ========================================
    createReadingTask('w5d2-vectordb-deep', 'ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‹¬ì¸µ ì´í•´', 45, {
      videoUrl: 'https://youtu.be/ShzjGfzTC1E',
      introduction: `
## í•™ìŠµ ëª©í‘œ
- ë²¡í„° DBì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- ì£¼ìš” ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ì„ ë¹„êµí•œë‹¤
- ë²¡í„° DB ì„ íƒ ê¸°ì¤€ì„ í•™ìŠµí•œë‹¤

---

## ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë€?

### ì¼ë°˜ DB vs ë²¡í„° DB

\`\`\`
ì¼ë°˜ ë°ì´í„°ë² ì´ìŠ¤ (PostgreSQL, MySQL):
- B-Tree ì¸ë±ìŠ¤: ì •í™•í•œ ê°’ ë§¤ì¹­
- ë²”ìœ„ ì¿¼ë¦¬: WHERE price > 100
- í…ìŠ¤íŠ¸ ê²€ìƒ‰: LIKE '%keyword%'
âŒ "ë¹„ìŠ·í•œ ì˜ë¯¸" ê²€ìƒ‰ ë¶ˆê°€

ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤:
- ANN ì¸ë±ìŠ¤: ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰
- ìœ ì‚¬ë„ ì¿¼ë¦¬: ê°€ì¥ ë¹„ìŠ·í•œ Kê°œ ì°¾ê¸°
- ì˜ë¯¸ ê²€ìƒ‰: "í–‰ë³µ" ê²€ìƒ‰ â†’ "ê¸°ì¨", "ì¦ê±°ì›€" í¬í•¨
âœ… ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ê²€ìƒ‰
\`\`\`

---

## ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ ì‹¬ì¸µ ë¶„ì„

### 1. Flat Index (Brute Force)

\`\`\`
ë°©ë²•: ëª¨ë“  ë²¡í„°ì™€ ì¿¼ë¦¬ ë²¡í„°ë¥¼ ì§ì ‘ ë¹„êµ

ì¥ì :
- ì •í™•ë„ 100% (ì •í™•í•œ ìµœê·¼ì ‘ ì´ì›ƒ)
- êµ¬í˜„ ê°„ë‹¨

ë‹¨ì :
- O(n) ì‹œê°„ ë³µì¡ë„
- ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ë§¤ìš° ëŠë¦¼

ì‚¬ìš©ì²˜:
- 10ë§Œ ê°œ ì´í•˜ ë°ì´í„°
- ì •í™•ë„ê°€ ìµœìš°ì„ ì¸ ê²½ìš°
\`\`\`

### 2. IVF (Inverted File Index)

\`\`\`
ë°©ë²•: ë²¡í„°ë¥¼ í´ëŸ¬ìŠ¤í„°ë¡œ ê·¸ë£¹í™”, í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ë§Œ ê²€ìƒ‰

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â—   â—      â—â—  â—    â—   â—        â”‚
â”‚    â—  â•”â•â•â•â•— â—      â•”â•â•â•â•—    â—     â”‚
â”‚  â—    â•‘ C1â•‘   â—    â•‘ C2â•‘  â—   â—   â”‚
â”‚     â— â•šâ•â•â•â•  â—  â—  â•šâ•â•â•â•     â—    â”‚
â”‚   â—   â—   â—     â—    â—   â—   â—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ê²€ìƒ‰ ê³¼ì •:
1. ì¿¼ë¦¬ ë²¡í„°ê°€ ì†í•  í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (nprobeê°œ)
2. í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ë‚´ ë²¡í„°ë§Œ ë¹„êµ

íŒŒë¼ë¯¸í„°:
- nlist: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ë³´í†µ âˆšn)
- nprobe: ê²€ìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜ (ì •í™•ë„â†”ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)
\`\`\`

\`\`\`python
import faiss
import numpy as np

# IVF ì¸ë±ìŠ¤ ìƒì„±
dimension = 1536
nlist = 100  # 100ê°œ í´ëŸ¬ìŠ¤í„°

# ì–‘ìí™”ê¸° + IVF
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist)

# í•™ìŠµ í•„ìš” (í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ê³„ì‚°)
vectors = np.random.rand(10000, dimension).astype('float32')
index.train(vectors)
index.add(vectors)

# ê²€ìƒ‰
index.nprobe = 10  # 10ê°œ í´ëŸ¬ìŠ¤í„° ê²€ìƒ‰
D, I = index.search(query_vector, k=5)
\`\`\`

### 3. HNSW (Hierarchical Navigable Small World)

\`\`\`
ë°©ë²•: ë‹¤ì¸µ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ íš¨ìœ¨ì ì¸ íƒìƒ‰

Layer 2 (sparse):    â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
                      â•²             â•±
Layer 1 (medium):  â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—
                    â•² â•± â•² â•± â•² â•± â•² â•±
Layer 0 (dense):  â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—

ê²€ìƒ‰ ê³¼ì •:
1. ìµœìƒìœ„ ë ˆì´ì–´ì—ì„œ ì‹œì‘
2. ê·¸ë˜í”„ ì—£ì§€ ë”°ë¼ ì´ë™
3. í•˜ìœ„ ë ˆì´ì–´ë¡œ ë‚´ë ¤ê°€ë©° ì •ë°€ íƒìƒ‰

ì¥ì :
- ë§¤ìš° ë¹ ë¥¸ ê²€ìƒ‰ (O(log n))
- ë†’ì€ ì¬í˜„ìœ¨ (recall)
- ë™ì  ì¶”ê°€ ê°€ëŠ¥

ë‹¨ì :
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ (ê·¸ë˜í”„ ì €ì¥)
- ì‚­ì œ ì–´ë ¤ì›€
\`\`\`

\`\`\`python
# HNSW ì¸ë±ìŠ¤ (FAISS)
index = faiss.IndexHNSWFlat(dimension, M=32)
# M: ê° ë…¸ë“œì˜ ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•, ëŠë¦¼)

# HNSW ì¸ë±ìŠ¤ (Chroma ê¸°ë³¸ê°’)
# ChromaëŠ” ë‚´ë¶€ì ìœ¼ë¡œ HNSW ì‚¬ìš©
\`\`\`

### 4. PQ (Product Quantization)

\`\`\`
ë°©ë²•: ë²¡í„°ë¥¼ ì„œë¸Œë²¡í„°ë¡œ ë¶„í•  í›„ ì–‘ìí™”

ì›ë³¸: [0.12, -0.34, 0.56, 0.78, -0.23, 0.45, 0.67, -0.89]
       â””â”€â”€ sub1 â”€â”€â”˜  â””â”€â”€ sub2 â”€â”€â”˜  â””â”€â”€ sub3 â”€â”€â”˜  â””â”€â”€ sub4 â”€â”€â”˜
                â†“         â†“           â†“           â†“
ì–‘ìí™”:      [code1]   [code2]     [code3]     [code4]
               â†“         â†“           â†“           â†“
ê²°ê³¼:        [42,       17,         89,         3]

ì¥ì :
- ë©”ëª¨ë¦¬ 90% ì ˆì•½ (1536D float â†’ ìˆ˜ì‹­ ë°”ì´íŠ¸)
- ìˆ˜ì‹­ì–µ ë²¡í„° ì²˜ë¦¬ ê°€ëŠ¥

ë‹¨ì :
- ì •í™•ë„ ì†ì‹¤ (ê·¼ì‚¬)
- í•™ìŠµ í•„ìš”
\`\`\`

### ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ ë¹„êµí‘œ

| ì•Œê³ ë¦¬ì¦˜ | ì†ë„ | ì •í™•ë„ | ë©”ëª¨ë¦¬ | ë™ì  ì¶”ê°€ | ì‚¬ìš©ì²˜ |
|----------|------|--------|--------|----------|--------|
| Flat | âš¡ | ğŸ’¯100% | ğŸ“Šë†’ìŒ | âœ… | <10ë§Œ |
| IVF | âš¡âš¡ | 95-99% | ğŸ“Šì¤‘ê°„ | âŒ (ì¬í•™ìŠµ) | 10ë§Œ-1000ë§Œ |
| HNSW | âš¡âš¡âš¡ | 95-99% | ğŸ“Šë†’ìŒ | âœ… | 100ë§Œ-1ì–µ |
| PQ | âš¡âš¡ | 85-95% | ğŸ“Šë‚®ìŒ | âŒ (ì¬í•™ìŠµ) | 1ì–µ+ |
| IVF+PQ | âš¡âš¡âš¡ | 90-95% | ğŸ“Šë§¤ìš°ë‚®ìŒ | âŒ | 10ì–µ+ |

---

## ì£¼ìš” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¹„êµ

### 1. Chroma

\`\`\`
ìœ í˜•: ì„ë² ë””ë“œ/ë¡œì»¬
ì¸ë±ì‹±: HNSW
íŠ¹ì§•:
- ì„¤ì¹˜ ì—†ì´ ì‹œì‘ (pip install chromadb)
- LangChain ì™„ë²½ í†µí•©
- ì˜êµ¬ ì €ì¥ ì§€ì›

ì‚¬ìš©ì²˜: í”„ë¡œí† íƒ€ì´í•‘, ì†Œê·œëª¨ í”„ë¡œë•ì…˜
ê°€ê²©: ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)
\`\`\`

### 2. Pinecone

\`\`\`
ìœ í˜•: ê´€ë¦¬í˜• í´ë¼ìš°ë“œ
ì¸ë±ì‹±: ë…ì ì•Œê³ ë¦¬ì¦˜ (ìµœì í™”ëœ HNSW)
íŠ¹ì§•:
- ì„œë²„ë¦¬ìŠ¤ ì˜µì…˜
- ìˆ˜ì‹­ì–µ ë²¡í„° ì§€ì›
- 99.9% SLA

ì‚¬ìš©ì²˜: ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜
ê°€ê²©: Free tier (100K ë²¡í„°) + $0.096/1M ë²¡í„°/ì›”
\`\`\`

### 3. Weaviate

\`\`\`
ìœ í˜•: ì˜¤í”ˆì†ŒìŠ¤/í´ë¼ìš°ë“œ
ì¸ë±ì‹±: HNSW
íŠ¹ì§•:
- GraphQL API
- ìì²´ ì„ë² ë”© ìƒì„± ê°€ëŠ¥
- ë©€í‹°ëª¨ë‹¬ ì§€ì›

ì‚¬ìš©ì²˜: GraphQL ì„ í˜¸, ë©€í‹°ëª¨ë‹¬
ê°€ê²©: ë¬´ë£Œ (ì…€í”„í˜¸ìŠ¤íŠ¸) / í´ë¼ìš°ë“œ ìœ ë£Œ
\`\`\`

### 4. Qdrant

\`\`\`
ìœ í˜•: ì˜¤í”ˆì†ŒìŠ¤/í´ë¼ìš°ë“œ
ì¸ë±ì‹±: HNSW + ì–‘ìí™”
íŠ¹ì§•:
- Rust ê¸°ë°˜ ê³ ì„±ëŠ¥
- í’ë¶€í•œ í•„í„°ë§
- gRPC ì§€ì›

ì‚¬ìš©ì²˜: ê³ ì„±ëŠ¥ ìš”êµ¬, ë³µì¡í•œ í•„í„°ë§
ê°€ê²©: ë¬´ë£Œ (ì…€í”„í˜¸ìŠ¤íŠ¸) / í´ë¼ìš°ë“œ ìœ ë£Œ
\`\`\`

### 5. pgvector (PostgreSQL í™•ì¥)

\`\`\`
ìœ í˜•: PostgreSQL í™•ì¥
ì¸ë±ì‹±: IVFFlat, HNSW
íŠ¹ì§•:
- ê¸°ì¡´ PostgreSQL í™œìš©
- SQLë¡œ ë²¡í„° ê²€ìƒ‰
- ACID ë³´ì¥

ì‚¬ìš©ì²˜: ì´ë¯¸ PostgreSQL ì‚¬ìš© ì¤‘
ê°€ê²©: ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)
\`\`\`

\`\`\`sql
-- pgvector ì˜ˆì‹œ
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding vector(1536)
);

CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);

-- ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
SELECT id, content, 1 - (embedding <=> query_embedding) AS similarity
FROM documents
ORDER BY embedding <=> query_embedding
LIMIT 5;
\`\`\`

---

## ë²¡í„° DB ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬

\`\`\`
ì‹œì‘
â”œâ”€â”€ ì´ë¯¸ PostgreSQL ì‚¬ìš© ì¤‘?
â”‚   â””â”€â”€ Yes â†’ pgvector (ê¸°ì¡´ ì¸í”„ë¼ í™œìš©)
â”‚
â”œâ”€â”€ í”„ë¡œí† íƒ€ì´í•‘ ë‹¨ê³„?
â”‚   â””â”€â”€ Yes â†’ Chroma (ê°€ì¥ ì‰¬ìš´ ì‹œì‘)
â”‚
â”œâ”€â”€ ê´€ë¦¬ ë¶€ë‹´ ìµœì†Œí™”?
â”‚   â””â”€â”€ Yes â†’ Pinecone (ì™„ì „ ê´€ë¦¬í˜•)
â”‚
â”œâ”€â”€ ê³ ì„±ëŠ¥ + ë³µì¡í•œ í•„í„°ë§?
â”‚   â””â”€â”€ Yes â†’ Qdrant
â”‚
â”œâ”€â”€ GraphQL ì„ í˜¸?
â”‚   â””â”€â”€ Yes â†’ Weaviate
â”‚
â””â”€â”€ ìµœëŒ€ í™•ì¥ì„± + ì…€í”„í˜¸ìŠ¤íŠ¸?
    â””â”€â”€ Yes â†’ Milvus
\`\`\`
      `,
      keyPoints: [
        'HNSW: ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš© ì‚¬ë¡€ì— ìµœì  (ë¹ ë¥´ê³  ì •í™•)',
        'ë°ì´í„° ê·œëª¨ì— ë”°ë¼ ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ ì„ íƒ',
        'Chroma(í”„ë¡œí† íƒ€ì…) â†’ Pinecone(í”„ë¡œë•ì…˜) ì„±ì¥ ê²½ë¡œ',
      ],
      practiceGoal: 'ë²¡í„° DBì˜ ë‚´ë¶€ ë™ì‘ê³¼ ì„ íƒ ê¸°ì¤€ì„ ì´í•´í•œë‹¤',
    }),

    // ========================================
    // Task 4: Chroma í”„ë¡œë•ì…˜ íŒ¨í„´ (50ë¶„)
    // ========================================
    createCodeTask('w5d2-chroma-production', 'Chroma í”„ë¡œë•ì…˜ íŒ¨í„´ ì‹¤ìŠµ', 50, {
      introduction: `
## í•™ìŠµ ëª©í‘œ

- Chromaì˜ ì„¸ ê°€ì§€ í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œë¥¼ ì´í•´í•œë‹¤ (ì¸ë©”ëª¨ë¦¬, ì˜êµ¬, ì„œë²„)
- ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ ê²€ìƒ‰ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
- í”„ë¡œë•ì…˜ê¸‰ ë²¡í„° ì €ì¥ì†Œ í´ë˜ìŠ¤ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤
- LangChainê³¼ Chromaë¥¼ í†µí•©í•  ìˆ˜ ìˆë‹¤

---

## ì™œ ë°°ìš°ëŠ”ê°€?

**ë¬¸ì œ**: Chromaë¥¼ ì²˜ìŒ ì“°ë©´ "ì¸ë©”ëª¨ë¦¬ë¡œ ë°ì´í„°ê°€ ë‚ ì•„ê°”ì–´ìš”!", "ê²€ìƒ‰ì´ ëŠë ¤ìš”!", "ë©”íƒ€ë°ì´í„° í•„í„°ëŠ” ì–´ë–»ê²Œ ì¨ìš”?" ê°™ì€ ë¬¸ì œì— ë¶€ë”ªí™ë‹ˆë‹¤.

**í•´ê²°**: í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ Chroma ì‚¬ìš©ë²•ì„ ìµíˆë©´ ë°ì´í„° ì˜êµ¬ ì €ì¥, ë¹ ë¥¸ ê²€ìƒ‰, ì •ë°€ í•„í„°ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## ë¹„ìœ : Chroma = ë˜‘ë˜‘í•œ íŒŒì¼ ìºë¹„ë‹›

\`\`\`
ì¼ë°˜ íŒŒì¼ ìºë¹„ë‹› (ê¸°ì¡´ DB)
- íŒŒì¼ ì´ë¦„ìœ¼ë¡œë§Œ ê²€ìƒ‰ (ì •í™•í•œ í‚¤ì›Œë“œ í•„ìš”)
- "2024ë…„ AI ë¬¸ì„œ" â†’ ëª» ì°¾ìŒ

Chroma (ë²¡í„° ê²€ìƒ‰ ìºë¹„ë‹›)
- ë‚´ìš©ì˜ ì˜ë¯¸ë¡œ ê²€ìƒ‰
- "ì¸ê³µì§€ëŠ¥ ìµœì‹  ê¸°ìˆ " â†’ "2024ë…„ AI ë¬¸ì„œ" ì°¾ìŒ
- íƒœê·¸(ë©”íƒ€ë°ì´í„°)ë¡œë„ í•„í„°: "2024ë…„" AND "AI" AND "ê¹€ì² ìˆ˜ ì‘ì„±"
\`\`\`

---

## Chroma ì•„í‚¤í…ì²˜ ì´í•´

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Chroma ì•„í‚¤í…ì²˜                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ë¬¸ì„œ (Text) â”‚ â”€â”€â†’ â”‚  ì„ë² ë”© í•¨ìˆ˜  â”‚ â”€â”€â†’ â”‚  ë²¡í„° DB   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ (OpenAI ë“±) â”‚     â”‚  (HNSW)   â”‚ â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â†“        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ë©”íƒ€ë°ì´í„°   â”‚ â”€â”€â†’ â”‚ í•„í„° ì—”ì§„    â”‚ â”€â”€â†’ â”‚  ê²°ê³¼ ì¡°í•©  â”‚ â”‚
â”‚  â”‚ (JSON)      â”‚     â”‚ ($eq, $and) â”‚     â”‚           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”‘ í•µì‹¬ ì»´í¬ë„ŒíŠ¸:
1. Collection: ë¬¸ì„œ ê·¸ë£¹ (DBì˜ í…Œì´ë¸” ê°œë…)
2. Document: í…ìŠ¤íŠ¸ ë‚´ìš©
3. Embedding: ë²¡í„° í‘œí˜„ (ìë™ ìƒì„± ë˜ëŠ” ì§ì ‘ ì œê³µ)
4. Metadata: í•„í„°ë§ìš© JSON ë°ì´í„°
5. ID: ë¬¸ì„œ ê³ ìœ  ì‹ë³„ì
\`\`\`

---

## í•µì‹¬ êµ¬í˜„ (ê°„ì†Œí™”)

\`\`\`python
# ğŸ“Œ Step 1: ì˜êµ¬ ì €ì¥ ì„¤ì •
import chromadb

client = chromadb.PersistentClient(
    path="./my_vectordb"  # âœ… ì´ í´ë”ì— ì˜êµ¬ ì €ì¥
)

collection = client.get_or_create_collection(
    name="my_docs",
    metadata={"hnsw:space": "cosine"}  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
)

# ğŸ“Œ Step 2: ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë¬¸ì„œ ì¶”ê°€
documents = ["RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤.", "ChromaëŠ” ë²¡í„° DBì…ë‹ˆë‹¤."]
metadatas = [
    {"category": "AI", "year": 2024, "author": "ê¹€ì² ìˆ˜"},
    {"category": "Database", "year": 2024, "author": "ì´ì˜í¬"}
]
ids = ["doc1", "doc2"]

collection.add(
    documents=documents,
    metadatas=metadatas,
    ids=ids
)

# ğŸ“Œ Step 3: í•„í„°ë§ ê²€ìƒ‰
results = collection.query(
    query_texts=["ë°ì´í„°ë² ì´ìŠ¤ ê¸°ìˆ "],
    n_results=2,
    where={"category": "Database"}  # ğŸ” í•„í„°: Database ì¹´í…Œê³ ë¦¬ë§Œ
)

print(results)

# ğŸ“Œ Step 4: LangChain í†µí•©
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma(
    collection_name="my_docs",
    embedding_function=OpenAIEmbeddings(),
    persist_directory="./my_vectordb"
)

# Retrieverë¡œ ì‚¬ìš©
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
docs = retriever.invoke("RAGë€?")
\`\`\`

---

## ì „ì²´ ì½”ë“œ (ìƒì„¸)

### ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

\`\`\`bash
pip install chromadb langchain-chroma langchain-openai
\`\`\`

---

## Chroma í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œ

ChromaëŠ” ì„¸ ê°€ì§€ í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ìƒí™©ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”.

| ëª¨ë“œ | ìš©ë„ | ë°ì´í„° ì˜ì†ì„± | ì„±ëŠ¥ |
|------|------|--------------|------|
| **Client()** | ê°œë°œ/í…ŒìŠ¤íŠ¸ | âŒ íœ˜ë°œì„± | ë¹ ë¦„ |
| **PersistentClient()** | ë‹¨ì¼ ì„œë²„ í”„ë¡œë•ì…˜ | âœ… ì˜êµ¬ ì €ì¥ | ì¤‘ê°„ |
| **HttpClient()** | ë¶„ì‚°/ë©€í‹° ì¸ìŠ¤í„´ìŠ¤ | âœ… ì„œë²„ì— ì €ì¥ | ë„¤íŠ¸ì›Œí¬ ì§€ì—° |

\`\`\`python
import chromadb
from chromadb.config import Settings

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì¸ë©”ëª¨ë¦¬ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
# âš ï¸ ì£¼ì˜: í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ëª¨ë“  ë°ì´í„° ì‚­ì œë¨!
client_memory = chromadb.Client()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì˜êµ¬ ì €ì¥ (í”„ë¡œë•ì…˜ ê¶Œì¥) â­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : ë‹¨ì¼ ì„œë²„ ë°°í¬, ë°ì´í„° ì˜ì†ì„± í•„ìš” ì‹œ
# ğŸ“ ì €ì¥ ìœ„ì¹˜: pathì— ì§€ì •í•œ í´ë”ì— SQLite + Parquet íŒŒì¼ ìƒì„±
client_persistent = chromadb.PersistentClient(
    path="./chroma_db",  # ì´ í´ë”ì— ë°ì´í„° ì €ì¥
    settings=Settings(
        anonymized_telemetry=False,  # í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™” (ê°œì¸ì •ë³´ ë³´í˜¸)
        allow_reset=True  # ë¦¬ì…‹ í—ˆìš© (ê°œë°œ ì‹œì—ë§Œ, í”„ë¡œë•ì…˜ì—ì„œëŠ” False)
    )
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ëª¨ë“œ (ë¶„ì‚° í™˜ê²½)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : ì—¬ëŸ¬ ì•±ì—ì„œ ë™ì¼ DB ì ‘ê·¼, ìˆ˜í‰ í™•ì¥
# ğŸ’¡ ë¨¼ì € ì„œë²„ ì‹¤í–‰ í•„ìš”: chroma run --path ./chroma_data --port 8000
client_http = chromadb.HttpClient(
    host="localhost",
    port=8000,
    settings=Settings(
        chroma_client_auth_provider="chromadb.auth.token.TokenAuthClientProvider",
        chroma_client_auth_credentials="your-token"  # ë³´ì•ˆì„ ìœ„í•œ ì¸ì¦ í† í°
    )
)
\`\`\`

---

## ì»¬ë ‰ì…˜ ìƒì„± ë° ì„¤ì •

ì»¬ë ‰ì…˜ì€ ë¬¸ì„œë¥¼ ê·¸ë£¹í™”í•˜ëŠ” ë‹¨ìœ„ì…ë‹ˆë‹¤. PostgreSQLì˜ í…Œì´ë¸”, MongoDBì˜ ì»¬ë ‰ì…˜ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤.

\`\`\`python
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì»¬ë ‰ì…˜ ìƒì„± ì˜µì…˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
collection = client.get_or_create_collection(
    name="my_documents",

    # HNSW ì¸ë±ìŠ¤ ì„¤ì • (ê²€ìƒ‰ ì„±ëŠ¥ì— ì˜í–¥)
    metadata={
        # ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹ ì„ íƒ
        "hnsw:space": "cosine",  # cosine (ê¶Œì¥), l2, ip(ë‚´ì )

        # HNSW íŒŒë¼ë¯¸í„° (ê³ ê¸‰)
        "hnsw:M": 16,          # ë…¸ë“œë‹¹ ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•â†‘, ë©”ëª¨ë¦¬â†‘)
        "hnsw:ef_construction": 100,  # ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œ íƒìƒ‰ ë²”ìœ„
        "hnsw:ef_search": 50   # ê²€ìƒ‰ ì‹œ íƒìƒ‰ ë²”ìœ„
    },

    # ì„ë² ë”© í•¨ìˆ˜ ì§€ì • (ìƒëµ ì‹œ ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©)
    embedding_function=embedding_functions.OpenAIEmbeddingFunction(
        model_name="text-embedding-3-small"
    )
)

# ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
print(f"ì»¬ë ‰ì…˜ëª…: {collection.name}")
print(f"ë¬¸ì„œ ìˆ˜: {collection.count()}")
print(f"ë©”íƒ€ë°ì´í„°: {collection.metadata}")
\`\`\`

### HNSW íŒŒë¼ë¯¸í„° ê°€ì´ë“œ

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… | ê¶Œì¥ ì¡°ì • |
|---------|--------|------|----------|
| **M** | 16 | ê·¸ë˜í”„ ì—°ê²° ìˆ˜ | ëŒ€ìš©ëŸ‰: 32-64 |
| **ef_construction** | 100 | ì¸ë±ìŠ¤ êµ¬ì¶• ì •ë°€ë„ | ì •í™•ë„ ì¤‘ìš”: 200+ |
| **ef_search** | 50 | ê²€ìƒ‰ ì •ë°€ë„ | ì¬í˜„ìœ¨ ì¤‘ìš”: 100+ |

---

## í”„ë¡œë•ì…˜ê¸‰ ë²¡í„° ì €ì¥ì†Œ í´ë˜ìŠ¤

ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ë²¡í„° ì €ì¥ì†Œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥: ì¤‘ë³µ ë°©ì§€, ë°°ì¹˜ ì²˜ë¦¬, í•„í„° ê²€ìƒ‰, í†µê³„ ì¡°íšŒ

### í´ë˜ìŠ¤ êµ¬ì¡° ì„¤ëª…

\`\`\`
ChromaVectorStore
â”œâ”€â”€ __init__()        â†’ í´ë¼ì´ì–¸íŠ¸/ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
â”œâ”€â”€ _generate_id()    â†’ ì½˜í…ì¸  ê¸°ë°˜ ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)
â”œâ”€â”€ add_documents()   â†’ ë°°ì¹˜ ë¬¸ì„œ ì¶”ê°€
â”œâ”€â”€ search()          â†’ ìœ ì‚¬ë„ ê²€ìƒ‰
â”œâ”€â”€ search_with_filter() â†’ ë³µì¡í•œ í•„í„° ì¡°ê±´ ê²€ìƒ‰
â”œâ”€â”€ delete()          â†’ ë¬¸ì„œ ì‚­ì œ
â””â”€â”€ get_stats()       â†’ í†µê³„ ì¡°íšŒ
\`\`\`

\`\`\`python
from dataclasses import dataclass
from typing import Optional
import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
import hashlib
import json

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : ë¬¸ì„œ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ íƒ€ì… ì•ˆì „ì„± í™•ë³´
# ğŸ’¡ ì™œ í•„ìš”?: dictë³´ë‹¤ IDE ìë™ì™„ì„±, íƒ€ì… ì²´í¬ ê°€ëŠ¥
@dataclass
class Document:
    """ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str           # ê³ ìœ  ì‹ë³„ì (ì¤‘ë³µ ë°©ì§€ìš©)
    content: str      # í…ìŠ¤íŠ¸ ë‚´ìš© (ì„ë² ë”© ëŒ€ìƒ)
    metadata: dict    # í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„° (category, date ë“±)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡œë•ì…˜ê¸‰ ë²¡í„° ì €ì¥ì†Œ í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ChromaVectorStore:
    """
    í”„ë¡œë•ì…˜ê¸‰ Chroma ë²¡í„° ì €ì¥ì†Œ

    ğŸ¯ ì—­í• : RAG ì‹œìŠ¤í…œì˜ ë¬¸ì„œ ì €ì¥/ê²€ìƒ‰ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸

    ğŸ’¡ ì„¤ê³„ ì›ì¹™:
    1. ì˜êµ¬ ì €ì¥: PersistentClientë¡œ ë°ì´í„° ì˜ì†ì„± ë³´ì¥
    2. ì¤‘ë³µ ë°©ì§€: ì½˜í…ì¸  í•´ì‹œ ê¸°ë°˜ ID ìƒì„±
    3. ë°°ì¹˜ ì²˜ë¦¬: ëŒ€ëŸ‰ ë¬¸ì„œ íš¨ìœ¨ì  ì²˜ë¦¬
    4. í•„í„° ê²€ìƒ‰: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì •ë°€ ê²€ìƒ‰
    """

    def __init__(
        self,
        collection_name: str,
        persist_directory: str = "./chroma_db",
        embedding_model: str = "text-embedding-3-small"
    ):
        """
        ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”

        Args:
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„ (DB í…Œì´ë¸” ê°œë…)
            persist_directory: ë°ì´í„° ì €ì¥ í´ë” ê²½ë¡œ
            embedding_model: OpenAI ì„ë² ë”© ëª¨ë¸ëª…
        """
        # Chroma í´ë¼ì´ì–¸íŠ¸ (ì˜êµ¬ ì €ì¥ ëª¨ë“œ)
        self.client = chromadb.PersistentClient(path=persist_directory)

        # OpenAI ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
        # ğŸ’¡ Chromaê°€ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ â†’ ë²¡í„° ë³€í™˜ ìˆ˜í–‰
        self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(
            model_name=embedding_model
        )

        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ
        # ğŸ’¡ get_or_create: ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ê¸°ì¡´ ê²ƒ ë°˜í™˜
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            embedding_function=self.embedding_fn,
            metadata={
                "description": f"RAG collection: {collection_name}",
                "embedding_model": embedding_model,
                "hnsw:space": "cosine"  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©
            }
        )

    def _generate_id(self, content: str) -> str:
        """
        ì½˜í…ì¸  ê¸°ë°˜ ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)

        ğŸ¯ ì—­í• : ë™ì¼í•œ ë‚´ìš© â†’ ë™ì¼í•œ ID â†’ ìë™ ì¤‘ë³µ ë°©ì§€
        ğŸ’¡ SHA256 í•´ì‹œì˜ ì• 16ìë¦¬ ì‚¬ìš© (ì¶©ëŒ í™•ë¥  ê·¹íˆ ë‚®ìŒ)
        """
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def add_documents(
        self,
        documents: list[Document],
        batch_size: int = 100
    ) -> int:
        """
        ë¬¸ì„œ ë°°ì¹˜ ì¶”ê°€

        ğŸ¯ ì—­í• : ëŒ€ëŸ‰ì˜ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë²¡í„° DBì— ì €ì¥

        ğŸ’¡ ì™œ ë°°ì¹˜ ì²˜ë¦¬?
        - ë©”ëª¨ë¦¬ íš¨ìœ¨: í•œ ë²ˆì— ëª¨ë“  ë¬¸ì„œ ë¡œë“œí•˜ë©´ OOM ìœ„í—˜
        - API ì œí•œ: OpenAI ì„ë² ë”© APIëŠ” í•œ ë²ˆì— ì²˜ë¦¬ëŸ‰ ì œí•œ
        - ì§„í–‰ ìƒí™©: ë°°ì¹˜ë³„ ì§„í–‰ë¥  í‘œì‹œ ê°€ëŠ¥

        Args:
            documents: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ 100)

        Returns:
            ì‹¤ì œ ì¶”ê°€ëœ ë¬¸ì„œ ìˆ˜ (ì¤‘ë³µ ì œì™¸)
        """
        added = 0

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]

            # ID ìƒì„± (ì—†ìœ¼ë©´ ì½˜í…ì¸  í•´ì‹œë¡œ ìë™ ìƒì„±)
            ids = [doc.id or self._generate_id(doc.content) for doc in batch]
            contents = [doc.content for doc in batch]
            metadatas = [doc.metadata for doc in batch]

            # âš ï¸ ì¤‘ë³µ ì²´í¬: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” IDëŠ” ê±´ë„ˆëœ€
            existing = self.collection.get(ids=ids)
            new_indices = [
                j for j, id_ in enumerate(ids)
                if id_ not in existing['ids']
            ]

            if new_indices:
                self.collection.add(
                    ids=[ids[j] for j in new_indices],
                    documents=[contents[j] for j in new_indices],
                    metadatas=[metadatas[j] for j in new_indices]
                )
                added += len(new_indices)

            print(f"Progress: {min(i + batch_size, len(documents))}/{len(documents)}")

        return added

    def search(
        self,
        query: str,
        k: int = 5,
        filter: Optional[dict] = None,
        include_distances: bool = True
    ) -> list[dict]:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

        ğŸ¯ ì—­í• : ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ kê°œ ë°˜í™˜

        ğŸ’¡ ë™ì‘ ë°©ì‹:
        1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ â†’ ë²¡í„° ë³€í™˜ (ì„ë² ë”©)
        2. HNSW ì¸ë±ìŠ¤ë¡œ ìœ ì‚¬ ë²¡í„° ê²€ìƒ‰
        3. ë©”íƒ€ë°ì´í„° í•„í„° ì ìš© (ìˆëŠ” ê²½ìš°)
        4. ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            filter: ë©”íƒ€ë°ì´í„° í•„í„° (ì˜ˆ: {"category": "AI"})
            include_distances: ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨ ì—¬ë¶€

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [{id, content, metadata, similarity}, ...]
        """
        results = self.collection.query(
            query_texts=[query],
            n_results=k,
            where=filter,
            include=["documents", "metadatas", "distances"]
        )

        # ê²°ê³¼ í¬ë§·íŒ… (ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜)
        formatted = []
        for i in range(len(results['ids'][0])):
            doc = {
                "id": results['ids'][0][i],
                "content": results['documents'][0][i],
                "metadata": results['metadatas'][0][i]
            }
            if include_distances:
                doc["distance"] = results['distances'][0][i]
                # ğŸ’¡ cosine distance â†’ similarity ë³€í™˜
                # distance=0 â†’ similarity=1 (ì™„ì „ ì¼ì¹˜)
                doc["similarity"] = 1 - results['distances'][0][i]
            formatted.append(doc)

        return formatted

    def search_with_filter(
        self,
        query: str,
        k: int = 5,
        category: Optional[str] = None,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None
    ) -> list[dict]:
        """
        ë³µì¡í•œ í•„í„° ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰

        ğŸ¯ ì—­í• : ì¹´í…Œê³ ë¦¬, ë‚ ì§œ ë²”ìœ„ ë“± ì—¬ëŸ¬ ì¡°ê±´ì„ ì¡°í•©í•œ ì •ë°€ ê²€ìƒ‰

        ğŸ’¡ Chroma í•„í„° ì—°ì‚°ì:
        - $eq: ê°™ìŒ
        - $ne: ë‹¤ë¦„
        - $gt, $gte: í¬ë‹¤, í¬ê±°ë‚˜ ê°™ë‹¤
        - $lt, $lte: ì‘ë‹¤, ì‘ê±°ë‚˜ ê°™ë‹¤
        - $in: ë¦¬ìŠ¤íŠ¸ì— í¬í•¨
        - $and, $or: ì¡°ê±´ ì¡°í•©

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜ ë¬¸ì„œ ìˆ˜
            category: ì¹´í…Œê³ ë¦¬ í•„í„° (ì˜ˆ: "AI", "Database")
            date_from: ì‹œì‘ ë‚ ì§œ (ì˜ˆ: "2024-01-01")
            date_to: ì¢…ë£Œ ë‚ ì§œ (ì˜ˆ: "2024-12-31")
        """
        where_conditions = []

        if category:
            where_conditions.append({"category": {"$eq": category}})

        if date_from:
            where_conditions.append({"date": {"$gte": date_from}})

        if date_to:
            where_conditions.append({"date": {"$lte": date_to}})

        # ì¡°ê±´ ì¡°í•©: ì—¬ëŸ¬ ì¡°ê±´ì€ $andë¡œ ë¬¶ìŒ
        filter_dict = None
        if len(where_conditions) == 1:
            filter_dict = where_conditions[0]
        elif len(where_conditions) > 1:
            filter_dict = {"$and": where_conditions}

        return self.search(query, k=k, filter=filter_dict)

    def delete(self, ids: list[str]) -> int:
        """
        ë¬¸ì„œ ì‚­ì œ

        ğŸ¯ ì—­í• : IDë¡œ ë¬¸ì„œ ì‚­ì œ (ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬, GDPR ì‚­ì œ ìš”ì²­ ë“±)
        âš ï¸ ì£¼ì˜: ì‚­ì œëŠ” ë³µêµ¬ ë¶ˆê°€! ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©
        """
        existing = self.collection.get(ids=ids)
        valid_ids = existing['ids']
        if valid_ids:
            self.collection.delete(ids=valid_ids)
        return len(valid_ids)

    def get_stats(self) -> dict:
        """
        ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ

        ğŸ¯ ì—­í• : ì»¬ë ‰ì…˜ ìƒíƒœ ëª¨ë‹ˆí„°ë§ (ë¬¸ì„œ ìˆ˜, ì„¤ì • ë“±)
        ğŸ’¡ í™œìš©: ëŒ€ì‹œë³´ë“œ í‘œì‹œ, í—¬ìŠ¤ì²´í¬, ë””ë²„ê¹…
        """
        return {
            "name": self.collection.name,
            "count": self.collection.count(),
            "metadata": self.collection.metadata
        }
\`\`\`

---

## ì‚¬ìš© ì˜ˆì‹œ

ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ChromaVectorStoreë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.

\`\`\`python
# 1. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
store = ChromaVectorStore(
    collection_name="tech_articles",
    persist_directory="./production_db"
)

# 2. ë¬¸ì„œ ì¶”ê°€
documents = [
    Document(
        id="doc1",
        content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ, LLMì˜ í™˜ê°ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤.",
        metadata={"category": "AI", "date": "2024-01-15", "author": "í™ê¸¸ë™"}
    ),
    Document(
        id="doc2",
        content="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        metadata={"category": "DB", "date": "2024-01-20", "author": "ê¹€ì² ìˆ˜"}
    ),
    Document(
        id="doc3",
        content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
        metadata={"category": "AI", "date": "2024-02-01", "author": "ì´ì˜í¬"}
    )
]

added = store.add_documents(documents)
print(f"ì¶”ê°€ëœ ë¬¸ì„œ: {added}ê°œ")

# 3. ê¸°ë³¸ ê²€ìƒ‰
results = store.search("LLM í™˜ê° ë°©ì§€ ê¸°ìˆ ", k=2)
for r in results:
    print(f"[{r['similarity']:.2%}] {r['content'][:50]}...")

# 4. í•„í„° ê²€ìƒ‰
ai_results = store.search_with_filter(
    query="ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ",
    k=5,
    category="AI",
    date_from="2024-01-01"
)

# 5. í†µê³„ í™•ì¸
stats = store.get_stats()
print(f"ì»¬ë ‰ì…˜: {stats['name']}, ë¬¸ì„œ ìˆ˜: {stats['count']}")
\`\`\`

---

## LangChain í†µí•©

LangChainì„ ì‚¬ìš©í•˜ë©´ Chromaë¥¼ ë” ì‰½ê²Œ RAG íŒŒì´í”„ë¼ì¸ì— í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### LangChain + Chroma ì•„í‚¤í…ì²˜

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LangChain RAG íŒŒì´í”„ë¼ì¸                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ì‚¬ìš©ì ì§ˆë¬¸                                             â”‚
â”‚       â†“                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Retriever  â”‚ â”€â”€â†’ â”‚    Chroma    â”‚ â”€â”€â†’ â”‚  ê´€ë ¨ ë¬¸ì„œ  â”‚ â”‚
â”‚  â”‚(as_retriever)â”‚     â”‚ (VectorStore)â”‚     â”‚  kê°œ ë°˜í™˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                â†“        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    Chain    â”‚ â†â”€â”€ â”‚    LLM      â”‚ â†â”€â”€ â”‚ í”„ë¡¬í”„íŠ¸   â”‚ â”‚
â”‚  â”‚(RetrievalQA)â”‚     â”‚ (GPT-4o)    â”‚     â”‚ + ì»¨í…ìŠ¤íŠ¸ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â†“                                                 â”‚
â”‚    ìµœì¢… ë‹µë³€                                             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

\`\`\`python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document as LCDocument
from langchain.chains import RetrievalQA

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : LangChainì—ì„œ Chromaë¥¼ VectorStoreë¡œ ì‚¬ìš©
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

vectorstore = Chroma(
    collection_name="langchain_demo",
    embedding_function=embeddings,
    persist_directory="./langchain_chroma"  # ì˜êµ¬ ì €ì¥
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ë¬¸ì„œ ì¶”ê°€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ’¡ LangChain Document í˜•ì‹ ì‚¬ìš© (page_content + metadata)
docs = [
    LCDocument(
        page_content="RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤.",
        metadata={"source": "tutorial", "page": 1}
    ),
    LCDocument(
        page_content="ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.",
        metadata={"source": "tutorial", "page": 2}
    )
]

vectorstore.add_documents(docs)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Retriever ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : ì¿¼ë¦¬ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ì„ ì¶”ìƒí™”
retriever = vectorstore.as_retriever(
    # ê²€ìƒ‰ ì „ëµ ì„ íƒ
    search_type="mmr",  # Maximum Marginal Relevance (ë‹¤ì–‘ì„± ê³ ë ¤)
    # ëŒ€ì•ˆ: "similarity" (ìˆœìˆ˜ ìœ ì‚¬ë„), "similarity_score_threshold"

    search_kwargs={
        "k": 5,              # ìµœì¢… ë°˜í™˜ ë¬¸ì„œ ìˆ˜
        "fetch_k": 20,       # MMR í›„ë³´êµ° (ë” ë§ì´ ê°€ì ¸ì™€ì„œ ë‹¤ì–‘ì„± í™•ë³´)
        "lambda_mult": 0.5   # ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜ (0=ë‹¤ì–‘ì„±, 1=ìœ ì‚¬ë„)
    }
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. QA ì²´ì¸ êµ¬ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : ê²€ìƒ‰ + ìƒì„±ì„ í•˜ë‚˜ì˜ ì²´ì¸ìœ¼ë¡œ ì—°ê²°
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— (ì‘ì€ ë¬¸ì„œì…‹)
    # ëŒ€ì•ˆ: "map_reduce" (ëŒ€ìš©ëŸ‰), "refine" (ìˆœì°¨ ê°œì„ )
    retriever=retriever,
    return_source_documents=True  # ì¶œì²˜ ë¬¸ì„œë„ ë°˜í™˜
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ì§ˆì˜ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
result = qa_chain.invoke({"query": "RAGì™€ ì„ë² ë”©ì˜ ê´€ê³„ëŠ”?"})
print(f"ë‹µë³€: {result['result']}")
print(f"ì°¸ì¡°: {[doc.metadata for doc in result['source_documents']]}")
\`\`\`

### ê²€ìƒ‰ íƒ€ì… ë¹„êµ

| search_type | ì„¤ëª… | ì‚¬ìš© ìƒí™© |
|-------------|------|----------|
| **similarity** | ìˆœìˆ˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ | ê¸°ë³¸, ë¹ ë¥¸ ê²€ìƒ‰ |
| **mmr** | ìœ ì‚¬ë„ + ë‹¤ì–‘ì„± | ì¤‘ë³µ ë¬¸ì„œ ë°©ì§€ |
| **similarity_score_threshold** | ì„ê³„ê°’ ì´ìƒë§Œ | í’ˆì§ˆ ë³´ì¥ í•„ìš” ì‹œ |

---

## Chroma ì„œë²„ ëª¨ë“œ (ë¶„ì‚° í™˜ê²½)

ì—¬ëŸ¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë™ì¼í•œ Chroma DBì— ì ‘ê·¼í•´ì•¼ í•  ë•Œ ì„œë²„ ëª¨ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

### ì–¸ì œ ì„œë²„ ëª¨ë“œê°€ í•„ìš”í•œê°€?

| ìƒí™© | PersistentClient | HttpClient (ì„œë²„) |
|------|-----------------|-------------------|
| ë‹¨ì¼ ì•±, ë¡œì»¬ ê°œë°œ | âœ… ì í•© | âŒ ë¶ˆí•„ìš” |
| ì—¬ëŸ¬ ì•±ì—ì„œ ê³µìœ  | âŒ íŒŒì¼ ì ê¸ˆ ë¬¸ì œ | âœ… ì í•© |
| K8s ë©€í‹° íŒŒë“œ | âŒ ê³µìœ  ìŠ¤í† ë¦¬ì§€ í•„ìš” | âœ… ì í•© |
| ê³ ê°€ìš©ì„± í•„ìš” | âŒ ë‹¨ì¼ ì¥ì• ì  | âœ… ë³µì œ ê°€ëŠ¥ |

\`\`\`bash
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë²„ ì‹œì‘ (CLI)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
chroma run --path ./chroma_server_data --port 8000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Dockerë¡œ ì‹¤í–‰ (í”„ë¡œë•ì…˜ ê¶Œì¥)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
docker run -p 8000:8000 -v ./chroma_data:/chroma/chroma chromadb/chroma
\`\`\`

\`\`\`python
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì—°ê²°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import chromadb

# ğŸ¯ ì—­í• : ì›ê²© Chroma ì„œë²„ì— ì—°ê²°
# ğŸ’¡ PersistentClientì™€ ë™ì¼í•œ API ì‚¬ìš© ê°€ëŠ¥
client = chromadb.HttpClient(host="localhost", port=8000)
collection = client.get_or_create_collection("my_collection")

# ì´í›„ ì‚¬ìš©ë²•ì€ ë™ì¼!
collection.add(documents=["..."], ids=["..."])
results = collection.query(query_texts=["..."], n_results=5)
\`\`\`

## âš ï¸ Common Pitfalls (ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜)

### 1. [ì˜êµ¬ ì €ì¥] Client() ëŒ€ì‹  PersistentClient() ì‚¬ìš©
**ì¦ìƒ**: í”„ë¡œê·¸ë¨ ì¬ì‹œì‘ ì‹œ ëª¨ë“  ë°ì´í„° ì‚¬ë¼ì§
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ì¸ë©”ëª¨ë¦¬ í´ë¼ì´ì–¸íŠ¸
client = chromadb.Client()  # ë°ì´í„° íœ˜ë°œ!
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: Client()ëŠ” ì¸ë©”ëª¨ë¦¬, í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ë°ì´í„° ì†ì‹¤
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - ì˜êµ¬ ì €ì¥
client = chromadb.PersistentClient(path="./my_chroma_db")
\`\`\`
**ê¸°ì–µí•  ì **: í”„ë¡œë•ì…˜ì—ì„œëŠ” í•­ìƒ \`PersistentClient\` ì‚¬ìš©

### 2. [ì¤‘ë³µ ì¶”ê°€] ê°™ì€ IDë¡œ add ë°˜ë³µ
**ì¦ìƒ**: ì—ëŸ¬ ë°œìƒ ë˜ëŠ” ë°ì´í„° ë¬´ì‹œ
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ë™ì¼ ID ë°˜ë³µ add
collection.add(documents=["doc1"], ids=["id1"])
collection.add(documents=["doc1 updated"], ids=["id1"])  # ğŸ’¥ ì—ëŸ¬!
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: addëŠ” ìƒˆ ë¬¸ì„œ ì¶”ê°€ìš©, ê¸°ì¡´ IDëŠ” ì—ëŸ¬
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - upsert ì‚¬ìš©
collection.upsert(documents=["doc1 updated"], ids=["id1"])  # ì—…ë°ì´íŠ¸
\`\`\`
**ê¸°ì–µí•  ì **: ê¸°ì¡´ ë¬¸ì„œ ê°±ì‹ ì€ \`upsert()\`, ìƒˆ ë¬¸ì„œëŠ” \`add()\`

### 3. [ë©”íƒ€ë°ì´í„° í•„í„°] where ë¬¸ë²• ì˜¤ë¥˜
**ì¦ìƒ**: í•„í„°ê°€ ì‘ë™ ì•ˆ í•¨, ëª¨ë“  ê²°ê³¼ ë°˜í™˜
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ì˜ëª»ëœ í•„í„° ë¬¸ë²•
results = collection.query(
    query_texts=["AI"],
    where={"year": 2024}  # ë¬¸ìì—´ì´ì–´ì•¼ í•¨!
)
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: Chroma ë©”íƒ€ë°ì´í„° í•„í„°ëŠ” íŠ¹ì • ë¬¸ë²• í•„ìš”
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - ì˜¬ë°”ë¥¸ ì—°ì‚°ì ì‚¬ìš©
results = collection.query(
    query_texts=["AI"],
    where={"year": {"$eq": 2024}},  # $eq ì—°ì‚°ì
    # ë˜ëŠ” ë¬¸ìì—´: where={"category": "AI"}
)
\`\`\`
**ê¸°ì–µí•  ì **: ìˆ«ì ë¹„êµëŠ” \`$eq\`, \`$gt\`, \`$lt\` ë“± ì—°ì‚°ì í•„ìˆ˜
      `,
      keyPoints: [
        'ğŸ’¾ PersistentClientë¡œ ì˜êµ¬ ì €ì¥',
        'ğŸ” ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ì •ë°€ ê²€ìƒ‰',
        'ğŸ”— LangChain í†µí•©ìœ¼ë¡œ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•',
      ],
      practiceGoal: 'Chromaë¥¼ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 5: Pinecone í”„ë¡œë•ì…˜ íŒ¨í„´ (50ë¶„)
    // ========================================
    createCodeTask('w5d2-pinecone-production', 'Pinecone í”„ë¡œë•ì…˜ íŒ¨í„´ ì‹¤ìŠµ', 50, {
      introduction: `
## í•™ìŠµ ëª©í‘œ

- Pineconeì˜ ì•„í‚¤í…ì²˜ì™€ í•µì‹¬ ê°œë…ì„ ì´í•´í•œë‹¤
- Serverless vs Pod ë°°í¬ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤
- ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ í™œìš©í•œ ë©€í‹°í…Œë„ŒíŠ¸ êµ¬í˜„ì„ í•™ìŠµí•œë‹¤
- í”„ë¡œë•ì…˜ê¸‰ Pinecone ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤

---

## ì™œ ë°°ìš°ëŠ”ê°€?

**ë¬¸ì œ**: ChromaëŠ” í”„ë¡œí† íƒ€ì…ì— ì¢‹ì§€ë§Œ, ìˆ˜ë°±ë§Œ ë¬¸ì„œë¥¼ ë‹¤ë£¨ëŠ” í”„ë¡œë•ì…˜ì—ì„œëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
- ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ì–´ë ¤ì›€
- ì„±ëŠ¥ ìµœì í™” ì§ì ‘ í•´ì•¼ í•¨
- ì¸í”„ë¼ ê´€ë¦¬ ë¶€ë‹´

**í•´ê²°**: Pineconeì€ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤ë¡œ ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## Pinecone ì•„í‚¤í…ì²˜ ì´í•´

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Pinecone ì•„í‚¤í…ì²˜                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚  â”‚   Project   â”‚  â† í•˜ë‚˜ì˜ ê³„ì •/íŒ€                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚         â”‚                                               â”‚
â”‚         â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚   Index 1   â”‚     â”‚   Index 2   â”‚  â† ë²¡í„° ì €ì¥ ë‹¨ìœ„   â”‚
â”‚  â”‚ (dimension) â”‚     â”‚ (dimension) â”‚    (ì°¨ì› ê³ ì •)      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                                               â”‚
â”‚         â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚           Namespaces                  â”‚              â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚
â”‚  â”‚ customer â”‚ customer â”‚   default      â”‚  â† ë…¼ë¦¬ì     â”‚
â”‚  â”‚    _a    â”‚    _b    â”‚     ("")       â”‚    ë¶„ë¦¬      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”‘ í•µì‹¬ ê°œë…:
1. Project: ìµœìƒìœ„ ë‹¨ìœ„ (API í‚¤ ë²”ìœ„)
2. Index: ë²¡í„° ì €ì¥ ë‹¨ìœ„ (ì°¨ì› ê³ ì •, metric ì„ íƒ)
3. Namespace: ë…¼ë¦¬ì  ë¶„ë¦¬ (ë©€í‹°í…Œë„ŒíŠ¸ìš©)
4. Vector: ID + values(ì„ë² ë”©) + metadata
\`\`\`

---

## Serverless vs Pod ë¹„êµ

| í•­ëª© | Serverless | Pod |
|------|------------|-----|
| **ê³¼ê¸ˆ** | ì‚¬ìš©ëŸ‰ ê¸°ë°˜ (read/write units) | ì‹œê°„ ê¸°ë°˜ (pod ìˆ˜ Ã— ì‹œê°„) |
| **í™•ì¥** | ìë™ (0~ë¬´í•œ) | ìˆ˜ë™ (replica ì„¤ì •) |
| **ì‹œì‘ ë¹„ìš©** | $0 (Free tier) | ~$70/ì›”~ |
| **Cold start** | ìˆìŒ (ìˆ˜ ì´ˆ) | ì—†ìŒ |
| **ì‚¬ìš© ì‚¬ë¡€** | ë³€ë™ íŠ¸ë˜í”½, MVP | ì¼ì • íŠ¸ë˜í”½, ì§€ì—°ì‹œê°„ ì¤‘ìš” |

\`\`\`python
# Serverless ì¸ë±ìŠ¤ ìƒì„± (ê¶Œì¥, ë¹„ìš© íš¨ìœ¨ì )
pc.create_index(
    name="my-index",
    dimension=1536,
    metric="cosine",
    spec=ServerlessSpec(cloud="aws", region="us-east-1")
)

# Pod ì¸ë±ìŠ¤ ìƒì„± (ê³ ì„±ëŠ¥, ì¼ì • íŠ¸ë˜í”½)
pc.create_index(
    name="my-index",
    dimension=1536,
    metric="cosine",
    spec=PodSpec(environment="us-east-1-aws", pod_type="p1.x1")
)
\`\`\`

---

## ë¹„ìœ : Chroma vs Pinecone = ê°œì¸ ì„œë²„ vs í´ë¼ìš°ë“œ

\`\`\`
Chroma (ê°œì¸ ì„œë²„)
- ë‚´ê°€ ì§ì ‘ ê´€ë¦¬
- ì‘ì€ ê·œëª¨ì— ì í•©
- ë¬´ë£Œì§€ë§Œ ê´€ë¦¬ ë¶€ë‹´

Pinecone (AWS ê°™ì€ í´ë¼ìš°ë“œ)
- ìë™ í™•ì¥, ìë™ ë°±ì—…
- ìˆ˜ì‹­ì–µ ë²¡í„° ì²˜ë¦¬ ê°€ëŠ¥
- ìœ ë£Œì§€ë§Œ ê´€ë¦¬ í¸í•¨
- ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ë©€í‹°í…Œë„ŒíŠ¸ ì§€ì›
\`\`\`

---

## í•µì‹¬ êµ¬í˜„ (ê°„ì†Œí™”)

\`\`\`python
# ğŸ“Œ Step 1: Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
from pinecone import Pinecone
import os

pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])

# ğŸ“Œ Step 2: ì¸ë±ìŠ¤ ìƒì„± (í•œ ë²ˆë§Œ)
index_name = "my-rag-index"
if index_name not in [idx.name for idx in pc.list_indexes()]:
    pc.create_index(
        name=index_name,
        dimension=1536,  # text-embedding-3-small
        metric="cosine"
    )

index = pc.Index(index_name)

# ğŸ“Œ Step 3: ë¬¸ì„œ ì—…ì„œíŠ¸ (ë©”íƒ€ë°ì´í„° í¬í•¨)
from openai import OpenAI
client = OpenAI()

def get_embedding(text):
    return client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    ).data[0].embedding

vectors = [
    {
        "id": "doc1",
        "values": get_embedding("RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤."),
        "metadata": {"category": "AI", "year": 2024}
    },
    {
        "id": "doc2",
        "values": get_embedding("Pineconeì€ ë²¡í„° DBì…ë‹ˆë‹¤."),
        "metadata": {"category": "Database", "year": 2024}
    }
]

index.upsert(vectors=vectors)

# ğŸ“Œ Step 4: í•„í„°ë§ ê²€ìƒ‰
query_emb = get_embedding("ë°ì´í„°ë² ì´ìŠ¤ ê¸°ìˆ ")

results = index.query(
    vector=query_emb,
    top_k=2,
    filter={"category": {"$eq": "Database"}},  # ğŸ” í•„í„°
    include_metadata=True
)

print(results)

# ğŸ“Œ Step 5: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ë©€í‹°í…Œë„ŒíŠ¸
# ê³ ê° Aì˜ ë°ì´í„°
index.upsert(vectors=vectors_a, namespace="customer_a")

# ê³ ê° Bì˜ ë°ì´í„°
index.upsert(vectors=vectors_b, namespace="customer_b")

# ê³ ê° Aë§Œ ê²€ìƒ‰
results_a = index.query(vector=query_emb, namespace="customer_a", top_k=5)
\`\`\`

---

## ì „ì²´ ì½”ë“œ (ìƒì„¸)

### Pinecone ê³„ì • ì„¤ì •

\`\`\`
1. https://www.pinecone.io ê°€ì…
2. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
3. API í‚¤ ë°œê¸‰ (Settings â†’ API Keys)
4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

export PINECONE_API_KEY="your-api-key"
\`\`\`

---

## ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

\`\`\`bash
pip install pinecone-client langchain-pinecone
\`\`\`

---

## í”„ë¡œë•ì…˜ê¸‰ Pinecone ë²¡í„° ì €ì¥ì†Œ

ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ Pinecone ë²¡í„° ì €ì¥ì†Œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

### í´ë˜ìŠ¤ êµ¬ì¡° ì„¤ëª…

\`\`\`
PineconeVectorStore
â”œâ”€â”€ __init__()           â†’ í´ë¼ì´ì–¸íŠ¸/ì¸ë±ìŠ¤ ì´ˆê¸°í™”
â”œâ”€â”€ _ensure_index()      â†’ ì¸ë±ìŠ¤ ìë™ ìƒì„±
â”œâ”€â”€ _get_embedding()     â†’ ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©
â”œâ”€â”€ _get_embeddings_batch() â†’ ë°°ì¹˜ ì„ë² ë”© (ë¹„ìš© ì ˆê°)
â”œâ”€â”€ upsert_documents()   â†’ ë¬¸ì„œ ì—…ì„œíŠ¸ (ì¶”ê°€/ê°±ì‹ )
â”œâ”€â”€ search()             â†’ ìœ ì‚¬ë„ ê²€ìƒ‰
â”œâ”€â”€ search_with_filter() â†’ í•„í„° ì¡°ê±´ ê²€ìƒ‰
â”œâ”€â”€ delete()             â†’ ë²¡í„° ì‚­ì œ
â””â”€â”€ get_stats()          â†’ ì¸ë±ìŠ¤ í†µê³„
\`\`\`

\`\`\`python
from pinecone import Pinecone, ServerlessSpec
from openai import OpenAI
from dataclasses import dataclass
from typing import Optional
import hashlib
import os
import time

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : Pinecone ë²¡í„° ë°ì´í„° êµ¬ì¡°í™”
# ğŸ’¡ embeddingì€ ì„ íƒì  - ì—†ìœ¼ë©´ ìë™ ìƒì„±
@dataclass
class PineconeDocument:
    """ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str                                # ê³ ìœ  ì‹ë³„ì
    content: str                           # í…ìŠ¤íŠ¸ ë‚´ìš©
    metadata: dict                         # í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„°
    embedding: Optional[list[float]] = None  # ë¯¸ë¦¬ ê³„ì‚°ëœ ì„ë² ë”© (ì„ íƒ)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡œë•ì…˜ê¸‰ Pinecone ë²¡í„° ì €ì¥ì†Œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class PineconeVectorStore:
    """
    í”„ë¡œë•ì…˜ê¸‰ Pinecone ë²¡í„° ì €ì¥ì†Œ

    ğŸ¯ ì—­í• : ëŒ€ê·œëª¨ RAG ì‹œìŠ¤í…œì˜ ë²¡í„° ì €ì¥/ê²€ìƒ‰ ë‹´ë‹¹

    ğŸ’¡ ì„¤ê³„ ì›ì¹™:
    1. ìë™ ì¸ë±ìŠ¤ ìƒì„±: ì—†ìœ¼ë©´ Serverlessë¡œ ìë™ ìƒì„±
    2. ë°°ì¹˜ ì²˜ë¦¬: ëŒ€ëŸ‰ ë¬¸ì„œ íš¨ìœ¨ì  ì—…ì„œíŠ¸
    3. ë„¤ì„ìŠ¤í˜ì´ìŠ¤: ë©€í‹°í…Œë„ŒíŠ¸ ì§€ì›
    4. ë©”íƒ€ë°ì´í„° ì œí•œ: 40KB ì œí•œ ìë™ ì²˜ë¦¬
    """

    def __init__(
        self,
        index_name: str,
        dimension: int = 1536,
        metric: str = "cosine",
        create_if_not_exists: bool = True
    ):
        """
        ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”

        Args:
            index_name: Pinecone ì¸ë±ìŠ¤ ì´ë¦„
            dimension: ë²¡í„° ì°¨ì› (text-embedding-3-small = 1536)
            metric: ìœ ì‚¬ë„ ì¸¡ì • (cosine, dotproduct, euclidean)
            create_if_not_exists: ì¸ë±ìŠ¤ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        """
        # Pinecone & OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
        self.openai = OpenAI()
        self.dimension = dimension
        self.index_name = index_name

        # ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ì—°ê²°
        if create_if_not_exists:
            self._ensure_index(metric)

        self.index = self.pc.Index(index_name)

    def _ensure_index(self, metric: str):
        """
        ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±

        ğŸ¯ ì—­í• : Serverless ì¸ë±ìŠ¤ ìë™ í”„ë¡œë¹„ì €ë‹
        ğŸ’¡ Serverless ì„ íƒ ì´ìœ : ë¹„ìš© íš¨ìœ¨ì , ìë™ í™•ì¥
        """
        existing = [idx.name for idx in self.pc.list_indexes()]

        if self.index_name not in existing:
            print(f"Creating index: {self.index_name}")
            self.pc.create_index(
                name=self.index_name,
                dimension=self.dimension,
                metric=metric,
                spec=ServerlessSpec(
                    cloud="aws",
                    region="us-east-1"  # ê°€ì¥ ê°€ê¹Œìš´ ë¦¬ì „ ì„ íƒ
                )
            )
            # â³ ì¸ë±ìŠ¤ ì¤€ë¹„ ëŒ€ê¸° (ë³´í†µ 30ì´ˆ~1ë¶„)
            while not self.pc.describe_index(self.index_name).status['ready']:
                time.sleep(1)
            print(f"Index {self.index_name} ready!")

    def _get_embedding(self, text: str) -> list[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±

        ğŸ¯ ì—­í• : ì¿¼ë¦¬ í…ìŠ¤íŠ¸ â†’ ë²¡í„° ë³€í™˜
        ğŸ’¡ ê²€ìƒ‰ ì‹œ ì‚¬ìš© (ë‹¨ì¼ ì¿¼ë¦¬)
        """
        response = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

    def _get_embeddings_batch(self, texts: list[str]) -> list[list[float]]:
        """
        ë°°ì¹˜ ì„ë² ë”© ìƒì„±

        ğŸ¯ ì—­í• : ì—¬ëŸ¬ í…ìŠ¤íŠ¸ í•œ ë²ˆì— ì„ë² ë”©
        ğŸ’¡ ë¬¸ì„œ ì—…ì„œíŠ¸ ì‹œ ì‚¬ìš© (ë¹„ìš© ì ˆê°, API í˜¸ì¶œ ìµœì†Œí™”)
        âš ï¸ OpenAI ì œí•œ: í•œ ë²ˆì— ìµœëŒ€ 2048ê°œ
        """
        response = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [item.embedding for item in response.data]

    def upsert_documents(
        self,
        documents: list[PineconeDocument],
        namespace: str = "",
        batch_size: int = 100
    ) -> int:
        """
        ë¬¸ì„œ ë°°ì¹˜ ì—…ì„œíŠ¸ (ì¶”ê°€ ë˜ëŠ” ê°±ì‹ )

        ğŸ¯ ì—­í• : ëŒ€ëŸ‰ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ Pineconeì— ì €ì¥

        ğŸ’¡ Upsert vs Insert:
        - Insert: ì¤‘ë³µ IDë©´ ì—ëŸ¬
        - Upsert: ì¤‘ë³µ IDë©´ ë®ì–´ì“°ê¸° (ê¶Œì¥)

        Args:
            documents: PineconeDocument ë¦¬ìŠ¤íŠ¸
            namespace: ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ë©€í‹°í…Œë„ŒíŠ¸ìš©)
            batch_size: ë°°ì¹˜ í¬ê¸° (Pinecone ê¶Œì¥: 100)

        Returns:
            ì—…ì„œíŠ¸ëœ ë¬¸ì„œ ìˆ˜
        """
        total_upserted = 0

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]

            # ì„ë² ë”©ì´ ì—†ëŠ” ë¬¸ì„œëŠ” ìë™ ìƒì„±
            texts_to_embed = [
                (j, doc.content)
                for j, doc in enumerate(batch)
                if doc.embedding is None
            ]

            if texts_to_embed:
                indices, texts = zip(*texts_to_embed)
                embeddings = self._get_embeddings_batch(list(texts))
                for idx, emb in zip(indices, embeddings):
                    batch[idx].embedding = emb

            # ì—…ì„œíŠ¸ ë²¡í„° ì¤€ë¹„
            # âš ï¸ ë©”íƒ€ë°ì´í„° 40KB ì œí•œ ì£¼ì˜!
            vectors = [
                {
                    "id": doc.id,
                    "values": doc.embedding,
                    "metadata": {
                        "content": doc.content[:1000],  # ì²˜ìŒ 1000ìë§Œ (ì œí•œ ë°©ì§€)
                        **doc.metadata
                    }
                }
                for doc in batch
            ]

            # ì—…ì„œíŠ¸ ì‹¤í–‰
            self.index.upsert(vectors=vectors, namespace=namespace)
            total_upserted += len(vectors)
            print(f"Progress: {min(i + batch_size, len(documents))}/{len(documents)}")

        return total_upserted

    def search(
        self,
        query: str,
        k: int = 5,
        namespace: str = "",
        filter: Optional[dict] = None,
        include_metadata: bool = True
    ) -> list[dict]:
        """
        ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

        ğŸ¯ ì—­í• : ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ kê°œ ë°˜í™˜

        ğŸ’¡ ê²€ìƒ‰ íë¦„:
        1. ì¿¼ë¦¬ â†’ ì„ë² ë”© (OpenAI API)
        2. ì„ë² ë”© â†’ Pinecone ê²€ìƒ‰ (ANN)
        3. ê²°ê³¼ í¬ë§·íŒ…

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬ í…ìŠ¤íŠ¸
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            namespace: ê²€ìƒ‰í•  ë„¤ì„ìŠ¤í˜ì´ìŠ¤
            filter: ë©”íƒ€ë°ì´í„° í•„í„°
            include_metadata: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€

        Returns:
            [{id, score, content, metadata}, ...]
        """
        # ì¿¼ë¦¬ â†’ ì„ë² ë”©
        query_embedding = self._get_embedding(query)

        # Pinecone ê²€ìƒ‰
        results = self.index.query(
            vector=query_embedding,
            top_k=k,
            namespace=namespace,
            filter=filter,
            include_values=False,  # ë²¡í„° ê°’ ë¶ˆí•„ìš” (ëŒ€ì—­í­ ì ˆì•½)
            include_metadata=include_metadata
        )

        # ê²°ê³¼ í¬ë§·íŒ… (ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ)
        formatted = []
        for match in results.matches:
            doc = {
                "id": match.id,
                "score": match.score,  # 0~1, ë†’ì„ìˆ˜ë¡ ìœ ì‚¬
                "content": match.metadata.get("content", ""),
                "metadata": {
                    k: v for k, v in match.metadata.items()
                    if k != "content"  # contentëŠ” ë³„ë„ í•„ë“œë¡œ
                }
            }
            formatted.append(doc)

        return formatted

    def search_with_filter(
        self,
        query: str,
        k: int = 5,
        namespace: str = "",
        category: Optional[str] = None,
        date_from: Optional[str] = None,
        min_score: Optional[float] = None
    ) -> list[dict]:
        """
        ë³µì¡í•œ í•„í„° ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰

        ğŸ¯ ì—­í• : ì¹´í…Œê³ ë¦¬, ë‚ ì§œ, ìµœì†Œ ì ìˆ˜ ë“± ì¡°ê±´ë¶€ ê²€ìƒ‰

        ğŸ’¡ Pinecone í•„í„° ì—°ì‚°ì:
        - $eq: ê°™ìŒ
        - $ne: ë‹¤ë¦„
        - $gt, $gte: í¬ë‹¤, í¬ê±°ë‚˜ ê°™ë‹¤
        - $lt, $lte: ì‘ë‹¤, ì‘ê±°ë‚˜ ê°™ë‹¤
        - $in: ë¦¬ìŠ¤íŠ¸ì— í¬í•¨
        - $and, $or: ì¡°ê±´ ì¡°í•©
        """
        filter_conditions = {}

        if category:
            filter_conditions["category"] = {"$eq": category}

        if date_from:
            filter_conditions["date"] = {"$gte": date_from}

        # ìµœì†Œ ì ìˆ˜ í•„í„°ë§ ìœ„í•´ ì—¬ìœ ë¶„ ê²€ìƒ‰
        results = self.search(
            query=query,
            k=k * 2 if min_score else k,
            namespace=namespace,
            filter=filter_conditions if filter_conditions else None
        )

        # ìµœì†Œ ì ìˆ˜ í•„í„°ë§ (Pinecone ìì²´ ì§€ì› ì—†ìŒ â†’ í›„ì²˜ë¦¬)
        if min_score:
            results = [r for r in results if r["score"] >= min_score][:k]

        return results

    def delete(
        self,
        ids: Optional[list[str]] = None,
        namespace: str = "",
        delete_all: bool = False
    ):
        """
        ë²¡í„° ì‚­ì œ

        ğŸ¯ ì—­í• : IDë¡œ ì‚­ì œ ë˜ëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ì²´ ì‚­ì œ
        âš ï¸ ì£¼ì˜: delete_all=TrueëŠ” ë³µêµ¬ ë¶ˆê°€!
        """
        if delete_all:
            self.index.delete(delete_all=True, namespace=namespace)
        elif ids:
            self.index.delete(ids=ids, namespace=namespace)

    def get_stats(self) -> dict:
        """
        ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ

        ğŸ¯ ì—­í• : ë²¡í„° ìˆ˜, ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ í†µê³„, ì°¨ì› ë“± í™•ì¸
        ğŸ’¡ í™œìš©: ëª¨ë‹ˆí„°ë§, ë¹„ìš© ì˜ˆì¸¡, ë””ë²„ê¹…
        """
        return self.index.describe_index_stats()
\`\`\`

---

## ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™œìš©

ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ ë‚´ì—ì„œ ë°ì´í„°ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤.

### ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš© ì‚¬ë¡€

| ì‚¬ìš© ì‚¬ë¡€ | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì˜ˆì‹œ | ì¥ì  |
|----------|-----------------|------|
| **ë©€í‹°í…Œë„ŒíŠ¸ SaaS** | customer_a, customer_b | ê³ ê° ë°ì´í„° ê²©ë¦¬ |
| **í™˜ê²½ ë¶„ë¦¬** | dev, staging, prod | í™˜ê²½ë³„ ë°ì´í„° ë¶„ë¦¬ |
| **ë¬¸ì„œ ë²„ì „** | v1, v2, latest | ë²„ì „ë³„ ê²€ìƒ‰ |
| **ì–¸ì–´ë³„ ë¶„ë¦¬** | ko, en, ja | ì–¸ì–´ë³„ ê²€ìƒ‰ ìµœì í™” |

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Index: my-app                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ customer_a  â”‚  â”‚ customer_b  â”‚  â”‚ customer_c  â”‚     â”‚
â”‚  â”‚ 500 vectors â”‚  â”‚ 1200 vectorsâ”‚  â”‚ 300 vectors â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                         â”‚
â”‚  ğŸ’¡ ê° ê³ ê° ë°ì´í„°ëŠ” ì™„ì „íˆ ê²©ë¦¬                          â”‚
â”‚  ğŸ’¡ ê²€ìƒ‰ ì‹œ ë‹¤ë¥¸ ê³ ê° ë°ì´í„° ë…¸ì¶œ ë¶ˆê°€                    â”‚
â”‚  ğŸ’¡ ë¹„ìš©: ì¸ë±ìŠ¤ í•˜ë‚˜ë¡œ ì—¬ëŸ¬ ê³ ê° ì„œë¹„ìŠ¤                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

\`\`\`python
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤: ë™ì¼ ì¸ë±ìŠ¤ ë‚´ ë…¼ë¦¬ì  ë¶„ë¦¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
store = PineconeVectorStore(index_name="multi-tenant-app")

# ğŸ¢ ê³ ê°ë³„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ë°ì´í„° ì €ì¥
store.upsert_documents(customer_a_docs, namespace="customer_a")
store.upsert_documents(customer_b_docs, namespace="customer_b")

# ğŸ” ê³ ê° Aì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰ (B ë°ì´í„° ì ‘ê·¼ ë¶ˆê°€)
results_a = store.search("ì§ˆë¬¸", namespace="customer_a")

# ğŸ” ê³ ê° Bì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰ (A ë°ì´í„° ì ‘ê·¼ ë¶ˆê°€)
results_b = store.search("ì§ˆë¬¸", namespace="customer_b")

# ğŸ“Š ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ í†µê³„
stats = store.get_stats()
for ns, ns_stats in stats['namespaces'].items():
    print(f"Namespace {ns}: {ns_stats['vector_count']} vectors")
\`\`\`

---

## ì‚¬ìš© ì˜ˆì‹œ

\`\`\`python
# 1. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
store = PineconeVectorStore(
    index_name="rag-production",
    dimension=1536
)

# 2. ë¬¸ì„œ ì¶”ê°€
documents = [
    PineconeDocument(
        id="doc1",
        content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ, ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•©ë‹ˆë‹¤.",
        metadata={"category": "AI", "date": "2024-01-15", "source": "blog"}
    ),
    PineconeDocument(
        id="doc2",
        content="Pineconeì€ ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ì— ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.",
        metadata={"category": "DB", "date": "2024-01-20", "source": "docs"}
    ),
    PineconeDocument(
        id="doc3",
        content="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì€ í‚¤ì›Œë“œì™€ ì˜ë¯¸ ê²€ìƒ‰ì„ ê²°í•©í•©ë‹ˆë‹¤.",
        metadata={"category": "AI", "date": "2024-02-01", "source": "paper"}
    )
]

store.upsert_documents(documents)

# 3. ê¸°ë³¸ ê²€ìƒ‰
results = store.search("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë€?", k=2)
for r in results:
    print(f"[{r['score']:.3f}] {r['content'][:50]}...")

# 4. í•„í„° ê²€ìƒ‰
ai_results = store.search_with_filter(
    query="ê²€ìƒ‰ ê¸°ìˆ ",
    k=5,
    category="AI",
    min_score=0.7
)

# 5. í†µê³„
stats = store.get_stats()
print(f"ì´ ë²¡í„° ìˆ˜: {stats['total_vector_count']}")
\`\`\`

---

## LangChain í†µí•©

LangChainì„ ì‚¬ìš©í•˜ë©´ Pineconeì„ RAG íŒŒì´í”„ë¼ì¸ì— ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë‘ ê°€ì§€ ì—°ê²° ë°©ì‹

| ë°©ì‹ | ì‚¬ìš© ìƒí™© | ë©”ì„œë“œ |
|------|----------|--------|
| **ê¸°ì¡´ ì¸ë±ìŠ¤ ì—°ê²°** | ì´ë¯¸ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° | \`from_existing_index()\` |
| **ìƒˆë¡œ ìƒì„±** | ìƒˆ ë°ì´í„° ì¶”ê°€ ì‹œ | \`from_documents()\` |

\`\`\`python
from langchain_pinecone import PineconeVectorStore as LCPinecone
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain.chains import RetrievalQA

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´ ì—°ê²°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# ğŸ”— ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì—°ê²° (ë°ì´í„° ì´ë¯¸ ìˆëŠ” ê²½ìš°)
vectorstore = LCPinecone.from_existing_index(
    index_name="rag-production",
    embedding=embeddings,
    namespace="langchain"  # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì§€ì •
)

# â• ë˜ëŠ” ìƒˆ ë¬¸ì„œë¡œ ìƒì„± (ì‹ ê·œ ë°ì´í„°)
docs = [
    Document(
        page_content="Pineconeê³¼ LangChainì˜ í†µí•©ì€ ê°„ë‹¨í•©ë‹ˆë‹¤.",
        metadata={"source": "tutorial"}
    )
]

vectorstore = LCPinecone.from_documents(
    docs,
    embeddings,
    index_name="rag-production",
    namespace="langchain"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Retriever ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : ì¿¼ë¦¬ â†’ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¶”ìƒí™”
retriever = vectorstore.as_retriever(
    search_type="similarity",  # ë˜ëŠ” "mmr" (ë‹¤ì–‘ì„±)
    search_kwargs={"k": 5}
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. QA ì²´ì¸ êµ¬ì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¯ ì—­í• : ê²€ìƒ‰ + LLM ìƒì„±ì„ í•˜ë‚˜ì˜ ì²´ì¸ìœ¼ë¡œ
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o-mini"),
    retriever=retriever,
    return_source_documents=True  # ì¶œì²˜ ë¬¸ì„œ í¬í•¨
)

# ì§ˆì˜ ì‹¤í–‰
result = qa.invoke("Pineconeê³¼ LangChainì„ ì–´ë–»ê²Œ í†µí•©í•˜ë‚˜ìš”?")
print(f"ë‹µë³€: {result['result']}")
\`\`\`

---

## Chroma vs Pinecone ìµœì¢… ë¹„êµ

| í•­ëª© | Chroma | Pinecone |
|------|--------|----------|
| **íƒ€ì…** | ë¡œì»¬/ì„ë² ë””ë“œ | ê´€ë¦¬í˜• í´ë¼ìš°ë“œ |
| **í™•ì¥ì„±** | ~100ë§Œ ë²¡í„° | ìˆ˜ì‹­ì–µ ë²¡í„° |
| **ì‹œì‘** | pip install | ê³„ì • ìƒì„± í•„ìš” |
| **ë¹„ìš©** | ë¬´ë£Œ | Free tier + ìœ ë£Œ |
| **ìš´ì˜** | ì§ì ‘ ê´€ë¦¬ | ì™„ì „ ê´€ë¦¬í˜• |
| **ì‚¬ìš©ì²˜** | í”„ë¡œí† íƒ€ì…, ì†Œê·œëª¨ | ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ |
| **ì„±ì¥ ê²½ë¡œ** | ê°œë°œ â†’ Pinecone ì´ì „ | ë°”ë¡œ í”„ë¡œë•ì…˜ |

## âš ï¸ Common Pitfalls (ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜)

### 1. [ë©”íƒ€ë°ì´í„° í¬ê¸°] 40KB ì œí•œ ì´ˆê³¼
**ì¦ìƒ**: ì—…ì„œíŠ¸ ì‹œ ì—ëŸ¬ ë°œìƒ
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ì „ì²´ ë¬¸ì„œë¥¼ ë©”íƒ€ë°ì´í„°ì—
index.upsert(vectors=[{
    "id": "doc1",
    "values": embedding,
    "metadata": {"content": very_long_document}  # ğŸ’¥ 40KB ì´ˆê³¼!
}])
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: Pinecone ë©”íƒ€ë°ì´í„°ëŠ” ë²¡í„°ë‹¹ 40KB ì œí•œ
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - ìš”ì•½ë§Œ ì €ì¥, ì „ì²´ëŠ” ì™¸ë¶€ DB
index.upsert(vectors=[{
    "id": "doc1",
    "values": embedding,
    "metadata": {
        "content": document[:1000],  # ì²˜ìŒ 1000ìë§Œ
        "full_content_id": "external_db_id_123"  # ì „ì²´ëŠ” PostgreSQL ë“±ì—
    }
}])
\`\`\`
**ê¸°ì–µí•  ì **: ë©”íƒ€ë°ì´í„°ëŠ” í•„í„°ìš©, ì „ì²´ ë¬¸ì„œëŠ” ì™¸ë¶€ ì €ì¥ì†Œ

### 2. [ë„¤ì„ìŠ¤í˜ì´ìŠ¤] ì‚­ì œ ì‹œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¯¸ì§€ì •
**ì¦ìƒ**: ì˜ë„ì¹˜ ì•Šì€ ë°ì´í„° ì‚­ì œ ë˜ëŠ” ì‚­ì œ ì•ˆë¨
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì—†ì´ ì‚­ì œ
index.delete(ids=["doc1"])  # ì–´ëŠ ë„¤ì„ìŠ¤í˜ì´ìŠ¤?
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: ê¸°ë³¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤("")ì—ì„œë§Œ ì‚­ì œë¨
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª…ì‹œ
index.delete(ids=["doc1"], namespace="customer_a")
\`\`\`
**ê¸°ì–µí•  ì **: upsert/query/delete ëª¨ë‘ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª…ì‹œ

### 3. [ë¹„ìš©] ë¶ˆí•„ìš”í•œ include_values=True
**ì¦ìƒ**: ì‘ë‹µ ëŠë¦¼, ëŒ€ì—­í­ ë‚­ë¹„
\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ - ë²¡í„° ê°’ê¹Œì§€ ë°˜í™˜
results = index.query(
    vector=query_emb,
    top_k=10,
    include_values=True  # 1536ì°¨ì› * 10ê°œ = ëŒ€ì—­í­!
)
\`\`\`
**ì™œ ì˜ëª»ë˜ì—ˆë‚˜**: ê²€ìƒ‰ ê²°ê³¼ì— ë²¡í„° ê°’ í•„ìš” ì—†ëŠ” ê²½ìš°ê°€ ëŒ€ë¶€ë¶„
\`\`\`python
# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ - ë©”íƒ€ë°ì´í„°ë§Œ ë°˜í™˜
results = index.query(
    vector=query_emb,
    top_k=10,
    include_values=False,  # ê¸°ë³¸ê°’
    include_metadata=True
)
\`\`\`
**ê¸°ì–µí•  ì **: \`include_values\`ëŠ” ë””ë²„ê¹…/ë¶„ì„ìš©, í”„ë¡œë•ì…˜ì—ì„  False
      `,
      keyPoints: [
        'â˜ï¸ Pineconeì€ ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ì— ìµœì í™”ëœ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤',
        'ğŸ¢ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ë©€í‹°í…Œë„ŒíŠ¸ êµ¬í˜„',
        'ğŸ”— LangChain í†µí•©ìœ¼ë¡œ ë¹ ë¥¸ RAG êµ¬ì¶•',
      ],
      practiceGoal: 'Pineconeì„ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 6: Day 2 ì¢…í•© í€´ì¦ˆ (20ë¶„)
    // ========================================
    createQuizTask('w5d2-quiz', 'Day 2 ì¢…í•© í€´ì¦ˆ', 20, {
      introduction: `
## í€´ì¦ˆ ì•ˆë‚´

Day 2ì—ì„œ í•™ìŠµí•œ ì„ë² ë”©ê³¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê°œë…ì„ í™•ì¸í•©ë‹ˆë‹¤.
ê° ë¬¸ì œë¥¼ ì‹ ì¤‘í•˜ê²Œ ì½ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
      `,
      questions: [
        {
          id: 'w5d2-q1',
          question: 'Word2Vecì˜ "King - Man + Woman = Queen" ì˜ˆì‹œê°€ ë³´ì—¬ì£¼ëŠ” ì„ë² ë”©ì˜ íŠ¹ì„±ì€?',
          options: [
            'í…ìŠ¤íŠ¸ ì••ì¶• íš¨ìœ¨ì„±',
            'ë²¡í„° ê³µê°„ì—ì„œì˜ ì˜ë¯¸ì  ì—°ì‚° ê°€ëŠ¥ì„±',
            'ë¬¸ë²• ì˜¤ë¥˜ ê²€ì¶œ ëŠ¥ë ¥',
            'ë‹¤êµ­ì–´ ë²ˆì—­ ì •í™•ë„',
          ],
          correctAnswer: 1,
          explanation: 'Word2Vec ì„ë² ë”©ì€ ë²¡í„° ê³µê°„ì—ì„œ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ì‚°ìˆ  ì—°ì‚°ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "King - Man + Woman â‰ˆ Queen"ì€ ì„±ë³„ ê´€ê³„ê°€ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í‘œí˜„ë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.',
        },
        {
          id: 'w5d2-q2',
          question: 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì˜ ì¥ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?',
          options: [
            'ê³„ì‚°ì´ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë³´ë‹¤ ëŠë¦¼',
            'í…ìŠ¤íŠ¸ ê¸¸ì´(ë²¡í„° í¬ê¸°)ì— ì˜í–¥ë°›ì§€ ì•ŠìŒ',
            'ìŒìˆ˜ ê°’ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ',
            'ì •ìˆ˜ë§Œ ë°˜í™˜í•˜ì—¬ ë¹„êµê°€ ì‰¬ì›€',
          ],
          correctAnswer: 1,
          explanation: 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë²¡í„°ì˜ ë°©í–¥ë§Œ ë¹„êµí•˜ë¯€ë¡œ, í…ìŠ¤íŠ¸ ê¸¸ì´(ë²¡í„° í¬ê¸°)ì— ë¬´ê´€í•©ë‹ˆë‹¤. ì§§ì€ ë¬¸ì¥ê³¼ ê¸´ ë¬¸ì¥ë„ ì˜ë¯¸ê°€ ê°™ìœ¼ë©´ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.',
        },
        {
          id: 'w5d2-q3',
          question: 'HNSW ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ì˜ ì£¼ìš” íŠ¹ì§•ì€?',
          options: [
            'ì •í™•ë„ 100%, ê°€ì¥ ëŠë¦¼',
            'í´ëŸ¬ìŠ¤í„° ê¸°ë°˜, ì¬í•™ìŠµ í•„ìš”',
            'ê·¸ë˜í”„ ê¸°ë°˜, ë¹ ë¥´ê³  ë™ì  ì¶”ê°€ ê°€ëŠ¥',
            'ì••ì¶• ê¸°ë°˜, ë©”ëª¨ë¦¬ ìµœì†Œí™”',
          ],
          correctAnswer: 2,
          explanation: 'HNSW(Hierarchical Navigable Small World)ëŠ” ë‹¤ì¸µ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ O(log n) ì‹œê°„ì— ê²€ìƒ‰í•˜ë©°, ìƒˆ ë²¡í„°ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        },
        {
          id: 'w5d2-q4',
          question: 'OpenAI text-embedding-3-smallê³¼ text-embedding-3-largeì˜ ì£¼ìš” ì°¨ì´ì ì€?',
          options: [
            'smallì€ ì˜ì–´ë§Œ, largeëŠ” ë‹¤êµ­ì–´ ì§€ì›',
            'smallì€ 1536ì°¨ì›, largeëŠ” 3072ì°¨ì›',
            'smallì€ ë¬´ë£Œ, largeëŠ” ìœ ë£Œ',
            'smallì€ CPU, largeëŠ” GPU í•„ìš”',
          ],
          correctAnswer: 1,
          explanation: 'text-embedding-3-smallì€ 1536ì°¨ì›, largeëŠ” 3072ì°¨ì› ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. largeëŠ” ë” ë†’ì€ í’ˆì§ˆì´ì§€ë§Œ ë¹„ìš©ì´ ì•½ 6.5ë°° ë†’ìŠµë‹ˆë‹¤.',
        },
        {
          id: 'w5d2-q5',
          question: 'Chromaì™€ Pineconeì˜ ê°€ì¥ í° ì°¨ì´ì ì€?',
          options: [
            'ChromaëŠ” Python, Pineconeì€ JavaScript',
            'ChromaëŠ” ë¡œì»¬/ì„ë² ë””ë“œ, Pineconeì€ ê´€ë¦¬í˜• í´ë¼ìš°ë“œ',
            'ChromaëŠ” HNSW, Pineconeì€ IVF ì‚¬ìš©',
            'ChromaëŠ” ìœ ë£Œ, Pineconeì€ ë¬´ë£Œ',
          ],
          correctAnswer: 1,
          explanation: 'ChromaëŠ” ë¡œì»¬/ì„ë² ë””ë“œë¡œ í”„ë¡œí† íƒ€ì´í•‘ì— ì í•©í•˜ê³ , Pineconeì€ ê´€ë¦¬í˜• í´ë¼ìš°ë“œë¡œ ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.',
        },
      ],
      keyPoints: [
        'Word2Vec: ë²¡í„° ê³µê°„ì—ì„œ ì˜ë¯¸ì  ì—°ì‚° ê°€ëŠ¥',
        'ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë²¡í„° í¬ê¸° ë¬´ê´€, ë°©í–¥ë§Œ ë¹„êµ',
        'HNSW: ê·¸ë˜í”„ ê¸°ë°˜, ë¹ ë¥´ê³  ë™ì  ì¶”ê°€ ê°€ëŠ¥',
      ],
      practiceGoal: 'Day 2 í•µì‹¬ ê°œë…ì„ ì´í•´í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤',
    }),

    // ========================================
    // Challenge: ì„ë² ë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ êµ¬ì¶• (60ë¶„)
    // ========================================
    createChallengeTask('w5d2-challenge', 'ì„ë² ë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ êµ¬ì¶•', 60, {
      introduction: `
## ì±Œë¦°ì§€ ì‹œë‚˜ë¦¬ì˜¤

ë‹¹ì‹ ì€ AI ìŠ¤íƒ€íŠ¸ì—…ì˜ ë°ì´í„° ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. íšŒì‚¬ëŠ” RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ë ¤ í•˜ëŠ”ë°,
ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ê³¼ ë²¡í„° DB ì¡°í•©ì´ ìµœì ì¸ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

**ìš”êµ¬ì‚¬í•­:**
1. 3ê°€ì§€ ì´ìƒì˜ ì„ë² ë”© ëª¨ë¸ ë¹„êµ
2. Chromaì™€ Pinecone ì„±ëŠ¥ ë¹„êµ
3. ë¹„ìš©, ì†ë„, ì •í™•ë„ ì¸¡ì •
4. ìµœì¢… ê¶Œì¥ ì‚¬í•­ ë„ì¶œ

---

## í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ê¸°ì¤€ |
|------|------|------|
| ì„ë² ë”© ëª¨ë¸ ë¹„êµ | 25ì  | 3ê°œ ì´ìƒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ |
| ë²¡í„° DB ë¹„êµ | 25ì  | Chroma, Pinecone ë™ì¼ ì¡°ê±´ ë¹„êµ |
| ì¸¡ì • ì§€í‘œ | 25ì  | ì†ë„, ì •í™•ë„, ë¹„ìš© ëª¨ë‘ ì¸¡ì • |
| ë¶„ì„ ë³´ê³ ì„œ | 25ì  | ê²°ê³¼ í•´ì„ ë° ê¶Œì¥ ì‚¬í•­ |

---

## í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹

> **KorQuAD + Wikipedia ê¸°ë°˜** ë¦¬ì–¼ í•œêµ­ì–´ RAG ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹

### ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ

| ê·œëª¨ | ë¬¸ì„œ | ì¿¼ë¦¬ | ìš©ë„ | ë‹¤ìš´ë¡œë“œ |
|------|------|------|------|----------|
| ì†Œê·œëª¨ | 200 | 50 | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ | [korean-rag-benchmark.json](https://raw.githubusercontent.com/jeromwolf/fde-curriculum/main/public/datasets/korean-rag-benchmark.json) |
| **ì¤‘ê·œëª¨** | 1,000 | 200 | ì¼ë°˜ ë²¤ì¹˜ë§ˆí¬ | [korean-rag-benchmark-medium.json](https://raw.githubusercontent.com/jeromwolf/fde-curriculum/main/public/datasets/korean-rag-benchmark-medium.json) |
| **ëŒ€ê·œëª¨** | 5,000 | 500 | ë³¸ê²© ì„±ëŠ¥ í‰ê°€ | [korean-rag-benchmark-large.json](https://raw.githubusercontent.com/jeromwolf/fde-curriculum/main/public/datasets/korean-rag-benchmark-large.json) |

> ë°ì´í„° ì¶œì²˜: [KorQuAD v1.0](https://korquad.github.io/) (CC BY-ND 2.0 KR)
> GitHub: [fde-curriculum/public/datasets](https://github.com/jeromwolf/fde-curriculum/tree/main/public/datasets)

---

### ë°ì´í„°ì…‹ ë¡œë“œ ì½”ë“œ

\`\`\`python
import json
import requests

# GitHub raw URL ë² ì´ìŠ¤
BASE_URL = "https://raw.githubusercontent.com/jeromwolf/fde-curriculum/main/public/datasets"

def load_benchmark(size: str = "medium"):
    """
    í•œêµ­ì–´ RAG ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ë¡œë“œ (GitHubì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ)

    Args:
        size: "small" (200ë¬¸ì„œ), "medium" (1,000ë¬¸ì„œ), "large" (5,000ë¬¸ì„œ)
    """
    file_map = {
        "small": "korean-rag-benchmark.json",
        "medium": "korean-rag-benchmark-medium.json",
        "large": "korean-rag-benchmark-large.json"
    }

    url = f"{BASE_URL}/{file_map[size]}"
    response = requests.get(url)
    response.raise_for_status()
    data = response.json()

    return data['documents'], data['queries']

# ì‚¬ìš© ì˜ˆì‹œ
documents, queries = load_benchmark("large")  # 5,000ë¬¸ì„œ, 500ì¿¼ë¦¬
print(f"ë¬¸ì„œ ìˆ˜: {len(documents)}")  # 5000
print(f"ì¿¼ë¦¬ ìˆ˜: {len(queries)}")    # 500

# ë¡œì»¬ íŒŒì¼ ì‚¬ìš© ì‹œ
# with open('korean-rag-benchmark-large.json', 'r', encoding='utf-8') as f:
#     data = json.load(f)
\`\`\`

---

### ë°ì´í„°ì…‹ êµ¬ì¡°

\`\`\`json
{
  "documents": [
    {
      "id": "doc_0001",
      "title": "íŒŒìš°ìŠ¤íŠ¸_ì„œê³¡",
      "text": "1839ë…„ ë°”ê·¸ë„ˆëŠ” ê´´í…Œì˜ íŒŒìš°ìŠ¤íŠ¸ì„ ì²˜ìŒ ì½ê³ ...",
      "source": "KorQuAD/Wikipedia"
    }
  ],
  "queries": [
    {
      "id": "q_001",
      "text": "ë°”ê·¸ë„ˆëŠ” ê´´í…Œì˜ íŒŒìš°ìŠ¤íŠ¸ë¥¼ ì½ê³  ë¬´ì—‡ì„ ì“°ê³ ì í–ˆëŠ”ê°€?",
      "answer": "êµí–¥ê³¡",
      "relevant_docs": ["doc_0001"]
    }
  ]
}
\`\`\`

---

### ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì½”ë“œ

\`\`\`python
def evaluate_retrieval(retrieved_ids: list, relevant_ids: list, k: int = 5):
    """
    Top-K ì •í™•ë„ ë° ì¬í˜„ìœ¨ ê³„ì‚°

    Args:
        retrieved_ids: ê²€ìƒ‰ëœ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸ (ìˆœìœ„ìˆœ)
        relevant_ids: ì •ë‹µ ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ Kê°œ í‰ê°€

    Returns:
        dict: precision@k, recall@k, hit@k
    """
    top_k = set(retrieved_ids[:k])
    relevant = set(relevant_ids)
    hits = len(top_k & relevant)

    return {
        "precision@k": hits / k,
        "recall@k": hits / len(relevant) if relevant else 0,
        "hit@k": 1 if hits > 0 else 0
    }

# ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
def run_benchmark(retriever, documents, queries):
    results = []
    for query in queries:
        retrieved = retriever.search(query['text'], k=5)
        retrieved_ids = [doc['id'] for doc in retrieved]

        metrics = evaluate_retrieval(retrieved_ids, query['relevant_docs'], k=5)
        results.append(metrics)

    # í‰ê·  ê³„ì‚°
    avg_precision = sum(r['precision@k'] for r in results) / len(results)
    avg_recall = sum(r['recall@k'] for r in results) / len(results)
    avg_hit = sum(r['hit@k'] for r in results) / len(results)

    print(f"Precision@5: {avg_precision:.4f}")
    print(f"Recall@5: {avg_recall:.4f}")
    print(f"Hit Rate@5: {avg_hit:.4f}")
\`\`\`
      `,
      keyPoints: [
        '3ê°œ ì´ìƒì˜ ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ (OpenAI, ì˜¤í”ˆì†ŒìŠ¤ ë“±)',
        'Chromaì™€ Pineconeì—ì„œ ë™ì¼ ë°ì´í„°ë¡œ ë¹„êµ',
        'ê²€ìƒ‰ ì†ë„(ms), ì •í™•ë„(Top-1/Top-5), ë¹„ìš©($) ì¸¡ì •',
        'ê²°ê³¼ ë¶„ì„ ë° ìµœì¢… ê¶Œì¥ ì‚¬í•­ ë³´ê³ ì„œ ì‘ì„±',
      ],
      hints: [
        `## 1. í”„ë¡œì íŠ¸ êµ¬ì¡°

\`\`\`
embedding-benchmark/
â”œâ”€â”€ config.py          # ì„¤ì • (API í‚¤, ëª¨ë¸ ëª©ë¡)
â”œâ”€â”€ embeddings.py      # ì„ë² ë”© ëª¨ë¸ ë˜í¼
â”œâ”€â”€ vectorstores.py    # ë²¡í„° DB ë˜í¼
â”œâ”€â”€ benchmark.py       # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ë¡œì§
â”œâ”€â”€ report.py          # ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
â”œâ”€â”€ main.py            # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â””â”€â”€ requirements.txt   # ì˜ì¡´ì„±
\`\`\`

**config.py**
\`\`\`python
import os
from dataclasses import dataclass

@dataclass
class Config:
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY")
    PINECONE_API_KEY: str = os.getenv("PINECONE_API_KEY")

    # í…ŒìŠ¤íŠ¸í•  ì„ë² ë”© ëª¨ë¸ ëª©ë¡
    EMBEDDING_MODELS = [
        {"name": "openai-small", "type": "openai", "model": "text-embedding-3-small", "dim": 1536, "cost_per_1k": 0.00002},
        {"name": "openai-large", "type": "openai", "model": "text-embedding-3-large", "dim": 3072, "cost_per_1k": 0.00013},
        {"name": "multilingual-e5", "type": "sentence-transformer", "model": "intfloat/multilingual-e5-large", "dim": 1024, "cost_per_1k": 0},
        {"name": "ko-sroberta", "type": "sentence-transformer", "model": "jhgan/ko-sroberta-multitask", "dim": 768, "cost_per_1k": 0},
    ]

    # ë²¡í„° DB ì„¤ì •
    CHROMA_PERSIST_DIR = "./chroma_db"
    PINECONE_INDEX_NAME = "rag-benchmark"
\`\`\``,

        `## 2. ì„ë² ë”© ëª¨ë¸ ë˜í¼ (embeddings.py)

\`\`\`python
from abc import ABC, abstractmethod
from typing import List
import time
import openai
from sentence_transformers import SentenceTransformer

class BaseEmbedding(ABC):
    """ì„ë² ë”© ëª¨ë¸ ì¶”ìƒ í´ë˜ìŠ¤"""

    @abstractmethod
    def embed(self, texts: List[str]) -> List[List[float]]:
        pass

    @abstractmethod
    def get_dimension(self) -> int:
        pass

class OpenAIEmbedding(BaseEmbedding):
    """OpenAI ì„ë² ë”© ë˜í¼"""

    def __init__(self, model: str = "text-embedding-3-small"):
        self.model = model
        self.client = openai.OpenAI()
        self._dimensions = {"text-embedding-3-small": 1536, "text-embedding-3-large": 3072}

    def embed(self, texts: List[str]) -> List[List[float]]:
        response = self.client.embeddings.create(input=texts, model=self.model)
        return [item.embedding for item in response.data]

    def get_dimension(self) -> int:
        return self._dimensions.get(self.model, 1536)

class SentenceTransformerEmbedding(BaseEmbedding):
    """SentenceTransformer ì„ë² ë”© ë˜í¼ (ë¡œì»¬ ëª¨ë¸)"""

    def __init__(self, model: str = "intfloat/multilingual-e5-large"):
        self.model = SentenceTransformer(model)
        self._dim = self.model.get_sentence_embedding_dimension()

    def embed(self, texts: List[str]) -> List[List[float]]:
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()

    def get_dimension(self) -> int:
        return self._dim

def create_embedding_model(config: dict) -> BaseEmbedding:
    """íŒ©í† ë¦¬ í•¨ìˆ˜: ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ ì„ë² ë”© ëª¨ë¸ ìƒì„±"""
    if config["type"] == "openai":
        return OpenAIEmbedding(model=config["model"])
    elif config["type"] == "sentence-transformer":
        return SentenceTransformerEmbedding(model=config["model"])
    else:
        raise ValueError(f"Unknown embedding type: {config['type']}")
\`\`\``,

        `## 3. ë²¡í„° DB ë˜í¼ (vectorstores.py)

\`\`\`python
from abc import ABC, abstractmethod
from typing import List, Dict, Any
import chromadb
from pinecone import Pinecone, ServerlessSpec

class BaseVectorStore(ABC):
    """ë²¡í„° ìŠ¤í† ì–´ ì¶”ìƒ í´ë˜ìŠ¤"""

    @abstractmethod
    def add_documents(self, ids: List[str], embeddings: List[List[float]], metadatas: List[Dict]):
        pass

    @abstractmethod
    def search(self, query_embedding: List[float], k: int = 5) -> List[Dict]:
        pass

    @abstractmethod
    def clear(self):
        pass

class ChromaVectorStore(BaseVectorStore):
    """Chroma ë²¡í„° ìŠ¤í† ì–´"""

    def __init__(self, collection_name: str = "benchmark", persist_dir: str = "./chroma_db"):
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )

    def add_documents(self, ids: List[str], embeddings: List[List[float]], metadatas: List[Dict]):
        self.collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas)

    def search(self, query_embedding: List[float], k: int = 5) -> List[Dict]:
        results = self.collection.query(query_embeddings=[query_embedding], n_results=k)
        return [{"id": id, "score": 1 - dist} for id, dist in zip(results["ids"][0], results["distances"][0])]

    def clear(self):
        self.client.delete_collection(self.collection.name)
        self.collection = self.client.create_collection(name=self.collection.name)

class PineconeVectorStore(BaseVectorStore):
    """Pinecone ë²¡í„° ìŠ¤í† ì–´"""

    def __init__(self, index_name: str, dimension: int, api_key: str):
        self.pc = Pinecone(api_key=api_key)

        # ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±
        if index_name not in [idx.name for idx in self.pc.list_indexes()]:
            self.pc.create_index(
                name=index_name,
                dimension=dimension,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1")
            )
        self.index = self.pc.Index(index_name)

    def add_documents(self, ids: List[str], embeddings: List[List[float]], metadatas: List[Dict]):
        vectors = [{"id": id, "values": emb, "metadata": meta}
                   for id, emb, meta in zip(ids, embeddings, metadatas)]
        self.index.upsert(vectors=vectors, batch_size=100)

    def search(self, query_embedding: List[float], k: int = 5) -> List[Dict]:
        results = self.index.query(vector=query_embedding, top_k=k, include_metadata=True)
        return [{"id": match.id, "score": match.score} for match in results.matches]

    def clear(self):
        self.index.delete(delete_all=True)
\`\`\``,

        `## 4. ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (benchmark.py)

\`\`\`python
import time
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class BenchmarkResult:
    model_name: str
    vectorstore_name: str
    embedding_time_ms: float      # ì„ë² ë”© ìƒì„± ì‹œê°„
    indexing_time_ms: float       # ì¸ë±ì‹± ì‹œê°„
    search_time_ms: float         # í‰ê·  ê²€ìƒ‰ ì‹œê°„
    precision_at_1: float         # Top-1 ì •í™•ë„
    precision_at_5: float         # Top-5 ì •í™•ë„
    hit_rate_at_5: float          # Hit Rate @ 5
    estimated_cost: float         # ì¶”ì • ë¹„ìš© ($)

def run_benchmark(
    embedding_model,
    vectorstore,
    documents: List[Dict],
    queries: List[Dict],
    model_config: dict
) -> BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""

    # 1. ì„ë² ë”© ìƒì„± ì‹œê°„ ì¸¡ì •
    texts = [doc["text"] for doc in documents]
    start = time.time()
    embeddings = embedding_model.embed(texts)
    embedding_time = (time.time() - start) * 1000

    # 2. ì¸ë±ì‹± ì‹œê°„ ì¸¡ì •
    start = time.time()
    vectorstore.add_documents(
        ids=[doc["id"] for doc in documents],
        embeddings=embeddings,
        metadatas=[{"title": doc.get("title", "")} for doc in documents]
    )
    indexing_time = (time.time() - start) * 1000

    # 3. ê²€ìƒ‰ ë° ì •í™•ë„ ì¸¡ì •
    search_times = []
    hits_at_1 = 0
    hits_at_5 = 0
    hit_count = 0

    for query in queries:
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_emb = embedding_model.embed([query["text"]])[0]

        # ê²€ìƒ‰ ì‹œê°„ ì¸¡ì •
        start = time.time()
        results = vectorstore.search(query_emb, k=5)
        search_times.append((time.time() - start) * 1000)

        # ì •í™•ë„ ê³„ì‚°
        retrieved_ids = [r["id"] for r in results]
        relevant_ids = set(query["relevant_docs"])

        if retrieved_ids[0] in relevant_ids:
            hits_at_1 += 1
        if set(retrieved_ids[:5]) & relevant_ids:
            hit_count += 1
        hits_at_5 += len(set(retrieved_ids[:5]) & relevant_ids)

    # 4. ë¹„ìš© ì¶”ì •
    total_tokens = sum(len(text.split()) * 1.3 for text in texts)  # ëŒ€ëµì  í† í° ìˆ˜
    estimated_cost = (total_tokens / 1000) * model_config.get("cost_per_1k", 0)

    return BenchmarkResult(
        model_name=model_config["name"],
        vectorstore_name=vectorstore.__class__.__name__,
        embedding_time_ms=embedding_time,
        indexing_time_ms=indexing_time,
        search_time_ms=sum(search_times) / len(search_times),
        precision_at_1=hits_at_1 / len(queries),
        precision_at_5=hits_at_5 / (len(queries) * 5),
        hit_rate_at_5=hit_count / len(queries),
        estimated_cost=estimated_cost
    )
\`\`\``,

        `## 5. ë¦¬í¬íŠ¸ ìƒì„± (report.py)

\`\`\`python
from typing import List
from benchmark import BenchmarkResult
import json

def generate_report(results: List[BenchmarkResult]) -> str:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""

    report = []
    report.append("# RAG ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¦¬í¬íŠ¸\\n")

    # ê²°ê³¼ í…Œì´ë¸”
    report.append("## ì„±ëŠ¥ ë¹„êµí‘œ\\n")
    report.append("| ëª¨ë¸ | ë²¡í„°DB | ì„ë² ë”©(ms) | ê²€ìƒ‰(ms) | P@1 | P@5 | Hit@5 | ë¹„ìš©($) |")
    report.append("|------|--------|-----------|---------|-----|-----|-------|---------|")

    for r in results:
        report.append(
            f"| {r.model_name} | {r.vectorstore_name} | "
            f"{r.embedding_time_ms:.1f} | {r.search_time_ms:.2f} | "
            f"{r.precision_at_1:.3f} | {r.precision_at_5:.3f} | "
            f"{r.hit_rate_at_5:.3f} | {r.estimated_cost:.4f} |"
        )

    # ê¶Œì¥ ì‚¬í•­
    report.append("\\n## ê¶Œì¥ ì‚¬í•­\\n")

    # ì •í™•ë„ ê¸°ì¤€ ë² ìŠ¤íŠ¸
    best_accuracy = max(results, key=lambda x: x.hit_rate_at_5)
    report.append(f"- **ì •í™•ë„ ë² ìŠ¤íŠ¸**: {best_accuracy.model_name} + {best_accuracy.vectorstore_name}")

    # ì†ë„ ê¸°ì¤€ ë² ìŠ¤íŠ¸
    best_speed = min(results, key=lambda x: x.search_time_ms)
    report.append(f"- **ì†ë„ ë² ìŠ¤íŠ¸**: {best_speed.model_name} + {best_speed.vectorstore_name}")

    # ë¹„ìš© ê¸°ì¤€ ë² ìŠ¤íŠ¸ (ë¬´ë£Œ ì œì™¸)
    paid_results = [r for r in results if r.estimated_cost > 0]
    if paid_results:
        best_cost = min(paid_results, key=lambda x: x.estimated_cost)
        report.append(f"- **ë¹„ìš©íš¨ìœ¨ ë² ìŠ¤íŠ¸**: {best_cost.model_name}")

    # ë¬´ë£Œ ì˜µì…˜
    free_results = [r for r in results if r.estimated_cost == 0]
    if free_results:
        best_free = max(free_results, key=lambda x: x.hit_rate_at_5)
        report.append(f"- **ë¬´ë£Œ ë² ìŠ¤íŠ¸**: {best_free.model_name}")

    return "\\n".join(report)

def save_results_json(results: List[BenchmarkResult], filename: str):
    """ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
    data = [vars(r) for r in results]
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
\`\`\``,

        `## 6. ë©”ì¸ ì‹¤í–‰ (main.py)

\`\`\`python
import requests
from config import Config
from embeddings import create_embedding_model
from vectorstores import ChromaVectorStore, PineconeVectorStore
from benchmark import run_benchmark
from report import generate_report, save_results_json

def load_benchmark_data(size: str = "medium"):
    """GitHubì—ì„œ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ"""
    base_url = "https://raw.githubusercontent.com/jeromwolf/fde-curriculum/main/public/datasets"
    file_map = {
        "small": "korean-rag-benchmark.json",
        "medium": "korean-rag-benchmark-medium.json",
        "large": "korean-rag-benchmark-large.json"
    }
    response = requests.get(f"{base_url}/{file_map[size]}")
    data = response.json()
    return data["documents"], data["queries"]

def main():
    config = Config()

    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“¥ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ë¡œë“œ ì¤‘...")
    documents, queries = load_benchmark_data("medium")  # 1,000ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸
    print(f"  - ë¬¸ì„œ: {len(documents)}ê°œ, ì¿¼ë¦¬: {len(queries)}ê°œ")

    results = []

    # ê° ì„ë² ë”© ëª¨ë¸ + ë²¡í„° DB ì¡°í•© í…ŒìŠ¤íŠ¸
    for model_config in config.EMBEDDING_MODELS:
        print(f"\\nğŸ”„ í…ŒìŠ¤íŠ¸ ì¤‘: {model_config['name']}")

        # ì„ë² ë”© ëª¨ë¸ ìƒì„±
        embedding_model = create_embedding_model(model_config)

        # Chroma í…ŒìŠ¤íŠ¸
        print("  - Chroma í…ŒìŠ¤íŠ¸...")
        chroma = ChromaVectorStore(collection_name=f"bench_{model_config['name']}")
        result = run_benchmark(embedding_model, chroma, documents, queries, model_config)
        results.append(result)
        chroma.clear()

        # Pinecone í…ŒìŠ¤íŠ¸ (API í‚¤ê°€ ìˆì„ ë•Œë§Œ)
        if config.PINECONE_API_KEY:
            print("  - Pinecone í…ŒìŠ¤íŠ¸...")
            pinecone = PineconeVectorStore(
                index_name=f"bench-{model_config['name'].replace('_', '-')}",
                dimension=model_config["dim"],
                api_key=config.PINECONE_API_KEY
            )
            result = run_benchmark(embedding_model, pinecone, documents, queries, model_config)
            results.append(result)
            pinecone.clear()

    # ë¦¬í¬íŠ¸ ìƒì„±
    print("\\nğŸ“Š ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report = generate_report(results)
    print(report)

    # ê²°ê³¼ ì €ì¥
    save_results_json(results, "benchmark_results.json")
    with open("benchmark_report.md", "w", encoding="utf-8") as f:
        f.write(report)

    print("\\nâœ… ì™„ë£Œ! benchmark_report.md íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
\`\`\``,

        `## 7. ì‹¤í–‰ ê°€ì´ë“œ

### í™˜ê²½ ì„¤ì •
\`\`\`bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Windows: venv\\Scripts\\activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install openai chromadb pinecone-client sentence-transformers requests

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export OPENAI_API_KEY="sk-..."
export PINECONE_API_KEY="..."  # ì„ íƒì‚¬í•­
\`\`\`

### ì‹¤í–‰
\`\`\`bash
python main.py
\`\`\`

### ì˜ˆìƒ ê²°ê³¼

| ëª¨ë¸ | ë²¡í„°DB | ì„ë² ë”©(ms) | ê²€ìƒ‰(ms) | P@1 | Hit@5 | ë¹„ìš©($) |
|------|--------|-----------|---------|-----|-------|---------|
| openai-small | Chroma | 8,500 | 2.5 | 0.78 | 0.92 | 0.026 |
| openai-large | Chroma | 12,000 | 3.1 | 0.82 | 0.95 | 0.169 |
| multilingual-e5 | Chroma | 45,000 | 2.8 | 0.75 | 0.88 | 0.000 |
| ko-sroberta | Chroma | 35,000 | 2.2 | 0.71 | 0.85 | 0.000 |

### ê¶Œì¥ ì¡°í•©
- **í”„ë¡œí† íƒ€ì…**: text-embedding-3-small + Chroma (ë¹ ë¥¸ ê°œë°œ, ì €ë¹„ìš©)
- **í”„ë¡œë•ì…˜**: text-embedding-3-large + Pinecone (ìµœê³  ì •í™•ë„, í™•ì¥ì„±)
- **ë¹„ìš© ì œì•½**: multilingual-e5 + Chroma (ë¬´ë£Œ, ë¡œì»¬ ì‹¤í–‰)`,
      ],
    }),
  ],
}
