// Day 2: ì„ë² ë”© & ë²¡í„° DB (Chroma, Pinecone) - ì™„ì „ ë¦¬ë‰´ì–¼ ë²„ì „

import type { Day } from '../../types'
import {
  createVideoTask,
  createReadingTask,
  createCodeTask,
  createQuizTask,
  createChallengeTask,
} from './types'

export const day2EmbeddingsVectordb: Day = {
  slug: 'embeddings-vectordb',
  title: 'ì„ë² ë”© & ë²¡í„° DB',
  totalDuration: 300, // 5ì‹œê°„
  tasks: [
    // ========================================
    // Task 1: ì„ë² ë”©ì˜ ì—­ì‚¬ì™€ ì›ë¦¬ (40ë¶„)
    // ========================================
    createVideoTask('w5d2-embeddings-history', 'ì„ë² ë”©ì˜ ì—­ì‚¬ì™€ ìˆ˜í•™ì  ì›ë¦¬', 40, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- ì„ë² ë”© ê¸°ìˆ ì˜ ë°œì „ ì—­ì‚¬ë¥¼ ì´í•´í•œë‹¤
- ì„ë² ë”©ì˜ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- ì£¼ìš” ì„ë² ë”© ëª¨ë¸ë“¤ì˜ íŠ¹ì§•ì„ ë¹„êµí•œë‹¤

---

## ì„ë² ë”©ì˜ ì—­ì‚¬: Word2Vecì—ì„œ LLMê¹Œì§€

### 1ì„¸ëŒ€: í†µê³„ ê¸°ë°˜ (2003-2012)

**TF-IDF (Term Frequency-Inverse Document Frequency)**

\`\`\`python
# ì „í†µì ì¸ ë¬¸ì„œ í‘œí˜„ ë°©ì‹
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
documents = [
    "ê³ ì–‘ì´ê°€ ë°©ì—ì„œ ì”ë‹¤",
    "ê°•ì•„ì§€ê°€ ë§ˆë‹¹ì—ì„œ ë…¼ë‹¤",
    "ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ê°€ ì¹œêµ¬ë‹¤"
]

# í¬ì†Œ ë²¡í„° (ëŒ€ë¶€ë¶„ 0)
tfidf_matrix = vectorizer.fit_transform(documents)
print(f"ì°¨ì›: {tfidf_matrix.shape}")  # (3, 8) - ë‹¨ì–´ ìˆ˜ë§Œí¼
print(f"ë¹„ì˜ ìš”ì†Œ ë¹„ìœ¨: {tfidf_matrix.nnz / tfidf_matrix.size:.2%}")  # ~25%
\`\`\`

**í•œê³„ì :**
- ë‹¨ì–´ ìˆœì„œ ë¬´ì‹œ ("ê°œê°€ ì‚¬ëŒì„ ë¬¼ì—ˆë‹¤" vs "ì‚¬ëŒì´ ê°œë¥¼ ë¬¼ì—ˆë‹¤")
- ë™ì˜ì–´ ì¸ì‹ ë¶ˆê°€ ("ì°¨" vs "ìë™ì°¨")
- í¬ì†Œ ë²¡í„° â†’ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨

---

### 2ì„¸ëŒ€: ì‹ ê²½ë§ ê¸°ë°˜ Word Embeddings (2013-2017)

**Word2Vec (2013, Google)**

Mikolov et al.ì˜ í˜ì‹ ì  ë…¼ë¬¸ "Efficient Estimation of Word Representations in Vector Space"

\`\`\`
í•µì‹¬ ì•„ì´ë””ì–´: ë¹„ìŠ·í•œ ë¬¸ë§¥ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤

"ë‚˜ëŠ” [ì»¤í”¼]ë¥¼ ë§ˆì‹ ë‹¤"
"ë‚˜ëŠ” [ì°¨]ë¥¼ ë§ˆì‹ ë‹¤"
â†’ "ì»¤í”¼"ì™€ "ì°¨"ëŠ” ìœ ì‚¬í•œ ë²¡í„°
\`\`\`

**ë‘ ê°€ì§€ í•™ìŠµ ë°©ì‹:**

\`\`\`
CBOW (Continuous Bag of Words):
  - ì£¼ë³€ ë‹¨ì–´ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ ì˜ˆì¸¡
  - [ë‚˜ëŠ”, ë¥¼, ë§ˆì‹ ë‹¤] â†’ [ì»¤í”¼] ì˜ˆì¸¡
  - ë¹ ë¥¸ í•™ìŠµ, ë¹ˆë²ˆí•œ ë‹¨ì–´ì— ìœ ë¦¬

Skip-gram:
  - ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ ì˜ˆì¸¡
  - [ì»¤í”¼] â†’ [ë‚˜ëŠ”, ë¥¼, ë§ˆì‹ ë‹¤] ì˜ˆì¸¡
  - í¬ê·€ ë‹¨ì–´, ì‘ì€ ë°ì´í„°ì…‹ì— ìœ ë¦¬
\`\`\`

**Word2Vecì˜ ë†€ë¼ìš´ ì„±ì§ˆ - ë²¡í„° ì—°ì‚°:**

\`\`\`python
# ìœ ëª…í•œ ì˜ˆì‹œ
vector("King") - vector("Man") + vector("Woman") â‰ˆ vector("Queen")

# ì‹¤ì œ ì½”ë“œ
from gensim.models import Word2Vec

# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
import gensim.downloader as api
model = api.load('word2vec-google-news-300')

# ë²¡í„° ì—°ì‚°
result = model.most_similar(
    positive=['king', 'woman'],
    negative=['man'],
    topn=1
)
print(result)  # [('queen', 0.7118)]
\`\`\`

**GloVe (2014, Stanford)**

Global Vectors for Word Representation - ì „ì—­ í†µê³„ ì •ë³´ í™œìš©

\`\`\`
Word2Vec: ë¡œì»¬ ë¬¸ë§¥ë§Œ í•™ìŠµ
GloVe: ì „ì²´ ì½”í¼ìŠ¤ì˜ ë™ì‹œ ì¶œí˜„ í†µê³„ í™œìš©
â†’ ë” ì•ˆì •ì ì¸ ì„ë² ë”©
\`\`\`

**FastText (2016, Facebook)**

ì„œë¸Œì›Œë“œ(subword) ê¸°ë°˜ - ë¯¸ë“±ë¡ ë‹¨ì–´ ì²˜ë¦¬ ê°€ëŠ¥

\`\`\`
"ìì—°ì–´ì²˜ë¦¬" â†’ ["ìì—°", "ì—°ì–´", "ì–´ì²˜", "ì²˜ë¦¬", ...]
â†’ í•™ìŠµì— ì—†ëŠ” ë‹¨ì–´ë„ ì„œë¸Œì›Œë“œ ì¡°í•©ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
\`\`\`

---

### 3ì„¸ëŒ€: Transformer ê¸°ë°˜ (2018-í˜„ì¬)

**BERT (2018, Google)**

Bidirectional Encoder Representations from Transformers

\`\`\`
í•µì‹¬ í˜ì‹ : ì–‘ë°©í–¥ ë¬¸ë§¥ ì´í•´

Word2Vec/GloVe: "bank"ëŠ” í•­ìƒ ê°™ì€ ë²¡í„°
BERT: ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¥¸ ë²¡í„°
  - "I went to the bank to deposit money" â†’ ê¸ˆìœµ bank
  - "I sat on the river bank" â†’ ê°•ë‘‘ bank
\`\`\`

**Sentence-BERT (2019)**

ë¬¸ì¥ ë‹¨ìœ„ ì„ë² ë”©ì— ìµœì í™”

\`\`\`python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

sentences = [
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤",
    "The weather is nice today",  # ê°™ì€ ì˜ë¯¸, ë‹¤ë¥¸ ì–¸ì–´
    "ì£¼ì‹ ì‹œì¥ì´ í­ë½í–ˆë‹¤"
]

embeddings = model.encode(sentences)
# ì²« ë‘ ë¬¸ì¥ì€ ìœ ì‚¬í•œ ë²¡í„°, ì„¸ ë²ˆì§¸ëŠ” ë‹¤ë¦„
\`\`\`

**OpenAI Embeddings (2022-2024)**

\`\`\`
text-embedding-ada-002 (2022): 1536ì°¨ì›, $0.0001/1K tokens
text-embedding-3-small (2024): 1536ì°¨ì›, $0.00002/1K tokens (80% ì €ë ´!)
text-embedding-3-large (2024): 3072ì°¨ì›, $0.00013/1K tokens
\`\`\`

---

## ì„ë² ë”©ì˜ ìˆ˜í•™ì  ì›ë¦¬

### ë²¡í„° ê³µê°„ ëª¨ë¸ (Vector Space Model)

\`\`\`
Nì°¨ì› ê³µê°„ì—ì„œ ê° ì (ë²¡í„°)ì€ í…ìŠ¤íŠ¸ì˜ "ì˜ë¯¸"ë¥¼ í‘œí˜„

ì˜ˆ: 3ì°¨ì›ìœ¼ë¡œ ë‹¨ìˆœí™”
    ë™ë¬¼ ì¶•  ìŒì‹ ì¶•  ê¸°ìˆ  ì¶•
ê³ ì–‘ì´: [0.9,   0.1,   0.0]
ê°•ì•„ì§€: [0.8,   0.2,   0.0]
í”¼ì:   [0.0,   0.9,   0.1]
ì»´í“¨í„°: [0.0,   0.1,   0.9]

â†’ ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ëŠ” ê°€ê¹ê³ , í”¼ìì™€ëŠ” ë©€ë‹¤
\`\`\`

### ìœ ì‚¬ë„ ì¸¡ì • ë°©ë²•

**1. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Cosine Similarity) - ê°€ì¥ ì¼ë°˜ì **

\`\`\`python
import numpy as np

def cosine_similarity(a, b):
    """
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‘ ë²¡í„°ì˜ ê°ë„
    - ë°©í–¥ë§Œ ê³ ë ¤, í¬ê¸° ë¬´ì‹œ
    - í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë¬´ê´€
    """
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# ê°’ ë²”ìœ„: -1 ~ 1
# 1: ì™„ì „íˆ ê°™ì€ ë°©í–¥ (ì˜ë¯¸ ë™ì¼)
# 0: ì§êµ (ê´€ë ¨ ì—†ìŒ)
# -1: ë°˜ëŒ€ ë°©í–¥ (ì˜ë¯¸ ë°˜ëŒ€)

vec_cat = np.array([0.9, 0.8, 0.1])
vec_dog = np.array([0.85, 0.75, 0.15])
vec_pizza = np.array([0.1, 0.2, 0.9])

print(f"ê³ ì–‘ì´-ê°•ì•„ì§€: {cosine_similarity(vec_cat, vec_dog):.3f}")  # ~0.99
print(f"ê³ ì–‘ì´-í”¼ì: {cosine_similarity(vec_cat, vec_pizza):.3f}")  # ~0.35
\`\`\`

**2. ìœ í´ë¦¬ë“œ ê±°ë¦¬ (Euclidean Distance)**

\`\`\`python
def euclidean_distance(a, b):
    """
    ìœ í´ë¦¬ë“œ ê±°ë¦¬ = ë‘ ì  ì‚¬ì´ì˜ ì§ì„  ê±°ë¦¬
    - í¬ê¸°ë„ ê³ ë ¤
    - ì‘ì„ìˆ˜ë¡ ìœ ì‚¬
    """
    return np.linalg.norm(a - b)

# ì£¼ì˜: ê±°ë¦¬ì´ë¯€ë¡œ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬!
print(f"ê³ ì–‘ì´-ê°•ì•„ì§€ ê±°ë¦¬: {euclidean_distance(vec_cat, vec_dog):.3f}")
\`\`\`

**3. ë‚´ì  (Dot Product)**

\`\`\`python
def dot_product(a, b):
    """
    ë‚´ì  = í¬ê¸° Ã— ë°©í–¥
    - ì •ê·œí™”ëœ ë²¡í„°ì—ì„œëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼
    - ë¹ ë¥¸ ê³„ì‚°
    """
    return np.dot(a, b)
\`\`\`

**ì–¸ì œ ì–´ë–¤ ì¸¡ì • ë°©ë²•ì„?**

| ë°©ë²• | ì‚¬ìš© ìƒí™© | íŠ¹ì§• |
|------|----------|------|
| **ì½”ì‚¬ì¸ ìœ ì‚¬ë„** | ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ê²€ìƒ‰ | ê¸¸ì´ ë¬´ê´€, ì˜ë¯¸ë§Œ ë¹„êµ |
| **ìœ í´ë¦¬ë“œ ê±°ë¦¬** | í´ëŸ¬ìŠ¤í„°ë§, ì´ìƒì¹˜ íƒì§€ | ì ˆëŒ€ì  ìœ„ì¹˜ ì¤‘ìš” |
| **ë‚´ì ** | ì¶”ì²œ ì‹œìŠ¤í…œ, ì •ê·œí™”ëœ ë²¡í„° | ë¹ ë¥¸ ê³„ì‚° |

---

## ì°¨ì›ì˜ ì €ì£¼ì™€ í•´ê²°ì±…

\`\`\`
ë¬¸ì œ: ê³ ì°¨ì›ì—ì„œëŠ” ëª¨ë“  ì ì´ ë¹„ìŠ·í•˜ê²Œ "ë©€ë¦¬" ë³´ì„

1536ì°¨ì›ì—ì„œ:
- ë¬´ì‘ìœ„ ë‘ ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ê±°ì˜ 0ì— ê°€ê¹Œì›€
- ìœ ì‚¬í•œ ë²¡í„°ë§Œ ë†’ì€ ìœ ì‚¬ë„ â†’ ì¢‹ì€ ë¶„ë³„ë ¥
\`\`\`

**ì°¨ì› ì¶•ì†Œ ê¸°ë²•:**

\`\`\`python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap

# PCA: ë¹ ë¥´ì§€ë§Œ ì„ í˜•ì 
pca = PCA(n_components=2)
reduced_pca = pca.fit_transform(embeddings)

# t-SNE: ì‹œê°í™”ì— ì¢‹ìŒ, ëŠë¦¼
tsne = TSNE(n_components=2)
reduced_tsne = tsne.fit_transform(embeddings)

# UMAP: t-SNEë³´ë‹¤ ë¹ ë¥´ê³  ì „ì—­ êµ¬ì¡° ë³´ì¡´
reducer = umap.UMAP(n_components=2)
reduced_umap = reducer.fit_transform(embeddings)
\`\`\`
      `,
      keyPoints: [
        'Word2Vec(2013) â†’ GloVe â†’ FastText â†’ BERT â†’ OpenAI ì„ë² ë”© ë°œì „ì‚¬',
        'ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë°©í–¥ ê¸°ë°˜, í…ìŠ¤íŠ¸ ê¸¸ì´ ë¬´ê´€',
        'ê³ ì°¨ì› ë²¡í„°ëŠ” ì˜ë¯¸ì  ë¶„ë³„ë ¥ì´ ë†’ìŒ',
      ],
      practiceGoal: 'ì„ë² ë”©ì˜ ì—­ì‚¬ì™€ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤',
    }),

    // ========================================
    // Task 2: ì„ë² ë”© ëª¨ë¸ ë¹„êµ & ì„ íƒ ê°€ì´ë“œ (45ë¶„)
    // ========================================
    createReadingTask('w5d2-embedding-models', 'ì„ë² ë”© ëª¨ë¸ ë¹„êµ & ì„ íƒ ê°€ì´ë“œ', 45, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- ì£¼ìš” ì„ë² ë”© ëª¨ë¸ë“¤ì˜ íŠ¹ì„±ì„ ë¹„êµí•œë‹¤
- ìƒí™©ì— ë§ëŠ” ìµœì ì˜ ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•œë‹¤
- ë‹¤êµ­ì–´ ë° í•œêµ­ì–´ ì „ìš© ëª¨ë¸ì„ ì´í•´í•œë‹¤

---

## ì£¼ìš” ì„ë² ë”© ëª¨ë¸ ì¢…í•© ë¹„êµ

### OpenAI Embeddings (2024 ê¸°ì¤€)

| ëª¨ë¸ | ì°¨ì› | ê°€ê²©/1M í† í° | ìµœëŒ€ í† í° | íŠ¹ì§• |
|------|------|-------------|----------|------|
| **text-embedding-3-small** | 1536 | $0.02 | 8191 | ê°€ì„±ë¹„ ìµœê³ , ëŒ€ë¶€ë¶„ ì¶©ë¶„ |
| **text-embedding-3-large** | 3072 | $0.13 | 8191 | ìµœê³  í’ˆì§ˆ, ë³µì¡í•œ ë„ë©”ì¸ |
| text-embedding-ada-002 | 1536 | $0.10 | 8191 | ë ˆê±°ì‹œ, ì‚¬ìš© ì§€ì–‘ |

\`\`\`python
from openai import OpenAI
client = OpenAI()

# 3-small: ëŒ€ë¶€ë¶„ì˜ RAGì— ì¶©ë¶„
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ",
    encoding_format="float"
)

# ì°¨ì› ì¶•ì†Œ ì˜µì…˜ (ìƒˆë¡œìš´ ê¸°ëŠ¥!)
response_reduced = client.embeddings.create(
    model="text-embedding-3-large",
    input="ê²€ìƒ‰ ì¦ê°• ìƒì„± ê¸°ìˆ ",
    dimensions=1024  # 3072 â†’ 1024ë¡œ ì¶•ì†Œ
)
# ì €ì¥ ê³µê°„ ì ˆì•½ + ì„±ëŠ¥ ìœ ì§€
\`\`\`

---

### ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸

**Sentence-Transformers (Hugging Face)**

| ëª¨ë¸ | ì°¨ì› | ì–¸ì–´ | ì†ë„ | í’ˆì§ˆ |
|------|------|------|------|------|
| all-MiniLM-L6-v2 | 384 | ì˜ì–´ | âš¡ï¸âš¡ï¸âš¡ï¸ | â­ï¸â­ï¸â­ï¸ |
| all-mpnet-base-v2 | 768 | ì˜ì–´ | âš¡ï¸âš¡ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸ |
| paraphrase-multilingual-MiniLM-L12-v2 | 384 | 50+ | âš¡ï¸âš¡ï¸âš¡ï¸ | â­ï¸â­ï¸â­ï¸ |
| **multilingual-e5-large** | 1024 | 100+ | âš¡ï¸ | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ |

\`\`\`python
from sentence_transformers import SentenceTransformer

# ì˜ì–´ ì „ìš© - ë¹ ë¥´ê³  ê°€ë²¼ì›€
model_en = SentenceTransformer('all-MiniLM-L6-v2')

# ë‹¤êµ­ì–´ - í•œêµ­ì–´ í¬í•¨
model_multi = SentenceTransformer('intfloat/multilingual-e5-large')

# ì„ë² ë”© ìƒì„±
texts = [
    "query: RAG ì‹œìŠ¤í…œì´ë€?",  # E5 ëª¨ë¸ì€ "query:" í”„ë¦¬í”½ìŠ¤ í•„ìš”
    "passage: RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤"  # ë¬¸ì„œëŠ” "passage:"
]
embeddings = model_multi.encode(texts)
\`\`\`

---

### í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸

| ëª¨ë¸ | ì°¨ì› | íŠ¹ì§• | ì¶”ì²œ ìƒí™© |
|------|------|------|----------|
| **KoSentenceT5** | 768 | í•œêµ­ì–´ íŠ¹í™” | ìˆœìˆ˜ í•œêµ­ì–´ ë¬¸ì„œ |
| **ko-sbert-nli** | 768 | í•œêµ­ì–´ NLI í•™ìŠµ | ì˜ë¯¸ì  ìœ ì‚¬ë„ |
| **KoSimCSE** | 768 | ëŒ€ì¡° í•™ìŠµ | ë¹„ì§€ë„ í•™ìŠµ |
| **multilingual-e5** | 1024 | 100ê°œ ì–¸ì–´ | ë‹¤êµ­ì–´ í˜¼í•© |

\`\`\`python
from sentence_transformers import SentenceTransformer

# í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
model_ko = SentenceTransformer('jhgan/ko-sroberta-multitask')

korean_texts = [
    "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë°œì „í•˜ê³  ìˆë‹¤",
    "AI ê¸°ìˆ ì´ ì§„ë³´í•˜ê³  ìˆë‹¤",
    "ì˜¤ëŠ˜ ì ì‹¬ì€ ê¹€ì¹˜ì°Œê°œ"
]

embeddings = model_ko.encode(korean_texts)
\`\`\`

---

## ì„ë² ë”© ëª¨ë¸ ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬

\`\`\`
ì‹œì‘
â”œâ”€â”€ ì˜ˆì‚°ì´ ìˆëŠ”ê°€?
â”‚   â”œâ”€â”€ ìˆë‹¤ â†’ OpenAI
â”‚   â”‚   â”œâ”€â”€ ì¼ë°˜ RAG â†’ text-embedding-3-small
â”‚   â”‚   â””â”€â”€ ê³ í’ˆì§ˆ í•„ìš” â†’ text-embedding-3-large
â”‚   â”‚
â”‚   â””â”€â”€ ì—†ë‹¤ (ë¬´ë£Œ) â†’ ì˜¤í”ˆì†ŒìŠ¤
â”‚       â”œâ”€â”€ ì˜ì–´ë§Œ? â†’ all-MiniLM-L6-v2 (ë¹ ë¦„)
â”‚       â”œâ”€â”€ ë‹¤êµ­ì–´? â†’ multilingual-e5-large
â”‚       â””â”€â”€ í•œêµ­ì–´? â†’ ko-sroberta-multitask
â”‚
â”œâ”€â”€ ë„ë©”ì¸ íŠ¹ìˆ˜ì„±ì´ ìˆëŠ”ê°€?
â”‚   â”œâ”€â”€ ë²•ë¥ /ì˜ë£Œ â†’ Fine-tuning ê³ ë ¤
â”‚   â”œâ”€â”€ ì½”ë“œ â†’ CodeBERT, text-embedding-3-large
â”‚   â””â”€â”€ ì¼ë°˜ â†’ ë²”ìš© ëª¨ë¸
â”‚
â””â”€â”€ ì§€ì—° ì‹œê°„ ìš”êµ¬ì‚¬í•­?
    â”œâ”€â”€ ì‹¤ì‹œê°„ (<100ms) â†’ ë¡œì»¬ ëª¨ë¸ + ìºì‹±
    â””â”€â”€ ë°°ì¹˜ ê°€ëŠ¥ â†’ OpenAI API
\`\`\`

---

## ë²¤ì¹˜ë§ˆí¬: MTEB (Massive Text Embedding Benchmark)

\`\`\`
MTEB: ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ í‘œì¤€

7ê°€ì§€ íƒœìŠ¤í¬:
1. Retrieval (ê²€ìƒ‰)
2. Semantic Textual Similarity (ì˜ë¯¸ì  ìœ ì‚¬ë„)
3. Clustering (í´ëŸ¬ìŠ¤í„°ë§)
4. Classification (ë¶„ë¥˜)
5. Reranking (ì¬ìˆœìœ„)
6. Pair Classification (ìŒ ë¶„ë¥˜)
7. Summarization (ìš”ì•½)

2024ë…„ ìƒìœ„ ëª¨ë¸ (ê²€ìƒ‰ íƒœìŠ¤í¬ ê¸°ì¤€):
1. text-embedding-3-large (OpenAI): 64.6
2. multilingual-e5-large-instruct: 62.4
3. text-embedding-3-small (OpenAI): 62.3
4. all-mpnet-base-v2: 57.8
\`\`\`

---

## í”„ë¡œë•ì…˜ ê³ ë ¤ì‚¬í•­

### 1. ë¹„ìš© ê³„ì‚°

\`\`\`python
def calculate_embedding_cost():
    """
    ì˜ˆì‹œ: 100ë§Œ ê°œ ë¬¸ì„œ (í‰ê·  500 í† í°)
    """
    documents = 1_000_000
    avg_tokens = 500
    total_tokens = documents * avg_tokens  # 500M í† í°

    costs = {
        "text-embedding-3-small": total_tokens / 1_000_000 * 0.02,
        "text-embedding-3-large": total_tokens / 1_000_000 * 0.13,
        "ada-002 (legacy)": total_tokens / 1_000_000 * 0.10,
        "ì˜¤í”ˆì†ŒìŠ¤": 0  # GPU ë¹„ìš© ë³„ë„
    }

    for model, cost in costs.items():
        print(f"{model}: \${cost:,.2f}")

# text-embedding-3-small: $10.00 â† ê°€ì„±ë¹„!
# text-embedding-3-large: $65.00
# ada-002 (legacy): $50.00
\`\`\`

### 2. ì§€ì—° ì‹œê°„ ë¹„êµ

| ë°©ì‹ | ì§€ì—° ì‹œê°„ | ì¥ë‹¨ì  |
|------|----------|--------|
| OpenAI API | 100-500ms | ê°„í¸, ë„¤íŠ¸ì›Œí¬ ì˜ì¡´ |
| ë¡œì»¬ CPU | 50-200ms | GPU ë¶ˆí•„ìš”, ì¤‘ê°„ ì†ë„ |
| ë¡œì»¬ GPU | 5-20ms | ë¹ ë¦„, GPU í•„ìš” |
| ìºì‹± | <1ms | ê°€ì¥ ë¹ ë¦„, íˆíŠ¸ìœ¨ ì¤‘ìš” |

### 3. ì„ë² ë”© ìºì‹± ì „ëµ

\`\`\`python
import hashlib
import redis
import json
import numpy as np

class EmbeddingCache:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis = redis.from_url(redis_url)
        self.ttl = 86400 * 30  # 30ì¼

    def _hash_text(self, text: str, model: str) -> str:
        """í…ìŠ¤íŠ¸ì™€ ëª¨ë¸ ì¡°í•©ì˜ í•´ì‹œ"""
        content = f"{model}:{text}"
        return hashlib.sha256(content.encode()).hexdigest()

    def get(self, text: str, model: str) -> np.ndarray | None:
        """ìºì‹œì—ì„œ ì„ë² ë”© ì¡°íšŒ"""
        key = self._hash_text(text, model)
        cached = self.redis.get(key)
        if cached:
            return np.array(json.loads(cached))
        return None

    def set(self, text: str, model: str, embedding: list):
        """ìºì‹œì— ì„ë² ë”© ì €ì¥"""
        key = self._hash_text(text, model)
        self.redis.setex(key, self.ttl, json.dumps(embedding))

# ì‚¬ìš© ì˜ˆì‹œ
cache = EmbeddingCache()

def get_embedding_with_cache(text: str, model: str = "text-embedding-3-small"):
    # 1. ìºì‹œ í™•ì¸
    cached = cache.get(text, model)
    if cached is not None:
        return cached

    # 2. API í˜¸ì¶œ
    response = client.embeddings.create(model=model, input=text)
    embedding = response.data[0].embedding

    # 3. ìºì‹œ ì €ì¥
    cache.set(text, model, embedding)

    return embedding
\`\`\`
      `,
      keyPoints: [
        'text-embedding-3-smallì´ ëŒ€ë¶€ë¶„ì˜ RAGì— ì¶©ë¶„ (ê°€ì„±ë¹„ ìµœê³ )',
        'í•œêµ­ì–´ëŠ” multilingual-e5 ë˜ëŠ” ko-sroberta ì¶”ì²œ',
        'ì„ë² ë”© ìºì‹±ìœ¼ë¡œ ë¹„ìš©ê³¼ ì§€ì—°ì‹œê°„ ìµœì í™”',
      ],
      practiceGoal: 'ìƒí™©ì— ë§ëŠ” ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 3: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‹¬ì¸µ ì´í•´ (45ë¶„)
    // ========================================
    createReadingTask('w5d2-vectordb-deep', 'ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‹¬ì¸µ ì´í•´', 45, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- ë²¡í„° DBì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- ì£¼ìš” ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ì„ ë¹„êµí•œë‹¤
- ë²¡í„° DB ì„ íƒ ê¸°ì¤€ì„ í•™ìŠµí•œë‹¤

---

## ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë€?

### ì¼ë°˜ DB vs ë²¡í„° DB

\`\`\`
ì¼ë°˜ ë°ì´í„°ë² ì´ìŠ¤ (PostgreSQL, MySQL):
- B-Tree ì¸ë±ìŠ¤: ì •í™•í•œ ê°’ ë§¤ì¹­
- ë²”ìœ„ ì¿¼ë¦¬: WHERE price > 100
- í…ìŠ¤íŠ¸ ê²€ìƒ‰: LIKE '%keyword%'
âŒ "ë¹„ìŠ·í•œ ì˜ë¯¸" ê²€ìƒ‰ ë¶ˆê°€

ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤:
- ANN ì¸ë±ìŠ¤: ê·¼ì‚¬ ìµœê·¼ì ‘ ì´ì›ƒ ê²€ìƒ‰
- ìœ ì‚¬ë„ ì¿¼ë¦¬: ê°€ì¥ ë¹„ìŠ·í•œ Kê°œ ì°¾ê¸°
- ì˜ë¯¸ ê²€ìƒ‰: "í–‰ë³µ" ê²€ìƒ‰ â†’ "ê¸°ì¨", "ì¦ê±°ì›€" í¬í•¨
âœ… ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ê²€ìƒ‰
\`\`\`

---

## ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ ì‹¬ì¸µ ë¶„ì„

### 1. Flat Index (Brute Force)

\`\`\`
ë°©ë²•: ëª¨ë“  ë²¡í„°ì™€ ì¿¼ë¦¬ ë²¡í„°ë¥¼ ì§ì ‘ ë¹„êµ

ì¥ì :
- ì •í™•ë„ 100% (ì •í™•í•œ ìµœê·¼ì ‘ ì´ì›ƒ)
- êµ¬í˜„ ê°„ë‹¨

ë‹¨ì :
- O(n) ì‹œê°„ ë³µì¡ë„
- ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ë§¤ìš° ëŠë¦¼

ì‚¬ìš©ì²˜:
- 10ë§Œ ê°œ ì´í•˜ ë°ì´í„°
- ì •í™•ë„ê°€ ìµœìš°ì„ ì¸ ê²½ìš°
\`\`\`

### 2. IVF (Inverted File Index)

\`\`\`
ë°©ë²•: ë²¡í„°ë¥¼ í´ëŸ¬ìŠ¤í„°ë¡œ ê·¸ë£¹í™”, í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ë§Œ ê²€ìƒ‰

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â—   â—      â—â—  â—    â—   â—        â”‚
â”‚    â—  â•”â•â•â•â•— â—      â•”â•â•â•â•—    â—     â”‚
â”‚  â—    â•‘ C1â•‘   â—    â•‘ C2â•‘  â—   â—   â”‚
â”‚     â— â•šâ•â•â•â•  â—  â—  â•šâ•â•â•â•     â—    â”‚
â”‚   â—   â—   â—     â—    â—   â—   â—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ê²€ìƒ‰ ê³¼ì •:
1. ì¿¼ë¦¬ ë²¡í„°ê°€ ì†í•  í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (nprobeê°œ)
2. í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ë‚´ ë²¡í„°ë§Œ ë¹„êµ

íŒŒë¼ë¯¸í„°:
- nlist: í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ (ë³´í†µ âˆšn)
- nprobe: ê²€ìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜ (ì •í™•ë„â†”ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)
\`\`\`

\`\`\`python
import faiss
import numpy as np

# IVF ì¸ë±ìŠ¤ ìƒì„±
dimension = 1536
nlist = 100  # 100ê°œ í´ëŸ¬ìŠ¤í„°

# ì–‘ìí™”ê¸° + IVF
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist)

# í•™ìŠµ í•„ìš” (í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ê³„ì‚°)
vectors = np.random.rand(10000, dimension).astype('float32')
index.train(vectors)
index.add(vectors)

# ê²€ìƒ‰
index.nprobe = 10  # 10ê°œ í´ëŸ¬ìŠ¤í„° ê²€ìƒ‰
D, I = index.search(query_vector, k=5)
\`\`\`

### 3. HNSW (Hierarchical Navigable Small World)

\`\`\`
ë°©ë²•: ë‹¤ì¸µ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ íš¨ìœ¨ì ì¸ íƒìƒ‰

Layer 2 (sparse):    â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
                      â•²             â•±
Layer 1 (medium):  â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—
                    â•² â•± â•² â•± â•² â•± â•² â•±
Layer 0 (dense):  â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—

ê²€ìƒ‰ ê³¼ì •:
1. ìµœìƒìœ„ ë ˆì´ì–´ì—ì„œ ì‹œì‘
2. ê·¸ë˜í”„ ì—£ì§€ ë”°ë¼ ì´ë™
3. í•˜ìœ„ ë ˆì´ì–´ë¡œ ë‚´ë ¤ê°€ë©° ì •ë°€ íƒìƒ‰

ì¥ì :
- ë§¤ìš° ë¹ ë¥¸ ê²€ìƒ‰ (O(log n))
- ë†’ì€ ì¬í˜„ìœ¨ (recall)
- ë™ì  ì¶”ê°€ ê°€ëŠ¥

ë‹¨ì :
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ (ê·¸ë˜í”„ ì €ì¥)
- ì‚­ì œ ì–´ë ¤ì›€
\`\`\`

\`\`\`python
# HNSW ì¸ë±ìŠ¤ (FAISS)
index = faiss.IndexHNSWFlat(dimension, M=32)
# M: ê° ë…¸ë“œì˜ ì—°ê²° ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì •í™•, ëŠë¦¼)

# HNSW ì¸ë±ìŠ¤ (Chroma ê¸°ë³¸ê°’)
# ChromaëŠ” ë‚´ë¶€ì ìœ¼ë¡œ HNSW ì‚¬ìš©
\`\`\`

### 4. PQ (Product Quantization)

\`\`\`
ë°©ë²•: ë²¡í„°ë¥¼ ì„œë¸Œë²¡í„°ë¡œ ë¶„í•  í›„ ì–‘ìí™”

ì›ë³¸: [0.12, -0.34, 0.56, 0.78, -0.23, 0.45, 0.67, -0.89]
       â””â”€â”€ sub1 â”€â”€â”˜  â””â”€â”€ sub2 â”€â”€â”˜  â””â”€â”€ sub3 â”€â”€â”˜  â””â”€â”€ sub4 â”€â”€â”˜
                â†“         â†“           â†“           â†“
ì–‘ìí™”:      [code1]   [code2]     [code3]     [code4]
               â†“         â†“           â†“           â†“
ê²°ê³¼:        [42,       17,         89,         3]

ì¥ì :
- ë©”ëª¨ë¦¬ 90% ì ˆì•½ (1536D float â†’ ìˆ˜ì‹­ ë°”ì´íŠ¸)
- ìˆ˜ì‹­ì–µ ë²¡í„° ì²˜ë¦¬ ê°€ëŠ¥

ë‹¨ì :
- ì •í™•ë„ ì†ì‹¤ (ê·¼ì‚¬)
- í•™ìŠµ í•„ìš”
\`\`\`

### ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ ë¹„êµí‘œ

| ì•Œê³ ë¦¬ì¦˜ | ì†ë„ | ì •í™•ë„ | ë©”ëª¨ë¦¬ | ë™ì  ì¶”ê°€ | ì‚¬ìš©ì²˜ |
|----------|------|--------|--------|----------|--------|
| Flat | âš¡ | ğŸ’¯100% | ğŸ“Šë†’ìŒ | âœ… | <10ë§Œ |
| IVF | âš¡âš¡ | 95-99% | ğŸ“Šì¤‘ê°„ | âŒ (ì¬í•™ìŠµ) | 10ë§Œ-1000ë§Œ |
| HNSW | âš¡âš¡âš¡ | 95-99% | ğŸ“Šë†’ìŒ | âœ… | 100ë§Œ-1ì–µ |
| PQ | âš¡âš¡ | 85-95% | ğŸ“Šë‚®ìŒ | âŒ (ì¬í•™ìŠµ) | 1ì–µ+ |
| IVF+PQ | âš¡âš¡âš¡ | 90-95% | ğŸ“Šë§¤ìš°ë‚®ìŒ | âŒ | 10ì–µ+ |

---

## ì£¼ìš” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¹„êµ

### 1. Chroma

\`\`\`
ìœ í˜•: ì„ë² ë””ë“œ/ë¡œì»¬
ì¸ë±ì‹±: HNSW
íŠ¹ì§•:
- ì„¤ì¹˜ ì—†ì´ ì‹œì‘ (pip install chromadb)
- LangChain ì™„ë²½ í†µí•©
- ì˜êµ¬ ì €ì¥ ì§€ì›

ì‚¬ìš©ì²˜: í”„ë¡œí† íƒ€ì´í•‘, ì†Œê·œëª¨ í”„ë¡œë•ì…˜
ê°€ê²©: ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)
\`\`\`

### 2. Pinecone

\`\`\`
ìœ í˜•: ê´€ë¦¬í˜• í´ë¼ìš°ë“œ
ì¸ë±ì‹±: ë…ì ì•Œê³ ë¦¬ì¦˜ (ìµœì í™”ëœ HNSW)
íŠ¹ì§•:
- ì„œë²„ë¦¬ìŠ¤ ì˜µì…˜
- ìˆ˜ì‹­ì–µ ë²¡í„° ì§€ì›
- 99.9% SLA

ì‚¬ìš©ì²˜: ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜
ê°€ê²©: Free tier (100K ë²¡í„°) + $0.096/1M ë²¡í„°/ì›”
\`\`\`

### 3. Weaviate

\`\`\`
ìœ í˜•: ì˜¤í”ˆì†ŒìŠ¤/í´ë¼ìš°ë“œ
ì¸ë±ì‹±: HNSW
íŠ¹ì§•:
- GraphQL API
- ìì²´ ì„ë² ë”© ìƒì„± ê°€ëŠ¥
- ë©€í‹°ëª¨ë‹¬ ì§€ì›

ì‚¬ìš©ì²˜: GraphQL ì„ í˜¸, ë©€í‹°ëª¨ë‹¬
ê°€ê²©: ë¬´ë£Œ (ì…€í”„í˜¸ìŠ¤íŠ¸) / í´ë¼ìš°ë“œ ìœ ë£Œ
\`\`\`

### 4. Qdrant

\`\`\`
ìœ í˜•: ì˜¤í”ˆì†ŒìŠ¤/í´ë¼ìš°ë“œ
ì¸ë±ì‹±: HNSW + ì–‘ìí™”
íŠ¹ì§•:
- Rust ê¸°ë°˜ ê³ ì„±ëŠ¥
- í’ë¶€í•œ í•„í„°ë§
- gRPC ì§€ì›

ì‚¬ìš©ì²˜: ê³ ì„±ëŠ¥ ìš”êµ¬, ë³µì¡í•œ í•„í„°ë§
ê°€ê²©: ë¬´ë£Œ (ì…€í”„í˜¸ìŠ¤íŠ¸) / í´ë¼ìš°ë“œ ìœ ë£Œ
\`\`\`

### 5. pgvector (PostgreSQL í™•ì¥)

\`\`\`
ìœ í˜•: PostgreSQL í™•ì¥
ì¸ë±ì‹±: IVFFlat, HNSW
íŠ¹ì§•:
- ê¸°ì¡´ PostgreSQL í™œìš©
- SQLë¡œ ë²¡í„° ê²€ìƒ‰
- ACID ë³´ì¥

ì‚¬ìš©ì²˜: ì´ë¯¸ PostgreSQL ì‚¬ìš© ì¤‘
ê°€ê²©: ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)
\`\`\`

\`\`\`sql
-- pgvector ì˜ˆì‹œ
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    content TEXT,
    embedding vector(1536)
);

CREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops);

-- ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
SELECT id, content, 1 - (embedding <=> query_embedding) AS similarity
FROM documents
ORDER BY embedding <=> query_embedding
LIMIT 5;
\`\`\`

---

## ë²¡í„° DB ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬

\`\`\`
ì‹œì‘
â”œâ”€â”€ ì´ë¯¸ PostgreSQL ì‚¬ìš© ì¤‘?
â”‚   â””â”€â”€ Yes â†’ pgvector (ê¸°ì¡´ ì¸í”„ë¼ í™œìš©)
â”‚
â”œâ”€â”€ í”„ë¡œí† íƒ€ì´í•‘ ë‹¨ê³„?
â”‚   â””â”€â”€ Yes â†’ Chroma (ê°€ì¥ ì‰¬ìš´ ì‹œì‘)
â”‚
â”œâ”€â”€ ê´€ë¦¬ ë¶€ë‹´ ìµœì†Œí™”?
â”‚   â””â”€â”€ Yes â†’ Pinecone (ì™„ì „ ê´€ë¦¬í˜•)
â”‚
â”œâ”€â”€ ê³ ì„±ëŠ¥ + ë³µì¡í•œ í•„í„°ë§?
â”‚   â””â”€â”€ Yes â†’ Qdrant
â”‚
â”œâ”€â”€ GraphQL ì„ í˜¸?
â”‚   â””â”€â”€ Yes â†’ Weaviate
â”‚
â””â”€â”€ ìµœëŒ€ í™•ì¥ì„± + ì…€í”„í˜¸ìŠ¤íŠ¸?
    â””â”€â”€ Yes â†’ Milvus
\`\`\`
      `,
      keyPoints: [
        'HNSW: ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš© ì‚¬ë¡€ì— ìµœì  (ë¹ ë¥´ê³  ì •í™•)',
        'ë°ì´í„° ê·œëª¨ì— ë”°ë¼ ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ ì„ íƒ',
        'Chroma(í”„ë¡œí† íƒ€ì…) â†’ Pinecone(í”„ë¡œë•ì…˜) ì„±ì¥ ê²½ë¡œ',
      ],
      practiceGoal: 'ë²¡í„° DBì˜ ë‚´ë¶€ ë™ì‘ê³¼ ì„ íƒ ê¸°ì¤€ì„ ì´í•´í•œë‹¤',
    }),

    // ========================================
    // Task 4: Chroma í”„ë¡œë•ì…˜ íŒ¨í„´ (50ë¶„)
    // ========================================
    createCodeTask('w5d2-chroma-production', 'Chroma í”„ë¡œë•ì…˜ íŒ¨í„´ ì‹¤ìŠµ', 50, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- Chromaë¥¼ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•œë‹¤
- ì˜êµ¬ ì €ì¥, ë©”íƒ€ë°ì´í„° í•„í„°ë§, ë°°ì¹˜ ì²˜ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤
- LangChainê³¼ í†µí•©í•˜ì—¬ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•œë‹¤

---

## ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

\`\`\`bash
pip install chromadb langchain-chroma langchain-openai
\`\`\`

---

## Chroma í´ë¼ì´ì–¸íŠ¸ ëª¨ë“œ

\`\`\`python
import chromadb
from chromadb.config import Settings

# 1. ì¸ë©”ëª¨ë¦¬ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
client_memory = chromadb.Client()

# 2. ì˜êµ¬ ì €ì¥ (í”„ë¡œë•ì…˜ ê¶Œì¥)
client_persistent = chromadb.PersistentClient(
    path="./chroma_db",
    settings=Settings(
        anonymized_telemetry=False,  # í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
        allow_reset=True  # ë¦¬ì…‹ í—ˆìš© (ê°œë°œ ì‹œ)
    )
)

# 3. í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ëª¨ë“œ (ë¶„ì‚° í™˜ê²½)
client_http = chromadb.HttpClient(
    host="localhost",
    port=8000,
    settings=Settings(
        chroma_client_auth_provider="chromadb.auth.token.TokenAuthClientProvider",
        chroma_client_auth_credentials="your-token"
    )
)
\`\`\`

---

## í”„ë¡œë•ì…˜ê¸‰ ë²¡í„° ì €ì¥ì†Œ í´ë˜ìŠ¤

\`\`\`python
from dataclasses import dataclass
from typing import Optional
import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
import hashlib
import json

@dataclass
class Document:
    """ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    content: str
    metadata: dict

class ChromaVectorStore:
    """í”„ë¡œë•ì…˜ê¸‰ Chroma ë²¡í„° ì €ì¥ì†Œ"""

    def __init__(
        self,
        collection_name: str,
        persist_directory: str = "./chroma_db",
        embedding_model: str = "text-embedding-3-small"
    ):
        # Chroma í´ë¼ì´ì–¸íŠ¸
        self.client = chromadb.PersistentClient(path=persist_directory)

        # OpenAI ì„ë² ë”© í•¨ìˆ˜
        self.embedding_fn = embedding_functions.OpenAIEmbeddingFunction(
            model_name=embedding_model
        )

        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            embedding_function=self.embedding_fn,
            metadata={
                "description": f"RAG collection: {collection_name}",
                "embedding_model": embedding_model,
                "hnsw:space": "cosine"  # ìœ ì‚¬ë„ ì¸¡ì • ë°©ì‹
            }
        )

    def _generate_id(self, content: str) -> str:
        """ì½˜í…ì¸  ê¸°ë°˜ ID ìƒì„± (ì¤‘ë³µ ë°©ì§€)"""
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def add_documents(
        self,
        documents: list[Document],
        batch_size: int = 100
    ) -> int:
        """ë¬¸ì„œ ë°°ì¹˜ ì¶”ê°€"""
        added = 0

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]

            ids = [doc.id or self._generate_id(doc.content) for doc in batch]
            contents = [doc.content for doc in batch]
            metadatas = [doc.metadata for doc in batch]

            # ì¤‘ë³µ ì²´í¬
            existing = self.collection.get(ids=ids)
            new_indices = [
                j for j, id_ in enumerate(ids)
                if id_ not in existing['ids']
            ]

            if new_indices:
                self.collection.add(
                    ids=[ids[j] for j in new_indices],
                    documents=[contents[j] for j in new_indices],
                    metadatas=[metadatas[j] for j in new_indices]
                )
                added += len(new_indices)

            print(f"Progress: {min(i + batch_size, len(documents))}/{len(documents)}")

        return added

    def search(
        self,
        query: str,
        k: int = 5,
        filter: Optional[dict] = None,
        include_distances: bool = True
    ) -> list[dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        results = self.collection.query(
            query_texts=[query],
            n_results=k,
            where=filter,
            include=["documents", "metadatas", "distances"]
        )

        # ê²°ê³¼ í¬ë§·íŒ…
        formatted = []
        for i in range(len(results['ids'][0])):
            doc = {
                "id": results['ids'][0][i],
                "content": results['documents'][0][i],
                "metadata": results['metadatas'][0][i]
            }
            if include_distances:
                doc["distance"] = results['distances'][0][i]
                doc["similarity"] = 1 - results['distances'][0][i]  # cosine â†’ ìœ ì‚¬ë„
            formatted.append(doc)

        return formatted

    def search_with_filter(
        self,
        query: str,
        k: int = 5,
        category: Optional[str] = None,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None
    ) -> list[dict]:
        """ë³µì¡í•œ í•„í„° ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰"""
        where_conditions = []

        if category:
            where_conditions.append({"category": {"$eq": category}})

        if date_from:
            where_conditions.append({"date": {"$gte": date_from}})

        if date_to:
            where_conditions.append({"date": {"$lte": date_to}})

        # ì¡°ê±´ ì¡°í•©
        filter_dict = None
        if len(where_conditions) == 1:
            filter_dict = where_conditions[0]
        elif len(where_conditions) > 1:
            filter_dict = {"$and": where_conditions}

        return self.search(query, k=k, filter=filter_dict)

    def delete(self, ids: list[str]) -> int:
        """ë¬¸ì„œ ì‚­ì œ"""
        existing = self.collection.get(ids=ids)
        valid_ids = existing['ids']
        if valid_ids:
            self.collection.delete(ids=valid_ids)
        return len(valid_ids)

    def get_stats(self) -> dict:
        """ì»¬ë ‰ì…˜ í†µê³„"""
        return {
            "name": self.collection.name,
            "count": self.collection.count(),
            "metadata": self.collection.metadata
        }
\`\`\`

---

## ì‚¬ìš© ì˜ˆì‹œ

\`\`\`python
# 1. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
store = ChromaVectorStore(
    collection_name="tech_articles",
    persist_directory="./production_db"
)

# 2. ë¬¸ì„œ ì¶”ê°€
documents = [
    Document(
        id="doc1",
        content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ, LLMì˜ í™˜ê°ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤.",
        metadata={"category": "AI", "date": "2024-01-15", "author": "í™ê¸¸ë™"}
    ),
    Document(
        id="doc2",
        content="ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        metadata={"category": "DB", "date": "2024-01-20", "author": "ê¹€ì² ìˆ˜"}
    ),
    Document(
        id="doc3",
        content="LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
        metadata={"category": "AI", "date": "2024-02-01", "author": "ì´ì˜í¬"}
    )
]

added = store.add_documents(documents)
print(f"ì¶”ê°€ëœ ë¬¸ì„œ: {added}ê°œ")

# 3. ê¸°ë³¸ ê²€ìƒ‰
results = store.search("LLM í™˜ê° ë°©ì§€ ê¸°ìˆ ", k=2)
for r in results:
    print(f"[{r['similarity']:.2%}] {r['content'][:50]}...")

# 4. í•„í„° ê²€ìƒ‰
ai_results = store.search_with_filter(
    query="ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ",
    k=5,
    category="AI",
    date_from="2024-01-01"
)

# 5. í†µê³„ í™•ì¸
stats = store.get_stats()
print(f"ì»¬ë ‰ì…˜: {stats['name']}, ë¬¸ì„œ ìˆ˜: {stats['count']}")
\`\`\`

---

## LangChain í†µí•©

\`\`\`python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document as LCDocument
from langchain.chains import RetrievalQA

# 1. ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

vectorstore = Chroma(
    collection_name="langchain_demo",
    embedding_function=embeddings,
    persist_directory="./langchain_chroma"
)

# 2. ë¬¸ì„œ ì¶”ê°€
docs = [
    LCDocument(
        page_content="RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤.",
        metadata={"source": "tutorial", "page": 1}
    ),
    LCDocument(
        page_content="ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.",
        metadata={"source": "tutorial", "page": 2}
    )
]

vectorstore.add_documents(docs)

# 3. Retriever ìƒì„±
retriever = vectorstore.as_retriever(
    search_type="mmr",  # Maximum Marginal Relevance (ë‹¤ì–‘ì„± ê³ ë ¤)
    search_kwargs={
        "k": 5,
        "fetch_k": 20,  # MMR í›„ë³´êµ°
        "lambda_mult": 0.5  # ë‹¤ì–‘ì„± ê°€ì¤‘ì¹˜
    }
)

# 4. QA ì²´ì¸
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

# 5. ì§ˆì˜
result = qa_chain.invoke({"query": "RAGì™€ ì„ë² ë”©ì˜ ê´€ê³„ëŠ”?"})
print(f"ë‹µë³€: {result['result']}")
print(f"ì°¸ì¡°: {[doc.metadata for doc in result['source_documents']]}")
\`\`\`

---

## Chroma ì„œë²„ ëª¨ë“œ (ë¶„ì‚° í™˜ê²½)

\`\`\`bash
# ì„œë²„ ì‹œì‘
chroma run --path ./chroma_server_data --port 8000

# Dockerë¡œ ì‹¤í–‰
docker run -p 8000:8000 -v ./chroma_data:/chroma/chroma chromadb/chroma
\`\`\`

\`\`\`python
# í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì—°ê²°
import chromadb

client = chromadb.HttpClient(host="localhost", port=8000)
collection = client.get_or_create_collection("my_collection")
\`\`\`
      `,
      keyPoints: [
        'PersistentClientë¡œ ì˜êµ¬ ì €ì¥',
        'ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ì •ë°€ ê²€ìƒ‰',
        'LangChain í†µí•©ìœ¼ë¡œ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•',
      ],
      practiceGoal: 'Chromaë¥¼ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 5: Pinecone í”„ë¡œë•ì…˜ íŒ¨í„´ (50ë¶„)
    // ========================================
    createCodeTask('w5d2-pinecone-production', 'Pinecone í”„ë¡œë•ì…˜ íŒ¨í„´ ì‹¤ìŠµ', 50, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- Pineconeì„ ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•œë‹¤
- ë„¤ì„ìŠ¤í˜ì´ìŠ¤, ë©”íƒ€ë°ì´í„° í•„í„°ë§, ë°°ì¹˜ ì—…ì„œíŠ¸ë¥¼ êµ¬í˜„í•œë‹¤
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense + Sparse)ì„ ì´í•´í•œë‹¤

---

## Pinecone ê³„ì • ì„¤ì •

\`\`\`
1. https://www.pinecone.io ê°€ì…
2. ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
3. API í‚¤ ë°œê¸‰ (Settings â†’ API Keys)
4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

export PINECONE_API_KEY="your-api-key"
\`\`\`

---

## ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

\`\`\`bash
pip install pinecone-client langchain-pinecone
\`\`\`

---

## í”„ë¡œë•ì…˜ê¸‰ Pinecone ë²¡í„° ì €ì¥ì†Œ

\`\`\`python
from pinecone import Pinecone, ServerlessSpec
from openai import OpenAI
from dataclasses import dataclass
from typing import Optional
import hashlib
import os
import time

@dataclass
class PineconeDocument:
    """ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    content: str
    metadata: dict
    embedding: Optional[list[float]] = None

class PineconeVectorStore:
    """í”„ë¡œë•ì…˜ê¸‰ Pinecone ë²¡í„° ì €ì¥ì†Œ"""

    def __init__(
        self,
        index_name: str,
        dimension: int = 1536,
        metric: str = "cosine",
        create_if_not_exists: bool = True
    ):
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.pc = Pinecone(api_key=os.environ["PINECONE_API_KEY"])
        self.openai = OpenAI()
        self.dimension = dimension
        self.index_name = index_name

        # ì¸ë±ìŠ¤ ìƒì„± ë˜ëŠ” ì—°ê²°
        if create_if_not_exists:
            self._ensure_index(metric)

        self.index = self.pc.Index(index_name)

    def _ensure_index(self, metric: str):
        """ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±"""
        existing = [idx.name for idx in self.pc.list_indexes()]

        if self.index_name not in existing:
            print(f"Creating index: {self.index_name}")
            self.pc.create_index(
                name=self.index_name,
                dimension=self.dimension,
                metric=metric,
                spec=ServerlessSpec(
                    cloud="aws",
                    region="us-east-1"
                )
            )
            # ì¸ë±ìŠ¤ ì¤€ë¹„ ëŒ€ê¸°
            while not self.pc.describe_index(self.index_name).status['ready']:
                time.sleep(1)
            print(f"Index {self.index_name} ready!")

    def _get_embedding(self, text: str) -> list[float]:
        """OpenAI ì„ë² ë”© ìƒì„±"""
        response = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding

    def _get_embeddings_batch(self, texts: list[str]) -> list[list[float]]:
        """ë°°ì¹˜ ì„ë² ë”© ìƒì„±"""
        response = self.openai.embeddings.create(
            model="text-embedding-3-small",
            input=texts
        )
        return [item.embedding for item in response.data]

    def upsert_documents(
        self,
        documents: list[PineconeDocument],
        namespace: str = "",
        batch_size: int = 100
    ) -> int:
        """ë¬¸ì„œ ë°°ì¹˜ ì—…ì„œíŠ¸"""
        total_upserted = 0

        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]

            # ì„ë² ë”©ì´ ì—†ëŠ” ë¬¸ì„œëŠ” ìƒì„±
            texts_to_embed = [
                (j, doc.content)
                for j, doc in enumerate(batch)
                if doc.embedding is None
            ]

            if texts_to_embed:
                indices, texts = zip(*texts_to_embed)
                embeddings = self._get_embeddings_batch(list(texts))
                for idx, emb in zip(indices, embeddings):
                    batch[idx].embedding = emb

            # ì—…ì„œíŠ¸ ë²¡í„° ì¤€ë¹„
            vectors = [
                {
                    "id": doc.id,
                    "values": doc.embedding,
                    "metadata": {
                        "content": doc.content[:1000],  # ë©”íƒ€ë°ì´í„° í¬ê¸° ì œí•œ
                        **doc.metadata
                    }
                }
                for doc in batch
            ]

            # ì—…ì„œíŠ¸
            self.index.upsert(vectors=vectors, namespace=namespace)
            total_upserted += len(vectors)
            print(f"Progress: {min(i + batch_size, len(documents))}/{len(documents)}")

        return total_upserted

    def search(
        self,
        query: str,
        k: int = 5,
        namespace: str = "",
        filter: Optional[dict] = None,
        include_metadata: bool = True
    ) -> list[dict]:
        """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self._get_embedding(query)

        # ê²€ìƒ‰
        results = self.index.query(
            vector=query_embedding,
            top_k=k,
            namespace=namespace,
            filter=filter,
            include_values=False,
            include_metadata=include_metadata
        )

        # ê²°ê³¼ í¬ë§·íŒ…
        formatted = []
        for match in results.matches:
            doc = {
                "id": match.id,
                "score": match.score,
                "content": match.metadata.get("content", ""),
                "metadata": {
                    k: v for k, v in match.metadata.items()
                    if k != "content"
                }
            }
            formatted.append(doc)

        return formatted

    def search_with_filter(
        self,
        query: str,
        k: int = 5,
        namespace: str = "",
        category: Optional[str] = None,
        date_from: Optional[str] = None,
        min_score: Optional[float] = None
    ) -> list[dict]:
        """ë³µì¡í•œ í•„í„° ì¡°ê±´ìœ¼ë¡œ ê²€ìƒ‰"""
        filter_conditions = {}

        if category:
            filter_conditions["category"] = {"$eq": category}

        if date_from:
            filter_conditions["date"] = {"$gte": date_from}

        results = self.search(
            query=query,
            k=k * 2 if min_score else k,  # í•„í„°ë§ ì—¬ìœ ë¶„
            namespace=namespace,
            filter=filter_conditions if filter_conditions else None
        )

        # ìµœì†Œ ì ìˆ˜ í•„í„°ë§
        if min_score:
            results = [r for r in results if r["score"] >= min_score][:k]

        return results

    def delete(
        self,
        ids: Optional[list[str]] = None,
        namespace: str = "",
        delete_all: bool = False
    ):
        """ë²¡í„° ì‚­ì œ"""
        if delete_all:
            self.index.delete(delete_all=True, namespace=namespace)
        elif ids:
            self.index.delete(ids=ids, namespace=namespace)

    def get_stats(self) -> dict:
        """ì¸ë±ìŠ¤ í†µê³„"""
        return self.index.describe_index_stats()
\`\`\`

---

## ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™œìš©

\`\`\`python
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤: ë™ì¼ ì¸ë±ìŠ¤ ë‚´ ë…¼ë¦¬ì  ë¶„ë¦¬

store = PineconeVectorStore(index_name="multi-tenant-app")

# ê³ ê°ë³„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
store.upsert_documents(customer_a_docs, namespace="customer_a")
store.upsert_documents(customer_b_docs, namespace="customer_b")

# ê³ ê° Aì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰
results_a = store.search("ì§ˆë¬¸", namespace="customer_a")

# ê³ ê° Bì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰
results_b = store.search("ì§ˆë¬¸", namespace="customer_b")

# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ í†µê³„
stats = store.get_stats()
for ns, ns_stats in stats['namespaces'].items():
    print(f"Namespace {ns}: {ns_stats['vector_count']} vectors")
\`\`\`

---

## ì‚¬ìš© ì˜ˆì‹œ

\`\`\`python
# 1. ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™”
store = PineconeVectorStore(
    index_name="rag-production",
    dimension=1536
)

# 2. ë¬¸ì„œ ì¶”ê°€
documents = [
    PineconeDocument(
        id="doc1",
        content="RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ, ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•©ë‹ˆë‹¤.",
        metadata={"category": "AI", "date": "2024-01-15", "source": "blog"}
    ),
    PineconeDocument(
        id="doc2",
        content="Pineconeì€ ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ì— ìµœì í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.",
        metadata={"category": "DB", "date": "2024-01-20", "source": "docs"}
    ),
    PineconeDocument(
        id="doc3",
        content="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì€ í‚¤ì›Œë“œì™€ ì˜ë¯¸ ê²€ìƒ‰ì„ ê²°í•©í•©ë‹ˆë‹¤.",
        metadata={"category": "AI", "date": "2024-02-01", "source": "paper"}
    )
]

store.upsert_documents(documents)

# 3. ê¸°ë³¸ ê²€ìƒ‰
results = store.search("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë€?", k=2)
for r in results:
    print(f"[{r['score']:.3f}] {r['content'][:50]}...")

# 4. í•„í„° ê²€ìƒ‰
ai_results = store.search_with_filter(
    query="ê²€ìƒ‰ ê¸°ìˆ ",
    k=5,
    category="AI",
    min_score=0.7
)

# 5. í†µê³„
stats = store.get_stats()
print(f"ì´ ë²¡í„° ìˆ˜: {stats['total_vector_count']}")
\`\`\`

---

## LangChain í†µí•©

\`\`\`python
from langchain_pinecone import PineconeVectorStore as LCPinecone
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema import Document
from langchain.chains import RetrievalQA

# 1. ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

vectorstore = LCPinecone.from_existing_index(
    index_name="rag-production",
    embedding=embeddings,
    namespace="langchain"
)

# ë˜ëŠ” ìƒˆë¡œ ìƒì„±
docs = [
    Document(
        page_content="Pineconeê³¼ LangChainì˜ í†µí•©ì€ ê°„ë‹¨í•©ë‹ˆë‹¤.",
        metadata={"source": "tutorial"}
    )
]

vectorstore = LCPinecone.from_documents(
    docs,
    embeddings,
    index_name="rag-production",
    namespace="langchain"
)

# 2. Retriever
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# 3. QA ì²´ì¸
qa = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o-mini"),
    retriever=retriever
)

answer = qa.invoke("Pineconeê³¼ LangChainì„ ì–´ë–»ê²Œ í†µí•©í•˜ë‚˜ìš”?")
\`\`\`

---

## Chroma vs Pinecone ìµœì¢… ë¹„êµ

| í•­ëª© | Chroma | Pinecone |
|------|--------|----------|
| **íƒ€ì…** | ë¡œì»¬/ì„ë² ë””ë“œ | ê´€ë¦¬í˜• í´ë¼ìš°ë“œ |
| **í™•ì¥ì„±** | ~100ë§Œ ë²¡í„° | ìˆ˜ì‹­ì–µ ë²¡í„° |
| **ì‹œì‘** | pip install | ê³„ì • ìƒì„± í•„ìš” |
| **ë¹„ìš©** | ë¬´ë£Œ | Free tier + ìœ ë£Œ |
| **ìš´ì˜** | ì§ì ‘ ê´€ë¦¬ | ì™„ì „ ê´€ë¦¬í˜• |
| **ì‚¬ìš©ì²˜** | í”„ë¡œí† íƒ€ì…, ì†Œê·œëª¨ | ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ |
| **ì„±ì¥ ê²½ë¡œ** | ê°œë°œ â†’ Pinecone ì´ì „ | ë°”ë¡œ í”„ë¡œë•ì…˜ |
      `,
      keyPoints: [
        'Pineconeì€ ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ì— ìµœì í™”ëœ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤',
        'ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ ë©€í‹°í…Œë„ŒíŠ¸ êµ¬í˜„',
        'LangChain í†µí•©ìœ¼ë¡œ ë¹ ë¥¸ RAG êµ¬ì¶•',
      ],
      practiceGoal: 'Pineconeì„ í”„ë¡œë•ì…˜ ìˆ˜ì¤€ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 6: Day 2 ì¢…í•© í€´ì¦ˆ (20ë¶„)
    // ========================================
    createQuizTask('w5d2-quiz', 'Day 2 ì¢…í•© í€´ì¦ˆ', 20, {
      introduction: `
## í€´ì¦ˆ ì•ˆë‚´

Day 2ì—ì„œ í•™ìŠµí•œ ì„ë² ë”©ê³¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê°œë…ì„ í™•ì¸í•©ë‹ˆë‹¤.
ê° ë¬¸ì œë¥¼ ì‹ ì¤‘í•˜ê²Œ ì½ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
      `,
      questions: [
        {
          id: 'w5d2-q1',
          question: 'Word2Vecì˜ "King - Man + Woman = Queen" ì˜ˆì‹œê°€ ë³´ì—¬ì£¼ëŠ” ì„ë² ë”©ì˜ íŠ¹ì„±ì€?',
          options: [
            'í…ìŠ¤íŠ¸ ì••ì¶• íš¨ìœ¨ì„±',
            'ë²¡í„° ê³µê°„ì—ì„œì˜ ì˜ë¯¸ì  ì—°ì‚° ê°€ëŠ¥ì„±',
            'ë¬¸ë²• ì˜¤ë¥˜ ê²€ì¶œ ëŠ¥ë ¥',
            'ë‹¤êµ­ì–´ ë²ˆì—­ ì •í™•ë„',
          ],
          correctAnswer: 1,
          explanation: 'Word2Vec ì„ë² ë”©ì€ ë²¡í„° ê³µê°„ì—ì„œ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ì‚°ìˆ  ì—°ì‚°ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "King - Man + Woman â‰ˆ Queen"ì€ ì„±ë³„ ê´€ê³„ê°€ ë²¡í„° ì—°ì‚°ìœ¼ë¡œ í‘œí˜„ë¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.',
        },
        {
          id: 'w5d2-q2',
          question: 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì˜ ì¥ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ê²ƒì€?',
          options: [
            'ê³„ì‚°ì´ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë³´ë‹¤ ëŠë¦¼',
            'í…ìŠ¤íŠ¸ ê¸¸ì´(ë²¡í„° í¬ê¸°)ì— ì˜í–¥ë°›ì§€ ì•ŠìŒ',
            'ìŒìˆ˜ ê°’ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŒ',
            'ì •ìˆ˜ë§Œ ë°˜í™˜í•˜ì—¬ ë¹„êµê°€ ì‰¬ì›€',
          ],
          correctAnswer: 1,
          explanation: 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë²¡í„°ì˜ ë°©í–¥ë§Œ ë¹„êµí•˜ë¯€ë¡œ, í…ìŠ¤íŠ¸ ê¸¸ì´(ë²¡í„° í¬ê¸°)ì— ë¬´ê´€í•©ë‹ˆë‹¤. ì§§ì€ ë¬¸ì¥ê³¼ ê¸´ ë¬¸ì¥ë„ ì˜ë¯¸ê°€ ê°™ìœ¼ë©´ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.',
        },
        {
          id: 'w5d2-q3',
          question: 'HNSW ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ì˜ ì£¼ìš” íŠ¹ì§•ì€?',
          options: [
            'ì •í™•ë„ 100%, ê°€ì¥ ëŠë¦¼',
            'í´ëŸ¬ìŠ¤í„° ê¸°ë°˜, ì¬í•™ìŠµ í•„ìš”',
            'ê·¸ë˜í”„ ê¸°ë°˜, ë¹ ë¥´ê³  ë™ì  ì¶”ê°€ ê°€ëŠ¥',
            'ì••ì¶• ê¸°ë°˜, ë©”ëª¨ë¦¬ ìµœì†Œí™”',
          ],
          correctAnswer: 2,
          explanation: 'HNSW(Hierarchical Navigable Small World)ëŠ” ë‹¤ì¸µ ê·¸ë˜í”„ êµ¬ì¡°ë¡œ O(log n) ì‹œê°„ì— ê²€ìƒ‰í•˜ë©°, ìƒˆ ë²¡í„°ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        },
        {
          id: 'w5d2-q4',
          question: 'OpenAI text-embedding-3-smallê³¼ text-embedding-3-largeì˜ ì£¼ìš” ì°¨ì´ì ì€?',
          options: [
            'smallì€ ì˜ì–´ë§Œ, largeëŠ” ë‹¤êµ­ì–´ ì§€ì›',
            'smallì€ 1536ì°¨ì›, largeëŠ” 3072ì°¨ì›',
            'smallì€ ë¬´ë£Œ, largeëŠ” ìœ ë£Œ',
            'smallì€ CPU, largeëŠ” GPU í•„ìš”',
          ],
          correctAnswer: 1,
          explanation: 'text-embedding-3-smallì€ 1536ì°¨ì›, largeëŠ” 3072ì°¨ì› ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. largeëŠ” ë” ë†’ì€ í’ˆì§ˆì´ì§€ë§Œ ë¹„ìš©ì´ ì•½ 6.5ë°° ë†’ìŠµë‹ˆë‹¤.',
        },
        {
          id: 'w5d2-q5',
          question: 'Chromaì™€ Pineconeì˜ ê°€ì¥ í° ì°¨ì´ì ì€?',
          options: [
            'ChromaëŠ” Python, Pineconeì€ JavaScript',
            'ChromaëŠ” ë¡œì»¬/ì„ë² ë””ë“œ, Pineconeì€ ê´€ë¦¬í˜• í´ë¼ìš°ë“œ',
            'ChromaëŠ” HNSW, Pineconeì€ IVF ì‚¬ìš©',
            'ChromaëŠ” ìœ ë£Œ, Pineconeì€ ë¬´ë£Œ',
          ],
          correctAnswer: 1,
          explanation: 'ChromaëŠ” ë¡œì»¬/ì„ë² ë””ë“œë¡œ í”„ë¡œí† íƒ€ì´í•‘ì— ì í•©í•˜ê³ , Pineconeì€ ê´€ë¦¬í˜• í´ë¼ìš°ë“œë¡œ ëŒ€ê·œëª¨ í”„ë¡œë•ì…˜ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.',
        },
      ],
      keyPoints: [
        'Word2Vec: ë²¡í„° ê³µê°„ì—ì„œ ì˜ë¯¸ì  ì—°ì‚° ê°€ëŠ¥',
        'ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë²¡í„° í¬ê¸° ë¬´ê´€, ë°©í–¥ë§Œ ë¹„êµ',
        'HNSW: ê·¸ë˜í”„ ê¸°ë°˜, ë¹ ë¥´ê³  ë™ì  ì¶”ê°€ ê°€ëŠ¥',
      ],
      practiceGoal: 'Day 2 í•µì‹¬ ê°œë…ì„ ì´í•´í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤',
    }),

    // ========================================
    // Challenge: ì„ë² ë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ êµ¬ì¶• (60ë¶„)
    // ========================================
    createChallengeTask('w5d2-challenge', 'ì„ë² ë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ êµ¬ì¶•', 60, {
      introduction: `
## ì±Œë¦°ì§€ ì‹œë‚˜ë¦¬ì˜¤

ë‹¹ì‹ ì€ AI ìŠ¤íƒ€íŠ¸ì—…ì˜ ë°ì´í„° ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. íšŒì‚¬ëŠ” RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ë ¤ í•˜ëŠ”ë°,
ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ê³¼ ë²¡í„° DB ì¡°í•©ì´ ìµœì ì¸ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

**ìš”êµ¬ì‚¬í•­:**
1. 3ê°€ì§€ ì´ìƒì˜ ì„ë² ë”© ëª¨ë¸ ë¹„êµ
2. Chromaì™€ Pinecone ì„±ëŠ¥ ë¹„êµ
3. ë¹„ìš©, ì†ë„, ì •í™•ë„ ì¸¡ì •
4. ìµœì¢… ê¶Œì¥ ì‚¬í•­ ë„ì¶œ

---

## í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ê¸°ì¤€ |
|------|------|------|
| ì„ë² ë”© ëª¨ë¸ ë¹„êµ | 25ì  | 3ê°œ ì´ìƒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ |
| ë²¡í„° DB ë¹„êµ | 25ì  | Chroma, Pinecone ë™ì¼ ì¡°ê±´ ë¹„êµ |
| ì¸¡ì • ì§€í‘œ | 25ì  | ì†ë„, ì •í™•ë„, ë¹„ìš© ëª¨ë‘ ì¸¡ì • |
| ë¶„ì„ ë³´ê³ ì„œ | 25ì  | ê²°ê³¼ í•´ì„ ë° ê¶Œì¥ ì‚¬í•­ |

---

## í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹

\`\`\`python
# í•œêµ­ì–´ RAG í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°
SAMPLE_DOCUMENTS = [
    "ì¸ê³µì§€ëŠ¥(AI)ì€ ê¸°ê³„ê°€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” AIì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì…ë‹ˆë‹¤.",
    "ìì—°ì–´ì²˜ë¦¬(NLP)ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ë¶„ì•¼ì…ë‹ˆë‹¤.",
    "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ìœ¼ë¡œ LLMì˜ í™˜ê°ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤.",
    "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
    "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
    "TransformerëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ê¸°ë°˜ì˜ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.",
    "GPTëŠ” ìƒì„±í˜• ì‚¬ì „í•™ìŠµ Transformer ëª¨ë¸ì…ë‹ˆë‹¤.",
    "BERTëŠ” ì–‘ë°©í–¥ ì¸ì½”ë” ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤."
]

SAMPLE_QUERIES = [
    ("AI ê¸°ìˆ ì´ë€?", 0),  # ì •ë‹µ ì¸ë±ìŠ¤
    ("ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê´€ê³„ëŠ”?", 1),
    ("ì–¸ì–´ ëª¨ë¸ì˜ ì¢…ë¥˜ëŠ”?", 8),
    ("ê²€ìƒ‰ ê¸°ë°˜ AI ê¸°ìˆ ì€?", 4),
    ("ë²¡í„° ê²€ìƒ‰ì´ë€?", 5)
]
\`\`\`
      `,
      keyPoints: [
        '3ê°œ ì´ìƒì˜ ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ (OpenAI, ì˜¤í”ˆì†ŒìŠ¤ ë“±)',
        'Chromaì™€ Pineconeì—ì„œ ë™ì¼ ë°ì´í„°ë¡œ ë¹„êµ',
        'ê²€ìƒ‰ ì†ë„(ms), ì •í™•ë„(Top-1/Top-5), ë¹„ìš©($) ì¸¡ì •',
        'ê²°ê³¼ ë¶„ì„ ë° ìµœì¢… ê¶Œì¥ ì‚¬í•­ ë³´ê³ ì„œ ì‘ì„±',
      ],
      hints: [
        `**í”„ë¡œì íŠ¸ êµ¬ì¡°**: embedding-benchmark/ í´ë”ì— config.py, embeddings.py, vectorstores.py, benchmark.py, report.py, main.py êµ¬ì„±`,
        `**ì„ë² ë”© ëª¨ë¸ ë˜í¼**: OpenAIì™€ ë¡œì»¬ ëª¨ë¸(SentenceTransformer)ì„ í†µí•© ì¸í„°í˜ì´ìŠ¤ë¡œ ë˜í•‘í•˜ì—¬ embed(texts) í•¨ìˆ˜ë¡œ ì¼ê´€ëœ API ì œê³µ`,
        `**ë²¤ì¹˜ë§ˆí¬ ì¸¡ì • í•­ëª©**: ì„ë² ë”© ìƒì„± ì‹œê°„, ê²€ìƒ‰ ì‹œê°„, Top-1/Top-5 ì •í™•ë„, ë¹„ìš© ì¶”ì •`,
        `**ê¶Œì¥ ì¡°í•©**: í”„ë¡œí† íƒ€ì…ì€ text-embedding-3-small + Chroma, í”„ë¡œë•ì…˜ì€ Pinecone ì‚¬ìš©`,
      ],
    }),
  ],
}
