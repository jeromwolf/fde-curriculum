// Day 9: LLM Fine-tuning (QLoRA ì‹¤ìŠµ)

import type { Day } from '../../types'
import {
  createVideoTask,
  createReadingTask,
  createCodeTask,
  createQuizTask,
  createChallengeTask,
} from './types'

export const day9Finetuning: Day = {
  slug: 'finetuning',
  title: 'LLM Fine-tuning (QLoRA ì‹¤ìŠµ)',
  totalDuration: 300,
  tasks: [
    // ========================================
    // Task 1: Fine-tuning ì´í•´ (40ë¶„)
    // ========================================
    createReadingTask('w5d9-finetuning-overview', 'Fine-tuning ì´í•´: ì–¸ì œ, ì™œ í•„ìš”í•œê°€', 40, {
      introduction: `
## Fine-tuningì´ë€?

Fine-tuningì€ ì‚¬ì „ í•™ìŠµëœ LLMì„ íŠ¹ì • ë„ë©”ì¸/íƒœìŠ¤í¬ì— ë§ê²Œ **ì¶”ê°€ í•™ìŠµ**í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

> "ëª¨ë¸ì´ ì§€ì‹ì„ 'í•™ìŠµ'í•˜ì—¬ ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ ë‹µë³€"

---

## RAG vs Fine-tuning ë¹„êµ

### RAG (ìš°ë¦¬ê°€ ë°°ìš´ ê²ƒ)

\`\`\`
[ë¬¸ì„œ] â†’ ê²€ìƒ‰ â†’ [ê´€ë ¨ ë‚´ìš©] â†’ LLM â†’ ë‹µë³€

íŠ¹ì§•:
âœ… ë¹ ë¥¸ ì ìš© (ëª‡ ì‹œê°„)
âœ… ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‰¬ì›€
âœ… ë¹„ìš© ì €ë ´
âŒ ë§¤ë²ˆ ê²€ìƒ‰ í•„ìš”
âŒ ëª¨ë¸ ìì²´ëŠ” ë³€í•˜ì§€ ì•ŠìŒ
\`\`\`

### Fine-tuning (ì˜¤ëŠ˜ ë°°ìš¸ ê²ƒ)

\`\`\`
[ë°ì´í„°] â†’ í•™ìŠµ â†’ [ìƒˆë¡œìš´ ëª¨ë¸] â†’ ë‹µë³€

íŠ¹ì§•:
âœ… ëª¨ë¸ì´ ì§€ì‹ì„ "í•™ìŠµ"
âœ… ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ ë‹µë³€
âœ… ì¼ê´€ëœ ìŠ¤íƒ€ì¼/í†¤
âŒ í•™ìŠµ ì‹œê°„ í•„ìš”
âŒ ì—…ë°ì´íŠ¸ ì‹œ ì¬í•™ìŠµ
\`\`\`

---

## ì–¸ì œ ë­˜ ì¨ì•¼ í•˜ë‚˜?

### RAG ì¶”ì²œ

\`\`\`
âœ… ë¬¸ì„œê°€ ìì£¼ ì—…ë°ì´íŠ¸ë¨
âœ… ì¶œì²˜ í‘œì‹œê°€ ì¤‘ìš”í•¨
âœ… ë¹ ë¥¸ êµ¬ì¶•ì´ í•„ìš”í•¨
âœ… ìµœì‹  ì •ë³´ê°€ ì¤‘ìš”í•¨

ì˜ˆ: ë‰´ìŠ¤ ê²€ìƒ‰, ì‹¤ì‹œê°„ ì •ë³´
\`\`\`

### Fine-tuning ì¶”ì²œ

\`\`\`
âœ… ì¼ê´€ëœ ìŠ¤íƒ€ì¼/í†¤ í•„ìš”
âœ… íŠ¹ì • ë„ë©”ì¸ ì „ë¬¸ì„±
âœ… ê²€ìƒ‰ ì—†ì´ ë¹ ë¥¸ ì‘ë‹µ
âœ… ëª¨ë¸ í¬ê¸° ì¤„ì´ê¸° (ì‘ì€ ëª¨ë¸ë¡œ ì„±ëŠ¥ up)

ì˜ˆ: ê³ ê° ìƒë‹´ ë´‡, ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸
\`\`\`

### ìµœì  ì¡°í•©: RAG + Fine-tuning

\`\`\`
Fine-tuned ëª¨ë¸ + RAG
= ë„ë©”ì¸ ì§€ì‹ + ìµœì‹  ë¬¸ì„œ
= ìµœê°• ì¡°í•©! ğŸ’ª
\`\`\`

---

## Fine-tuning ë°©ì‹ ë¹„êµ

### 1. Full Fine-tuning

\`\`\`
ì „ì²´ íŒŒë¼ë¯¸í„° í•™ìŠµ

Qwen3-1.7B: 1.7B íŒŒë¼ë¯¸í„° ì „ì²´ í•™ìŠµ
- ë¹„ìš©: ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°
- ì‹œê°„: â±ï¸â±ï¸â±ï¸â±ï¸â±ï¸
- GPU: 40GB+ VRAM í•„ìš”
\`\`\`

### 2. LoRA (Low-Rank Adaptation)

\`\`\`
ì¼ë¶€ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ (1~5%)

ì›ë³¸ ëª¨ë¸ì€ ê·¸ëŒ€ë¡œ + Adapter ì¶”ê°€
- ë¹„ìš©: ğŸ’°ğŸ’°
- ì‹œê°„: â±ï¸â±ï¸
- GPU: 16GB VRAM
\`\`\`

### 3. QLoRA (Quantized LoRA) â­ ì˜¤ëŠ˜ ì‚¬ìš©

\`\`\`
ì–‘ìí™” + LoRA

4bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
- ë¹„ìš©: ğŸ’°
- ì‹œê°„: â±ï¸
- GPU: 8GB VRAM (ë˜ëŠ” Colab ë¬´ë£Œ)
\`\`\`

---

## QLoRA ì‘ë™ ì›ë¦¬

### ê°œë…

\`\`\`
[ì›ë³¸ ëª¨ë¸ - 16bit]
       â†“ ì–‘ìí™”
[ì–‘ìí™” ëª¨ë¸ - 4bit] (ê³ ì •, í•™ìŠµ ì•ˆí•¨)
       +
[LoRA Adapter] (ì´ê²ƒë§Œ í•™ìŠµ!)
       â†“
[ê²°ê³¼ ëª¨ë¸]
\`\`\`

### ë©”ëª¨ë¦¬ ë¹„êµ

| ë°©ì‹ | Qwen3-1.7B VRAM |
|------|-----------------|
| Full Fine-tuning | ~14GB |
| LoRA | ~8GB |
| **QLoRA** | **~4GB** |

### ì„±ëŠ¥

\`\`\`
Full Fine-tuning: 100% (ê¸°ì¤€)
LoRA:             ~98%
QLoRA:            ~95%

â†’ 5% ì„±ëŠ¥ ì†ì‹¤ë¡œ 75% ë©”ëª¨ë¦¬ ì ˆì•½!
\`\`\`

---

## ë¹„ìš©/ì‹œê°„ ì˜ˆìƒ

### Qwen3-1.7B QLoRA ê¸°ì¤€

| í•­ëª© | ë¡œì»¬ (RTX 4090) | Colab (ë¬´ë£Œ) |
|------|-----------------|--------------|
| VRAM | 4~6GB ì‚¬ìš© | 15GB (T4) |
| í•™ìŠµ ì‹œê°„ | 30ë¶„~1ì‹œê°„ | 1~2ì‹œê°„ |
| ë¹„ìš© | ì „ê¸°ì„¸ë§Œ | ë¬´ë£Œ |

### ë°ì´í„° ê·œëª¨ë³„

| ë°ì´í„° ìˆ˜ | í•™ìŠµ ì‹œê°„ | ê¶Œì¥ epoch |
|-----------|-----------|------------|
| 100ê°œ | 10ë¶„ | 3-5 |
| 500ê°œ | 30ë¶„ | 2-3 |
| 1,000ê°œ | 1ì‹œê°„ | 1-2 |
| 5,000ê°œ | 3ì‹œê°„ | 1 |
      `,
      keyPoints: [
        'RAG: ê²€ìƒ‰ ê¸°ë°˜, ë¹ ë¥¸ ì ìš©, ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‰¬ì›€',
        'Fine-tuning: ëª¨ë¸ í•™ìŠµ, ì¼ê´€ëœ ìŠ¤íƒ€ì¼, ê²€ìƒ‰ ë¶ˆí•„ìš”',
        'QLoRA: 4bit ì–‘ìí™” + LoRAë¡œ 8GB GPUì—ì„œ í•™ìŠµ ê°€ëŠ¥',
        'ìµœì  ì¡°í•©: Fine-tuned ëª¨ë¸ + RAG',
      ],
    }),

    // ========================================
    // Task 2: í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (35ë¶„)
    // ========================================
    createCodeTask('w5d9-data-preparation', 'í•™ìŠµ ë°ì´í„° ì¤€ë¹„: Instruction Format', 35, {
      introduction: `
## í•™ìŠµ ë°ì´í„° í˜•ì‹

Fine-tuningì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ **ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„°**ì…ë‹ˆë‹¤.

---

## Instruction Format

### ê¸°ë³¸ êµ¬ì¡°

\`\`\`json
{
  "instruction": "ì§ˆë¬¸ ë˜ëŠ” ì§€ì‹œ",
  "input": "ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)",
  "output": "ì›í•˜ëŠ” ë‹µë³€"
}
\`\`\`

### ì‹¤ì œ ì˜ˆì‹œ (ì—°ë§ì •ì‚°)

\`\`\`json
{
  "instruction": "ìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
  "input": "",
  "output": "ìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n- 1ëª…: 25ë§Œì›\\n- 2ëª…: 55ë§Œì› (25ë§Œì› + 30ë§Œì›)\\n- 3ëª… ì´ìƒ: 55ë§Œì› + 30ë§Œì› Ã— (ìë…€ìˆ˜-2)"
}
\`\`\`

---

## ë°ì´í„° ìˆ˜ì§‘ 4ê°€ì§€ ë°©ë²•

### ë°©ë²• 1: ìˆ˜ë™ ì‘ì„± (ê¶Œì¥ ì‹œì‘ì )

\`\`\`python
data = [
    {
        "instruction": "ì—°ë§ì •ì‚°ì´ ë­”ê°€ìš”?",
        "input": "",
        "output": "ì—°ë§ì •ì‚°ì€ ê·¼ë¡œìê°€ 1ë…„ê°„ ë‚©ë¶€í•œ ì†Œë“ì„¸ë¥¼..."
    },
    {
        "instruction": "ì‹ ìš©ì¹´ë“œ ì†Œë“ê³µì œ í•œë„ëŠ”?",
        "input": "",
        "output": "ì‹ ìš©ì¹´ë“œ ì†Œë“ê³µì œ í•œë„ëŠ” ì—°ê°„ 300ë§Œì›ì…ë‹ˆë‹¤..."
    },
    # ... ìµœì†Œ 50~100ê°œ ê¶Œì¥
]
\`\`\`

### ë°©ë²• 2: RAG ë‹µë³€ í™œìš©

\`\`\`python
# ê¸°ì¡´ RAGë¡œ ë‹µë³€ ìƒì„± â†’ ê²€ìˆ˜ â†’ í•™ìŠµ ë°ì´í„°
questions = ["ìë…€ì„¸ì•¡ê³µì œë€?", "ì˜ë£Œë¹„ ê³µì œ í•œë„ëŠ”?", ...]

for q in questions:
    answer = rag_chatbot(q)  # RAGë¡œ ë‹µë³€ ìƒì„±
    # ì‚¬ëŒì´ ê²€ìˆ˜ í›„ ì €ì¥
    data.append({"instruction": q, "output": answer})
\`\`\`

### ë°©ë²• 3: GPT/Claudeë¡œ ìƒì„±

\`\`\`python
prompt = """
ì—°ë§ì •ì‚° ê´€ë ¨ Q&A ë°ì´í„°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

í˜•ì‹:
Q: [ì§ˆë¬¸]
A: [ìƒì„¸í•œ ë‹µë³€]

ì£¼ì œ: ìë…€ì„¸ì•¡ê³µì œ

5ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
"""

# ìƒì„± í›„ ê²€ìˆ˜ í•„ìˆ˜!
\`\`\`

### ë°©ë²• 4: ê¸°ì¡´ FAQ í™œìš©

\`\`\`
êµ­ì„¸ì²­ FAQ, íšŒì‚¬ ë‚´ë¶€ Q&A ë“±
â†’ ì •ì œ í›„ í•™ìŠµ ë°ì´í„°ë¡œ ë³€í™˜
\`\`\`

### âš ï¸ ì£¼ì˜ì‚¬í•­

- ë°˜ë“œì‹œ ì‚¬ëŒì´ ê²€ìˆ˜!
- ì˜ëª»ëœ ì •ë³´ = ì˜ëª» í•™ìŠµ
- ë‹¤ì–‘í•œ í‘œí˜„ í¬í•¨ (ê°™ì€ ì§ˆë¬¸, ë‹¤ë¥¸ í‘œí˜„)

---

## ë°ì´í„°ì…‹ ì˜ˆì‹œ (JSON)

\`\`\`json
// tax_qa_dataset.json
[
  {
    "instruction": "ìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡ì´ ì–¼ë§ˆì¸ê°€ìš”?",
    "input": "",
    "output": "ìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. ê¸°ë³¸ ê³µì œ\\n- ìë…€ 1ëª…: 25ë§Œì›\\n- ìë…€ 2ëª…: 55ë§Œì›\\n- ìë…€ 3ëª… ì´ìƒ: 55ë§Œì› + 30ë§Œì› Ã— (ìë…€ìˆ˜-2)\\n\\n2. ì¶œì‚°Â·ì…ì–‘ ê³µì œ\\n- ì²«ì§¸: 30ë§Œì›\\n- ë‘˜ì§¸: 50ë§Œì›\\n- ì…‹ì§¸ ì´ìƒ: 70ë§Œì›"
  },
  {
    "instruction": "ì‹ ìš©ì¹´ë“œ ì†Œë“ê³µì œ ë°›ìœ¼ë ¤ë©´ ì–¼ë§ˆë‚˜ ì¨ì•¼ í•˜ë‚˜ìš”?",
    "input": "",
    "output": "ì‹ ìš©ì¹´ë“œ ì†Œë“ê³µì œë¥¼ ë°›ìœ¼ë ¤ë©´ ì´ê¸‰ì—¬ì˜ 25%ë¥¼ ì´ˆê³¼í•˜ì—¬ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\\n\\nì˜ˆì‹œ:\\n- ì—°ë´‰ 4,000ë§Œì›ì¸ ê²½ìš°\\n- ìµœì†Œ ì‚¬ìš©ì•¡: 1,000ë§Œì› (25%)\\n- 1,000ë§Œì› ì´ˆê³¼ë¶„ë¶€í„° ê³µì œ ì ìš©\\n\\nê³µì œìœ¨:\\n- ì‹ ìš©ì¹´ë“œ: 15%\\n- ì²´í¬ì¹´ë“œ/í˜„ê¸ˆì˜ìˆ˜ì¦: 30%\\n- ì „í†µì‹œì¥/ëŒ€ì¤‘êµí†µ: 40%"
  },
  {
    "instruction": "ì˜ë£Œë¹„ ì„¸ì•¡ê³µì œ ëŒ€ìƒì€ ëˆ„êµ¬ì¸ê°€ìš”?",
    "input": "",
    "output": "ì˜ë£Œë¹„ ì„¸ì•¡ê³µì œ ëŒ€ìƒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1. ë³¸ì¸\\n2. ë°°ìš°ì\\n3. ë¶€ì–‘ê°€ì¡± (ë‚˜ì´/ì†Œë“ ì œí•œ ì—†ìŒ)\\n\\nê³µì œìœ¨: 15%\\n\\ní•œë„:\\n- ë³¸ì¸, 65ì„¸ ì´ìƒ, ì¥ì• ì¸: í•œë„ ì—†ìŒ\\n- ê·¸ ì™¸: ì—° 700ë§Œì›"
  }
]
\`\`\`

---

## ë°ì´í„° í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ì¢‹ì€ ë°ì´í„°

\`\`\`
- ëª…í™•í•œ ì§ˆë¬¸ê³¼ ë‹µë³€
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ í¬í•¨
- ì¼ê´€ëœ ë‹µë³€ ìŠ¤íƒ€ì¼
- ë‹¤ì–‘í•œ ì§ˆë¬¸ í‘œí˜„
- ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ ë°˜ì˜
\`\`\`

### âŒ ë‚˜ìœ ë°ì´í„°

\`\`\`
- ëª¨í˜¸í•œ ë‹µë³€ ("ìƒí™©ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤")
- ì˜¤ë˜ëœ ì •ë³´ (2024ë…„ ì´ì „ ê¸°ì¤€)
- ì¤‘ë³µ ë°ì´í„°
- ë„ˆë¬´ ì§§ì€ ë‹µë³€
- ì˜¤ë¥˜ê°€ ìˆëŠ” ì •ë³´
\`\`\`

### ê¶Œì¥ ë°ì´í„° ìˆ˜

| ëª©ì  | ìµœì†Œ | ê¶Œì¥ |
|------|------|------|
| í…ŒìŠ¤íŠ¸/í•™ìŠµ | 50ê°œ | 100ê°œ |
| ì‹¤ë¬´ ì ìš© | 200ê°œ | 500ê°œ+ |
| ê³ í’ˆì§ˆ ëª¨ë¸ | 1,000ê°œ | 5,000ê°œ+ |

---

## HuggingFace Datasets ë¡œë“œ

\`\`\`python
from datasets import Dataset
import json

# JSON ë¡œë“œ
with open("tax_qa_dataset.json", "r", encoding="utf-8") as f:
    data = json.load(f)

# Dataset ë³€í™˜
dataset = Dataset.from_list(data)

# í™•ì¸
print(dataset)
# Dataset({
#     features: ['instruction', 'input', 'output'],
#     num_rows: 100
# })

# Train/Test ë¶„í• 
dataset = dataset.train_test_split(test_size=0.1)
print(f"Train: {len(dataset['train'])}, Test: {len(dataset['test'])}")
\`\`\`
      `,
      keyPoints: [
        'Instruction Format: instruction + input + output',
        'ë°ì´í„° ìˆ˜ì§‘: ìˆ˜ë™ ì‘ì„±, RAG í™œìš©, GPT ìƒì„±, FAQ',
        'í’ˆì§ˆ > ìˆ˜ëŸ‰: 100ê°œ ê³ í’ˆì§ˆ ë°ì´í„°ê°€ 1000ê°œ ì €í’ˆì§ˆë³´ë‹¤ ë‚˜ìŒ',
        'ë°˜ë“œì‹œ ì‚¬ëŒì´ ê²€ìˆ˜: ì˜ëª»ëœ ì •ë³´ = ì˜ëª»ëœ í•™ìŠµ',
      ],
      practiceGoal: 'ìì‹ ì˜ ë„ë©”ì¸ì— ë§ëŠ” í•™ìŠµ ë°ì´í„°ì…‹ì„ ì„¤ê³„í•˜ê³  ìˆ˜ì§‘í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 3: QLoRA ì‹¤ìŠµ (50ë¶„)
    // ========================================
    createCodeTask('w5d9-qlora-training', 'QLoRA ì‹¤ìŠµ: 4bit ì–‘ìí™” í•™ìŠµ', 50, {
      introduction: `
## í™˜ê²½ ì„ íƒ

### ì˜µì…˜ 1: ë¡œì»¬ GPU

\`\`\`
í•„ìš” ì‚¬ì–‘:
- NVIDIA GPU (RTX 3060 ì´ìƒ)
- VRAM 8GB ì´ìƒ
- RAM 16GB ì´ìƒ

ì¥ì : ììœ ë¡œìš´ ì‹¤í—˜
ë‹¨ì : ì´ˆê¸° íˆ¬ì í•„ìš”
\`\`\`

### ì˜µì…˜ 2: Google Colab (ë¬´ë£Œ) â­ ì¶”ì²œ

\`\`\`
ì œê³µ ì‚¬ì–‘:
- T4 GPU (15GB VRAM)
- RAM 12GB

ì¥ì : ë¬´ë£Œ, ë°”ë¡œ ì‹œì‘
ë‹¨ì : ì‹œê°„ ì œí•œ, ì„¸ì…˜ ëŠê¹€

ì„¤ì •: ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > T4 GPU
\`\`\`

---

## íŒ¨í‚¤ì§€ ì„¤ì¹˜

\`\`\`bash
pip install transformers datasets peft accelerate bitsandbytes trl
\`\`\`

| íŒ¨í‚¤ì§€ | ìš©ë„ | í¬ê¸° |
|--------|------|------|
| **transformers** | ëª¨ë¸ ë¡œë“œ/í•™ìŠµ | ~50MB |
| **datasets** | ë°ì´í„°ì…‹ ì²˜ë¦¬ | ~20MB |
| **peft** | LoRA/QLoRA | ~5MB |
| **accelerate** | í•™ìŠµ ê°€ì† | ~10MB |
| **bitsandbytes** | ì–‘ìí™” | ~20MB |
| **trl** | í•™ìŠµ íŠ¸ë ˆì´ë„ˆ | ~10MB |

---

## Step 1: ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)

\`\`\`python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                    # 4bit ì–‘ìí™”
    bnb_4bit_quant_type="nf4",            # ì–‘ìí™” íƒ€ì…
    bnb_4bit_compute_dtype=torch.bfloat16 # ì—°ì‚° íƒ€ì…
)

# ëª¨ë¸ ë¡œë“œ
model_name = "Qwen/Qwen3-1.7B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
\`\`\`

---

## Step 2: LoRA ì„¤ì •

\`\`\`python
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ëª¨ë¸ ì¤€ë¹„ (ì–‘ìí™”ëœ ëª¨ë¸ìš©)
model = prepare_model_for_kbit_training(model)

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,                    # LoRA rank (í´ìˆ˜ë¡ ì„±ëŠ¥â†‘, ë©”ëª¨ë¦¬â†‘)
    lora_alpha=32,           # LoRA alpha
    lora_dropout=0.05,       # Dropout
    bias="none",             # Bias í•™ìŠµ ì—¬ë¶€
    task_type="CAUSAL_LM",   # íƒœìŠ¤í¬ íƒ€ì…
    target_modules=[         # ì ìš©í•  ë ˆì´ì–´
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)

# LoRA ì ìš©
model = get_peft_model(model, lora_config)

# í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° í™•ì¸
model.print_trainable_parameters()
# trainable params: 13,631,488 || all params: 1,777,565,696
# trainable%: 0.77%  â† 1% ë¯¸ë§Œë§Œ í•™ìŠµ!
\`\`\`

---

## Step 3: í”„ë¡¬í”„íŠ¸ í¬ë§·

\`\`\`python
def format_prompt(example):
    """í•™ìŠµ ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

    if example['input']:
        prompt = f"""### ì§€ì‹œ:
{example['instruction']}

### ì…ë ¥:
{example['input']}

### ë‹µë³€:
{example['output']}"""
    else:
        prompt = f"""### ì§€ì‹œ:
{example['instruction']}

### ë‹µë³€:
{example['output']}"""

    return {"text": prompt}

# ë°ì´í„°ì…‹ì— ì ìš©
formatted_dataset = dataset.map(format_prompt)

# í™•ì¸
print(formatted_dataset['train'][0]['text'])
\`\`\`

---

## Step 4: í•™ìŠµ ì„¤ì •

\`\`\`python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./tax-qwen-lora",      # ì €ì¥ ê²½ë¡œ
    num_train_epochs=3,                 # ì—í­ ìˆ˜
    per_device_train_batch_size=4,      # ë°°ì¹˜ í¬ê¸°
    gradient_accumulation_steps=4,      # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
    learning_rate=2e-4,                 # í•™ìŠµë¥ 
    warmup_steps=100,                   # ì›œì—…
    logging_steps=10,                   # ë¡œê¹… ì£¼ê¸°
    save_strategy="epoch",              # ì €ì¥ ì „ëµ
    fp16=True,                          # í˜¼í•© ì •ë°€ë„
    optim="paged_adamw_8bit",          # ì˜µí‹°ë§ˆì´ì €
    report_to="none",                   # ë¦¬í¬íŠ¸ (wandb ë“±)
)
\`\`\`

---

## Step 5: í•™ìŠµ ì‹¤í–‰

\`\`\`python
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    train_dataset=formatted_dataset["train"],
    eval_dataset=formatted_dataset["test"],
    tokenizer=tokenizer,
    args=training_args,
    dataset_text_field="text",
    max_seq_length=1024,
    packing=False,
)

# í•™ìŠµ ì‹œì‘!
print("ğŸš€ í•™ìŠµ ì‹œì‘...")
trainer.train()
print("âœ… í•™ìŠµ ì™„ë£Œ!")
\`\`\`

### í•™ìŠµ ë¡œê·¸ ì˜ˆì‹œ

\`\`\`
Step 10/300: loss=2.45
Step 20/300: loss=1.89
Step 30/300: loss=1.52
...
Step 300/300: loss=0.78

Training completed! âœ…
\`\`\`

---

## Step 6: ëª¨ë¸ ì €ì¥

\`\`\`python
# LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥ (ìˆ˜ì‹­ MB)
model.save_pretrained("./tax-qwen-lora")
tokenizer.save_pretrained("./tax-qwen-lora")

print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
# ./tax-qwen-lora/
# â”œâ”€â”€ adapter_config.json
# â”œâ”€â”€ adapter_model.safetensors  â† ~50MB
# â””â”€â”€ tokenizer files
\`\`\`

### ì „ì²´ ëª¨ë¸ë¡œ ë³‘í•© (ì„ íƒ)

\`\`\`python
from peft import PeftModel

# ë³‘í•©ëœ ëª¨ë¸ ìƒì„±
merged_model = model.merge_and_unload()

# ì €ì¥
merged_model.save_pretrained("./tax-qwen-merged")
# í¬ê¸°: ~3.4GB (ì „ì²´ ëª¨ë¸)
\`\`\`
      `,
      keyPoints: [
        'BitsAndBytesConfig: 4bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½',
        'LoraConfig: r=16, target_modulesë¡œ í•™ìŠµ ëŒ€ìƒ ì§€ì •',
        'SFTTrainer: Supervised Fine-Tuning ì „ìš© íŠ¸ë ˆì´ë„ˆ',
        'í•™ìŠµ íŒŒë¼ë¯¸í„° 1% ë¯¸ë§Œìœ¼ë¡œ ì „ì²´ ëª¨ë¸ ì„±ëŠ¥ 95% ë‹¬ì„±',
      ],
      practiceGoal: 'QLoRAë¡œ ì§ì ‘ ëª¨ë¸ì„ Fine-tuningí•˜ê³  ì €ì¥í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 4: í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (25ë¶„)
    // ========================================
    createCodeTask('w5d9-model-testing', 'í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ì™€ í‰ê°€', 25, {
      introduction: `
## ì¶”ë¡  í…ŒìŠ¤íŠ¸

### ë‹µë³€ ìƒì„± í•¨ìˆ˜

\`\`\`python
def generate_answer(instruction, model, tokenizer):
    """Fine-tuned ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±"""
    prompt = f"""### ì§€ì‹œ:
{instruction}

### ë‹µë³€:
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("### ë‹µë³€:")[-1].strip()

# í…ŒìŠ¤íŠ¸
question = "ìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡ì´ ì–¼ë§ˆì¸ê°€ìš”?"
answer = generate_answer(question, model, tokenizer)
print(answer)
\`\`\`

---

## í•™ìŠµ ì „í›„ ë¹„êµ

### Before (ê¸°ë³¸ Qwen3-1.7B)

\`\`\`
Q: ìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡ì´ ì–¼ë§ˆì¸ê°€ìš”?

A: ìë…€ì„¸ì•¡ê³µì œì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
   ì„¸ì•¡ê³µì œëŠ” ë‚©ë¶€í•  ì„¸ê¸ˆì—ì„œ ì§ì ‘ ì°¨ê°ë˜ëŠ”...
   (ì¼ë°˜ì ì¸ ì„¤ëª…, êµ¬ì²´ì  ê¸ˆì•¡ ì—†ìŒ)
\`\`\`

### After (Fine-tuned)

\`\`\`
Q: ìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡ì´ ì–¼ë§ˆì¸ê°€ìš”?

A: ìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

   1. ê¸°ë³¸ ê³µì œ
   - ìë…€ 1ëª…: 25ë§Œì›
   - ìë…€ 2ëª…: 55ë§Œì›
   - ìë…€ 3ëª… ì´ìƒ: 55ë§Œì› + 30ë§Œì› Ã— (ìë…€ìˆ˜-2)

   2. ì¶œì‚°Â·ì…ì–‘ ê³µì œ
   - ì²«ì§¸: 30ë§Œì›
   - ë‘˜ì§¸: 50ë§Œì›
   - ì…‹ì§¸ ì´ìƒ: 70ë§Œì›

   (í•™ìŠµëœ ì •í™•í•œ ì •ë³´!)
\`\`\`

---

## ì •ëŸ‰ì  í‰ê°€

### í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ í‰ê°€

\`\`\`python
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)

results = []
for example in test_dataset:
    prediction = generate_answer(example['instruction'], model, tokenizer)
    reference = example['output']

    scores = scorer.score(reference, prediction)
    results.append({
        'rouge1': scores['rouge1'].fmeasure,
        'rougeL': scores['rougeL'].fmeasure
    })

avg_rouge1 = sum(r['rouge1'] for r in results) / len(results)
avg_rougeL = sum(r['rougeL'] for r in results) / len(results)

print(f"ROUGE-1: {avg_rouge1:.3f}")
print(f"ROUGE-L: {avg_rougeL:.3f}")
\`\`\`

### ì„±ëŠ¥ ë¹„êµí‘œ

| ì§€í‘œ | ê¸°ë³¸ ëª¨ë¸ | Fine-tuned |
|------|----------|------------|
| ì •í™•ë„ | 72% | **89%** |
| ë‹µë³€ ì¼ê´€ì„± | 65% | **92%** |
| ì‘ë‹µ ì†ë„ | 1.2ì´ˆ | 1.1ì´ˆ |
| ROUGE-1 | 0.45 | **0.78** |
| ROUGE-L | 0.38 | **0.72** |
      `,
      keyPoints: [
        'generate() í•¨ìˆ˜ë¡œ Fine-tuned ëª¨ë¸ ì¶”ë¡ ',
        'í•™ìŠµ ì „í›„ ë¹„êµë¡œ ê°œì„  íš¨ê³¼ í™•ì¸',
        'ROUGE ì ìˆ˜ë¡œ ì •ëŸ‰ì  í‰ê°€',
        'Fine-tuning í›„ ì •í™•ë„ 17% í–¥ìƒ ê°€ëŠ¥',
      ],
      practiceGoal: 'Fine-tuned ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 5: Fine-tuned + RAG í†µí•© (40ë¶„)
    // ========================================
    createCodeTask('w5d9-finetuned-rag', 'Fine-tuned ëª¨ë¸ + RAG í†µí•©', 40, {
      introduction: `
## ì™œ Fine-tuned + RAGì¸ê°€?

### Fine-tuningë§Œ

\`\`\`
âœ… í•™ìŠµëœ ë‚´ìš© ë¹ ë¥¸ ë‹µë³€
âŒ í•™ìŠµ ì•ˆ ëœ ë‚´ìš© ë‹µë³€ ë¶ˆê°€
âŒ ìµœì‹  ì •ë³´ ë°˜ì˜ ì–´ë ¤ì›€
\`\`\`

### RAGë§Œ

\`\`\`
âœ… ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
âŒ ì¼ë°˜ì ì¸ LLM ì‚¬ìš©
âŒ ë„ë©”ì¸ ì§€ì‹ ë¶€ì¡±
\`\`\`

### Fine-tuned + RAG â­

\`\`\`
âœ… ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ (Fine-tuning)
âœ… ìµœì‹  ë¬¸ì„œ ê²€ìƒ‰ (RAG)
âœ… ì •í™•í•˜ê³  ì¼ê´€ëœ ë‹µë³€
= ìµœê°• ì¡°í•©!
\`\`\`

---

## Ollamaì— ì»¤ìŠ¤í…€ ëª¨ë¸ ë“±ë¡

### Step 1: GGUF ë³€í™˜

\`\`\`bash
# llama.cpp ë„êµ¬ ì‚¬ìš©
python convert_hf_to_gguf.py ./tax-qwen-merged \\
    --outfile tax-qwen.gguf \\
    --outtype q4_k_m
\`\`\`

### Step 2: Modelfile ì‘ì„±

\`\`\`dockerfile
# Modelfile
FROM ./tax-qwen.gguf

TEMPLATE """{{ .Prompt }}"""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER stop "### ì§€ì‹œ:"

SYSTEM """ë‹¹ì‹ ì€ ì—°ë§ì •ì‚° ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
\`\`\`

### Step 3: Ollamaì— ë“±ë¡

\`\`\`bash
ollama create tax-qwen -f Modelfile
ollama list  # tax-qwen í™•ì¸
\`\`\`

---

## RAGì— ì ìš©

### ëª¨ë¸ êµì²´ (í•œ ì¤„ë§Œ ë³€ê²½!)

\`\`\`python
from langchain_community.llms import Ollama

# Before (ì¼ë°˜ ëª¨ë¸)
# MODEL_NAME = "qwen2.5:7b"

# After (ì»¤ìŠ¤í…€ ëª¨ë¸!)
MODEL_NAME = "tax-qwen"

# ë‚˜ë¨¸ì§€ RAG ì½”ë“œëŠ” ë™ì¼
llm = Ollama(model=MODEL_NAME)
\`\`\`

### ì „ì²´ RAG íŒŒì´í”„ë¼ì¸

\`\`\`python
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# ì„ë² ë”© & ë²¡í„°ìŠ¤í† ì–´
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embeddings)

# Fine-tuned ëª¨ë¸ë¡œ RAG ì²´ì¸
llm = Ollama(model="tax-qwen")  # ì»¤ìŠ¤í…€ ëª¨ë¸!

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
)

# ì§ˆë¬¸
response = qa_chain.invoke("2024ë…„ ìë…€ì„¸ì•¡ê³µì œ ë³€ê²½ì‚¬í•­ì€?")
print(response)
\`\`\`

---

## í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ

### ìƒí™©ë³„ ëª¨ë¸ ì„ íƒ

\`\`\`python
def get_model(question: str):
    """ì§ˆë¬¸ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ"""

    # ì„¸ê¸ˆ ê´€ë ¨ â†’ ì»¤ìŠ¤í…€ ëª¨ë¸
    tax_keywords = ["ê³µì œ", "ì„¸ì•¡", "ì—°ë§ì •ì‚°", "ì„¸ê¸ˆ", "ì†Œë“"]
    if any(kw in question for kw in tax_keywords):
        return Ollama(model="tax-qwen")

    # ì¼ë°˜ ì§ˆë¬¸ â†’ ë²”ìš© ëª¨ë¸
    return Ollama(model="qwen2.5:7b")

# ì‚¬ìš©
llm = get_model(user_question)
response = qa_chain.invoke(user_question)
\`\`\`

### ì•™ìƒë¸” (ê³ ê¸‰)

\`\`\`python
def ensemble_answer(question: str):
    """ë‘ ëª¨ë¸ì˜ ë‹µë³€ì„ ì¢…í•©"""

    # ë‘ ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
    custom_llm = Ollama(model="tax-qwen")
    general_llm = Ollama(model="qwen2.5:7b")

    answer1 = custom_llm.invoke(question)
    answer2 = general_llm.invoke(question)

    # ì¢…í•© í”„ë¡¬í”„íŠ¸
    combine_prompt = f"""
ë‘ AIì˜ ë‹µë³€ì„ ê²€í† í•˜ê³  ê°€ì¥ ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ë‹µë³€ 1: {answer1}
ë‹µë³€ 2: {answer2}

ì¢…í•© ë‹µë³€:
"""

    return general_llm.invoke(combine_prompt)
\`\`\`
      `,
      keyPoints: [
        'Fine-tuned + RAG = ë„ë©”ì¸ ì§€ì‹ + ìµœì‹  ë¬¸ì„œ',
        'GGUF ë³€í™˜ â†’ Modelfile â†’ Ollama ë“±ë¡',
        'ê¸°ì¡´ RAG ì½”ë“œì—ì„œ ëª¨ë¸ëª…ë§Œ ë³€ê²½í•˜ë©´ ë',
        'í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ: ì§ˆë¬¸ë³„ ìµœì  ëª¨ë¸ ì„ íƒ',
      ],
      practiceGoal: 'Fine-tuned ëª¨ë¸ì„ Ollamaì— ë“±ë¡í•˜ê³  RAG íŒŒì´í”„ë¼ì¸ì— í†µí•©í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 6: HuggingFace ë°°í¬ (35ë¶„)
    // ========================================
    createCodeTask('w5d9-huggingface-deploy', 'HuggingFace Hub ë°°í¬ì™€ ê³µìœ ', 35, {
      introduction: `
## HuggingFace Hub ì†Œê°œ

HuggingFace HubëŠ” **ëª¨ë¸ ê³µìœ  í”Œë«í¼**ìœ¼ë¡œ, ì „ ì„¸ê³„ ê°œë°œìë“¤ì´ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ê³µìœ í•©ë‹ˆë‹¤.

> "ë‚´ê°€ ë§Œë“  Fine-tuned ëª¨ë¸ì„ ì„¸ìƒê³¼ ê³µìœ !"

---

## ê³„ì • ì¤€ë¹„

### 1. íšŒì›ê°€ì…

\`\`\`
https://huggingface.co/join
\`\`\`

### 2. Access Token ë°œê¸‰

\`\`\`
Settings > Access Tokens > New token
- Name: tax-qwen-upload
- Type: Write
\`\`\`

### 3. ë¡œê·¸ì¸

\`\`\`python
from huggingface_hub import login

login(token="hf_your_token_here")
# ë˜ëŠ” CLI: huggingface-cli login
\`\`\`

---

## ëª¨ë¸ ì—…ë¡œë“œ

### LoRA Adapter ì—…ë¡œë“œ

\`\`\`python
from huggingface_hub import HfApi

api = HfApi()

# ë ˆí¬ì§€í† ë¦¬ ìƒì„±
api.create_repo(
    repo_id="your-username/tax-qwen-lora",
    repo_type="model",
    private=False  # ê³µê°œ ì—¬ë¶€
)

# ì—…ë¡œë“œ
api.upload_folder(
    folder_path="./tax-qwen-lora",
    repo_id="your-username/tax-qwen-lora",
    repo_type="model"
)

print("ì—…ë¡œë“œ ì™„ë£Œ!")
print("https://huggingface.co/your-username/tax-qwen-lora")
\`\`\`

---

## Model Card ì‘ì„±

### README.md (í•„ìˆ˜!)

\`\`\`markdown
---
license: apache-2.0
language:
- ko
library_name: peft
base_model: Qwen/Qwen3-1.7B
tags:
- tax
- korean
- lora
- qlora
---

# Tax-Qwen-LoRA ğŸ‡°ğŸ‡·

ì—°ë§ì •ì‚° ì „ë¬¸ í•œêµ­ì–´ LLM (QLoRA Fine-tuned)

## ëª¨ë¸ ì„¤ëª…

Qwen3-1.7Bë¥¼ ì—°ë§ì •ì‚° Q&A ë°ì´í„°ë¡œ QLoRA íŒŒì¸íŠœë‹í•œ ëª¨ë¸ì…ë‹ˆë‹¤.

## ì‚¬ìš©ë²•

\\\`\\\`\\\`python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-1.7B")
model = PeftModel.from_pretrained(base_model, "your-username/tax-qwen-lora")
\\\`\\\`\\\`

## í•™ìŠµ ë°ì´í„°

- ì—°ë§ì •ì‚° Q&A 500ê°œ
- ì¶œì²˜: êµ­ì„¸ì²­ ìë£Œ ê¸°ë°˜

## ì„±ëŠ¥

| ì§€í‘œ | ì ìˆ˜ |
|------|------|
| ì •í™•ë„ | 89% |
| ì¼ê´€ì„± | 92% |

## ë¼ì´ì„ ìŠ¤

Apache 2.0 (ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥)
\`\`\`

---

## ë‹¤ë¥¸ ì‚¬ëŒì´ ì‚¬ìš©í•˜ê¸°

### ì‚¬ìš© ë°©ë²• ì•ˆë‚´

\`\`\`python
# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
# pip install transformers peft accelerate

# 2. ëª¨ë¸ ë¡œë“œ
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-1.7B",
    device_map="auto"
)
model = PeftModel.from_pretrained(
    base_model,
    "your-username/tax-qwen-lora"
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-1.7B")

# 3. ì‚¬ìš©
prompt = "### ì§€ì‹œ:\\nìë…€ì„¸ì•¡ê³µì œ ê¸ˆì•¡?\\n\\n### ë‹µë³€:\\n"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0]))
\`\`\`

---

## ë¼ì´ì„ ìŠ¤ ê³ ë ¤ì‚¬í•­

### ë² ì´ìŠ¤ ëª¨ë¸ ë¼ì´ì„ ìŠ¤ í™•ì¸

\`\`\`
Qwen3: Apache 2.0 âœ…
- ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥
- ìˆ˜ì •/ë°°í¬ ê°€ëŠ¥
- ì €ì‘ê¶Œ í‘œì‹œ í•„ìš”
\`\`\`

### ë‚´ ëª¨ë¸ ë¼ì´ì„ ìŠ¤ ì„ íƒ

| ë¼ì´ì„ ìŠ¤ | ìƒì—…ì  ì‚¬ìš© | ì„¤ëª… |
|----------|------------|------|
| Apache 2.0 | âœ… ê°€ëŠ¥ | ê°€ì¥ ë„ë¦¬ ì‚¬ìš© |
| MIT | âœ… ê°€ëŠ¥ | ê°€ì¥ ììœ ë¡œì›€ |
| CC-BY-NC | âŒ ë¶ˆê°€ | ë¹„ìƒì—…ì ë§Œ í—ˆìš© |

### âš ï¸ ì£¼ì˜ì‚¬í•­

\`\`\`
âš ï¸ í•™ìŠµ ë°ì´í„° ì €ì‘ê¶Œ í™•ì¸
âš ï¸ ë¯¼ê°í•œ ì •ë³´ í¬í•¨ ì—¬ë¶€
âš ï¸ ë² ì´ìŠ¤ ëª¨ë¸ ë¼ì´ì„ ìŠ¤ ì¤€ìˆ˜
\`\`\`
      `,
      keyPoints: [
        'HuggingFace Hub: ëª¨ë¸ ê³µìœ  í”Œë«í¼',
        'Access Token (Write)ìœ¼ë¡œ ì—…ë¡œë“œ ê¶Œí•œ',
        'Model Card: README.mdë¡œ ëª¨ë¸ ì„¤ëª… í•„ìˆ˜',
        'ë¼ì´ì„ ìŠ¤: ë² ì´ìŠ¤ ëª¨ë¸ + í•™ìŠµ ë°ì´í„° ì €ì‘ê¶Œ í™•ì¸',
      ],
      practiceGoal: 'Fine-tuned ëª¨ë¸ì„ HuggingFace Hubì— ì—…ë¡œë“œí•˜ê³  ê³µìœ í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 7: íŠ¸ëŸ¬ë¸”ìŠˆíŒ… & í€´ì¦ˆ (25ë¶„)
    // ========================================
    createQuizTask('w5d9-troubleshooting-quiz', 'Fine-tuning íŠ¸ëŸ¬ë¸”ìŠˆíŒ… & í€´ì¦ˆ', 25, {
      introduction: `
## ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ í•´ê²°ë²•

### 1. CUDA Out of Memory

\`\`\`
âŒ ì˜¤ë¥˜: CUDA out of memory

âœ… í•´ê²°:
1. batch_size ì¤„ì´ê¸°: 4 â†’ 2 â†’ 1
2. gradient_accumulation ëŠ˜ë¦¬ê¸°
3. max_seq_length ì¤„ì´ê¸°: 1024 â†’ 512
4. gradient_checkpointing=True
\`\`\`

### 2. í•™ìŠµì´ ì•ˆ ë¨ (loss ì•ˆ ì¤„ì–´ë“¦)

\`\`\`
âŒ ë¬¸ì œ: lossê°€ ê³„ì† ë†’ìŒ

âœ… í•´ê²°:
1. learning_rate ì¡°ì •: 2e-4 â†’ 1e-4
2. ë°ì´í„° í˜•ì‹ í™•ì¸
3. í† í¬ë‚˜ì´ì € ì„¤ì • í™•ì¸
\`\`\`

### 3. Colab ì„¸ì…˜ ëŠê¹€

\`\`\`
âŒ ë¬¸ì œ: ì¤‘ê°„ì— ì—°ê²° ëŠê¹€

âœ… í•´ê²°:
1. Google Drive ë§ˆìš´íŠ¸
   from google.colab import drive
   drive.mount('/content/drive')

2. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
   save_strategy="steps"
   save_steps=100
\`\`\`

### 4. ê³¼ì í•© (Overfitting)

\`\`\`
âŒ ë¬¸ì œ: Train lossëŠ” ë‚®ì€ë° Test lossê°€ ë†’ìŒ

âœ… í•´ê²°:
1. LoRA rank ë‚®ì¶”ê¸° (r=8)
2. Dropout ë†’ì´ê¸° (0.1)
3. Epoch ì¤„ì´ê¸° (1-2)
4. ë°ì´í„° ë‹¤ì–‘ì„± ëŠ˜ë¦¬ê¸°
\`\`\`

---

## ì „ì²´ íŒŒì´í”„ë¼ì¸ ì •ë¦¬

\`\`\`
[1. ë°ì´í„° ì¤€ë¹„]
    â”‚
    â–¼ Instruction Format JSON
    â”‚
[2. ëª¨ë¸ ë¡œë“œ]
    â”‚
    â–¼ BitsAndBytesConfig (4bit)
    â”‚
[3. LoRA ì„¤ì •]
    â”‚
    â–¼ LoraConfig + get_peft_model
    â”‚
[4. í•™ìŠµ]
    â”‚
    â–¼ SFTTrainer.train()
    â”‚
[5. ì €ì¥]
    â”‚
    â–¼ model.save_pretrained()
    â”‚
[6. í…ŒìŠ¤íŠ¸]
    â”‚
    â–¼ generate_answer()
    â”‚
[7. RAG í†µí•©]
    â”‚
    â–¼ Ollama ë“±ë¡ ë˜ëŠ” ì§ì ‘ ì‚¬ìš©
    â”‚
[8. ë°°í¬]
    â”‚
    â–¼ HuggingFace Hub
\`\`\`
      `,
      questions: [
        {
          id: 'w5d9-q1',
          question: 'QLoRAì—ì„œ "Q"ëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?',
          options: [
            'Quality (í’ˆì§ˆ)',
            'Quantized (ì–‘ìí™”)',
            'Quick (ë¹ ë¥¸)',
            'Query (ì¿¼ë¦¬)',
          ],
          correctAnswer: 1,
          explanation: 'QLoRAì˜ QëŠ” Quantized(ì–‘ìí™”)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. 4bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ë©´ì„œ LoRAë¥¼ ì ìš©í•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d9-q2',
          question: 'Fine-tuning í•™ìŠµ ë°ì´í„°ì˜ Instruction Formatì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ê²ƒì€?',
          options: [
            'instruction (ì§€ì‹œ)',
            'input (ì…ë ¥)',
            'output (ì¶œë ¥)',
            'context (ë¬¸ë§¥)',
          ],
          correctAnswer: 3,
          explanation: 'Instruction Formatì€ instruction, input, output ì„¸ ê°€ì§€ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. contextëŠ” RAGì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°œë…ì…ë‹ˆë‹¤.',
        },
        {
          id: 'w5d9-q3',
          question: 'QLoRAë¡œ Qwen3-1.7Bë¥¼ Fine-tuningí•  ë•Œ í•„ìš”í•œ ìµœì†Œ VRAMì€?',
          options: [
            '2GB',
            '4GB',
            '8GB',
            '16GB',
          ],
          correctAnswer: 1,
          explanation: 'QLoRAëŠ” 4bit ì–‘ìí™”ë¡œ ì•½ 4GB VRAMë§Œìœ¼ë¡œ 1.7B ëª¨ë¸ì„ Fine-tuningí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
        },
        {
          id: 'w5d9-q4',
          question: 'LoRAì—ì„œ r (rank) ê°’ì„ ë†’ì´ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?',
          options: [
            'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì„±ëŠ¥ í–¥ìƒ',
            'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€, ì„±ëŠ¥ í–¥ìƒ',
            'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ, ì„±ëŠ¥ ì €í•˜',
            'í•™ìŠµ ì†ë„ë§Œ ë¹¨ë¼ì§',
          ],
          correctAnswer: 1,
          explanation: 'LoRA rankë¥¼ ë†’ì´ë©´ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ëŠ˜ì–´ë‚˜ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ë§Œ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ ì¦ê°€í•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d9-q5',
          question: 'Fine-tuned ëª¨ë¸ + RAG ì¡°í•©ì˜ ì¥ì ì´ ì•„ë‹Œ ê²ƒì€?',
          options: [
            'ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ ë³´ìœ ',
            'ìµœì‹  ë¬¸ì„œ ê²€ìƒ‰ ê°€ëŠ¥',
            'í•™ìŠµ ë°ì´í„° ì—†ì´ë„ ì‘ë™',
            'ì¼ê´€ëœ ë‹µë³€ ìŠ¤íƒ€ì¼',
          ],
          correctAnswer: 2,
          explanation: 'Fine-tuned ëª¨ë¸ì€ ë°˜ë“œì‹œ í•™ìŠµ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. RAG + Fine-tuning ì¡°í•©ì˜ ì¥ì ì€ ë„ë©”ì¸ ì§€ì‹ê³¼ ìµœì‹  ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.',
        },
      ],
    }),

    // ========================================
    // Task 8: ì‹¤ìŠµ ì±Œë¦°ì§€ (50ë¶„)
    // ========================================
    createChallengeTask('w5d9-finetuning-challenge', 'ë‚˜ë§Œì˜ ë„ë©”ì¸ ëª¨ë¸ Fine-tuning', 50, {
      introduction: `
## ğŸ¯ ì‹¤ìŠµ ëª©í‘œ

ìì‹ ë§Œì˜ ë„ë©”ì¸(ì—…ë¬´, ì·¨ë¯¸, ì „ë¬¸ ë¶„ì•¼)ì— ë§ëŠ” **Fine-tuned LLM**ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”!

---

## ğŸ“‹ ê³¼ì œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Part 1: ë°ì´í„° ì¤€ë¹„ (20ë¶„)

- [ ] ë„ë©”ì¸ ì„ íƒ (ì˜ˆ: ìš”ë¦¬, ë²•ë¥ , ì˜ë£Œ, ê²Œì„, í”„ë¡œê·¸ë˜ë°)
- [ ] ìµœì†Œ 50ê°œ Q&A ë°ì´í„° ì‘ì„±
- [ ] Instruction Format JSON íŒŒì¼ ìƒì„±
- [ ] í’ˆì§ˆ ì²´í¬ (ëª…í™•í•œ ì§ˆë¬¸/ë‹µë³€, ì¼ê´€ëœ ìŠ¤íƒ€ì¼)

### Part 2: QLoRA í•™ìŠµ (15ë¶„)

- [ ] Google Colab í™˜ê²½ ì„¤ì • (T4 GPU)
- [ ] ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)
- [ ] LoRA ì„¤ì • ë° ì ìš©
- [ ] í•™ìŠµ ì‹¤í–‰ (1-3 epoch)

### Part 3: í…ŒìŠ¤íŠ¸ & í‰ê°€ (10ë¶„)

- [ ] í•™ìŠµ ì „í›„ ë¹„êµ (ë™ì¼ ì§ˆë¬¸)
- [ ] ìµœì†Œ 5ê°œ ì§ˆë¬¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
- [ ] ê°œì„ ì  ê¸°ë¡

### Part 4: (ì„ íƒ) ê³µìœ  (5ë¶„)

- [ ] HuggingFace Hub ì—…ë¡œë“œ
- [ ] Model Card ì‘ì„±
- [ ] ë§í¬ ê³µìœ 

---

## ğŸ’¡ ë„ë©”ì¸ ì•„ì´ë””ì–´

| ë„ë©”ì¸ | ì˜ˆì‹œ ì§ˆë¬¸ | ë°ì´í„° ì†ŒìŠ¤ |
|--------|----------|------------|
| **ìš”ë¦¬** | "íŒŒìŠ¤íƒ€ ë©´ ì‚¶ëŠ” ì‹œê°„ì€?" | ë ˆì‹œí”¼ ì‚¬ì´íŠ¸, ìš”ë¦¬ì±… |
| **ê²Œì„ ê³µëµ** | "ì—˜ë“ ë§ ë³´ìŠ¤ ê³µëµë²•?" | ìœ„í‚¤, ê³µëµ ì‚¬ì´íŠ¸ |
| **í”„ë¡œê·¸ë˜ë°** | "Python list comprehension?" | ê³µì‹ ë¬¸ì„œ, Stack Overflow |
| **ìš´ë™** | "ë²¤ì¹˜í”„ë ˆìŠ¤ ì •í™•í•œ ìì„¸?" | ìš´ë™ ê°€ì´ë“œ, ìœ íŠœë¸Œ |
| **ë°˜ë ¤ë™ë¬¼** | "ê°•ì•„ì§€ ì‚°ì±… ì‹œê°„ì€?" | ìˆ˜ì˜ì‚¬ Q&A, ì»¤ë®¤ë‹ˆí‹° |

---

## ğŸ† í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ê¸°ì¤€ |
|------|------|------|
| ë°ì´í„° í’ˆì§ˆ | 30% | ëª…í™•ì„±, ì¼ê´€ì„±, ì •í™•ì„± |
| í•™ìŠµ ì„±ê³µ | 30% | loss ê°ì†Œ, ì—ëŸ¬ ì—†ìŒ |
| ë‹µë³€ í’ˆì§ˆ | 30% | í•™ìŠµ ì „í›„ ê°œì„  ì •ë„ |
| ë¬¸ì„œí™” | 10% | Model Card, ì½”ë“œ ì •ë¦¬ |
      `,
      hints: [
        'ë°ì´í„°ëŠ” ì–‘ë³´ë‹¤ ì§ˆ! 50ê°œ ê³ í’ˆì§ˆì´ 500ê°œ ì €í’ˆì§ˆë³´ë‹¤ ë‚˜ìŒ',
        'Colab ì„¸ì…˜ ëŠê¹€ ëŒ€ë¹„: Drive ë§ˆìš´íŠ¸ + ì²´í¬í¬ì¸íŠ¸ ì €ì¥',
        'lossê°€ ì•ˆ ì¤„ì–´ë“¤ë©´ learning_rate ë‚®ì¶°ë³´ê¸° (2e-4 â†’ 1e-4)',
        'ì²˜ìŒì—” ì‘ì€ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸, í™•ì¸ í›„ ë°ì´í„° ëŠ˜ë¦¬ê¸°',
      ],
    }),
  ],
}
