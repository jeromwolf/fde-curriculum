// Day 4: LangChain RAG íŒŒì´í”„ë¼ì¸ - ì™„ì „ ë¦¬ë‰´ì–¼ ë²„ì „

import type { Day } from '../../types'
import {
  createVideoTask,
  createReadingTask,
  createCodeTask,
  createQuizTask,
  createChallengeTask,
} from './types'

export const day4LangchainRag: Day = {
  slug: 'langchain-rag',
  title: 'LangChain RAG íŒŒì´í”„ë¼ì¸',
  totalDuration: 300, // 5ì‹œê°„
  tasks: [
    // ========================================
    // Task 1: LangChain ì•„í‚¤í…ì²˜ì™€ LCEL ì‹¬í™” (40ë¶„)
    // ========================================
    createVideoTask('w5d4-langchain-architecture', 'LangChain ì•„í‚¤í…ì²˜ì™€ LCEL ì‹¬í™”', 40, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- LangChainì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•œë‹¤
- LCEL(LangChain Expression Language)ì„ ë§ˆìŠ¤í„°í•œë‹¤
- Runnable ì¸í„°í˜ì´ìŠ¤ì™€ ì²´ì¸ êµ¬ì„±ì„ í•™ìŠµí•œë‹¤

---

## LangChain ìƒíƒœê³„ (2024-2025)

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LangChain ìƒíƒœê³„                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  langchain  â”‚  â”‚ langchain-  â”‚  â”‚ langchain-  â”‚     â”‚
â”‚  â”‚   (core)    â”‚  â”‚   openai    â”‚  â”‚   chroma    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                â”‚                â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                          â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  langgraph  â”‚  â”‚  langsmith  â”‚  â”‚  langserve  â”‚     â”‚
â”‚  â”‚  (ì—ì´ì „íŠ¸)  â”‚  â”‚  (ëª¨ë‹ˆí„°ë§)  â”‚  â”‚   (ë°°í¬)    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

langchain: í•µì‹¬ ì¶”ìƒí™”, ì²´ì¸, í”„ë¡¬í”„íŠ¸
langchain-core: LCEL, Runnable ì¸í„°í˜ì´ìŠ¤
langchain-openai: OpenAI í†µí•©
langchain-community: 100+ í†µí•© (Chroma, Pinecone, etc.)
langgraph: ë³µì¡í•œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°
langsmith: ì¶”ì , í‰ê°€, ëª¨ë‹ˆí„°ë§
langserve: REST APIë¡œ ë°°í¬
\`\`\`

---

## LCEL (LangChain Expression Language)

### LCELì´ë€?

\`\`\`python
# ì „í†µì ì¸ ë°©ì‹ (Legacy Chain)
from langchain.chains import LLMChain

chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(input_text)

# LCEL ë°©ì‹ (í˜„ëŒ€ì )
chain = prompt | llm | output_parser
result = chain.invoke(input_text)
\`\`\`

**LCELì˜ ì¥ì :**
- ì„ ì–¸ì ì´ê³  ì§ê´€ì ì¸ ë¬¸ë²•
- ìë™ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
- ìë™ ë°°ì¹˜ ì²˜ë¦¬
- ìë™ ë³‘ë ¬ ì‹¤í–‰
- íƒ€ì… ì•ˆì „ì„±
- LangSmith í†µí•©

---

### Runnable ì¸í„°í˜ì´ìŠ¤

\`\`\`python
from langchain_core.runnables import Runnable

# ëª¨ë“  LCEL ì»´í¬ë„ŒíŠ¸ëŠ” Runnable ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
class Runnable:
    # ë™ê¸° ì‹¤í–‰
    def invoke(self, input) -> output:
        pass

    # ë°°ì¹˜ ì‹¤í–‰
    def batch(self, inputs: list) -> list[output]:
        pass

    # ìŠ¤íŠ¸ë¦¬ë° (ë™ê¸°)
    def stream(self, input) -> Iterator[output]:
        pass

    # ë¹„ë™ê¸° ì‹¤í–‰
    async def ainvoke(self, input) -> output:
        pass

    # ë¹„ë™ê¸° ë°°ì¹˜
    async def abatch(self, inputs: list) -> list[output]:
        pass

    # ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë°
    async def astream(self, input) -> AsyncIterator[output]:
        pass
\`\`\`

---

### íŒŒì´í”„ ì—°ì‚°ì (|)

\`\`\`python
# | ì—°ì‚°ì: ì™¼ìª½ ì¶œë ¥ â†’ ì˜¤ë¥¸ìª½ ì…ë ¥

chain = prompt | llm | output_parser
#       â†“        â†“        â†“
#   PromptValue â†’ Message â†’ String

# ì‹¤í–‰
result = chain.invoke({"question": "RAGë€?"})
\`\`\`

---

### ì£¼ìš” Runnable í´ë˜ìŠ¤

\`\`\`python
from langchain_core.runnables import (
    RunnablePassthrough,   # ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
    RunnableLambda,        # í•¨ìˆ˜ë¥¼ Runnableë¡œ ë˜í•‘
    RunnableParallel,      # ë³‘ë ¬ ì‹¤í–‰
    RunnableBranch,        # ì¡°ê±´ë¶€ ë¶„ê¸°
    RunnableSequence,      # ìˆœì°¨ ì‹¤í–‰ (| ì—°ì‚°ì)
)

# 1. RunnablePassthrough: ì…ë ¥ ê·¸ëŒ€ë¡œ ì „ë‹¬
chain = {
    "context": retriever,
    "question": RunnablePassthrough()  # ì…ë ¥ëœ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ
}

# 2. RunnableLambda: í•¨ìˆ˜ ë˜í•‘
def format_docs(docs):
    return "\\n".join(doc.page_content for doc in docs)

chain = retriever | RunnableLambda(format_docs)

# 3. RunnableParallel: ë³‘ë ¬ ì‹¤í–‰
parallel = RunnableParallel({
    "context": retriever | format_docs,
    "question": RunnablePassthrough(),
    "history": get_history
})

# 4. RunnableBranch: ì¡°ê±´ë¶€ ë¶„ê¸°
from langchain_core.runnables import RunnableBranch

branch = RunnableBranch(
    (lambda x: "ì½”ë“œ" in x, code_chain),
    (lambda x: "ìš”ì•½" in x, summary_chain),
    default_chain  # ê¸°ë³¸ê°’
)
\`\`\`

---

### itemgetterì™€ ì¡°í•©

\`\`\`python
from operator import itemgetter

# ë”•ì…”ë„ˆë¦¬ì—ì„œ íŠ¹ì • í‚¤ ì¶”ì¶œ
chain = (
    {
        "context": itemgetter("docs") | format_docs,
        "question": itemgetter("question")
    }
    | prompt
    | llm
)

# ì‚¬ìš©
result = chain.invoke({
    "docs": [doc1, doc2],
    "question": "RAGë€?"
})
\`\`\`

---

## ì²´ì¸ êµ¬ì„± íŒ¨í„´

### íŒ¨í„´ 1: ê¸°ë³¸ RAG ì²´ì¸

\`\`\`python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

llm = ChatOpenAI(model="gpt-4o-mini")

prompt = ChatPromptTemplate.from_template("""
Context: {context}
Question: {question}
Answer:""")

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
\`\`\`

### íŒ¨í„´ 2: ì†ŒìŠ¤ í¬í•¨ RAG

\`\`\`python
from langchain_core.runnables import RunnableParallel

rag_with_sources = RunnableParallel({
    "answer": rag_chain,
    "sources": retriever | (lambda docs: [d.metadata for d in docs])
})

result = rag_with_sources.invoke("RAGë€?")
# {"answer": "...", "sources": [{"source": "doc1.pdf"}, ...]}
\`\`\`

### íŒ¨í„´ 3: ì¬ì‹œë„ ë˜í¼

\`\`\`python
from langchain_core.runnables import RunnableWithFallbacks

reliable_llm = llm.with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True
).with_fallbacks([backup_llm])

rag_chain = prompt | reliable_llm | StrOutputParser()
\`\`\`
      `,
      keyPoints: [
        'LCEL: ì„ ì–¸ì ì´ê³  íƒ€ì… ì•ˆì „í•œ ì²´ì¸ êµ¬ì„±',
        'Runnable ì¸í„°í˜ì´ìŠ¤: invoke, batch, stream ì§€ì›',
        'RunnableParallelë¡œ ë³‘ë ¬ ì‹¤í–‰, RunnableBranchë¡œ ì¡°ê±´ë¶€ ë¶„ê¸°',
      ],
      practiceGoal: 'LangChain ì•„í‚¤í…ì²˜ì™€ LCELì„ ì´í•´í•œë‹¤',
    }),

    // ========================================
    // Task 2: í”„ë¡œë•ì…˜ê¸‰ RAG ì²´ì¸ êµ¬ì¶• (50ë¶„)
    // ========================================
    createCodeTask('w5d4-production-rag-chain', 'í”„ë¡œë•ì…˜ê¸‰ RAG ì²´ì¸ êµ¬ì¶•', 50, {
      introduction: `
## ì™œ ë°°ìš°ëŠ”ê°€?

**ë¬¸ì œ**: LangChain íŠœí† ë¦¬ì–¼ì„ ë³´ë©´ "chain = prompt | llm"ì²˜ëŸ¼ 5ì¤„ë¡œ RAGê°€ ëë‚˜ëŠ”ë°, í”„ë¡œë•ì…˜ì— ì˜¬ë¦¬ë©´ í­ë°œí•©ë‹ˆë‹¤.
- API ì—ëŸ¬ ë‚˜ë©´ ì‹œìŠ¤í…œ ì „ì²´ ë‹¤ìš´
- ëŠë¦° ì‘ë‹µ (10ì´ˆ+)
- ì†ŒìŠ¤ ì¶”ì  ë¶ˆê°€ëŠ¥
- ë¹„ìš© í­íƒ„

**í•´ê²°**: ì¬ì‹œë„, í´ë°±, ìºì‹±, ì†ŒìŠ¤ ì¶”ì ì´ ìˆëŠ” í”„ë¡œë•ì…˜ê¸‰ ì²´ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

---

## ë¹„ìœ : RAG ì²´ì¸ = ìë™ì°¨ ê³µì¥ ì¡°ë¦½ ë¼ì¸

\`\`\`
íŠœí† ë¦¬ì–¼ RAG = ìˆ˜ì œ ìë™ì°¨
- ì§ì ‘ ì¡°ë¦½ (ëŠë¦¼)
- ê³ ì¥ ë‚˜ë©´ ë©ˆì¶¤
- ì–´ë””ì„œ ê³ ì¥ë‚¬ëŠ”ì§€ ëª¨ë¦„

í”„ë¡œë•ì…˜ RAG = í˜„ëŒ€ ìë™ì°¨ ê³µì¥
- ìë™í™” ì¡°ë¦½ ë¼ì¸ (ë¹ ë¦„)
- ê³ ì¥ ë‚˜ë©´ ë°±ì—… ë¼ì¸ìœ¼ë¡œ ì „í™˜
- ê° ë‹¨ê³„ ëª¨ë‹ˆí„°ë§
- í’ˆì§ˆ ê²€ì‚¬ (ì†ŒìŠ¤ ì¶”ì )
\`\`\`

---

## í•µì‹¬ êµ¬í˜„ (ê°„ì†Œí™”)

\`\`\`python
# ğŸ“Œ Step 1: ì¬ì‹œë„ ì„¤ì •
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini").with_retry(
    stop_after_attempt=3,  # 3ë²ˆê¹Œì§€ ì¬ì‹œë„
    wait_exponential_jitter=True  # ì§€ìˆ˜ ë°±ì˜¤í”„
)

# ğŸ“Œ Step 2: RAG ì²´ì¸ (ì†ŒìŠ¤ ì¶”ì  í¬í•¨)
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

chain = RunnableParallel({
    "docs": retriever,  # ë¬¸ì„œ ê²€ìƒ‰
    "question": RunnablePassthrough()
}) | RunnableParallel({
    "answer": (
        lambda x: {"context": format_docs(x["docs"]), "question": x["question"]}
        | prompt
        | llm
    ),
    "sources": lambda x: [d.metadata for d in x["docs"]]  # âœ… ì†ŒìŠ¤ ì¶”ì 
})

result = chain.invoke("RAGë€?")
print(f"Answer: {result['answer']}")
print(f"Sources: {result['sources']}")

# ğŸ“Œ Step 3: ìŠ¤íŠ¸ë¦¬ë°
for chunk in chain.stream("RAGë€?"):
    if "answer" in chunk:
        print(chunk["answer"], end="", flush=True)

# ğŸ“Œ Step 4: ë°°ì¹˜ ì²˜ë¦¬
questions = ["RAGë€?", "ë²¡í„° DBë€?", "LangChainì´ë€?"]
results = chain.batch(questions)
\`\`\`

---

## ì „ì²´ ì½”ë“œ (ìƒì„¸)

### ì„¤ì¹˜

\`\`\`bash
pip install langchain langchain-openai langchain-chroma chromadb
\`\`\`

---

## í”„ë¡œë•ì…˜ê¸‰ RAG í´ë˜ìŠ¤

\`\`\`python
from dataclasses import dataclass
from typing import Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import Document
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RAGConfig:
    """RAG ì„¤ì •"""
    model: str = "gpt-4o-mini"
    embedding_model: str = "text-embedding-3-small"
    chunk_size: int = 500
    chunk_overlap: int = 50
    k: int = 5
    temperature: float = 0
    max_retries: int = 3

@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ"""
    answer: str
    sources: list[dict]
    context: str

class ProductionRAGChain:
    """í”„ë¡œë•ì…˜ê¸‰ RAG ì²´ì¸"""

    def __init__(
        self,
        config: RAGConfig = RAGConfig(),
        persist_directory: Optional[str] = None
    ):
        self.config = config
        self.persist_directory = persist_directory

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._init_components()

    def _init_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        # ì„ë² ë”©
        self.embeddings = OpenAIEmbeddings(
            model=self.config.embedding_model
        )

        # LLM (ì¬ì‹œë„ + í´ë°± ì„¤ì •)
        self.llm = ChatOpenAI(
            model=self.config.model,
            temperature=self.config.temperature
        ).with_retry(
            stop_after_attempt=self.config.max_retries,
            wait_exponential_jitter=True
        )

        # í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\\n\\n", "\\n", ".", " ", ""]
        )

        # í”„ë¡¬í”„íŠ¸
        self.prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
3. ë‹µë³€ì€ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
4. ê°€ëŠ¥í•˜ë©´ ê·¼ê±°ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:""")

        # ë²¡í„° ì €ì¥ì†Œ (ë‚˜ì¤‘ì— ì´ˆê¸°í™”)
        self.vectorstore = None
        self.retriever = None

    def load_documents(self, documents: list[Document]):
        """ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        logger.info(f"Loading {len(documents)} documents...")

        # ë¬¸ì„œ ë¶„í• 
        splits = self.splitter.split_documents(documents)
        logger.info(f"Split into {len(splits)} chunks")

        # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        if self.persist_directory:
            self.vectorstore = Chroma.from_documents(
                splits,
                self.embeddings,
                persist_directory=self.persist_directory
            )
        else:
            self.vectorstore = Chroma.from_documents(
                splits,
                self.embeddings
            )

        # ê²€ìƒ‰ê¸° ìƒì„±
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": self.config.k}
        )

        logger.info("Vector store created successfully")

    def _format_docs(self, docs: list[Document]) -> str:
        """ë¬¸ì„œ í¬ë§¤íŒ…"""
        return "\\n\\n---\\n\\n".join(doc.page_content for doc in docs)

    def _extract_sources(self, docs: list[Document]) -> list[dict]:
        """ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        sources = []
        for doc in docs:
            source = {
                "content_preview": doc.page_content[:100] + "...",
                **doc.metadata
            }
            sources.append(source)
        return sources

    def _build_chain(self):
        """RAG ì²´ì¸ êµ¬ì„±"""
        return (
            RunnableParallel({
                "docs": self.retriever,
                "question": RunnablePassthrough()
            })
            | RunnableParallel({
                "answer": (
                    lambda x: {
                        "context": self._format_docs(x["docs"]),
                        "question": x["question"]
                    }
                )
                | self.prompt
                | self.llm
                | StrOutputParser(),
                "sources": lambda x: self._extract_sources(x["docs"]),
                "context": lambda x: self._format_docs(x["docs"])
            })
        )

    def query(self, question: str) -> RAGResponse:
        """ì§ˆë¬¸ì— ë‹µë³€"""
        if not self.retriever:
            raise ValueError("ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”: load_documents()")

        chain = self._build_chain()

        try:
            result = chain.invoke(question)
            return RAGResponse(
                answer=result["answer"],
                sources=result["sources"],
                context=result["context"]
            )
        except Exception as e:
            logger.error(f"Query failed: {e}")
            raise

    def stream(self, question: str):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        if not self.retriever:
            raise ValueError("ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”: load_documents()")

        # ìŠ¤íŠ¸ë¦¬ë°ìš© ê°„ì†Œí™”ëœ ì²´ì¸
        stream_chain = (
            {"context": self.retriever | self._format_docs, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

        for chunk in stream_chain.stream(question):
            yield chunk

    def batch_query(self, questions: list[str]) -> list[RAGResponse]:
        """ë°°ì¹˜ ì§ˆë¬¸"""
        if not self.retriever:
            raise ValueError("ë¬¸ì„œë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”: load_documents()")

        chain = self._build_chain()
        results = chain.batch(questions)

        return [
            RAGResponse(
                answer=r["answer"],
                sources=r["sources"],
                context=r["context"]
            )
            for r in results
        ]
\`\`\`

---

## ì‚¬ìš© ì˜ˆì‹œ

\`\`\`python
# 1. ë¬¸ì„œ ì¤€ë¹„
documents = [
    Document(
        page_content="""
        RAG(Retrieval-Augmented Generation)ëŠ” 2020ë…„ Meta AIì—ì„œ ë°œí‘œí•œ ê¸°ìˆ ì…ë‹ˆë‹¤.
        ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ ê°œì„ í•©ë‹ˆë‹¤.
        ì£¼ìš” ì¥ì ìœ¼ë¡œëŠ” í™˜ê° ê°ì†Œ, ìµœì‹  ì •ë³´ í™œìš©, ë„ë©”ì¸ íŠ¹í™” ë“±ì´ ìˆìŠµë‹ˆë‹¤.
        """,
        metadata={"source": "rag_intro.md", "page": 1}
    ),
    Document(
        page_content="""
        LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
        LCEL(LangChain Expression Language)ì„ í†µí•´ ì„ ì–¸ì ìœ¼ë¡œ ì²´ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ë‹¤ì–‘í•œ ë²¡í„° ì €ì¥ì†Œ, LLM, ë„êµ¬ì™€ í†µí•©ë©ë‹ˆë‹¤.
        """,
        metadata={"source": "langchain_intro.md", "page": 1}
    ),
    Document(
        page_content="""
        ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•©ë‹ˆë‹¤.
        ëŒ€í‘œì ì¸ ë²¡í„° DBë¡œëŠ” Chroma, Pinecone, Weaviate, Qdrant ë“±ì´ ìˆìŠµë‹ˆë‹¤.
        HNSW, IVF ë“±ì˜ ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """,
        metadata={"source": "vectordb_intro.md", "page": 1}
    )
]

# 2. RAG ì²´ì¸ ìƒì„±
config = RAGConfig(
    model="gpt-4o-mini",
    k=3,
    temperature=0
)

rag = ProductionRAGChain(
    config=config,
    persist_directory="./production_rag_db"
)

# 3. ë¬¸ì„œ ë¡œë“œ
rag.load_documents(documents)

# 4. ì§ˆë¬¸
response = rag.query("RAGì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€ìš”?")
print(f"Answer: {response.answer}")
print(f"Sources: {response.sources}")

# 5. ìŠ¤íŠ¸ë¦¬ë°
print("\\nStreaming: ", end="")
for chunk in rag.stream("LangChainì´ë€?"):
    print(chunk, end="", flush=True)
print()

# 6. ë°°ì¹˜ ì§ˆë¬¸
questions = ["RAGë€?", "ë²¡í„° DBë€?", "LangChainì˜ íŠ¹ì§•ì€?"]
responses = rag.batch_query(questions)
for q, r in zip(questions, responses):
    print(f"\\nQ: {q}")
    print(f"A: {r.answer[:100]}...")
\`\`\`
      `,
      keyPoints: [
        'ğŸ” with_retry()ë¡œ ì¬ì‹œë„ ë¡œì§ êµ¬í˜„',
        'âš¡ RunnableParallelë¡œ ì†ŒìŠ¤ì™€ ì»¨í…ìŠ¤íŠ¸ ë™ì‹œ ì¶”ì¶œ',
        'ğŸ“¦ batch()ë¡œ ë°°ì¹˜ ì²˜ë¦¬, stream()ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°',
      ],
      practiceGoal: 'í”„ë¡œë•ì…˜ê¸‰ RAG ì²´ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 3: ëŒ€í™”í˜• RAG (History-aware RAG) (45ë¶„)
    // ========================================
    createCodeTask('w5d4-conversational-rag', 'ëŒ€í™”í˜• RAG (History-aware RAG)', 45, {
      introduction: `
## ì™œ ë°°ìš°ëŠ”ê°€?

**ë¬¸ì œ**: ì¼ë°˜ RAGëŠ” ë§¤ ì§ˆë¬¸ì„ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì„œ, "ê·¸ê²ƒ", "ì´ì „ì—" ê°™ì€ ëŒ€ëª…ì‚¬ë¥¼ ì´í•´ ëª» í•©ë‹ˆë‹¤.
- Q1: "RAGë€?" â†’ A1: "RAGëŠ”..."
- Q2: "ê·¸ê²ƒì˜ ì¥ì ì€?" â†’ A2: ??? ("ê·¸ê²ƒ"ì´ ë­ì§€?)

**í•´ê²°**: ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ë©´ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

## ë¹„ìœ : ëŒ€í™”í˜• RAG = ê¸°ì–µí•˜ëŠ” ì±—ë´‡

\`\`\`
ì¼ë°˜ RAG = ê¸ˆë¶•ì–´ (3ì´ˆ ê¸°ì–µ)
- Q: "ì‚¼ì„±ì „ì ì£¼ê°€ëŠ”?"
- A: "10ë§Œì›ì…ë‹ˆë‹¤"
- Q: "ê·¸ íšŒì‚¬ ì‹¤ì ì€?"
- A: ??? ("ê·¸ íšŒì‚¬"ê°€ ë­ì§€?)

ëŒ€í™”í˜• RAG = ì¸ê°„ (ë¬¸ë§¥ ê¸°ì–µ)
- Q: "ì‚¼ì„±ì „ì ì£¼ê°€ëŠ”?"
- A: "10ë§Œì›ì…ë‹ˆë‹¤"
- Q: "ê·¸ íšŒì‚¬ ì‹¤ì ì€?"
- â†’ [ì§ˆë¬¸ ì¬ì‘ì„±: "ì‚¼ì„±ì „ì ì‹¤ì ì€?"]
- A: "ì˜ì—…ì´ìµ 15ì¡°ì›ì…ë‹ˆë‹¤"
\`\`\`

---

## í•µì‹¬ êµ¬í˜„ (ê°„ì†Œí™”)

\`\`\`python
# ğŸ“Œ Step 1: ì§ˆë¬¸ ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

contextualize_prompt = ChatPromptTemplate.from_messages([
    ("system", "ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë³´ê³  ì§ˆë¬¸ì„ ë…ë¦½ì ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}")
])

# ğŸ“Œ Step 2: íˆìŠ¤í† ë¦¬ ê´€ë¦¬
class ConversationalRAG:
    def __init__(self):
        self.history = []  # ëŒ€í™” íˆìŠ¤í† ë¦¬

    def query(self, question: str) -> str:
        # 1ï¸âƒ£ ì§ˆë¬¸ ì¬ì‘ì„± (íˆìŠ¤í† ë¦¬ ìˆìœ¼ë©´)
        if self.history:
            contextualized_q = (contextualize_prompt | llm).invoke({
                "history": self.history,
                "question": question
            })
        else:
            contextualized_q = question

        # 2ï¸âƒ£ ê²€ìƒ‰ & ë‹µë³€
        docs = retriever.invoke(contextualized_q)
        answer = (qa_prompt | llm).invoke({
            "context": format_docs(docs),
            "question": question
        })

        # 3ï¸âƒ£ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.history.append(HumanMessage(content=question))
        self.history.append(AIMessage(content=answer))

        return answer

# ğŸ“Œ Step 3: ì‚¬ìš©
rag = ConversationalRAG()
print(rag.query("RAGë€?"))
print(rag.query("ê·¸ê²ƒì˜ ì¥ì ì€?"))  # âœ… "RAGì˜ ì¥ì ì€?"ìœ¼ë¡œ ì¬ì‘ì„±ë¨!
\`\`\`

---

## ì „ì²´ ì½”ë“œ (ìƒì„¸)

### êµ¬í˜„

\`\`\`python
from typing import Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import Document
from dataclasses import dataclass, field

@dataclass
class ConversationMessage:
    """ëŒ€í™” ë©”ì‹œì§€"""
    role: str  # "human" or "ai"
    content: str

    def to_langchain(self) -> BaseMessage:
        if self.role == "human":
            return HumanMessage(content=self.content)
        return AIMessage(content=self.content)

class ConversationalRAG:
    """ëŒ€í™”í˜• RAG ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        vectorstore: Chroma,
        model: str = "gpt-4o-mini",
        k: int = 5,
        max_history: int = 10
    ):
        self.vectorstore = vectorstore
        self.retriever = vectorstore.as_retriever(search_kwargs={"k": k})
        self.max_history = max_history

        # LLM
        self.llm = ChatOpenAI(model=model, temperature=0)

        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.history: list[ConversationMessage] = []

        # í”„ë¡¬í”„íŠ¸ ì„¤ì •
        self._setup_prompts()

    def _setup_prompts(self):
        """í”„ë¡¬í”„íŠ¸ ì„¤ì •"""
        # 1. ì§ˆë¬¸ ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸
        self.contextualize_prompt = ChatPromptTemplate.from_messages([
            ("system", """ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ,
íˆìŠ¤í† ë¦¬ ì—†ì´ë„ ì´í•´í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

ê·œì¹™:
1. ëŒ€ëª…ì‚¬(ê·¸ê²ƒ, ì´ê²ƒ, ê·¸)ë¥¼ êµ¬ì²´ì ì¸ ëª…ì‚¬ë¡œ ë°”ê¾¸ì„¸ìš”.
2. ìƒëµëœ ì£¼ì–´ë‚˜ ëª©ì ì–´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
3. ì¬ì‘ì„±ì´ í•„ìš” ì—†ìœ¼ë©´ ì›ë˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
4. ì§ˆë¬¸ í˜•ì‹ì„ ìœ ì§€í•˜ì„¸ìš”.

ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}")
        ])

        # 2. QA í”„ë¡¬í”„íŠ¸
        self.qa_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ê·œì¹™:
1. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
2. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
3. ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}"""),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{question}")
        ])

    def _get_history_messages(self) -> list[BaseMessage]:
        """íˆìŠ¤í† ë¦¬ë¥¼ LangChain ë©”ì‹œì§€ë¡œ ë³€í™˜"""
        # ìµœê·¼ Nê°œë§Œ ì‚¬ìš©
        recent = self.history[-self.max_history:]
        return [msg.to_langchain() for msg in recent]

    def _contextualize_question(self, question: str) -> str:
        """ì§ˆë¬¸ ì¬ì‘ì„± (í•„ìš”í•œ ê²½ìš°)"""
        if not self.history:
            return question

        chain = self.contextualize_prompt | self.llm | StrOutputParser()

        contextualized = chain.invoke({
            "history": self._get_history_messages(),
            "question": question
        })

        return contextualized.strip()

    def _format_docs(self, docs: list[Document]) -> str:
        return "\\n\\n---\\n\\n".join(doc.page_content for doc in docs)

    def query(self, question: str) -> str:
        """ì§ˆë¬¸ì— ë‹µë³€"""
        # 1. ì§ˆë¬¸ ì¬ì‘ì„±
        contextualized_q = self._contextualize_question(question)
        print(f"[Debug] Original: {question}")
        print(f"[Debug] Contextualized: {contextualized_q}")

        # 2. ê²€ìƒ‰
        docs = self.retriever.invoke(contextualized_q)
        context = self._format_docs(docs)

        # 3. ë‹µë³€ ìƒì„±
        chain = self.qa_prompt | self.llm | StrOutputParser()
        answer = chain.invoke({
            "context": context,
            "history": self._get_history_messages(),
            "question": question  # ì›ë˜ ì§ˆë¬¸ ì‚¬ìš©
        })

        # 4. íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.history.append(ConversationMessage("human", question))
        self.history.append(ConversationMessage("ai", answer))

        return answer

    def clear_history(self):
        """íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.history = []

    def get_history(self) -> list[ConversationMessage]:
        """íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
        return self.history.copy()
\`\`\`

---

## ì‚¬ìš© ì˜ˆì‹œ

\`\`\`python
# 1. ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ (ì´ì „ì— ìƒì„±)
# vectorstore = ...

# 2. ëŒ€í™”í˜• RAG ìƒì„±
conv_rag = ConversationalRAG(
    vectorstore=vectorstore,
    max_history=10
)

# 3. ëŒ€í™” ì‹œì‘
print("=== ëŒ€í™”í˜• RAG í…ŒìŠ¤íŠ¸ ===\\n")

# ì²« ë²ˆì§¸ ì§ˆë¬¸
q1 = "RAGë€ ë¬´ì—‡ì¸ê°€ìš”?"
a1 = conv_rag.query(q1)
print(f"User: {q1}")
print(f"AI: {a1}\\n")

# ë‘ ë²ˆì§¸ ì§ˆë¬¸ (ëŒ€ëª…ì‚¬ ì‚¬ìš©)
q2 = "ê·¸ê²ƒì˜ ì£¼ìš” ì¥ì ì€?"
a2 = conv_rag.query(q2)
print(f"User: {q2}")
print(f"AI: {a2}\\n")

# ì„¸ ë²ˆì§¸ ì§ˆë¬¸ (ì´ì „ ë§¥ë½ ì°¸ì¡°)
q3 = "ë‹¤ë¥¸ ê¸°ìˆ ê³¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ê°€ìš”?"
a3 = conv_rag.query(q3)
print(f"User: {q3}")
print(f"AI: {a3}\\n")

# íˆìŠ¤í† ë¦¬ í™•ì¸
print("=== ëŒ€í™” íˆìŠ¤í† ë¦¬ ===")
for msg in conv_rag.get_history():
    print(f"[{msg.role}]: {msg.content[:50]}...")

# íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
conv_rag.clear_history()
print("\\níˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
\`\`\`

---

## ì„¸ì…˜ ê´€ë¦¬ (ë©€í‹° ìœ ì €)

\`\`\`python
from uuid import uuid4

class SessionManager:
    """ì„¸ì…˜ë³„ ëŒ€í™” ê´€ë¦¬"""

    def __init__(self, vectorstore: Chroma):
        self.vectorstore = vectorstore
        self.sessions: dict[str, ConversationalRAG] = {}

    def get_or_create_session(self, session_id: str = None) -> tuple[str, ConversationalRAG]:
        """ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        if session_id is None:
            session_id = str(uuid4())

        if session_id not in self.sessions:
            self.sessions[session_id] = ConversationalRAG(
                vectorstore=self.vectorstore
            )

        return session_id, self.sessions[session_id]

    def delete_session(self, session_id: str):
        """ì„¸ì…˜ ì‚­ì œ"""
        if session_id in self.sessions:
            del self.sessions[session_id]

# ì‚¬ìš©
manager = SessionManager(vectorstore)

# ìœ ì € A
session_a, rag_a = manager.get_or_create_session()
rag_a.query("RAGë€?")

# ìœ ì € B (ë³„ë„ ì„¸ì…˜)
session_b, rag_b = manager.get_or_create_session()
rag_b.query("ë²¡í„° DBë€?")

# ê° ì„¸ì…˜ì€ ë…ë¦½ì ì¸ íˆìŠ¤í† ë¦¬
print(f"Session A history: {len(rag_a.get_history())} messages")
print(f"Session B history: {len(rag_b.get_history())} messages")
\`\`\`
      `,
      keyPoints: [
        'ğŸ’¬ ì§ˆë¬¸ ì¬ì‘ì„±ìœ¼ë¡œ ëŒ€ëª…ì‚¬ í•´ê²°',
        'ğŸ“ MessagesPlaceholderë¡œ íˆìŠ¤í† ë¦¬ ê´€ë¦¬',
        'ğŸ‘¥ SessionManagerë¡œ ë©€í‹° ìœ ì € ì§€ì›',
      ],
      practiceGoal: 'ëŒ€í™”í˜• RAGë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 4: ê³ ê¸‰ RAG íŒ¨í„´ (40ë¶„)
    // ========================================
    createReadingTask('w5d4-advanced-patterns', 'ê³ ê¸‰ RAG íŒ¨í„´', 40, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- Self-Query Retrieverë¥¼ ì´í•´í•œë‹¤
- Parent Document Retrieverë¥¼ ì´í•´í•œë‹¤
- Multi-Query Retrieverë¥¼ ì´í•´í•œë‹¤

---

## 1. Self-Query Retriever

### ë¬¸ì œ

\`\`\`
ì‚¬ìš©ì: "2024ë…„ ì´í›„ AI ê´€ë ¨ ê¸°ìˆ  ë¬¸ì„œ ì°¾ì•„ì¤˜"

ì¼ë°˜ ê²€ìƒ‰: "2024ë…„", "AI", "ê¸°ìˆ  ë¬¸ì„œ" í‚¤ì›Œë“œë¡œ ì˜ë¯¸ ê²€ìƒ‰
â†’ 2023ë…„ ë¬¸ì„œë„ ë°˜í™˜ë  ìˆ˜ ìˆìŒ

Self-Query: ì¿¼ë¦¬ ë¶„ì„ â†’ í•„í„° ìë™ ìƒì„±
â†’ filter: {"year": {"$gte": 2024}, "category": "AI"}
â†’ ì •í™•í•œ ê²°ê³¼
\`\`\`

### êµ¬í˜„

\`\`\`python
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.schema import AttributeInfo

# ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
metadata_field_info = [
    AttributeInfo(
        name="category",
        description="ë¬¸ì„œì˜ ì¹´í…Œê³ ë¦¬. ì˜ˆ: tech, business, science",
        type="string"
    ),
    AttributeInfo(
        name="year",
        description="ë¬¸ì„œ ì‘ì„± ì—°ë„",
        type="integer"
    ),
    AttributeInfo(
        name="author",
        description="ë¬¸ì„œ ì‘ì„±ì",
        type="string"
    ),
]

# Self-Query Retriever ìƒì„±
self_query_retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectorstore,
    document_contents="ê¸°ìˆ  ë¬¸ì„œ ë° ë…¼ë¬¸",
    metadata_field_info=metadata_field_info,
    verbose=True
)

# ì‚¬ìš©
results = self_query_retriever.invoke("2024ë…„ ê¹€ì² ìˆ˜ê°€ ì“´ AI ê´€ë ¨ ë¬¸ì„œ")
# ë‚´ë¶€ì ìœ¼ë¡œ:
# query: "AI ê´€ë ¨ ë¬¸ì„œ"
# filter: {"year": 2024, "author": "ê¹€ì² ìˆ˜"}
\`\`\`

---

## 2. Parent Document Retriever

### ë¬¸ì œ

\`\`\`
ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰:
  âœ… ì •í™•í•œ ê²€ìƒ‰ (íŠ¹ì • ë¬¸ì¥ ë§¤ì¹­)
  âŒ ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±

í° ì²­í¬ë¡œ ê²€ìƒ‰:
  âœ… í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸
  âŒ ê²€ìƒ‰ ì •í™•ë„ ì €í•˜

í•´ê²°: ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰, í° ì²­í¬ ë°˜í™˜!
\`\`\`

### êµ¬í˜„

\`\`\`python
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

# ì‘ì€ ì²­í¬ (ê²€ìƒ‰ìš©)
child_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=20
)

# í° ì²­í¬ (ë°˜í™˜ìš©)
parent_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
)

# ë¬¸ì„œ ì €ì¥ì†Œ
docstore = InMemoryStore()

# Parent Document Retriever ìƒì„±
parent_retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=docstore,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter
)

# ë¬¸ì„œ ì¶”ê°€
parent_retriever.add_documents(documents)

# ê²€ìƒ‰ (ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰ â†’ ë¶€ëª¨ ì²­í¬ ë°˜í™˜)
results = parent_retriever.invoke("RAGì˜ ê²€ìƒ‰ ë‹¨ê³„")
# ì‘ì€ ì²­í¬ì—ì„œ ì •í™•í•œ ë§¤ì¹­ â†’ 1000ì ë¶€ëª¨ ì²­í¬ ë°˜í™˜
\`\`\`

---

## 3. Multi-Query Retriever

### ë¬¸ì œ

\`\`\`
ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰:
  ì¿¼ë¦¬: "RAG ì„±ëŠ¥ í–¥ìƒ"
  â†’ ì œí•œëœ ê´€ì ì˜ ê²€ìƒ‰ ê²°ê³¼

Multi-Query ê²€ìƒ‰:
  ì›ë³¸: "RAG ì„±ëŠ¥ í–¥ìƒ"
  í™•ì¥1: "RAG ìµœì í™” ë°©ë²•"
  í™•ì¥2: "ê²€ìƒ‰ ì¦ê°• ìƒì„± ê°œì„ "
  í™•ì¥3: "RAG ì‹œìŠ¤í…œ íŠœë‹"
  â†’ ë‹¤ì–‘í•œ ê´€ì ì˜ ê²€ìƒ‰ ê²°ê³¼
\`\`\`

### êµ¬í˜„

\`\`\`python
from langchain.retrievers.multi_query import MultiQueryRetriever

# Multi-Query Retriever ìƒì„±
multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
    llm=llm
)

# ê²€ìƒ‰ (ë‚´ë¶€ì ìœ¼ë¡œ ì—¬ëŸ¬ ì¿¼ë¦¬ ìƒì„±)
results = multi_query_retriever.invoke("RAG ì„±ëŠ¥ í–¥ìƒ ë°©ë²•")

# ì»¤ìŠ¤í…€ ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
from langchain.prompts import PromptTemplate

custom_prompt = PromptTemplate(
    input_variables=["question"],
    template="""ë‹¹ì‹ ì€ AI ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ 3ê°€ì§€ ë‹¤ë¥¸ ê´€ì ì˜ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
ê° ì¿¼ë¦¬ëŠ” ìƒˆ ì¤„ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ëŒ€ì²´ ì¿¼ë¦¬:"""
)

multi_query_custom = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm,
    prompt=custom_prompt
)
\`\`\`

---

## 4. Contextual Compression

### ë¬¸ì œ

\`\`\`
ê¸´ ì²­í¬ì—ì„œ ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œ

ì›ë³¸ ì²­í¬ (500ì):
"... RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ LLMì˜ í™˜ê°ì„ ì¤„ì…ë‹ˆë‹¤.
í•œí¸ ë‚ ì”¨ëŠ” ë§‘ì•˜ê³ , ì ì‹¬ì€ ê¹€ì¹˜ì°Œê°œì˜€ìŠµë‹ˆë‹¤. ë‹¤ì‹œ RAGë¡œ ëŒì•„ê°€ë©´..."

ì§ˆë¬¸: "RAGë€?"

ì••ì¶•ëœ ì²­í¬:
"RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ LLMì˜ í™˜ê°ì„ ì¤„ì…ë‹ˆë‹¤."
\`\`\`

### êµ¬í˜„

\`\`\`python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# LLM ê¸°ë°˜ ì••ì¶•ê¸°
compressor = LLMChainExtractor.from_llm(llm)

# Compression Retriever ìƒì„±
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 10})
)

# ê²€ìƒ‰ (ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œ)
results = compression_retriever.invoke("RAGë€?")
\`\`\`

---

## íŒ¨í„´ ì¡°í•©

\`\`\`python
# í•˜ì´ë¸Œë¦¬ë“œ + Re-ranking + Compression
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# 1. BM25 + ë²¡í„° ì•™ìƒë¸”
bm25 = BM25Retriever.from_documents(docs, k=10)
vector = vectorstore.as_retriever(search_kwargs={"k": 10})
ensemble = EnsembleRetriever(
    retrievers=[bm25, vector],
    weights=[0.4, 0.6]
)

# 2. Re-ranking ì¶”ê°€
from langchain.retrievers.document_compressors import CohereRerank

reranker = CohereRerank(top_n=5)
reranking_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=ensemble
)

# 3. ìµœì¢… ì••ì¶•
final_retriever = ContextualCompressionRetriever(
    base_compressor=LLMChainExtractor.from_llm(llm),
    base_retriever=reranking_retriever
)
\`\`\`

---

## íŒ¨í„´ ì„ íƒ ê°€ì´ë“œ

| íŒ¨í„´ | ë¬¸ì œ ìƒí™© | í•´ê²°ì±… |
|------|----------|--------|
| Self-Query | ë©”íƒ€ë°ì´í„° í•„í„° ìì£¼ í•„ìš” | ìì—°ì–´ â†’ í•„í„° ìë™ ë³€í™˜ |
| Parent Document | ê²€ìƒ‰ ì •í™•ë„ vs ì»¨í…ìŠ¤íŠ¸ | ì‘ì€ ì²­í¬ ê²€ìƒ‰, í° ì²­í¬ ë°˜í™˜ |
| Multi-Query | ë‹¨ì¼ ê´€ì  í•œê³„ | ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ë‹¤ì–‘í•œ ê²°ê³¼ |
| Compression | ê¸´ ì²­í¬ì— ë…¸ì´ì¦ˆ | ê´€ë ¨ ë¶€ë¶„ë§Œ ì¶”ì¶œ |
      `,
      keyPoints: [
        'Self-Query: ìì—°ì–´ì—ì„œ ë©”íƒ€ë°ì´í„° í•„í„° ìë™ ì¶”ì¶œ',
        'Parent Document: ì‘ì€ ì²­í¬ ê²€ìƒ‰, í° ì²­í¬ ë°˜í™˜',
        'Multi-Query: ì—¬ëŸ¬ ê´€ì ì˜ ì¿¼ë¦¬ë¡œ ë‹¤ì–‘í•œ ê²°ê³¼',
      ],
      practiceGoal: 'ê³ ê¸‰ RAG íŒ¨í„´ì„ ì´í•´í•˜ê³  ì ìš©í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 5: LangSmith ëª¨ë‹ˆí„°ë§ & í‰ê°€ (35ë¶„)
    // ========================================
    createReadingTask('w5d4-langsmith', 'LangSmith ëª¨ë‹ˆí„°ë§ & í‰ê°€', 35, {
      introduction: `
## í•™ìŠµ ëª©í‘œ
- LangSmithì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ì´í•´í•œë‹¤
- RAG íŒŒì´í”„ë¼ì¸ ì¶”ì ì„ ì„¤ì •í•œë‹¤
- ìë™í™”ëœ í‰ê°€ë¥¼ êµ¬í˜„í•œë‹¤

---

## LangSmithë€?

\`\`\`
LangSmith = LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ê´€ì¸¡ì„± í”Œë«í¼

í•µì‹¬ ê¸°ëŠ¥:
1. Tracing: ì²´ì¸ ì‹¤í–‰ ì¶”ì 
2. Debugging: ì—ëŸ¬ ë””ë²„ê¹…
3. Evaluation: ìë™í™”ëœ í‰ê°€
4. Datasets: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ê´€ë¦¬
5. Monitoring: í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§
\`\`\`

---

## ì„¤ì •

\`\`\`bash
# ì„¤ì¹˜
pip install langsmith

# í™˜ê²½ ë³€ìˆ˜
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY=your-api-key
export LANGCHAIN_PROJECT=my-rag-project
\`\`\`

\`\`\`python
# Pythonì—ì„œ ì„¤ì •
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-rag-project"
\`\`\`

---

## ìë™ ì¶”ì 

\`\`\`python
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ë§Œ í•˜ë©´ ìë™ìœ¼ë¡œ ì¶”ì !
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

llm = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_template("ì§ˆë¬¸: {question}")

chain = prompt | llm

# ì´ í˜¸ì¶œì€ ìë™ìœ¼ë¡œ LangSmithì— ê¸°ë¡ë¨
result = chain.invoke({"question": "RAGë€?"})
\`\`\`

---

## ì»¤ìŠ¤í…€ ì¶”ì 

\`\`\`python
from langsmith import traceable

@traceable(name="my_rag_pipeline")
def rag_pipeline(question: str) -> str:
    """RAG íŒŒì´í”„ë¼ì¸"""
    # ê²€ìƒ‰
    docs = retriever.invoke(question)

    # ë‹µë³€ ìƒì„±
    answer = chain.invoke({
        "context": format_docs(docs),
        "question": question
    })

    return answer

# íƒœê·¸ì™€ ë©”íƒ€ë°ì´í„° ì¶”ê°€
@traceable(
    name="rag_with_metadata",
    tags=["production", "v2"],
    metadata={"version": "2.0", "team": "ml"}
)
def rag_v2(question: str) -> str:
    ...
\`\`\`

---

## í”¼ë“œë°± ìˆ˜ì§‘

\`\`\`python
from langsmith import Client

client = Client()

# ì‹¤í–‰ í›„ í”¼ë“œë°± ì¶”ê°€
run = client.read_run(run_id)
client.create_feedback(
    run.id,
    key="user-rating",
    score=1.0,  # 0.0 ~ 1.0
    comment="ì •í™•í•œ ë‹µë³€"
)

# í”„ë¡œê·¸ë˜ë° ë°©ì‹ í”¼ë“œë°±
client.create_feedback(
    run.id,
    key="retrieval-precision",
    score=0.8,
    comment="5ê°œ ì¤‘ 4ê°œ ê´€ë ¨ ë¬¸ì„œ"
)
\`\`\`

---

## ìë™í™”ëœ í‰ê°€

\`\`\`python
from langsmith.evaluation import LangChainStringEvaluator

# 1. ë°ì´í„°ì…‹ ìƒì„±
dataset = client.create_dataset("rag-test-v1")

# ì˜ˆì‹œ ì¶”ê°€
client.create_examples(
    dataset_id=dataset.id,
    inputs=[
        {"question": "RAGë€?"},
        {"question": "ë²¡í„° DBì˜ ì¥ì ì€?"}
    ],
    outputs=[
        {"answer": "RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì…ë‹ˆë‹¤."},
        {"answer": "ë²¡í„° DBëŠ” ìœ ì‚¬ë„ ê²€ìƒ‰ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."}
    ]
)

# 2. í‰ê°€ì ì •ì˜
from langsmith.evaluation import evaluate

evaluators = [
    # ì •í™•ë„ í‰ê°€
    LangChainStringEvaluator("qa"),
    # ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±
    LangChainStringEvaluator("context_qa"),
    # ê¸¸ì´ í‰ê°€
    lambda inputs, outputs: {
        "key": "length",
        "score": 1.0 if len(outputs["output"]) < 500 else 0.5
    }
]

# 3. í‰ê°€ ì‹¤í–‰
results = evaluate(
    lambda inputs: rag_chain.invoke(inputs["question"]),
    data=dataset.name,
    evaluators=evaluators,
    experiment_prefix="rag-eval-v1"
)

print(results.to_pandas())
\`\`\`

---

## í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§

\`\`\`python
# ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë©”íŠ¸ë¦­:

# 1. ì§€ì—° ì‹œê°„ (Latency)
# - P50, P95, P99 ì§€ì—° ì‹œê°„
# - ë‹¨ê³„ë³„ ì§€ì—° ì‹œê°„ ë¶„ì„

# 2. í† í° ì‚¬ìš©ëŸ‰
# - ì…ë ¥/ì¶œë ¥ í† í° ìˆ˜
# - ë¹„ìš© ì¶”ì •

# 3. ì—ëŸ¬ìœ¨
# - ì—ëŸ¬ ìœ í˜•ë³„ ë¶„ë¥˜
# - ì—ëŸ¬ ë°œìƒ íŒ¨í„´

# 4. í”¼ë“œë°± ì ìˆ˜
# - ì‚¬ìš©ì ë§Œì¡±ë„
# - ìë™í™”ëœ í‰ê°€ ì ìˆ˜
\`\`\`

---

## ë¹„ìš© ìµœì í™” íŒ

\`\`\`python
# 1. ìƒ˜í”Œë§
import os
os.environ["LANGCHAIN_TRACING_SAMPLE_RATE"] = "0.1"  # 10%ë§Œ ì¶”ì 

# 2. ê°œë°œ ì¤‘ì—ë§Œ ì¶”ì 
if os.environ.get("ENV") == "development":
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
else:
    os.environ["LANGCHAIN_TRACING_V2"] = "false"

# 3. íŠ¹ì • ì²´ì¸ë§Œ ì¶”ì 
from langchain.callbacks import tracing_v2_enabled

with tracing_v2_enabled():
    # ì´ ë¸”ë¡ ë‚´ì˜ í˜¸ì¶œë§Œ ì¶”ì 
    result = chain.invoke(input)
\`\`\`
      `,
      keyPoints: [
        'LangSmith: LLM ì•± ê´€ì¸¡ì„± í”Œë«í¼',
        '@traceableë¡œ ì»¤ìŠ¤í…€ ì¶”ì ',
        'ìë™í™”ëœ í‰ê°€ë¡œ í’ˆì§ˆ ê´€ë¦¬',
      ],
      practiceGoal: 'LangSmithë¡œ RAG ì‹œìŠ¤í…œì„ ëª¨ë‹ˆí„°ë§í•˜ê³  í‰ê°€í•  ìˆ˜ ìˆë‹¤',
    }),

    // ========================================
    // Task 6: Day 4 ì¢…í•© í€´ì¦ˆ (20ë¶„)
    // ========================================
    createQuizTask('w5d4-quiz', 'Day 4 ì¢…í•© í€´ì¦ˆ', 20, {
      introduction: `
## í€´ì¦ˆ ì•ˆë‚´

Day 4ì—ì„œ í•™ìŠµí•œ LangChain RAG íŒŒì´í”„ë¼ì¸ ê°œë…ì„ í™•ì¸í•©ë‹ˆë‹¤.
ê° ë¬¸ì œë¥¼ ì‹ ì¤‘í•˜ê²Œ ì½ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
      `,
      questions: [
        {
          id: 'w5d4-q1',
          question: 'LCELì—ì„œ | (íŒŒì´í”„) ì—°ì‚°ìì˜ ì—­í• ì€?',
          options: [
            'OR ì¡°ê±´ ì—°ì‚°',
            'ì»´í¬ë„ŒíŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°',
            'ë³‘ë ¬ ì‹¤í–‰',
            'ì¡°ê±´ë¶€ ë¶„ê¸°',
          ],
          correctAnswer: 1,
          explanation: 'LCELì—ì„œ | ì—°ì‚°ìëŠ” ì™¼ìª½ ì»´í¬ë„ŒíŠ¸ì˜ ì¶œë ¥ì„ ì˜¤ë¥¸ìª½ ì»´í¬ë„ŒíŠ¸ì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d4-q2',
          question: 'RunnablePassthroughì˜ ì—­í• ì€?',
          options: [
            'ì—ëŸ¬ë¥¼ ë¬´ì‹œí•˜ê³  ë‹¤ìŒìœ¼ë¡œ ì „ë‹¬',
            'ì…ë ¥ì„ ë³€í™˜ ì—†ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬',
            'ì¶œë ¥ì„ ìºì‹±',
            'ë³‘ë ¬ ì‹¤í–‰ ê²°ê³¼ ë³‘í•©',
          ],
          correctAnswer: 1,
          explanation: 'RunnablePassthroughëŠ” ì…ë ¥ì„ ì–´ë–¤ ë³€í™˜ë„ ì—†ì´ ê·¸ëŒ€ë¡œ ë‹¤ìŒ ì»´í¬ë„ŒíŠ¸ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. ì£¼ë¡œ ë”•ì…”ë„ˆë¦¬ êµ¬ì„± ì‹œ ì›ë³¸ ì…ë ¥ì„ ìœ ì§€í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d4-q3',
          question: 'ëŒ€í™”í˜• RAGì—ì„œ ì§ˆë¬¸ ì¬ì‘ì„±(Contextualization)ì´ í•„ìš”í•œ ì´ìœ ëŠ”?',
          options: [
            'ê²€ìƒ‰ ì†ë„ í–¥ìƒ',
            'ëŒ€ëª…ì‚¬ ë“± ë¬¸ë§¥ ì˜ì¡´ì  í‘œí˜„ì„ í•´ê²°í•˜ê¸° ìœ„í•´',
            'í† í° ë¹„ìš© ì ˆê°',
            'ë³´ì•ˆ ê°•í™”',
          ],
          correctAnswer: 1,
          explanation: 'ëŒ€ëª…ì‚¬("ê·¸ê²ƒ", "ì´ì „ì—")ë‚˜ ìƒëµëœ ì£¼ì–´ ë“± ë¬¸ë§¥ ì˜ì¡´ì  í‘œí˜„ì„ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•´ì•¼ ê²€ìƒ‰ì´ ì •í™•í•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d4-q4',
          question: 'Parent Document Retrieverì˜ í•µì‹¬ ì›ë¦¬ëŠ”?',
          options: [
            'í° ì²­í¬ë¡œ ê²€ìƒ‰, ì‘ì€ ì²­í¬ ë°˜í™˜',
            'ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰, í° ì²­í¬(ë¶€ëª¨) ë°˜í™˜',
            'ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰, ê²°ê³¼ ë³‘í•©',
            'ìì—°ì–´ì—ì„œ í•„í„° ìë™ ì¶”ì¶œ',
          ],
          correctAnswer: 1,
          explanation: 'Parent Document RetrieverëŠ” ì‘ì€ ì²­í¬ë¡œ ì •í™•í•œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ê²€ìƒ‰ëœ ì²­í¬ì˜ ë¶€ëª¨(í° ì²­í¬)ë¥¼ ë°˜í™˜í•˜ì—¬ í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.',
        },
        {
          id: 'w5d4-q5',
          question: 'LangSmithì˜ @traceable ë°ì½”ë ˆì´í„°ì˜ ì—­í• ì€?',
          options: [
            'í•¨ìˆ˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìºì‹±',
            'í•¨ìˆ˜ ì‹¤í–‰ì„ LangSmithì— ì¶”ì  ê¸°ë¡',
            'í•¨ìˆ˜ ì—ëŸ¬ë¥¼ ìë™ ë³µêµ¬',
            'í•¨ìˆ˜ ì‹¤í–‰ì„ ë³‘ë ¬í™”',
          ],
          correctAnswer: 1,
          explanation: '@traceable ë°ì½”ë ˆì´í„°ëŠ” í•¨ìˆ˜ì˜ ì…ë ¥, ì¶œë ¥, ì‹¤í–‰ ì‹œê°„ ë“±ì„ LangSmithì— ìë™ìœ¼ë¡œ ê¸°ë¡í•˜ì—¬ ë””ë²„ê¹…ê³¼ ëª¨ë‹ˆí„°ë§ì— í™œìš©í•©ë‹ˆë‹¤.',
        },
      ],
      keyPoints: [
        '| ì—°ì‚°ì: ì»´í¬ë„ŒíŠ¸ ìˆœì°¨ ì—°ê²°',
        'ì§ˆë¬¸ ì¬ì‘ì„±: ë¬¸ë§¥ ì˜ì¡´ì  í‘œí˜„ í•´ê²°',
        'Parent Document: ì‘ì€ ì²­í¬ ê²€ìƒ‰, í° ì²­í¬ ë°˜í™˜',
      ],
      practiceGoal: 'Day 4 í•µì‹¬ ê°œë…ì„ ì´í•´í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤',
    }),

    // ========================================
    // Challenge: ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œ êµ¬ì¶• (70ë¶„)
    // ========================================
    createChallengeTask('w5d4-challenge', 'ë©€í‹° ê¸°ëŠ¥ RAG ì‹œìŠ¤í…œ êµ¬ì¶•', 70, {
      introduction: `
## ì±Œë¦°ì§€ ì‹œë‚˜ë¦¬ì˜¤

ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ í”Œë«í¼ì˜ AI ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. íšŒì‚¬ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ê°–ì¶˜ RAG ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ë ¤ í•©ë‹ˆë‹¤:

1. **ê¸°ë³¸ RAG**: ë¬¸ì„œ ê²€ìƒ‰ ê¸°ë°˜ Q&A
2. **ëŒ€í™”í˜• RAG**: íˆìŠ¤í† ë¦¬ ìœ ì§€
3. **Self-Query**: ë©”íƒ€ë°ì´í„° í•„í„°ë§
4. **ìŠ¤íŠ¸ë¦¬ë°**: ì‹¤ì‹œê°„ ì‘ë‹µ

**ìš”êµ¬ì‚¬í•­:**
- LangChain LCEL ì‚¬ìš©
- í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ì—ëŸ¬ ì²˜ë¦¬
- LangSmith í†µí•© (ì„ íƒ)

---

## í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ê¸°ì¤€ |
|------|------|------|
| ê¸°ë³¸ RAG | 25ì  | LCEL ì²´ì¸ êµ¬ì„±, ì†ŒìŠ¤ ì¶”ì  |
| ëŒ€í™”í˜• RAG | 25ì  | íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ì§ˆë¬¸ ì¬ì‘ì„± |
| Self-Query | 25ì  | ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ, í•„í„° ì¶”ì¶œ |
| ìŠ¤íŠ¸ë¦¬ë° | 25ì  | ë™ê¸°/ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° |

---

## í…ŒìŠ¤íŠ¸ ë°ì´í„°

\`\`\`python
from langchain.schema import Document

TECH_DOCUMENTS = [
    Document(
        page_content="""
        RAG(Retrieval-Augmented Generation)ëŠ” 2020ë…„ Meta AIì—ì„œ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.
        ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ LLMì˜ í™˜ê°ì„ ì¤„ì´ê³  ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        ì£¼ìš” êµ¬ì„±ìš”ì†Œ: ê²€ìƒ‰ê¸°(Retriever), ìƒì„±ê¸°(Generator), ë²¡í„° ì €ì¥ì†Œ
        """,
        metadata={"category": "AI", "year": 2024, "author": "ê¹€ì² ìˆ˜"}
    ),
    Document(
        page_content="""
        LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
        LCEL(LangChain Expression Language)ë¡œ ì„ ì–¸ì  ì²´ì¸ êµ¬ì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
        100ê°œ ì´ìƒì˜ í†µí•©(OpenAI, Chroma, Pinecone ë“±)ì„ ì§€ì›í•©ë‹ˆë‹¤.
        """,
        metadata={"category": "Framework", "year": 2024, "author": "ì´ì˜í¬"}
    ),
    Document(
        page_content="""
        ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        ëŒ€í‘œì ì¸ ì¸ë±ì‹± ì•Œê³ ë¦¬ì¦˜: HNSW, IVF, PQ
        ì£¼ìš” ì†”ë£¨ì…˜: Chroma(ë¡œì»¬), Pinecone(í´ë¼ìš°ë“œ), pgvector(PostgreSQL)
        """,
        metadata={"category": "Database", "year": 2023, "author": "ë°•ì§€ë¯¼"}
    ),
    Document(
        page_content="""
        í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì€ LLMì—ì„œ ì›í•˜ëŠ” ì¶œë ¥ì„ ì–»ê¸° ìœ„í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.
        ì£¼ìš” ê¸°ë²•: Few-shot, Chain-of-Thought, ReAct
        ì¢‹ì€ í”„ë¡¬í”„íŠ¸ëŠ” ëª…í™•í•œ ì§€ì‹œ, ì¶©ë¶„í•œ ì»¨í…ìŠ¤íŠ¸, ì˜ˆì‹œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
        """,
        metadata={"category": "AI", "year": 2023, "author": "ê¹€ì² ìˆ˜"}
    )
]
\`\`\`
      `,
      keyPoints: [
        'LCEL ê¸°ë°˜ RAG ì²´ì¸ (ì†ŒìŠ¤ ì¶”ì  í¬í•¨)',
        'ëŒ€í™”í˜• RAG (íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ì§ˆë¬¸ ì¬ì‘ì„±)',
        'Self-Query Retriever (category, year, author í•„í„°)',
        'ë™ê¸°/ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° ì§€ì›',
      ],
      hints: [
        '**í”„ë¡œì íŠ¸ êµ¬ì¡°**: multi_rag/ í´ë”ì— chains/(basic_rag.py, conversational.py, self_query.py), retrievers.py, prompts.py, main.py êµ¬ì„±',
        '**ê¸°ë³¸ RAG ì²´ì¸**: RunnableParallelë¡œ docsì™€ question ë³‘ë ¬ ì²˜ë¦¬, ì†ŒìŠ¤ ì¶”ì¶œ í¬í•¨',
        '**Self-Query**: AttributeInfoë¡œ ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜, "2024ë…„ AI ë¬¸ì„œ" â†’ {"category": "AI", "year": 2024} ìë™ í•„í„°',
        '**í†µí•© ì¸í„°í˜ì´ìŠ¤**: mode íŒŒë¼ë¯¸í„°ë¡œ basic/conversational/self_query ì „í™˜, stream ë©”ì„œë“œë¡œ í† í° ë‹¨ìœ„ ì¶œë ¥',
      ],
    }),
  ],
}
