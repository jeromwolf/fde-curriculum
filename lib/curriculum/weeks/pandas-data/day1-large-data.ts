// Day 1: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
import type { Task } from '../../types'

export const day1Tasks: Task[] = [
  {
    id: 'large-data-intro-video',
    type: 'video',
    title: 'ì™œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ê°€ ì–´ë ¤ìš´ê°€?',
    duration: 15,
    content: {
      objectives: [
        'ë©”ëª¨ë¦¬ ì œì•½ê³¼ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì˜ ë„ì „ì„ ì´í•´í•œë‹¤',
        'pandasì˜ ë©”ëª¨ë¦¬ ì‚¬ìš© íŒ¨í„´ì„ íŒŒì•…í•œë‹¤',
        'ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì „ëµì˜ ì¢…ë¥˜ë¥¼ ì•ˆë‹¤'
      ],
      videoUrl: 'https://www.youtube.com/watch?v=u4_c2LDi4b8',
      transcript: `
## ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ í˜„ì‹¤

ì‹¤ë¬´ì—ì„œ ë§Œë‚˜ëŠ” ë°ì´í„°ëŠ” GB ë‹¨ìœ„ë¥¼ ë„˜ì–´ê°€ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ ë°©ë²•ìœ¼ë¡œ pandasë¥¼ ì‚¬ìš©í•˜ë©´ ë©”ëª¨ë¦¬ ë¬¸ì œì— ë¶€ë”ªí™ë‹ˆë‹¤.

### ë©”ëª¨ë¦¬ ë¬¸ì œì˜ ì›ì¸

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì™œ pandasëŠ” ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í• ê¹Œ?                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ                             â”‚
â”‚  2. ë¬¸ìì—´ì€ Python ê°ì²´ë¡œ ì €ì¥ (ì˜¤ë²„í—¤ë“œ í¼)               â”‚
â”‚  3. ê¸°ë³¸ dtypeì´ ë„‰ë„‰í•¨ (int64, float64)                   â”‚
â”‚  4. ì—°ì‚° ì¤‘ ì„ì‹œ ë³µì‚¬ë³¸ ìƒì„±                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •

| CSV í¬ê¸° | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ëŒ€ëµ) | ì´ìœ  |
|----------|---------------------|------|
| 1 GB | 2-5 GB | ë¬¸ìì—´ ë³€í™˜, dtype ì˜¤ë²„í—¤ë“œ |
| 5 GB | 10-25 GB | ë™ì¼ ë¹„ìœ¨ |
| 10 GB | ë¶ˆê°€ëŠ¥ | ëŒ€ë¶€ë¶„ PCì—ì„œ OOM |

### í•´ê²° ì „ëµ Overview

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì „ëµ                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Chunk ì²˜ë¦¬    â†’ ë°ì´í„°ë¥¼ ì¡°ê°ë‚´ì–´ ìˆœì°¨ ì²˜ë¦¬              â”‚
â”‚  2. dtype ìµœì í™”  â†’ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50-90% ì ˆì•½               â”‚
â”‚  3. í•„ìš” ì»¬ëŸ¼ë§Œ   â†’ usecolsë¡œ í•„ìš”í•œ ê²ƒë§Œ ë¡œë“œ              â”‚
â”‚  4. íŒŒì¼ í¬ë§·     â†’ CSV â†’ Parquet (5-10ë°° íš¨ìœ¨)            â”‚
â”‚  5. ëŒ€ì•ˆ ë„êµ¬     â†’ Polars, Dask, Vaex                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### í•µì‹¬ ë©”ì‹œì§€

> "1GB íŒŒì¼ì„ ì½ìœ¼ë ¤ë©´ 3GB ë©”ëª¨ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆë‹¤. ì „ëµ ì—†ì´ \`pd.read_csv()\`ë¥¼ í˜¸ì¶œí•˜ì§€ ë§ˆë¼."
      `,
      keyPoints: [
        'pandasëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ',
        'CSV í¬ê¸°ì˜ 2-5ë°° ë©”ëª¨ë¦¬ í•„ìš”',
        'Chunk, dtype, usecols, Parquet ë“± ì „ëµ í•„ìš”',
        'ì „ëµ ì—†ìœ¼ë©´ OOM(Out of Memory) ë°œìƒ'
      ]
    }
  },
  {
    id: 'chunk-processing-video',
    type: 'video',
    title: 'Chunk ë‹¨ìœ„ ì²˜ë¦¬ íŒ¨í„´',
    duration: 15,
    content: {
      objectives: [
        'chunksize íŒŒë¼ë¯¸í„°ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤',
        'ì²­í¬ ë‹¨ìœ„ ì§‘ê³„ì™€ ê²°í•© íŒ¨í„´ì„ ìµíŒë‹¤',
        'ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•œë‹¤'
      ],
      videoUrl: 'https://www.youtube.com/watch?v=5J5RiMFj8Ag',
      transcript: `
## Chunk ì²˜ë¦¬ë€?

**Chunk ì²˜ë¦¬**ëŠ” ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì‘ì€ ì¡°ê°(chunk)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### ê¸°ë³¸ íŒ¨í„´

\`\`\`python
import pandas as pd

# chunksizeë¡œ ì´í„°ë ˆì´í„° ìƒì„±
chunks = pd.read_csv('large.csv', chunksize=100_000)

# ê° ì²­í¬ë¥¼ ìˆœì°¨ ì²˜ë¦¬
results = []
for chunk in chunks:
    processed = process(chunk)
    results.append(processed)

# ê²°ê³¼ í•©ì¹˜ê¸°
final = pd.concat(results, ignore_index=True)
\`\`\`

### ì§‘ê³„ ì—°ì‚° íŒ¨í„´

\`\`\`python
# íŒ¨í„´ 1: í•©ê³„/ê°œìˆ˜ ì§‘ê³„
total_sum = 0
total_count = 0

for chunk in pd.read_csv('large.csv', chunksize=100_000):
    total_sum += chunk['amount'].sum()
    total_count += len(chunk)

average = total_sum / total_count
\`\`\`

\`\`\`python
# íŒ¨í„´ 2: ê·¸ë£¹ë³„ ì§‘ê³„
from collections import defaultdict

group_sums = defaultdict(float)
group_counts = defaultdict(int)

for chunk in pd.read_csv('large.csv', chunksize=100_000):
    for category, amount in zip(chunk['category'], chunk['amount']):
        group_sums[category] += amount
        group_counts[category] += 1

# ê·¸ë£¹ë³„ í‰ê· 
group_avg = {k: group_sums[k] / group_counts[k] for k in group_sums}
\`\`\`

### Chunk í¬ê¸° ì„ ì •

| RAM | ê¶Œì¥ chunksize | ì´ìœ  |
|-----|---------------|------|
| 8GB | 50,000 - 100,000 | ì—¬ìœ  ë©”ëª¨ë¦¬ í™•ë³´ |
| 16GB | 100,000 - 500,000 | ì ì ˆí•œ ê· í˜• |
| 32GB+ | 500,000 - 1,000,000 | ì†ë„ ìš°ì„  |

\`\`\`python
# ë™ì  chunksize ê³„ì‚°
import psutil

available_mb = psutil.virtual_memory().available / 1024 / 1024
# ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬ì˜ 10%ë§Œ ì‚¬ìš©
chunksize = int(available_mb * 1000 / 10)  # ëŒ€ëµì  ê³„ì‚°
\`\`\`

### í•µì‹¬ ë©”ì‹œì§€

> "Chunk ì²˜ë¦¬ì˜ í•µì‹¬ì€ **ì¤‘ê°„ ê²°ê³¼ í•©ì¹˜ê¸°** ì „ëµì´ë‹¤. í•©ê³„, ê°œìˆ˜, min, maxëŠ” ì‰½ì§€ë§Œ ì¤‘ì•™ê°’, ë¶„ìœ„ìˆ˜ëŠ” ì–´ë µë‹¤."
      `,
      keyPoints: [
        'chunksizeë¡œ ì´í„°ë ˆì´í„° ìƒì„±',
        'ì²­í¬ë³„ ì²˜ë¦¬ í›„ ê²°ê³¼ í•©ì¹˜ê¸°',
        'RAMì— ë”°ë¼ chunksize ì¡°ì ˆ',
        'í•©ê³„/ê°œìˆ˜ëŠ” ì‰½ì§€ë§Œ ì¤‘ì•™ê°’ì€ ë³µì¡'
      ]
    }
  },
  {
    id: 'chunk-processing-code',
    type: 'code',
    title: 'Chunk ì²˜ë¦¬ ì‹¤ìŠµ',
    duration: 20,
    content: {
      objectives: [
        'chunksizeë¥¼ ì‚¬ìš©í•œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤',
        'ì²­í¬ë³„ ì§‘ê³„ í›„ ê²°ê³¼ë¥¼ í•©ì¹œë‹¤',
        'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•œë‹¤'
      ],
      instructions: `
Chunk ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

## ìš”êµ¬ì‚¬í•­
1. chunksize=100,000ìœ¼ë¡œ íŒŒì¼ ì½ê¸°
2. ê° ì²­í¬ì—ì„œ categoryë³„ amount í•©ê³„ ê³„ì‚°
3. ì²­í¬ ê²°ê³¼ë“¤ì„ í•©ì³ì„œ ìµœì¢… ì§‘ê³„
4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
      `,
      starterCode: `
import pandas as pd
import sys

def get_memory_usage():
    """í˜„ì¬ DataFrameë“¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


def process_chunk(chunk: pd.DataFrame) -> pd.DataFrame:
    """ì²­í¬ë³„ ì§‘ê³„ ì²˜ë¦¬

    Args:
        chunk: ì…ë ¥ ì²­í¬

    Returns:
        pd.DataFrame: categoryë³„ amount í•©ê³„ì™€ ê°œìˆ˜
    """
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


def aggregate_results(results: list) -> pd.DataFrame:
    """ì²­í¬ ê²°ê³¼ë“¤ì„ í•©ì³ì„œ ìµœì¢… ì§‘ê³„

    Args:
        results: ì²­í¬ë³„ ì§‘ê³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

    Returns:
        pd.DataFrame: ìµœì¢… ì§‘ê³„ ê²°ê³¼
    """
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


def process_large_file(filepath: str, chunksize: int = 100_000) -> pd.DataFrame:
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬

    Args:
        filepath: CSV íŒŒì¼ ê²½ë¡œ
        chunksize: ì²­í¬ í¬ê¸°

    Returns:
        pd.DataFrame: ìµœì¢… ì§‘ê³„ ê²°ê³¼
    """
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
import numpy as np

np.random.seed(42)
n_rows = 500_000  # 50ë§Œ í–‰

sample_data = pd.DataFrame({
    'id': range(n_rows),
    'category': np.random.choice(['A', 'B', 'C', 'D'], n_rows),
    'amount': np.random.uniform(10, 1000, n_rows),
    'date': pd.date_range('2024-01-01', periods=n_rows, freq='s')
})
sample_data.to_csv('/tmp/large_sample.csv', index=False)
print(f"ìƒ˜í”Œ íŒŒì¼ ìƒì„±: {n_rows:,}í–‰")

# ì²˜ë¦¬ ì‹¤í–‰
result = process_large_file('/tmp/large_sample.csv')
print("\\n=== ìµœì¢… ê²°ê³¼ ===")
print(result)
      `,
      solutionCode: `
import pandas as pd
import sys
import tracemalloc

def get_memory_usage():
    """í˜„ì¬ DataFrameë“¤ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)

    ğŸ’¡ tracemallocì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì¶”ì 
    """
    current, peak = tracemalloc.get_traced_memory()
    return current / 1024 / 1024  # MB


def process_chunk(chunk: pd.DataFrame) -> pd.DataFrame:
    """ì²­í¬ë³„ ì§‘ê³„ ì²˜ë¦¬

    ğŸ¯ ì—­í• : ê° ì²­í¬ì—ì„œ categoryë³„ í†µê³„ ê³„ì‚°

    ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:
    - groupbyë¡œ categoryë³„ ì§‘ê³„
    - sumê³¼ count ëª¨ë‘ ê³„ì‚° (ë‚˜ì¤‘ì— í‰ê·  ê³„ì‚°ìš©)
    """
    return chunk.groupby('category').agg(
        amount_sum=('amount', 'sum'),
        count=('amount', 'count')
    ).reset_index()


def aggregate_results(results: list) -> pd.DataFrame:
    """ì²­í¬ ê²°ê³¼ë“¤ì„ í•©ì³ì„œ ìµœì¢… ì§‘ê³„

    ğŸ¯ ì—­í• : ì²­í¬ë³„ ë¶€ë¶„ ì§‘ê³„ë¥¼ í•©ì³ ì „ì²´ í†µê³„ ê³„ì‚°

    ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:
    - concatìœ¼ë¡œ ëª¨ë“  ì²­í¬ ê²°ê³¼ í•©ì¹˜ê¸°
    - ë‹¤ì‹œ groupbyë¡œ ì „ì²´ í•©ê³„ ê³„ì‚°
    - í‰ê·  = ì´í•© / ì´ê°œìˆ˜
    """
    combined = pd.concat(results, ignore_index=True)

    final = combined.groupby('category').agg(
        total_amount=('amount_sum', 'sum'),
        total_count=('count', 'sum')
    ).reset_index()

    final['average'] = final['total_amount'] / final['total_count']
    return final.sort_values('total_amount', ascending=False)


def process_large_file(filepath: str, chunksize: int = 100_000) -> pd.DataFrame:
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬

    ğŸ¯ ì—­í• : ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬

    ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:
    - chunksizeë¡œ ì´í„°ë ˆì´í„° ìƒì„±
    - ê° ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë§Œ ì €ì¥
    - ì›ë³¸ ì²­í¬ëŠ” ì¦‰ì‹œ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œ
    """
    tracemalloc.start()

    results = []
    chunk_count = 0

    print("ì²­í¬ ì²˜ë¦¬ ì‹œì‘...")
    for chunk in pd.read_csv(filepath, chunksize=chunksize):
        chunk_count += 1
        processed = process_chunk(chunk)
        results.append(processed)

        if chunk_count % 3 == 0:
            print(f"  ì²­í¬ {chunk_count} ì™„ë£Œ, ë©”ëª¨ë¦¬: {get_memory_usage():.1f} MB")

    print(f"ì´ {chunk_count}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"ìµœì¢… ë©”ëª¨ë¦¬ ì‚¬ìš©: {get_memory_usage():.1f} MB")

    final = aggregate_results(results)

    tracemalloc.stop()
    return final


# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
import numpy as np

np.random.seed(42)
n_rows = 500_000  # 50ë§Œ í–‰

sample_data = pd.DataFrame({
    'id': range(n_rows),
    'category': np.random.choice(['A', 'B', 'C', 'D'], n_rows),
    'amount': np.random.uniform(10, 1000, n_rows),
    'date': pd.date_range('2024-01-01', periods=n_rows, freq='s')
})
sample_data.to_csv('/tmp/large_sample.csv', index=False)
print(f"ìƒ˜í”Œ íŒŒì¼ ìƒì„±: {n_rows:,}í–‰")

# ì²˜ë¦¬ ì‹¤í–‰
result = process_large_file('/tmp/large_sample.csv')
print("\\n=== ìµœì¢… ê²°ê³¼ ===")
print(result)

# ë©”ëª¨ë¦¬ ë¹„êµ: ì „ì²´ ë¡œë“œ vs ì²­í¬ ì²˜ë¦¬
print("\\n=== ë©”ëª¨ë¦¬ ë¹„êµ ===")
tracemalloc.start()
df_full = pd.read_csv('/tmp/large_sample.csv')
current, _ = tracemalloc.get_traced_memory()
print(f"ì „ì²´ ë¡œë“œ: {current / 1024 / 1024:.1f} MB")
tracemalloc.stop()
      `,
      keyPoints: [
        'chunksizeë¡œ ì´í„°ë ˆì´í„° ìƒì„±',
        'ì²­í¬ë³„ë¡œ ë¶€ë¶„ ì§‘ê³„ ê³„ì‚°',
        'ê²°ê³¼ í•©ì¹  ë•Œ sum, count ë”°ë¡œ ê´€ë¦¬',
        'tracemallocìœ¼ë¡œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§'
      ]
    }
  },
  {
    id: 'dtype-optimization-video',
    type: 'video',
    title: 'dtype ìµœì í™”ë¡œ ë©”ëª¨ë¦¬ 50-90% ì ˆì•½',
    duration: 15,
    content: {
      objectives: [
        'pandasì˜ ê¸°ë³¸ dtypeê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì´í•´í•œë‹¤',
        'ìµœì ì˜ dtypeì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ìµíŒë‹¤',
        'category íƒ€ì…ì˜ íš¨ê³¼ë¥¼ íŒŒì•…í•œë‹¤'
      ],
      transcript: `
## dtypeì´ ë©”ëª¨ë¦¬ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

pandasëŠ” ê¸°ë³¸ì ìœ¼ë¡œ **ë„‰ë„‰í•œ dtype**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë©”ëª¨ë¦¬ ë‚­ë¹„ì˜ ì£¼ë²”ì…ë‹ˆë‹¤.

### ê¸°ë³¸ dtype vs ìµœì í™” dtype

| ë°ì´í„° | ê¸°ë³¸ dtype | ìµœì  dtype | ì ˆì•½ë¥  |
|--------|-----------|-----------|--------|
| ì •ìˆ˜ (0-100) | int64 (8B) | int8 (1B) | 87.5% |
| ì •ìˆ˜ (0-65535) | int64 (8B) | uint16 (2B) | 75% |
| ì‹¤ìˆ˜ (ì¼ë°˜) | float64 (8B) | float32 (4B) | 50% |
| ì¹´í…Œê³ ë¦¬ | object (ê°€ë³€) | category | 90%+ |

### ì •ìˆ˜ íƒ€ì… ì„ íƒ ê°€ì´ë“œ

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì •ìˆ˜ íƒ€ì… ë²”ìœ„                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  int8:   -128 ~ 127                    (1 byte)           â”‚
â”‚  int16:  -32,768 ~ 32,767              (2 bytes)          â”‚
â”‚  int32:  -2.1B ~ 2.1B                  (4 bytes)          â”‚
â”‚  int64:  -9.2E ~ 9.2E                  (8 bytes)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  uint8:  0 ~ 255                       (1 byte)           â”‚
â”‚  uint16: 0 ~ 65,535                    (2 bytes)          â”‚
â”‚  uint32: 0 ~ 4.3B                      (4 bytes)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### Category íƒ€ì…ì˜ ë§ˆë²•

\`\`\`python
# ì¹´í…Œê³ ë¦¬ê°€ ì ì€ ë¬¸ìì—´ì— ë§¤ìš° íš¨ê³¼ì 
df['status'] = df['status'].astype('category')

# ì˜ˆ: 100ë§Œ í–‰, ìƒíƒœê°’ 3ê°œ (active, pending, closed)
# object: ~80 MB
# category: ~1 MB (80ë°° ì ˆì•½!)
\`\`\`

### ì½”ë“œ ì˜ˆì‹œ

\`\`\`python
# ë°©ë²• 1: read_csvì—ì„œ dtype ì§€ì •
dtypes = {
    'id': 'int32',
    'age': 'int8',
    'salary': 'float32',
    'department': 'category',
    'status': 'category'
}
df = pd.read_csv('data.csv', dtype=dtypes)

# ë°©ë²• 2: ë¡œë“œ í›„ ë³€í™˜
df['age'] = df['age'].astype('int8')
df['status'] = df['status'].astype('category')

# ë©”ëª¨ë¦¬ í™•ì¸
print(df.memory_usage(deep=True))
\`\`\`

### í•µì‹¬ ë©”ì‹œì§€

> "dtype ìµœì í™”ë§Œìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ 50-90% ì ˆì•½í•  ìˆ˜ ìˆë‹¤. ë°ì´í„°ì˜ ì‹¤ì œ ë²”ìœ„ë¥¼ í™•ì¸í•˜ê³  ì ì ˆí•œ íƒ€ì…ì„ ì„ íƒí•˜ë¼."
      `,
      keyPoints: [
        'ê¸°ë³¸ int64ë¥¼ int8/int16/int32ë¡œ ë³€ê²½í•˜ë©´ 87.5%ê¹Œì§€ ì ˆì•½',
        'float64 â†’ float32ë¡œ 50% ì ˆì•½',
        'category íƒ€ì…ì€ ë°˜ë³µ ë¬¸ìì—´ì— 90%+ ì ˆì•½',
        'memory_usage(deep=True)ë¡œ ì‹¤ì œ ë©”ëª¨ë¦¬ í™•ì¸'
      ]
    }
  },
  {
    id: 'dtype-optimization-code',
    type: 'code',
    title: 'dtype ìµœì í™” ì‹¤ìŠµ',
    duration: 20,
    content: {
      objectives: [
        'ì»¬ëŸ¼ë³„ ìµœì  dtypeì„ ìë™ìœ¼ë¡œ ì¶”ë¡ í•œë‹¤',
        'dtype ìµœì í™” ì „í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¹„êµí•œë‹¤',
        'ìµœì í™” í•¨ìˆ˜ë¥¼ ì¬ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ êµ¬í˜„í•œë‹¤'
      ],
      instructions: `
DataFrameì˜ dtypeì„ ìë™ìœ¼ë¡œ ìµœì í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

## ìš”êµ¬ì‚¬í•­
1. ì •ìˆ˜ ì»¬ëŸ¼: ê°’ ë²”ìœ„ì— ë§ëŠ” ìµœì†Œ int íƒ€ì… ì„ íƒ
2. ì‹¤ìˆ˜ ì»¬ëŸ¼: float32ë¡œ ë‹¤ìš´ìºìŠ¤íŠ¸
3. ë¬¸ìì—´ ì»¬ëŸ¼: ìœ ë‹ˆí¬ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ categoryë¡œ ë³€í™˜
4. ìµœì í™” ì „í›„ ë©”ëª¨ë¦¬ ë¹„êµ ì¶œë ¥
      `,
      starterCode: `
import pandas as pd
import numpy as np

def optimize_dtypes(df: pd.DataFrame, category_threshold: float = 0.5) -> pd.DataFrame:
    """DataFrameì˜ dtypeì„ ìµœì í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½

    Args:
        df: ì…ë ¥ DataFrame
        category_threshold: ìœ ë‹ˆí¬ ë¹„ìœ¨ì´ ì´ ê°’ ì´í•˜ë©´ categoryë¡œ ë³€í™˜

    Returns:
        pd.DataFrame: dtypeì´ ìµœì í™”ëœ DataFrame
    """
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


def optimize_integers(df: pd.DataFrame) -> pd.DataFrame:
    """ì •ìˆ˜ ì»¬ëŸ¼ì„ ìµœì†Œ í•„ìš” dtypeìœ¼ë¡œ ë³€í™˜"""
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


def optimize_floats(df: pd.DataFrame) -> pd.DataFrame:
    """ì‹¤ìˆ˜ ì»¬ëŸ¼ì„ float32ë¡œ ë³€í™˜"""
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


def optimize_objects(df: pd.DataFrame, threshold: float) -> pd.DataFrame:
    """ë¬¸ìì—´ ì»¬ëŸ¼ì„ categoryë¡œ ë³€í™˜ (ìœ ë‹ˆí¬ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´)"""
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


def compare_memory(before: pd.DataFrame, after: pd.DataFrame) -> None:
    """ìµœì í™” ì „í›„ ë©”ëª¨ë¦¬ ë¹„êµ ì¶œë ¥"""
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    pass


# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
np.random.seed(42)
n_rows = 100_000

df = pd.DataFrame({
    'id': np.arange(n_rows),  # 0 ~ 99999
    'age': np.random.randint(0, 100, n_rows),  # 0 ~ 99
    'score': np.random.uniform(0, 100, n_rows),  # ì‹¤ìˆ˜
    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows),  # 5ê°œ ì¹´í…Œê³ ë¦¬
    'status': np.random.choice(['active', 'inactive'], n_rows),  # 2ê°œ ìƒíƒœ
    'description': ['item_' + str(i % 1000) for i in range(n_rows)]  # 1000ê°œ ìœ ë‹ˆí¬
})

print("=== ì›ë³¸ DataFrame ===")
print(df.dtypes)
print(f"\\në©”ëª¨ë¦¬: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

# ìµœì í™” ì‹¤í–‰
df_optimized = optimize_dtypes(df)

print("\\n=== ìµœì í™” í›„ ===")
print(df_optimized.dtypes)
compare_memory(df, df_optimized)
      `,
      solutionCode: `
import pandas as pd
import numpy as np

def optimize_integers(df: pd.DataFrame) -> pd.DataFrame:
    """ì •ìˆ˜ ì»¬ëŸ¼ì„ ìµœì†Œ í•„ìš” dtypeìœ¼ë¡œ ë³€í™˜

    ğŸ¯ ì—­í• : int64ë¥¼ int8/int16/int32ë¡œ ë‹¤ìš´ìºìŠ¤íŠ¸

    ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:
    - ê°’ì˜ min/maxë¥¼ í™•ì¸í•˜ì—¬ ì ì ˆí•œ íƒ€ì… ì„ íƒ
    - unsigned ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ uintë¡œ ë³€í™˜ (ë²”ìœ„ 2ë°°)
    """
    int_cols = df.select_dtypes(include=['int64', 'int32']).columns

    for col in int_cols:
        col_min = df[col].min()
        col_max = df[col].max()

        # unsigned ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if col_min >= 0:
            if col_max <= 255:
                df[col] = df[col].astype('uint8')
            elif col_max <= 65535:
                df[col] = df[col].astype('uint16')
            elif col_max <= 4294967295:
                df[col] = df[col].astype('uint32')
        else:
            if col_min >= -128 and col_max <= 127:
                df[col] = df[col].astype('int8')
            elif col_min >= -32768 and col_max <= 32767:
                df[col] = df[col].astype('int16')
            elif col_min >= -2147483648 and col_max <= 2147483647:
                df[col] = df[col].astype('int32')

    return df


def optimize_floats(df: pd.DataFrame) -> pd.DataFrame:
    """ì‹¤ìˆ˜ ì»¬ëŸ¼ì„ float32ë¡œ ë³€í™˜

    ğŸ¯ ì—­í• : float64 â†’ float32ë¡œ ë©”ëª¨ë¦¬ 50% ì ˆì•½

    ğŸ’¡ ì£¼ì˜: ì •ë°€ë„ê°€ ì¤‘ìš”í•œ ê¸ˆìœµ/ê³¼í•™ ê³„ì‚°ì—ì„œëŠ” ì£¼ì˜
    """
    float_cols = df.select_dtypes(include=['float64']).columns

    for col in float_cols:
        df[col] = df[col].astype('float32')

    return df


def optimize_objects(df: pd.DataFrame, threshold: float) -> pd.DataFrame:
    """ë¬¸ìì—´ ì»¬ëŸ¼ì„ categoryë¡œ ë³€í™˜ (ìœ ë‹ˆí¬ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´)

    ğŸ¯ ì—­í• : ë°˜ë³µë˜ëŠ” ë¬¸ìì—´ì„ categoryë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½

    ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:
    - ìœ ë‹ˆí¬ ë¹„ìœ¨ = ìœ ë‹ˆí¬ ê°œìˆ˜ / ì „ì²´ í–‰ ìˆ˜
    - ë¹„ìœ¨ì´ ë‚®ì„ìˆ˜ë¡ category íš¨ê³¼ í¼
    """
    object_cols = df.select_dtypes(include=['object']).columns

    for col in object_cols:
        unique_ratio = df[col].nunique() / len(df)

        if unique_ratio <= threshold:
            df[col] = df[col].astype('category')
            print(f"  {col}: object â†’ category (ìœ ë‹ˆí¬ ë¹„ìœ¨: {unique_ratio:.2%})")

    return df


def compare_memory(before: pd.DataFrame, after: pd.DataFrame) -> None:
    """ìµœì í™” ì „í›„ ë©”ëª¨ë¦¬ ë¹„êµ ì¶œë ¥"""
    mem_before = before.memory_usage(deep=True).sum() / 1024 / 1024
    mem_after = after.memory_usage(deep=True).sum() / 1024 / 1024
    reduction = (1 - mem_after / mem_before) * 100

    print(f"\\nğŸ“Š ë©”ëª¨ë¦¬ ë¹„êµ:")
    print(f"  ìµœì í™” ì „: {mem_before:.2f} MB")
    print(f"  ìµœì í™” í›„: {mem_after:.2f} MB")
    print(f"  ì ˆì•½: {reduction:.1f}%")


def optimize_dtypes(df: pd.DataFrame, category_threshold: float = 0.5) -> pd.DataFrame:
    """DataFrameì˜ dtypeì„ ìµœì í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½

    ğŸ¯ ì—­í• : ëª¨ë“  ì»¬ëŸ¼ì˜ dtypeì„ ìµœì í™”

    Args:
        df: ì…ë ¥ DataFrame
        category_threshold: ìœ ë‹ˆí¬ ë¹„ìœ¨ì´ ì´ ê°’ ì´í•˜ë©´ categoryë¡œ ë³€í™˜

    Returns:
        pd.DataFrame: dtypeì´ ìµœì í™”ëœ DataFrame
    """
    df = df.copy()

    print("ğŸ”§ dtype ìµœì í™” ì‹œì‘...")
    df = optimize_integers(df)
    df = optimize_floats(df)
    df = optimize_objects(df, category_threshold)
    print("âœ… ìµœì í™” ì™„ë£Œ")

    return df


# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
np.random.seed(42)
n_rows = 100_000

df = pd.DataFrame({
    'id': np.arange(n_rows),  # 0 ~ 99999
    'age': np.random.randint(0, 100, n_rows),  # 0 ~ 99
    'score': np.random.uniform(0, 100, n_rows),  # ì‹¤ìˆ˜
    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows),  # 5ê°œ ì¹´í…Œê³ ë¦¬
    'status': np.random.choice(['active', 'inactive'], n_rows),  # 2ê°œ ìƒíƒœ
    'description': ['item_' + str(i % 1000) for i in range(n_rows)]  # 1000ê°œ ìœ ë‹ˆí¬
})

print("=== ì›ë³¸ DataFrame ===")
print(df.dtypes)
print(f"\\në©”ëª¨ë¦¬: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

# ìµœì í™” ì‹¤í–‰
df_optimized = optimize_dtypes(df)

print("\\n=== ìµœì í™” í›„ ===")
print(df_optimized.dtypes)
compare_memory(df, df_optimized)
      `,
      keyPoints: [
        'ì •ìˆ˜ëŠ” min/max í™•ì¸ í›„ ìµœì†Œ íƒ€ì… ì„ íƒ',
        'ìŒìˆ˜ ì—†ìœ¼ë©´ uint ì‚¬ìš© (ë²”ìœ„ 2ë°°)',
        'float64 â†’ float32ë¡œ 50% ì ˆì•½',
        'ìœ ë‹ˆí¬ ë¹„ìœ¨ ë‚®ìœ¼ë©´ categoryë¡œ ë³€í™˜'
      ]
    }
  },
  {
    id: 'usecols-skiprows-reading',
    type: 'reading',
    title: 'í•„ìš”í•œ ê²ƒë§Œ ì½ê¸°: usecols, skiprows',
    duration: 10,
    content: {
      objectives: [
        'usecolsë¡œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œí•˜ëŠ” ë°©ë²•ì„ ìµíŒë‹¤',
        'skiprowsì™€ nrowsë¡œ ì¼ë¶€ í–‰ë§Œ ì½ëŠ” ë°©ë²•ì„ ë°°ìš´ë‹¤',
        'ì¡°ê±´ë¶€ ë¡œë”© ì „ëµì„ ì´í•´í•œë‹¤'
      ],
      markdown: `
# í•„ìš”í•œ ê²ƒë§Œ ì½ê¸°

ê°€ì¥ íš¨ê³¼ì ì¸ ë©”ëª¨ë¦¬ ì ˆì•½ì€ **ì²˜ìŒë¶€í„° í•„ìš”í•œ ê²ƒë§Œ ì½ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.

## usecols: í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œ

\`\`\`python
# ë°©ë²• 1: ì»¬ëŸ¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
df = pd.read_csv('data.csv', usecols=['id', 'name', 'amount'])

# ë°©ë²• 2: ì¸ë±ìŠ¤ë¡œ ì§€ì •
df = pd.read_csv('data.csv', usecols=[0, 2, 5])

# ë°©ë²• 3: ëŒë‹¤ë¡œ ì¡°ê±´ ì§€ì •
df = pd.read_csv('data.csv', usecols=lambda x: x.startswith('sales_'))
\`\`\`

### íš¨ê³¼

| ì „ì²´ ì»¬ëŸ¼ | í•„ìš” ì»¬ëŸ¼ | ë©”ëª¨ë¦¬ ì ˆì•½ |
|----------|----------|------------|
| 100 | 10 | ~90% |
| 50 | 5 | ~90% |
| 20 | 10 | ~50% |

## skiprows & nrows: í•„ìš”í•œ í–‰ë§Œ ë¡œë“œ

\`\`\`python
# ì²˜ìŒ 1000í–‰ë§Œ (í”„ë¡œí† íƒ€ì… ê°œë°œìš©)
df = pd.read_csv('data.csv', nrows=1000)

# í—¤ë” + ì²˜ìŒ 1000í–‰ë§Œ
df = pd.read_csv('data.csv', nrows=1000)

# ì²˜ìŒ 100í–‰ ìŠ¤í‚µ (ì˜ëª»ëœ ë°ì´í„° ì œì™¸)
df = pd.read_csv('data.csv', skiprows=100)

# íŠ¹ì • í–‰ë§Œ ìŠ¤í‚µ (ë¦¬ìŠ¤íŠ¸ë¡œ ì§€ì •)
df = pd.read_csv('data.csv', skiprows=[1, 5, 10])  # 1, 5, 10ë²ˆ í–‰ ìŠ¤í‚µ

# ëŒë‹¤ë¡œ ì¡°ê±´ë¶€ ìŠ¤í‚µ (ì§ìˆ˜ í–‰ë§Œ ì½ê¸°)
df = pd.read_csv('data.csv', skiprows=lambda x: x % 2 == 1)
\`\`\`

## ë³µí•© ì‚¬ìš© ì˜ˆì‹œ

\`\`\`python
# ìµœì í™”ëœ ë¡œë”©: í•„ìš” ì»¬ëŸ¼ + í•„ìš” í–‰ + dtype ìµœì í™”
df = pd.read_csv(
    'large_data.csv',
    usecols=['id', 'category', 'amount'],  # í•„ìš” ì»¬ëŸ¼ë§Œ
    nrows=100_000,  # ì²˜ìŒ 10ë§Œ í–‰ë§Œ (ê°œë°œ ì¤‘)
    dtype={
        'id': 'int32',
        'category': 'category',
        'amount': 'float32'
    }
)
\`\`\`

## ì»¬ëŸ¼ ì´ë¦„ ë¯¸ë¦¬ í™•ì¸

\`\`\`python
# í—¤ë”ë§Œ ì½ì–´ì„œ ì»¬ëŸ¼ í™•ì¸
header = pd.read_csv('data.csv', nrows=0)
print(header.columns.tolist())

# ê²°ê³¼: ['id', 'name', 'category', 'amount', 'date', ...]
\`\`\`

## ì‹¤ë¬´ íŒ

| ìƒí™© | ì „ëµ |
|------|------|
| ê°œë°œ/í…ŒìŠ¤íŠ¸ | nrows=1000ìœ¼ë¡œ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… |
| EDA | usecolsë¡œ ë¶„ì„ ëŒ€ìƒë§Œ ë¡œë“œ |
| ë°°ì¹˜ ì²˜ë¦¬ | chunksize + usecols ì¡°í•© |
| íŠ¹ì • ê¸°ê°„ | skiprows + nrows (ìœ„ì¹˜ ì•Œ ë•Œ) |

> ğŸ’¡ **íŒ**: \`pd.read_csv\`ì˜ ì²« í˜¸ì¶œì€ \`nrows=5\`ë¡œ ë°ì´í„° êµ¬ì¡° íŒŒì•… í›„ ì „ëµì„ ì„¸ìš°ì„¸ìš”.
      `,
      externalLinks: [
        { title: 'pandas read_csv ë¬¸ì„œ', url: 'https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html' },
        { title: 'Efficiently Reading Large CSVs', url: 'https://realpython.com/python-csv/' }
      ],
      keyPoints: [
        'usecolsë¡œ í•„ìš” ì»¬ëŸ¼ë§Œ ë¡œë“œ',
        'nrowsë¡œ ê°œë°œ ì‹œ ì¼ë¶€ë§Œ ë¡œë“œ',
        'skiprowsë¡œ ë¶ˆí•„ìš”í•œ í–‰ ì œì™¸',
        'ë³µí•© ì‚¬ìš©ìœ¼ë¡œ ìµœëŒ€ íš¨ê³¼'
      ]
    }
  },
  {
    id: 'parquet-format-video',
    type: 'video',
    title: 'Parquet: CSVë³´ë‹¤ 10ë°° íš¨ìœ¨ì ì¸ í¬ë§·',
    duration: 10,
    content: {
      objectives: [
        'Parquet íŒŒì¼ í¬ë§·ì˜ ì¥ì ì„ ì´í•´í•œë‹¤',
        'CSVì™€ Parquetì˜ ì°¨ì´ë¥¼ íŒŒì•…í•œë‹¤',
        'Parquet ì½ê¸°/ì“°ê¸° ë°©ë²•ì„ ìµíŒë‹¤'
      ],
      transcript: `
## Parquetì´ë€?

**Parquet**ëŠ” ì»¬ëŸ¼ ê¸°ë°˜ ì €ì¥ í¬ë§·ìœ¼ë¡œ, ë¹…ë°ì´í„° ì²˜ë¦¬ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

### CSV vs Parquet

| í•­ëª© | CSV | Parquet |
|------|-----|---------|
| ì €ì¥ ë°©ì‹ | í–‰ ê¸°ë°˜ | ì»¬ëŸ¼ ê¸°ë°˜ |
| ì••ì¶• | ì—†ìŒ | ìë™ ì••ì¶• |
| íŒŒì¼ í¬ê¸° | ê¸°ì¤€ | 1/5 ~ 1/10 |
| ì½ê¸° ì†ë„ | ëŠë¦¼ | ë§¤ìš° ë¹ ë¦„ |
| dtype ë³´ì¡´ | âŒ | âœ… |

### ì™œ Parquetê°€ ë¹ ë¥¸ê°€?

\`\`\`
CSV (í–‰ ê¸°ë°˜):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id, name, amount, date                  â”‚
â”‚ 1, Alice, 100, 2024-01-01               â”‚
â”‚ 2, Bob, 200, 2024-01-02                 â”‚
â”‚ 3, Charlie, 150, 2024-01-03             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ íŠ¹ì • ì»¬ëŸ¼ë§Œ ì½ì–´ë„ ì „ì²´ í–‰ì„ ìŠ¤ìº”í•´ì•¼ í•¨

Parquet (ì»¬ëŸ¼ ê¸°ë°˜):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ id        â”‚ â”‚ name        â”‚ â”‚ amount   â”‚ â”‚ date          â”‚
â”‚ 1         â”‚ â”‚ Alice       â”‚ â”‚ 100      â”‚ â”‚ 2024-01-01    â”‚
â”‚ 2         â”‚ â”‚ Bob         â”‚ â”‚ 200      â”‚ â”‚ 2024-01-02    â”‚
â”‚ 3         â”‚ â”‚ Charlie     â”‚ â”‚ 150      â”‚ â”‚ 2024-01-03    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì½ìœ¼ë©´ ë¨
\`\`\`

### ì‚¬ìš©ë²•

\`\`\`python
# ì“°ê¸°
df.to_parquet('data.parquet', engine='pyarrow')

# ì½ê¸°
df = pd.read_parquet('data.parquet')

# íŠ¹ì • ì»¬ëŸ¼ë§Œ ì½ê¸° (ë§¤ìš° ë¹ ë¦„!)
df = pd.read_parquet('data.parquet', columns=['id', 'amount'])
\`\`\`

### í•µì‹¬ ë©”ì‹œì§€

> "ì‹¤ë¬´ì—ì„œëŠ” CSV ëŒ€ì‹  Parquetë¥¼ ì‚¬ìš©í•˜ë¼. 10ë°° ì‘ì€ íŒŒì¼, 10ë°° ë¹ ë¥¸ ë¡œë”©."
      `,
      keyPoints: [
        'ParquetëŠ” ì»¬ëŸ¼ ê¸°ë°˜ í¬ë§·',
        'CSV ëŒ€ë¹„ 1/5 ~ 1/10 íŒŒì¼ í¬ê¸°',
        'í•„ìš” ì»¬ëŸ¼ë§Œ ì½ìœ¼ë©´ 10ë°°+ ë¹ ë¦„',
        'dtype ìë™ ë³´ì¡´ (ì¬ì§€ì • ë¶ˆí•„ìš”)'
      ]
    }
  },
  {
    id: 'parquet-code',
    type: 'code',
    title: 'CSV vs Parquet ì„±ëŠ¥ ë¹„êµ',
    duration: 15,
    content: {
      objectives: [
        'CSVì™€ Parquet ê°„ ë³€í™˜ì„ ìˆ˜í–‰í•œë‹¤',
        'íŒŒì¼ í¬ê¸°ì™€ ì½ê¸° ì†ë„ë¥¼ ë¹„êµí•œë‹¤',
        'ì»¬ëŸ¼ ì„ íƒ ì½ê¸°ì˜ íš¨ê³¼ë¥¼ í™•ì¸í•œë‹¤'
      ],
      instructions: `
CSVì™€ Parquet í¬ë§·ì˜ ì„±ëŠ¥ì„ ì§ì ‘ ë¹„êµí•©ë‹ˆë‹¤.

## ìš”êµ¬ì‚¬í•­
1. ìƒ˜í”Œ ë°ì´í„°ë¥¼ CSVì™€ Parquetë¡œ ì €ì¥
2. íŒŒì¼ í¬ê¸° ë¹„êµ
3. ì „ì²´ ì½ê¸° ì†ë„ ë¹„êµ
4. íŠ¹ì • ì»¬ëŸ¼ë§Œ ì½ê¸° ì†ë„ ë¹„êµ
      `,
      starterCode: `
import pandas as pd
import numpy as np
import time
import os

def create_sample_data(n_rows: int = 500_000) -> pd.DataFrame:
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    return pd.DataFrame({
        'id': np.arange(n_rows),
        'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows),
        'value1': np.random.uniform(0, 1000, n_rows),
        'value2': np.random.uniform(0, 1000, n_rows),
        'value3': np.random.uniform(0, 1000, n_rows),
        'date': pd.date_range('2020-01-01', periods=n_rows, freq='s'),
        'description': ['item_' + str(i % 10000) for i in range(n_rows)]
    })


def benchmark_formats(df: pd.DataFrame, csv_path: str, parquet_path: str):
    """CSVì™€ Parquet í¬ë§· ë²¤ì¹˜ë§ˆí¬"""
    # TODO: êµ¬í˜„í•˜ì„¸ìš”
    # 1. íŒŒì¼ ì €ì¥
    # 2. íŒŒì¼ í¬ê¸° ë¹„êµ
    # 3. ì „ì²´ ì½ê¸° ì†ë„ ë¹„êµ
    # 4. íŠ¹ì • ì»¬ëŸ¼ë§Œ ì½ê¸° ì†ë„ ë¹„êµ
    pass


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
df = create_sample_data(500_000)
print(f"ë°ì´í„° í¬ê¸°: {len(df):,}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")

benchmark_formats(df, '/tmp/test.csv', '/tmp/test.parquet')
      `,
      solutionCode: `
import pandas as pd
import numpy as np
import time
import os

def create_sample_data(n_rows: int = 500_000) -> pd.DataFrame:
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    return pd.DataFrame({
        'id': np.arange(n_rows),
        'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows),
        'value1': np.random.uniform(0, 1000, n_rows),
        'value2': np.random.uniform(0, 1000, n_rows),
        'value3': np.random.uniform(0, 1000, n_rows),
        'date': pd.date_range('2020-01-01', periods=n_rows, freq='s'),
        'description': ['item_' + str(i % 10000) for i in range(n_rows)]
    })


def get_file_size_mb(path: str) -> float:
    """íŒŒì¼ í¬ê¸°ë¥¼ MBë¡œ ë°˜í™˜"""
    return os.path.getsize(path) / 1024 / 1024


def benchmark_formats(df: pd.DataFrame, csv_path: str, parquet_path: str):
    """CSVì™€ Parquet í¬ë§· ë²¤ì¹˜ë§ˆí¬

    ğŸ¯ ì—­í• : ë‘ í¬ë§·ì˜ íŒŒì¼ í¬ê¸°ì™€ ì½ê¸° ì†ë„ ë¹„êµ
    """
    print("=" * 60)
    print("CSV vs Parquet ë²¤ì¹˜ë§ˆí¬")
    print("=" * 60)

    # 1. íŒŒì¼ ì €ì¥
    print("\\nğŸ“ íŒŒì¼ ì €ì¥ ì¤‘...")
    df.to_csv(csv_path, index=False)
    df.to_parquet(parquet_path, engine='pyarrow')

    # 2. íŒŒì¼ í¬ê¸° ë¹„êµ
    csv_size = get_file_size_mb(csv_path)
    parquet_size = get_file_size_mb(parquet_path)

    print(f"\\nğŸ“Š íŒŒì¼ í¬ê¸°:")
    print(f"  CSV:     {csv_size:.2f} MB")
    print(f"  Parquet: {parquet_size:.2f} MB")
    print(f"  ì••ì¶•ë¥ :  {(1 - parquet_size/csv_size)*100:.1f}% ì ˆì•½")

    # 3. ì „ì²´ ì½ê¸° ì†ë„ ë¹„êµ
    print(f"\\nâ±ï¸ ì „ì²´ ì½ê¸° ì†ë„:")

    start = time.perf_counter()
    _ = pd.read_csv(csv_path)
    csv_time = time.perf_counter() - start
    print(f"  CSV:     {csv_time:.3f}ì´ˆ")

    start = time.perf_counter()
    _ = pd.read_parquet(parquet_path)
    parquet_time = time.perf_counter() - start
    print(f"  Parquet: {parquet_time:.3f}ì´ˆ")
    print(f"  ì†ë„ í–¥ìƒ: {csv_time/parquet_time:.1f}ë°° ë¹ ë¦„")

    # 4. íŠ¹ì • ì»¬ëŸ¼ë§Œ ì½ê¸° (2ê°œ ì»¬ëŸ¼)
    print(f"\\nâ±ï¸ 2ê°œ ì»¬ëŸ¼ë§Œ ì½ê¸° ì†ë„:")

    start = time.perf_counter()
    _ = pd.read_csv(csv_path, usecols=['id', 'value1'])
    csv_time_cols = time.perf_counter() - start
    print(f"  CSV:     {csv_time_cols:.3f}ì´ˆ")

    start = time.perf_counter()
    _ = pd.read_parquet(parquet_path, columns=['id', 'value1'])
    parquet_time_cols = time.perf_counter() - start
    print(f"  Parquet: {parquet_time_cols:.3f}ì´ˆ")
    print(f"  ì†ë„ í–¥ìƒ: {csv_time_cols/parquet_time_cols:.1f}ë°° ë¹ ë¦„")

    print("\\n" + "=" * 60)
    print("ğŸ’¡ ê²°ë¡ : ParquetëŠ” íŒŒì¼ í¬ê¸°, ì½ê¸° ì†ë„ ëª¨ë‘ ì••ë„ì ")
    print("=" * 60)


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
df = create_sample_data(500_000)
print(f"ë°ì´í„° í¬ê¸°: {len(df):,}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")

benchmark_formats(df, '/tmp/test.csv', '/tmp/test.parquet')
      `,
      keyPoints: [
        'ParquetëŠ” CSV ëŒ€ë¹„ 60-80% íŒŒì¼ í¬ê¸° ì ˆì•½',
        'ì „ì²´ ì½ê¸° ì†ë„ 3-5ë°° ë¹ ë¦„',
        'ì»¬ëŸ¼ ì„ íƒ ì½ê¸° ì‹œ 10ë°°+ ë¹ ë¦„',
        'ì‹¤ë¬´ì—ì„œëŠ” Parquet ì‚¬ìš© ê¶Œì¥'
      ]
    }
  },
  {
    id: 'large-data-quiz',
    type: 'quiz',
    title: 'ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í€´ì¦ˆ',
    duration: 5,
    content: {
      objectives: [
        'ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì „ëµì„ ì´í•´í–ˆëŠ”ì§€ í™•ì¸í•œë‹¤'
      ],
      questions: [
        {
          question: '1GB CSV íŒŒì¼ì„ pandasë¡œ ì½ì„ ë•Œ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€?',
          options: [
            'ì•½ 1GB (íŒŒì¼ í¬ê¸°ì™€ ë™ì¼)',
            'ì•½ 2-5GB (íŒŒì¼ í¬ê¸°ì˜ 2-5ë°°)',
            'ì•½ 500MB (íŒŒì¼ í¬ê¸°ì˜ ì ˆë°˜)',
            'ì•½ 10GB (íŒŒì¼ í¬ê¸°ì˜ 10ë°°)'
          ],
          answer: 1,
          explanation: 'pandasëŠ” ë¬¸ìì—´ ë³€í™˜, dtype ì˜¤ë²„í—¤ë“œ ë“±ìœ¼ë¡œ CSV í¬ê¸°ì˜ 2-5ë°° ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.'
        },
        {
          question: 'dtype ìµœì í™”ì—ì„œ ê°€ì¥ íš¨ê³¼ì ì¸ ì „ëµì€?',
          options: [
            'int64 â†’ int32ë¡œ ë³€ê²½',
            'float64 â†’ float32ë¡œ ë³€ê²½',
            'ë°˜ë³µë˜ëŠ” ë¬¸ìì—´ì„ categoryë¡œ ë³€ê²½',
            'ëª¨ë“  ì»¬ëŸ¼ì„ objectë¡œ ë³€ê²½'
          ],
          answer: 2,
          explanation: 'category íƒ€ì…ì€ ë°˜ë³µë˜ëŠ” ë¬¸ìì—´ì—ì„œ 90% ì´ìƒ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆì–´ ê°€ì¥ íš¨ê³¼ì ì…ë‹ˆë‹¤.'
        },
        {
          question: 'Parquet í¬ë§·ì˜ ì¥ì ì´ ì•„ë‹Œ ê²ƒì€?',
          options: [
            'ì»¬ëŸ¼ ê¸°ë°˜ ì €ì¥ìœ¼ë¡œ íŠ¹ì • ì»¬ëŸ¼ë§Œ ë¹ ë¥´ê²Œ ì½ê¸° ê°€ëŠ¥',
            'ìë™ ì••ì¶•ìœ¼ë¡œ íŒŒì¼ í¬ê¸° ê°ì†Œ',
            'dtype ì •ë³´ê°€ ë³´ì¡´ë¨',
            'í…ìŠ¤íŠ¸ ì—ë””í„°ë¡œ ì§ì ‘ í¸ì§‘ ê°€ëŠ¥'
          ],
          answer: 3,
          explanation: 'ParquetëŠ” ë°”ì´ë„ˆë¦¬ í¬ë§·ì´ë¯€ë¡œ í…ìŠ¤íŠ¸ ì—ë””í„°ë¡œ í¸ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” CSVì˜ ì¥ì ì…ë‹ˆë‹¤.'
        }
      ],
      keyPoints: [
        'CSVëŠ” ë©”ëª¨ë¦¬ì˜ 2-5ë°° í•„ìš”',
        'category íƒ€ì…ì´ ê°€ì¥ íš¨ê³¼ì ì¸ ìµœì í™”',
        'ParquetëŠ” ë°”ì´ë„ˆë¦¬ í¬ë§· (í¸ì§‘ ë¶ˆê°€)'
      ]
    }
  },
  {
    id: 'large-data-challenge',
    type: 'code',
    title: 'ğŸ† Daily Challenge: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ë¡œë”',
    duration: 30,
    content: {
      objectives: [
        'ëª¨ë“  ìµœì í™” ê¸°ë²•ì„ í†µí•©í•œ ë°ì´í„° ë¡œë”ë¥¼ êµ¬í˜„í•œë‹¤',
        'ìë™ dtype ì¶”ë¡ ê³¼ chunk ì²˜ë¦¬ë¥¼ ê²°í•©í•œë‹¤',
        'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ë©° ì²˜ë¦¬í•œë‹¤'
      ],
      instructions: `
## ğŸ† Daily Challenge

ëª¨ë“  ìµœì í™” ê¸°ë²•ì„ í†µí•©í•œ SmartDataLoader í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

### ìš”êµ¬ì‚¬í•­
1. ìë™ dtype ìµœì í™”
2. chunksize ìë™ ê³„ì‚° (ê°€ìš© ë©”ëª¨ë¦¬ ê¸°ë°˜)
3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
4. CSV/Parquet ìë™ ê°ì§€ ë° ì²˜ë¦¬
5. ì²˜ë¦¬ ì§„í–‰ë¥  í‘œì‹œ
      `,
      starterCode: `
import pandas as pd
import numpy as np
import os
import time
from typing import Iterator, Optional, Callable

class SmartDataLoader:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ë¡œë”

    Features:
    - ìë™ dtype ìµœì í™”
    - ìë™ chunksize ê³„ì‚°
    - ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
    - CSV/Parquet ìë™ ê°ì§€
    """

    def __init__(self, memory_limit_mb: int = 500):
        """
        Args:
            memory_limit_mb: ì‚¬ìš©í•  ìµœëŒ€ ë©”ëª¨ë¦¬ (MB)
        """
        self.memory_limit_mb = memory_limit_mb

    def load(
        self,
        filepath: str,
        usecols: Optional[list] = None,
        optimize_dtypes: bool = True
    ) -> pd.DataFrame:
        """íŒŒì¼ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œ

        Args:
            filepath: íŒŒì¼ ê²½ë¡œ
            usecols: ë¡œë“œí•  ì»¬ëŸ¼ ëª©ë¡
            optimize_dtypes: dtype ìë™ ìµœì í™” ì—¬ë¶€

        Returns:
            pd.DataFrame: ë¡œë“œëœ DataFrame
        """
        # TODO: êµ¬í˜„í•˜ì„¸ìš”
        pass

    def load_chunked(
        self,
        filepath: str,
        process_func: Callable[[pd.DataFrame], pd.DataFrame],
        usecols: Optional[list] = None
    ) -> Iterator[pd.DataFrame]:
        """ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ë©° ë¡œë“œ

        Args:
            filepath: íŒŒì¼ ê²½ë¡œ
            process_func: ê° ì²­í¬ì— ì ìš©í•  í•¨ìˆ˜
            usecols: ë¡œë“œí•  ì»¬ëŸ¼ ëª©ë¡

        Yields:
            pd.DataFrame: ì²˜ë¦¬ëœ ì²­í¬
        """
        # TODO: êµ¬í˜„í•˜ì„¸ìš”
        pass

    def _detect_format(self, filepath: str) -> str:
        """íŒŒì¼ í¬ë§· ê°ì§€"""
        # TODO: êµ¬í˜„í•˜ì„¸ìš”
        pass

    def _calculate_chunksize(self, filepath: str) -> int:
        """íŒŒì¼ í¬ê¸° ê¸°ë°˜ chunksize ê³„ì‚°"""
        # TODO: êµ¬í˜„í•˜ì„¸ìš”
        pass

    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """dtype ìë™ ìµœì í™”"""
        # TODO: êµ¬í˜„í•˜ì„¸ìš”
        pass


# í…ŒìŠ¤íŠ¸
np.random.seed(42)
n_rows = 200_000

df = pd.DataFrame({
    'id': np.arange(n_rows),
    'category': np.random.choice(['A', 'B', 'C'], n_rows),
    'amount': np.random.uniform(0, 1000, n_rows),
    'count': np.random.randint(0, 100, n_rows)
})

df.to_csv('/tmp/smart_test.csv', index=False)
df.to_parquet('/tmp/smart_test.parquet')

# SmartDataLoader í…ŒìŠ¤íŠ¸
loader = SmartDataLoader(memory_limit_mb=100)

print("=== CSV ë¡œë“œ ===")
df_csv = loader.load('/tmp/smart_test.csv', optimize_dtypes=True)
print(df_csv.dtypes)

print("\\n=== Parquet ë¡œë“œ ===")
df_parquet = loader.load('/tmp/smart_test.parquet')
print(df_parquet.dtypes)

print("\\n=== ì²­í¬ ì²˜ë¦¬ ===")
def aggregate(chunk):
    return chunk.groupby('category')['amount'].sum().reset_index()

results = list(loader.load_chunked('/tmp/smart_test.csv', aggregate))
print(f"ì²­í¬ ìˆ˜: {len(results)}")
      `,
      solutionCode: `
import pandas as pd
import numpy as np
import os
import time
import tracemalloc
from typing import Iterator, Optional, Callable

class SmartDataLoader:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ë¡œë”

    ğŸ¯ ì—­í• : ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œ

    Features:
    - ìë™ dtype ìµœì í™”
    - ìë™ chunksize ê³„ì‚°
    - ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
    - CSV/Parquet ìë™ ê°ì§€
    """

    def __init__(self, memory_limit_mb: int = 500):
        self.memory_limit_mb = memory_limit_mb

    def _detect_format(self, filepath: str) -> str:
        """íŒŒì¼ í¬ë§· ê°ì§€"""
        ext = os.path.splitext(filepath)[1].lower()
        if ext == '.parquet':
            return 'parquet'
        elif ext in ['.csv', '.txt']:
            return 'csv'
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬ë§·: {ext}")

    def _calculate_chunksize(self, filepath: str) -> int:
        """íŒŒì¼ í¬ê¸° ê¸°ë°˜ chunksize ê³„ì‚°

        ğŸ’¡ ì „ëµ: ë©”ëª¨ë¦¬ ì œí•œì˜ 10%ë¥¼ í•œ ì²­í¬ê°€ ì‚¬ìš©í•˜ë„ë¡
        """
        file_size_mb = os.path.getsize(filepath) / 1024 / 1024

        # ëŒ€ëµì ì¸ í–‰ ìˆ˜ ì¶”ì • (1KBë‹¹ 10í–‰ ê°€ì •)
        estimated_rows = file_size_mb * 1000 * 10

        # ë©”ëª¨ë¦¬ ì œí•œì˜ 10%ë¥¼ í•œ ì²­í¬ê°€ ì‚¬ìš©
        chunk_memory_mb = self.memory_limit_mb * 0.1
        memory_per_row_kb = file_size_mb * 1024 / estimated_rows * 3  # 3ë°° ì—¬ìœ 

        if memory_per_row_kb > 0:
            chunksize = int(chunk_memory_mb * 1024 / memory_per_row_kb)
        else:
            chunksize = 100_000

        return max(10_000, min(chunksize, 1_000_000))

    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """dtype ìë™ ìµœì í™”"""
        df = df.copy()

        # ì •ìˆ˜ ìµœì í™”
        for col in df.select_dtypes(include=['int64', 'int32']).columns:
            col_min, col_max = df[col].min(), df[col].max()
            if col_min >= 0:
                if col_max <= 255:
                    df[col] = df[col].astype('uint8')
                elif col_max <= 65535:
                    df[col] = df[col].astype('uint16')
            else:
                if col_min >= -128 and col_max <= 127:
                    df[col] = df[col].astype('int8')
                elif col_min >= -32768 and col_max <= 32767:
                    df[col] = df[col].astype('int16')

        # ì‹¤ìˆ˜ ìµœì í™”
        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = df[col].astype('float32')

        # ì¹´í…Œê³ ë¦¬ ìµœì í™”
        for col in df.select_dtypes(include=['object']).columns:
            if df[col].nunique() / len(df) < 0.5:
                df[col] = df[col].astype('category')

        return df

    def load(
        self,
        filepath: str,
        usecols: Optional[list] = None,
        optimize_dtypes: bool = True
    ) -> pd.DataFrame:
        """íŒŒì¼ì„ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë“œ"""
        format_type = self._detect_format(filepath)

        tracemalloc.start()
        start_time = time.perf_counter()

        if format_type == 'parquet':
            df = pd.read_parquet(filepath, columns=usecols)
        else:
            df = pd.read_csv(filepath, usecols=usecols)

        if optimize_dtypes and format_type == 'csv':
            df = self._optimize_dtypes(df)

        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        elapsed = time.perf_counter() - start_time

        print(f"ğŸ“Š ë¡œë“œ ì™„ë£Œ:")
        print(f"  - í–‰ ìˆ˜: {len(df):,}")
        print(f"  - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print(f"  - ë©”ëª¨ë¦¬: {current/1024/1024:.1f} MB (í”¼í¬: {peak/1024/1024:.1f} MB)")

        return df

    def load_chunked(
        self,
        filepath: str,
        process_func: Callable[[pd.DataFrame], pd.DataFrame],
        usecols: Optional[list] = None
    ) -> Iterator[pd.DataFrame]:
        """ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ë©° ë¡œë“œ"""
        format_type = self._detect_format(filepath)

        if format_type == 'parquet':
            df = pd.read_parquet(filepath, columns=usecols)
            yield process_func(df)
            return

        chunksize = self._calculate_chunksize(filepath)
        print(f"ğŸ“¦ ì²­í¬ í¬ê¸°: {chunksize:,}")

        chunk_count = 0
        for chunk in pd.read_csv(filepath, chunksize=chunksize, usecols=usecols):
            chunk_count += 1
            chunk = self._optimize_dtypes(chunk)
            yield process_func(chunk)

        print(f"âœ… ì´ {chunk_count}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")


# í…ŒìŠ¤íŠ¸
np.random.seed(42)
n_rows = 200_000

df = pd.DataFrame({
    'id': np.arange(n_rows),
    'category': np.random.choice(['A', 'B', 'C'], n_rows),
    'amount': np.random.uniform(0, 1000, n_rows),
    'count': np.random.randint(0, 100, n_rows)
})

df.to_csv('/tmp/smart_test.csv', index=False)
df.to_parquet('/tmp/smart_test.parquet')

# SmartDataLoader í…ŒìŠ¤íŠ¸
loader = SmartDataLoader(memory_limit_mb=100)

print("=== CSV ë¡œë“œ ===")
df_csv = loader.load('/tmp/smart_test.csv', optimize_dtypes=True)
print(df_csv.dtypes)

print("\\n=== Parquet ë¡œë“œ ===")
df_parquet = loader.load('/tmp/smart_test.parquet')
print(df_parquet.dtypes)

print("\\n=== ì²­í¬ ì²˜ë¦¬ ===")
def aggregate(chunk):
    return chunk.groupby('category')['amount'].sum().reset_index()

results = list(loader.load_chunked('/tmp/smart_test.csv', aggregate))
print(f"ì²­í¬ ìˆ˜: {len(results)}")

# ê²°ê³¼ í•©ì¹˜ê¸°
final = pd.concat(results).groupby('category')['amount'].sum().reset_index()
print("\\nìµœì¢… ì§‘ê³„ ê²°ê³¼:")
print(final)
      `,
      keyPoints: [
        'íŒŒì¼ í¬ë§· ìë™ ê°ì§€ (CSV/Parquet)',
        'ë©”ëª¨ë¦¬ ê¸°ë°˜ chunksize ìë™ ê³„ì‚°',
        'dtype ìë™ ìµœì í™”',
        'ì²­í¬ ì²˜ë¦¬ì™€ ì¼ë°˜ ì²˜ë¦¬ í†µí•©'
      ]
    }
  }
]
