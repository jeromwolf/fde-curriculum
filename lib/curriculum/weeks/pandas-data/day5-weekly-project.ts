// Day 5: Weekly Project - NYC Taxi ë°ì´í„° ë¶„ì„
import type { Task } from '../../types'

export const day5Tasks: Task[] = [
  // Task 1: í”„ë¡œì íŠ¸ ì†Œê°œ (Video)
  {
    id: 'w2d5-task1',
    title: 'ğŸš• Weekly Project: NYC Taxi ë°ì´í„° ë¶„ì„',
    type: 'video',
    duration: 10,
    content: {
      videoUrl: 'https://www.youtube.com/watch?v=placeholder',
      objectives: [
        'í”„ë¡œì íŠ¸ ëª©í‘œì™€ ë°ì´í„°ì…‹ ì´í•´',
        'ë¶„ì„ ìš”êµ¬ì‚¬í•­ íŒŒì•…',
        'ê¸°ìˆ ì  ì œì•½ ì¡°ê±´ ì´í•´',
        'í‰ê°€ ê¸°ì¤€ í™•ì¸'
      ],
      transcript: `
# Weekly Project: NYC Taxi ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸

ì´ë²ˆ ì£¼ì°¨ í”„ë¡œì íŠ¸ì—ì„œëŠ” **NYC Taxi ë°ì´í„°**ë¥¼ í™œìš©í•˜ì—¬ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê¸°ìˆ ì„ ì¢…í•©ì ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤.

## í”„ë¡œì íŠ¸ ëª©í‘œ

1. **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ëŠ¥ë ¥** ê²€ì¦
   - 1GB+ ë°ì´í„°ì…‹ ì²˜ë¦¬
   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©
   - ì„±ëŠ¥ ìµœì í™”

2. **ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ**
   - ì‹œê°„ëŒ€ë³„ ìˆ˜ìš” íŒ¨í„´
   - ì§€ì—­ë³„ ìš”ê¸ˆ ë¶„ì„
   - íŒ ì˜ˆì¸¡ ìš”ì¸ ë¶„ì„

3. **ê¸°ìˆ  ì¢…í•© í™œìš©**
   - pandas/Polars ë¹„êµ
   - ë©”ëª¨ë¦¬ ìµœì í™”
   - ë²¡í„°í™” ì—°ì‚°

## ë°ì´í„°ì…‹ ì†Œê°œ

### NYC TLC Trip Record Data

ë‰´ìš•ì‹œ íƒì‹œ & ë¦¬ë¬´ì§„ ìœ„ì›íšŒ(TLC) ê³µê°œ ë°ì´í„°
- **ì¶œì²˜**: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page
- **í¬ê¸°**: ì›”ê°„ ì•½ 1GB (2022ë…„ ë°ì´í„°)
- **í˜•ì‹**: Parquet

### ì£¼ìš” ì»¬ëŸ¼

| ì»¬ëŸ¼ëª… | ì„¤ëª… |
|--------|------|
| tpep_pickup_datetime | ìŠ¹ì°¨ ì‹œê° |
| tpep_dropoff_datetime | í•˜ì°¨ ì‹œê° |
| passenger_count | ìŠ¹ê° ìˆ˜ |
| trip_distance | ì´ë™ ê±°ë¦¬ (ë§ˆì¼) |
| PULocationID | ìŠ¹ì°¨ ì§€ì—­ ID |
| DOLocationID | í•˜ì°¨ ì§€ì—­ ID |
| fare_amount | ê¸°ë³¸ ìš”ê¸ˆ |
| tip_amount | íŒ ê¸ˆì•¡ |
| total_amount | ì´ ê¸ˆì•¡ |
| payment_type | ê²°ì œ ë°©ì‹ |

## ë¶„ì„ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ ê³¼ì œ (70ì )

1. **ë°ì´í„° ë¡œë”© ìµœì í™”** (20ì )
   - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë¡œë”©
   - í•„ìš” ì»¬ëŸ¼ë§Œ ì„ íƒ
   - dtype ìµœì í™”

2. **ì‹œê°„ëŒ€ë³„ ë¶„ì„** (25ì )
   - ìš”ì¼ë³„/ì‹œê°„ëŒ€ë³„ ìš´í–‰ íŒ¨í„´
   - í”¼í¬ ì‹œê°„ ë¶„ì„
   - í‰ê·  ìš”ê¸ˆ ì¶”ì´

3. **ì§€ì—­ë³„ ë¶„ì„** (25ì )
   - Top 10 ì¸ê¸° ìŠ¹ì°¨ ì§€ì—­
   - ì§€ì—­ ê°„ ì´ë™ íŒ¨í„´
   - ì§€ì—­ë³„ í‰ê·  ìš”ê¸ˆ

### ì„ íƒ ê³¼ì œ (30ì )

4. **íŒ ë¶„ì„** (15ì )
   - íŒ ê¸ˆì•¡ ë¶„í¬
   - íŒ ë¹„ìœ¨ ê³„ì‚°
   - íŒì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì¸

5. **pandas vs Polars ë¹„êµ** (15ì )
   - ë™ì¼ ë¶„ì„ ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ìˆ˜í–‰
   - ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸
   - ì½”ë“œ ë³µì¡ë„ ë¹„êµ

## ê¸°ìˆ ì  ì œì•½ ì¡°ê±´

- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 4GB ì´í•˜
- Parquet í˜•ì‹ í™œìš© ê¶Œì¥
- ê²°ê³¼ë¬¼: Jupyter Notebook ë˜ëŠ” Python ìŠ¤í¬ë¦½íŠ¸

## í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ê¸°ì¤€ |
|------|------|------|
| ì½”ë“œ í’ˆì§ˆ | 20% | ê°€ë…ì„±, ëª¨ë“ˆí™”, ì£¼ì„ |
| ì„±ëŠ¥ ìµœì í™” | 30% | ë©”ëª¨ë¦¬ íš¨ìœ¨, ì‹¤í–‰ ì‹œê°„ |
| ë¶„ì„ ê¹Šì´ | 30% | ì¸ì‚¬ì´íŠ¸ ë„ì¶œ, ì‹œê°í™” |
| ë¬¸ì„œí™” | 20% | ê²°ê³¼ í•´ì„, ë°œê²¬ì  ì •ë¦¬ |
      `,
      keyPoints: [
        'NYC Taxi ë°ì´í„°: ë‰´ìš•ì‹œ íƒì‹œ ìš´í–‰ ê¸°ë¡',
        '1GB+ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹¤ìŠµ',
        'ì‹œê°„ëŒ€ë³„, ì§€ì—­ë³„ ë¶„ì„ + íŒ ì˜ˆì¸¡',
        'ë©”ëª¨ë¦¬ 4GB ì´í•˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ëª©í‘œ'
      ]
    }
  },

  // Task 2: ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° íƒìƒ‰ (Reading)
  {
    id: 'w2d5-task2',
    title: 'ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì´ˆê¸° íƒìƒ‰',
    type: 'reading',
    duration: 10,
    content: {
      objectives: [
        'ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë°©ë²•',
        'ë°ì´í„° êµ¬ì¡° íŒŒì•…',
        'í’ˆì§ˆ ì´ìŠˆ í™•ì¸',
        'ë¶„ì„ ì „ëµ ìˆ˜ë¦½'
      ],
      markdown: `
# ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì´ˆê¸° íƒìƒ‰

## ë°ì´í„° ë‹¤ìš´ë¡œë“œ

### ë°©ë²• 1: ì§ì ‘ ë‹¤ìš´ë¡œë“œ
\`\`\`bash
# 2022ë…„ 1ì›” Yellow Taxi ë°ì´í„° (ì•½ 500MB)
wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet
\`\`\`

### ë°©ë²• 2: Pythonìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ
\`\`\`python
import urllib.request
import os

url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet'
filename = 'yellow_tripdata_2022-01.parquet'

if not os.path.exists(filename):
    print(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {filename}")
    urllib.request.urlretrieve(url, filename)
    print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
\`\`\`

### ì—¬ëŸ¬ ë‹¬ ë°ì´í„° ë³‘í•©
\`\`\`python
# 1-3ì›” ë°ì´í„° ë‹¤ìš´ë¡œë“œ
base_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-{:02d}.parquet'
months = [1, 2, 3]

for month in months:
    url = base_url.format(month)
    filename = f'yellow_tripdata_2022-{month:02d}.parquet'
    if not os.path.exists(filename):
        urllib.request.urlretrieve(url, filename)
        print(f"Downloaded: {filename}")
\`\`\`

## ì´ˆê¸° ë°ì´í„° íƒìƒ‰

### Parquet ë©”íƒ€ë°ì´í„° í™•ì¸
\`\`\`python
import pyarrow.parquet as pq

# Parquet íŒŒì¼ ë©”íƒ€ë°ì´í„° (ë¡œë”© ì—†ì´ í™•ì¸)
parquet_file = pq.ParquetFile('yellow_tripdata_2022-01.parquet')

print(f"í–‰ ìˆ˜: {parquet_file.metadata.num_rows:,}")
print(f"ì»¬ëŸ¼ ìˆ˜: {parquet_file.metadata.num_columns}")
print(f"íŒŒì¼ í¬ê¸°: {os.path.getsize(filename) / 1024**2:.1f} MB")

# ìŠ¤í‚¤ë§ˆ í™•ì¸
print("\\nì»¬ëŸ¼ ì •ë³´:")
for col in parquet_file.schema:
    print(f"  {col.name}: {col.physical_type}")
\`\`\`

### ìƒ˜í”Œ ë°ì´í„° í™•ì¸
\`\`\`python
import pandas as pd

# ì²˜ìŒ 1000í–‰ë§Œ ë¡œë“œ
df_sample = pd.read_parquet('yellow_tripdata_2022-01.parquet', nrows=1000)

print(df_sample.info())
print(df_sample.describe())
print(df_sample.head())
\`\`\`

## ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ

### ì¼ë°˜ì ì¸ ë¬¸ì œì 

1. **ëˆ„ë½ê°’**
   - passenger_count: 0 ë˜ëŠ” NULL
   - tip_amount: í˜„ê¸ˆ ê²°ì œ ì‹œ 0

2. **ì´ìƒì¹˜**
   - trip_distance: 0 ë˜ëŠ” ê·¹ë‹¨ì ìœ¼ë¡œ ê¸´ ê±°ë¦¬
   - fare_amount: ìŒìˆ˜ ë˜ëŠ” ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ ê°’
   - passenger_count: 0 ë˜ëŠ” 8 ì´ˆê³¼

3. **ë‚ ì§œ ì´ìŠˆ**
   - pickup_datetime > dropoff_datetime (ì‹œê°„ ì—­ì „)
   - ë¯¸ë˜ ë‚ ì§œ

### ë°ì´í„° ì •ì œ ì „ëµ
\`\`\`python
# ê¸°ë³¸ í•„í„°ë§ ì¡°ê±´
VALID_CONDITIONS = {
    'passenger_count': (1, 6),      # 1-6ëª…
    'trip_distance': (0.1, 100),    # 0.1-100ë§ˆì¼
    'fare_amount': (2.5, 500),      # $2.5-$500
    'tip_amount': (0, 200),         # $0-$200
}

def clean_taxi_data(df):
    """ê¸°ë³¸ ë°ì´í„° ì •ì œ"""
    initial_rows = len(df)

    # ëˆ„ë½ê°’ ì œê±°
    df = df.dropna(subset=['trip_distance', 'fare_amount'])

    # ë²”ìœ„ í•„í„°ë§
    df = df[
        (df['passenger_count'].between(1, 6)) &
        (df['trip_distance'].between(0.1, 100)) &
        (df['fare_amount'].between(2.5, 500)) &
        (df['tip_amount'].between(0, 200))
    ]

    # ì‹œê°„ ê²€ì¦
    df = df[df['tpep_dropoff_datetime'] > df['tpep_pickup_datetime']]

    print(f"ì •ì œ ì „: {initial_rows:,} â†’ ì •ì œ í›„: {len(df):,}")
    print(f"ì œê±°ëœ í–‰: {initial_rows - len(df):,} ({100*(initial_rows-len(df))/initial_rows:.1f}%)")

    return df
\`\`\`

## ë¶„ì„ ì „ëµ

### ê¶Œì¥ ì ‘ê·¼ ë°©ì‹

1. **Parquet í˜•ì‹ ìœ ì§€** - ì••ì¶• íš¨ìœ¨ ì¢‹ìŒ
2. **í•„ìš” ì»¬ëŸ¼ë§Œ ë¡œë“œ** - usecols í™œìš©
3. **dtype ìµœì í™”** - ë©”ëª¨ë¦¬ ì ˆì•½
4. **ì²­í¬ ì²˜ë¦¬** - í•„ìš”ì‹œ ë¶„í•  ì²˜ë¦¬
5. **ì¤‘ê°„ ê²°ê³¼ ìºì‹±** - ë°˜ë³µ ê³„ì‚° ë°©ì§€
      `,
      keyPoints: [
        'NYC TLC ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ Parquet í˜•ì‹ ë‹¤ìš´ë¡œë“œ',
        'ë©”íƒ€ë°ì´í„°ë¡œ ë¡œë”© ì „ ë°ì´í„° í¬ê¸° í™•ì¸',
        'ì´ìƒì¹˜ì™€ ëˆ„ë½ê°’ ì •ì œ í•„ìˆ˜',
        'Parquet + usecolsë¡œ íš¨ìœ¨ì  ë¡œë”©'
      ]
    }
  },

  // Task 3: ë°ì´í„° ë¡œë”© ìµœì í™” (Code)
  {
    id: 'w2d5-task3',
    title: 'Part 1: ë°ì´í„° ë¡œë”© ìµœì í™”',
    type: 'code',
    duration: 25,
    content: {
      objectives: [
        'ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ë¡œë”©',
        'dtype ìë™ ìµœì í™”',
        'ë°ì´í„° í’ˆì§ˆ ê²€ì¦',
        'ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹'
      ],
      starterCode: `import pandas as pd
import numpy as np
import os
import time
import gc

# ===========================================
# ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ë‹¤ìš´ë¡œë“œ í›„ ì‚¬ìš©)
# ===========================================
DATA_PATH = 'yellow_tripdata_2022-01.parquet'

# ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë°ì´í„° ì—†ì„ ê²½ìš°)
def create_sample_data(n_rows=500_000):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)

    # ì‹¤ì œ NYC Taxi ë°ì´í„°ì™€ ìœ ì‚¬í•œ êµ¬ì¡°
    start_date = pd.Timestamp('2022-01-01')
    pickup_times = start_date + pd.to_timedelta(
        np.random.randint(0, 31*24*60*60, n_rows), unit='s'
    )
    trip_durations = np.random.exponential(15, n_rows) * 60  # í‰ê·  15ë¶„

    df = pd.DataFrame({
        'VendorID': np.random.choice([1, 2], n_rows),
        'tpep_pickup_datetime': pickup_times,
        'tpep_dropoff_datetime': pickup_times + pd.to_timedelta(trip_durations, unit='s'),
        'passenger_count': np.random.choice([1, 1, 1, 2, 2, 3, 4, 5], n_rows),
        'trip_distance': np.random.exponential(3, n_rows),  # í‰ê·  3ë§ˆì¼
        'RatecodeID': np.random.choice([1, 1, 1, 2, 3, 4, 5, 6], n_rows),
        'PULocationID': np.random.randint(1, 265, n_rows),
        'DOLocationID': np.random.randint(1, 265, n_rows),
        'payment_type': np.random.choice([1, 1, 2, 2, 3, 4], n_rows),
        'fare_amount': np.random.exponential(15, n_rows),  # í‰ê·  $15
        'extra': np.random.choice([0, 0.5, 1, 2.5], n_rows),
        'mta_tax': 0.5,
        'tip_amount': np.random.exponential(3, n_rows) * (np.random.random(n_rows) > 0.3),
        'tolls_amount': np.random.choice([0, 0, 0, 5.76, 6.55], n_rows),
        'improvement_surcharge': 0.3,
        'total_amount': 0,  # ë‚˜ì¤‘ì— ê³„ì‚°
        'congestion_surcharge': np.random.choice([0, 2.5], n_rows),
        'airport_fee': np.random.choice([0, 0, 0, 1.25], n_rows)
    })

    # total_amount ê³„ì‚°
    df['total_amount'] = (df['fare_amount'] + df['extra'] + df['mta_tax'] +
                          df['tip_amount'] + df['tolls_amount'] +
                          df['improvement_surcharge'] + df['congestion_surcharge'])

    return df

# ë°ì´í„° ì¤€ë¹„
if not os.path.exists(DATA_PATH):
    print("ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    df_sample = create_sample_data()
    df_sample.to_parquet(DATA_PATH)
    print(f"ìƒ˜í”Œ ë°ì´í„° ì €ì¥: {DATA_PATH}")

# ===========================================
# Task 1: ê¸°ë³¸ ë¡œë”© vs ìµœì í™” ë¡œë”© ë¹„êµ
# ===========================================

def load_basic():
    """ê¸°ë³¸ ë¡œë”© (ìµœì í™” ì—†ìŒ)"""
    # TODO: pandas read_parquetë¡œ ì „ì²´ ë°ì´í„° ë¡œë”©
    pass

def load_optimized():
    """ìµœì í™”ëœ ë¡œë”©"""
    # TODO: êµ¬í˜„
    # 1. í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (usecols)
    # 2. dtype ìµœì í™” ì ìš©
    # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •

    # ë¶„ì„ì— í•„ìš”í•œ ì»¬ëŸ¼
    NEEDED_COLUMNS = [
        'tpep_pickup_datetime',
        'tpep_dropoff_datetime',
        'passenger_count',
        'trip_distance',
        'PULocationID',
        'DOLocationID',
        'payment_type',
        'fare_amount',
        'tip_amount',
        'total_amount'
    ]

    pass

# ===========================================
# Task 2: ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜
# ===========================================

def optimize_dtypes(df):
    """DataFrame dtype ìµœì í™”"""
    # TODO: êµ¬í˜„
    # 1. ì •ìˆ˜ ì»¬ëŸ¼ ë‹¤ìš´ìºìŠ¤íŒ…
    # 2. ì‹¤ìˆ˜ ì»¬ëŸ¼ ë‹¤ìš´ìºìŠ¤íŒ…
    # 3. ìµœì í™” ì „í›„ ë©”ëª¨ë¦¬ ë¹„êµ
    pass

# ===========================================
# Task 3: ë°ì´í„° í’ˆì§ˆ ê²€ì¦
# ===========================================

def validate_data(df):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    # TODO: êµ¬í˜„
    # 1. ëˆ„ë½ê°’ ë¹„ìœ¨ í™•ì¸
    # 2. ì´ìƒì¹˜ íƒì§€
    # 3. ë°ì´í„° ì •ì œ
    pass

# ===========================================
# Task 4: ì „ì²´ íŒŒì´í”„ë¼ì¸
# ===========================================

class TaxiDataLoader:
    """NYC Taxi ë°ì´í„° ë¡œë” í´ë˜ìŠ¤"""

    def __init__(self, filepath):
        self.filepath = filepath
        self.df = None
        self.load_stats = {}

    def load(self, optimize=True):
        """ë°ì´í„° ë¡œë”©"""
        # TODO: êµ¬í˜„
        pass

    def get_stats(self):
        """ë¡œë”© í†µê³„ ë°˜í™˜"""
        # TODO: êµ¬í˜„
        pass

# ===========================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ===========================================

if __name__ == "__main__":
    print("=== NYC Taxi ë°ì´í„° ë¡œë”© ìµœì í™” ===\\n")

    # ê¸°ë³¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    print("[1] ê¸°ë³¸ ë¡œë”©")
    # TODO: load_basic() í˜¸ì¶œ ë° ê²°ê³¼ ì¶œë ¥

    # ìµœì í™” ë¡œë”© í…ŒìŠ¤íŠ¸
    print("\\n[2] ìµœì í™” ë¡œë”©")
    # TODO: load_optimized() í˜¸ì¶œ ë° ê²°ê³¼ ì¶œë ¥

    # í´ë˜ìŠ¤ ì‚¬ìš© í…ŒìŠ¤íŠ¸
    print("\\n[3] TaxiDataLoader í…ŒìŠ¤íŠ¸")
    # TODO: TaxiDataLoader ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í…ŒìŠ¤íŠ¸
`,
      solutionCode: `import pandas as pd
import numpy as np
import os
import time
import gc
import tracemalloc

DATA_PATH = 'yellow_tripdata_2022-01.parquet'

def create_sample_data(n_rows=500_000):
    """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    np.random.seed(42)
    start_date = pd.Timestamp('2022-01-01')
    pickup_times = start_date + pd.to_timedelta(
        np.random.randint(0, 31*24*60*60, n_rows), unit='s'
    )
    trip_durations = np.random.exponential(15, n_rows) * 60

    df = pd.DataFrame({
        'VendorID': np.random.choice([1, 2], n_rows),
        'tpep_pickup_datetime': pickup_times,
        'tpep_dropoff_datetime': pickup_times + pd.to_timedelta(trip_durations, unit='s'),
        'passenger_count': np.random.choice([1, 1, 1, 2, 2, 3, 4, 5], n_rows),
        'trip_distance': np.random.exponential(3, n_rows),
        'RatecodeID': np.random.choice([1, 1, 1, 2, 3, 4, 5, 6], n_rows),
        'PULocationID': np.random.randint(1, 265, n_rows),
        'DOLocationID': np.random.randint(1, 265, n_rows),
        'payment_type': np.random.choice([1, 1, 2, 2, 3, 4], n_rows),
        'fare_amount': np.random.exponential(15, n_rows),
        'extra': np.random.choice([0, 0.5, 1, 2.5], n_rows),
        'mta_tax': 0.5,
        'tip_amount': np.random.exponential(3, n_rows) * (np.random.random(n_rows) > 0.3),
        'tolls_amount': np.random.choice([0, 0, 0, 5.76, 6.55], n_rows),
        'improvement_surcharge': 0.3,
        'total_amount': 0,
        'congestion_surcharge': np.random.choice([0, 2.5], n_rows),
        'airport_fee': np.random.choice([0, 0, 0, 1.25], n_rows)
    })
    df['total_amount'] = (df['fare_amount'] + df['extra'] + df['mta_tax'] +
                          df['tip_amount'] + df['tolls_amount'] +
                          df['improvement_surcharge'] + df['congestion_surcharge'])
    return df

if not os.path.exists(DATA_PATH):
    print("ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    df_sample = create_sample_data()
    df_sample.to_parquet(DATA_PATH)
    print(f"ìƒ˜í”Œ ë°ì´í„° ì €ì¥: {DATA_PATH}")

# ===========================================
# Task 1: ê¸°ë³¸ ë¡œë”© vs ìµœì í™” ë¡œë”© ë¹„êµ
# ===========================================

def load_basic():
    """ê¸°ë³¸ ë¡œë”© (ìµœì í™” ì—†ìŒ)"""
    tracemalloc.start()
    start = time.time()

    df = pd.read_parquet(DATA_PATH)

    elapsed = time.time() - start
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    print(f"  í–‰ ìˆ˜: {len(df):,}")
    print(f"  ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    print(f"  ë©”ëª¨ë¦¬: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    print(f"  ë¡œë”© ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"  í”¼í¬ ë©”ëª¨ë¦¬: {peak / 1024**2:.1f} MB")

    return df

def load_optimized():
    """ìµœì í™”ëœ ë¡œë”©"""
    NEEDED_COLUMNS = [
        'tpep_pickup_datetime',
        'tpep_dropoff_datetime',
        'passenger_count',
        'trip_distance',
        'PULocationID',
        'DOLocationID',
        'payment_type',
        'fare_amount',
        'tip_amount',
        'total_amount'
    ]

    tracemalloc.start()
    start = time.time()

    df = pd.read_parquet(DATA_PATH, columns=NEEDED_COLUMNS)
    df = optimize_dtypes(df)

    elapsed = time.time() - start
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    print(f"  í–‰ ìˆ˜: {len(df):,}")
    print(f"  ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}")
    print(f"  ë©”ëª¨ë¦¬: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    print(f"  ë¡œë”© ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"  í”¼í¬ ë©”ëª¨ë¦¬: {peak / 1024**2:.1f} MB")

    return df

# ===========================================
# Task 2: ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜
# ===========================================

def optimize_dtypes(df, verbose=False):
    """DataFrame dtype ìµœì í™”"""
    start_mem = df.memory_usage(deep=True).sum() / 1024**2

    for col in df.columns:
        col_type = df[col].dtype

        if col_type != 'object' and not pd.api.types.is_datetime64_any_dtype(df[col]):
            c_min, c_max = df[col].min(), df[col].max()

            if str(col_type).startswith('int') or str(col_type).startswith('uint'):
                if c_min >= 0:
                    if c_max <= 255:
                        df[col] = df[col].astype(np.uint8)
                    elif c_max <= 65535:
                        df[col] = df[col].astype(np.uint16)
                    elif c_max <= 4294967295:
                        df[col] = df[col].astype(np.uint32)
                else:
                    if c_min >= -128 and c_max <= 127:
                        df[col] = df[col].astype(np.int8)
                    elif c_min >= -32768 and c_max <= 32767:
                        df[col] = df[col].astype(np.int16)
                    elif c_min >= -2147483648 and c_max <= 2147483647:
                        df[col] = df[col].astype(np.int32)

            elif str(col_type).startswith('float'):
                df[col] = df[col].astype(np.float32)

    end_mem = df.memory_usage(deep=True).sum() / 1024**2

    if verbose:
        print(f"  ë©”ëª¨ë¦¬ ìµœì í™”: {start_mem:.1f} MB â†’ {end_mem:.1f} MB")
        print(f"  ì ˆì•½: {100 * (start_mem - end_mem) / start_mem:.1f}%")

    return df

# ===========================================
# Task 3: ë°ì´í„° í’ˆì§ˆ ê²€ì¦
# ===========================================

def validate_data(df, verbose=True):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì •ì œ"""
    initial_rows = len(df)
    issues = {}

    # 1. ëˆ„ë½ê°’ í™•ì¸
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0:
        issues['null_values'] = null_counts[null_counts > 0].to_dict()
        if verbose:
            print("\\n[ëˆ„ë½ê°’]")
            for col, count in issues['null_values'].items():
                print(f"  {col}: {count:,} ({100*count/len(df):.2f}%)")

    # 2. ì´ìƒì¹˜ íƒì§€
    issues['outliers'] = {}

    # passenger_count
    invalid_passengers = (df['passenger_count'] < 1) | (df['passenger_count'] > 6)
    issues['outliers']['passenger_count'] = invalid_passengers.sum()

    # trip_distance
    invalid_distance = (df['trip_distance'] <= 0) | (df['trip_distance'] > 100)
    issues['outliers']['trip_distance'] = invalid_distance.sum()

    # fare_amount
    invalid_fare = (df['fare_amount'] < 2.5) | (df['fare_amount'] > 500)
    issues['outliers']['fare_amount'] = invalid_fare.sum()

    # tip_amount
    invalid_tip = (df['tip_amount'] < 0) | (df['tip_amount'] > 200)
    issues['outliers']['tip_amount'] = invalid_tip.sum()

    if verbose:
        print("\\n[ì´ìƒì¹˜]")
        for col, count in issues['outliers'].items():
            print(f"  {col}: {count:,} ({100*count/len(df):.2f}%)")

    # 3. ë°ì´í„° ì •ì œ
    df_clean = df[
        (df['passenger_count'].between(1, 6)) &
        (df['trip_distance'].between(0.1, 100)) &
        (df['fare_amount'].between(2.5, 500)) &
        (df['tip_amount'].between(0, 200)) &
        (df['tpep_dropoff_datetime'] > df['tpep_pickup_datetime'])
    ].dropna()

    final_rows = len(df_clean)
    issues['removed_rows'] = initial_rows - final_rows

    if verbose:
        print(f"\\n[ì •ì œ ê²°ê³¼]")
        print(f"  ì›ë³¸: {initial_rows:,} í–‰")
        print(f"  ì •ì œ í›„: {final_rows:,} í–‰")
        print(f"  ì œê±°: {issues['removed_rows']:,} í–‰ ({100*issues['removed_rows']/initial_rows:.1f}%)")

    return df_clean, issues

# ===========================================
# Task 4: ì „ì²´ íŒŒì´í”„ë¼ì¸
# ===========================================

class TaxiDataLoader:
    """NYC Taxi ë°ì´í„° ë¡œë” í´ë˜ìŠ¤"""

    NEEDED_COLUMNS = [
        'tpep_pickup_datetime', 'tpep_dropoff_datetime',
        'passenger_count', 'trip_distance',
        'PULocationID', 'DOLocationID',
        'payment_type', 'fare_amount',
        'tip_amount', 'total_amount'
    ]

    def __init__(self, filepath):
        self.filepath = filepath
        self.df = None
        self.df_clean = None
        self.load_stats = {}

    def load(self, optimize=True, validate=True, verbose=True):
        """ë°ì´í„° ë¡œë”© íŒŒì´í”„ë¼ì¸"""
        tracemalloc.start()
        start = time.time()

        # 1. ë°ì´í„° ë¡œë”©
        if verbose:
            print("ë°ì´í„° ë¡œë”© ì¤‘...")
        self.df = pd.read_parquet(self.filepath, columns=self.NEEDED_COLUMNS)

        # 2. dtype ìµœì í™”
        if optimize:
            if verbose:
                print("dtype ìµœì í™” ì¤‘...")
            self.df = optimize_dtypes(self.df, verbose=verbose)

        # 3. ë°ì´í„° ê²€ì¦ ë° ì •ì œ
        if validate:
            if verbose:
                print("ë°ì´í„° ê²€ì¦ ì¤‘...")
            self.df_clean, validation_issues = validate_data(self.df, verbose=verbose)
            self.load_stats['validation'] = validation_issues
        else:
            self.df_clean = self.df

        # 4. í†µê³„ ìˆ˜ì§‘
        elapsed = time.time() - start
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        self.load_stats.update({
            'rows': len(self.df_clean),
            'columns': len(self.df_clean.columns),
            'memory_mb': self.df_clean.memory_usage(deep=True).sum() / 1024**2,
            'load_time': elapsed,
            'peak_memory_mb': peak / 1024**2
        })

        if verbose:
            print(f"\\në¡œë”© ì™„ë£Œ!")
            print(f"  í–‰ ìˆ˜: {self.load_stats['rows']:,}")
            print(f"  ë©”ëª¨ë¦¬: {self.load_stats['memory_mb']:.1f} MB")
            print(f"  ë¡œë”© ì‹œê°„: {self.load_stats['load_time']:.2f}ì´ˆ")

        return self

    def get_df(self, cleaned=True):
        """DataFrame ë°˜í™˜"""
        return self.df_clean if cleaned else self.df

    def get_stats(self):
        """ë¡œë”© í†µê³„ ë°˜í™˜"""
        return self.load_stats

# ===========================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ===========================================

if __name__ == "__main__":
    print("=== NYC Taxi ë°ì´í„° ë¡œë”© ìµœì í™” ===\\n")

    # ê¸°ë³¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    print("[1] ê¸°ë³¸ ë¡œë”©")
    df_basic = load_basic()
    basic_mem = df_basic.memory_usage(deep=True).sum() / 1024**2
    del df_basic
    gc.collect()

    # ìµœì í™” ë¡œë”© í…ŒìŠ¤íŠ¸
    print("\\n[2] ìµœì í™” ë¡œë”©")
    df_opt = load_optimized()
    opt_mem = df_opt.memory_usage(deep=True).sum() / 1024**2
    del df_opt
    gc.collect()

    # ë¹„êµ
    print(f"\\n[ë¹„êµ]")
    print(f"  ê¸°ë³¸: {basic_mem:.1f} MB")
    print(f"  ìµœì í™”: {opt_mem:.1f} MB")
    print(f"  ì ˆì•½: {100 * (basic_mem - opt_mem) / basic_mem:.1f}%")

    # í´ë˜ìŠ¤ ì‚¬ìš© í…ŒìŠ¤íŠ¸
    print("\\n[3] TaxiDataLoader í…ŒìŠ¤íŠ¸")
    loader = TaxiDataLoader(DATA_PATH)
    loader.load(optimize=True, validate=True)

    df = loader.get_df()
    stats = loader.get_stats()
    print(f"\\nìµœì¢… DataFrame Shape: {df.shape}")
`,
      instructions: 'NYC Taxi ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ë¡œë”©í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì»¬ëŸ¼ ì„ íƒ, dtype ìµœì í™”, ë°ì´í„° ê²€ì¦/ì •ì œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.',
      keyPoints: [
        'usecolsë¡œ í•„ìš” ì»¬ëŸ¼ë§Œ ë¡œë”©í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½',
        'dtype ë‹¤ìš´ìºìŠ¤íŒ…ìœ¼ë¡œ ì¶”ê°€ ë©”ëª¨ë¦¬ ìµœì í™”',
        'ì´ìƒì¹˜ì™€ ëˆ„ë½ê°’ ì œê±°ë¡œ ë°ì´í„° í’ˆì§ˆ í™•ë³´',
        'tracemallocìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§'
      ]
    }
  },

  // Task 4: ì‹œê°„ëŒ€ë³„ ë¶„ì„ (Code)
  {
    id: 'w2d5-task4',
    title: 'Part 2: ì‹œê°„ëŒ€ë³„ ìš´í–‰ íŒ¨í„´ ë¶„ì„',
    type: 'code',
    duration: 25,
    content: {
      objectives: [
        'ì‹œê°„ëŒ€ë³„ ìš´í–‰ íŒ¨í„´ ë¶„ì„',
        'ìš”ì¼ë³„ íŠ¹ì„± íŒŒì•…',
        'í”¼í¬ ì‹œê°„ ì‹ë³„',
        'ì‹œê°í™” êµ¬í˜„'
      ],
      starterCode: `import pandas as pd
import numpy as np

# ì´ì „ íƒœìŠ¤í¬ì—ì„œ ë¡œë”©í•œ ë°ì´í„° ì‚¬ìš©
# df = loader.get_df()

# ë˜ëŠ” ìƒ˜í”Œ ë°ì´í„° ë¡œë”©
df = pd.read_parquet('yellow_tripdata_2022-01.parquet',
    columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime',
             'passenger_count', 'trip_distance', 'fare_amount',
             'tip_amount', 'total_amount', 'PULocationID'])

print(f"ë°ì´í„° í¬ê¸°: {len(df):,} í–‰")

# ===========================================
# Task 1: ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
# ===========================================

def extract_time_features(df):
    """ë‚ ì§œ/ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ"""
    # TODO: ë‹¤ìŒ íŠ¹ì„± ì¶”ì¶œ
    # - hour: ì‹œê°„ (0-23)
    # - day_of_week: ìš”ì¼ (0=ì›”, 6=ì¼)
    # - day_name: ìš”ì¼ëª… ('Monday', ...)
    # - is_weekend: ì£¼ë§ ì—¬ë¶€
    # - trip_duration_min: ìš´í–‰ ì‹œê°„ (ë¶„)

    pass

# ===========================================
# Task 2: ì‹œê°„ëŒ€ë³„ ë¶„ì„
# ===========================================

def analyze_by_hour(df):
    """ì‹œê°„ëŒ€ë³„ ë¶„ì„"""
    # TODO: ì‹œê°„ëŒ€ë³„ë¡œ ë‹¤ìŒ ì§€í‘œ ê³„ì‚°
    # - ìš´í–‰ ê±´ìˆ˜
    # - í‰ê·  ìš”ê¸ˆ
    # - í‰ê·  ì´ë™ ê±°ë¦¬
    # - í‰ê·  íŒ

    pass

# ===========================================
# Task 3: ìš”ì¼ë³„ ë¶„ì„
# ===========================================

def analyze_by_day(df):
    """ìš”ì¼ë³„ ë¶„ì„"""
    # TODO: ìš”ì¼ë³„ë¡œ ë‹¤ìŒ ì§€í‘œ ê³„ì‚°
    # - ìš´í–‰ ê±´ìˆ˜
    # - í‰ê·  ìš”ê¸ˆ
    # - í”¼í¬ ì‹œê°„ (ê°€ì¥ ìš´í–‰ì´ ë§ì€ ì‹œê°„)

    pass

# ===========================================
# Task 4: í”¼í¬ ì‹œê°„ ë¶„ì„
# ===========================================

def find_peak_hours(df):
    """í”¼í¬ ì‹œê°„ ì‹ë³„"""
    # TODO: ë‹¤ìŒ ë¶„ì„ ìˆ˜í–‰
    # - ì „ì²´ í”¼í¬ ì‹œê°„ (ìƒìœ„ 3ê°œ)
    # - í‰ì¼ í”¼í¬ ì‹œê°„
    # - ì£¼ë§ í”¼í¬ ì‹œê°„
    # - ì¶œí‡´ê·¼ ì‹œê°„ ë¹„ì¤‘ ë¶„ì„

    pass

# ===========================================
# Task 5: ê²°ê³¼ ì‹œê°í™” (ì½˜ì†” ì¶œë ¥)
# ===========================================

def visualize_patterns(hourly_stats, daily_stats):
    """íŒ¨í„´ ì‹œê°í™” (í…ìŠ¤íŠ¸ ê¸°ë°˜)"""
    # TODO: ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
    # - íˆìŠ¤í† ê·¸ë¨ í˜•íƒœì˜ ë°” ì°¨íŠ¸ (í…ìŠ¤íŠ¸)
    # - ìš”ì•½ í†µê³„

    pass

# ===========================================
# ì‹¤í–‰
# ===========================================

if __name__ == "__main__":
    print("=== ì‹œê°„ëŒ€ë³„ ìš´í–‰ íŒ¨í„´ ë¶„ì„ ===\\n")

    # íŠ¹ì„± ì¶”ì¶œ
    df = extract_time_features(df)

    # ë¶„ì„ ì‹¤í–‰
    hourly = analyze_by_hour(df)
    daily = analyze_by_day(df)
    peaks = find_peak_hours(df)

    # ê²°ê³¼ ì¶œë ¥
    visualize_patterns(hourly, daily)
    print("\\ní”¼í¬ ì‹œê°„ ë¶„ì„:")
    print(peaks)
`,
      solutionCode: `import pandas as pd
import numpy as np

df = pd.read_parquet('yellow_tripdata_2022-01.parquet',
    columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime',
             'passenger_count', 'trip_distance', 'fare_amount',
             'tip_amount', 'total_amount', 'PULocationID'])

print(f"ë°ì´í„° í¬ê¸°: {len(df):,} í–‰")

# ===========================================
# Task 1: ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
# ===========================================

def extract_time_features(df):
    """ë‚ ì§œ/ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ"""
    df = df.copy()

    # ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
    df['hour'] = df['tpep_pickup_datetime'].dt.hour
    df['day_of_week'] = df['tpep_pickup_datetime'].dt.dayofweek
    df['day_name'] = df['tpep_pickup_datetime'].dt.day_name()
    df['is_weekend'] = df['day_of_week'].isin([5, 6])

    # ìš´í–‰ ì‹œê°„ ê³„ì‚° (ë¶„)
    df['trip_duration_min'] = (
        df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']
    ).dt.total_seconds() / 60

    # ì´ìƒì¹˜ ì œê±° (0ë¶„ ì´ˆê³¼, 180ë¶„ ì´í•˜)
    df = df[(df['trip_duration_min'] > 0) & (df['trip_duration_min'] <= 180)]

    print(f"íŠ¹ì„± ì¶”ì¶œ ì™„ë£Œ: {len(df):,} í–‰")
    print(f"ì¶”ê°€ëœ ì»¬ëŸ¼: hour, day_of_week, day_name, is_weekend, trip_duration_min")

    return df

# ===========================================
# Task 2: ì‹œê°„ëŒ€ë³„ ë¶„ì„
# ===========================================

def analyze_by_hour(df):
    """ì‹œê°„ëŒ€ë³„ ë¶„ì„"""
    hourly_stats = df.groupby('hour').agg({
        'fare_amount': ['count', 'mean'],
        'trip_distance': 'mean',
        'tip_amount': 'mean',
        'trip_duration_min': 'mean'
    }).round(2)

    hourly_stats.columns = ['trip_count', 'avg_fare', 'avg_distance', 'avg_tip', 'avg_duration']
    hourly_stats = hourly_stats.reset_index()

    return hourly_stats

# ===========================================
# Task 3: ìš”ì¼ë³„ ë¶„ì„
# ===========================================

def analyze_by_day(df):
    """ìš”ì¼ë³„ ë¶„ì„"""
    daily_stats = df.groupby(['day_of_week', 'day_name']).agg({
        'fare_amount': ['count', 'mean'],
        'trip_distance': 'mean',
        'tip_amount': 'mean'
    }).round(2)

    daily_stats.columns = ['trip_count', 'avg_fare', 'avg_distance', 'avg_tip']
    daily_stats = daily_stats.reset_index()

    # ìš”ì¼ë³„ í”¼í¬ ì‹œê°„ ê³„ì‚°
    peak_hours = df.groupby(['day_of_week', 'hour']).size().reset_index(name='count')
    peak_hours = peak_hours.loc[peak_hours.groupby('day_of_week')['count'].idxmax()]
    daily_stats = daily_stats.merge(
        peak_hours[['day_of_week', 'hour']].rename(columns={'hour': 'peak_hour'}),
        on='day_of_week'
    )

    return daily_stats

# ===========================================
# Task 4: í”¼í¬ ì‹œê°„ ë¶„ì„
# ===========================================

def find_peak_hours(df):
    """í”¼í¬ ì‹œê°„ ì‹ë³„"""
    results = {}

    # ì „ì²´ í”¼í¬ ì‹œê°„
    hourly_counts = df.groupby('hour').size().sort_values(ascending=False)
    results['overall_peak_hours'] = hourly_counts.head(3).to_dict()

    # í‰ì¼ í”¼í¬ ì‹œê°„
    weekday_counts = df[~df['is_weekend']].groupby('hour').size().sort_values(ascending=False)
    results['weekday_peak_hours'] = weekday_counts.head(3).to_dict()

    # ì£¼ë§ í”¼í¬ ì‹œê°„
    weekend_counts = df[df['is_weekend']].groupby('hour').size().sort_values(ascending=False)
    results['weekend_peak_hours'] = weekend_counts.head(3).to_dict()

    # ì¶œí‡´ê·¼ ì‹œê°„ ë¹„ì¤‘
    morning_rush = df['hour'].between(7, 9).sum()
    evening_rush = df['hour'].between(17, 19).sum()
    total_trips = len(df)

    results['commute_analysis'] = {
        'morning_rush_pct': round(100 * morning_rush / total_trips, 1),
        'evening_rush_pct': round(100 * evening_rush / total_trips, 1),
        'off_peak_pct': round(100 * (total_trips - morning_rush - evening_rush) / total_trips, 1)
    }

    return results

# ===========================================
# Task 5: ê²°ê³¼ ì‹œê°í™” (ì½˜ì†” ì¶œë ¥)
# ===========================================

def visualize_patterns(hourly_stats, daily_stats):
    """íŒ¨í„´ ì‹œê°í™” (í…ìŠ¤íŠ¸ ê¸°ë°˜)"""

    # ì‹œê°„ëŒ€ë³„ ìš´í–‰ íŒ¨í„´ (í…ìŠ¤íŠ¸ ë°” ì°¨íŠ¸)
    print("\\n=== ì‹œê°„ëŒ€ë³„ ìš´í–‰ íŒ¨í„´ ===")
    max_count = hourly_stats['trip_count'].max()

    for _, row in hourly_stats.iterrows():
        bar_len = int(50 * row['trip_count'] / max_count)
        bar = 'â–ˆ' * bar_len
        print(f"{row['hour']:2d}ì‹œ | {bar} {row['trip_count']:,}")

    # ìš”ì¼ë³„ ìš”ì•½
    print("\\n=== ìš”ì¼ë³„ ìš”ì•½ ===")
    print(f"{'ìš”ì¼':<12} {'ìš´í–‰ ìˆ˜':>12} {'í‰ê· ìš”ê¸ˆ':>10} {'í”¼í¬ì‹œê°„':>8}")
    print("-" * 45)

    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    for day in day_order:
        row = daily_stats[daily_stats['day_name'] == day].iloc[0]
        print(f"{row['day_name']:<12} {row['trip_count']:>12,} \${row['avg_fare']:>8.2f} {row['peak_hour']:>6}ì‹œ")

    # ìš”ì•½ í†µê³„
    print("\\n=== ìš”ì•½ í†µê³„ ===")
    total_trips = hourly_stats['trip_count'].sum()
    busiest_hour = hourly_stats.loc[hourly_stats['trip_count'].idxmax()]
    slowest_hour = hourly_stats.loc[hourly_stats['trip_count'].idxmin()]

    print(f"ì´ ìš´í–‰ ê±´ìˆ˜: {total_trips:,}")
    print(f"ê°€ì¥ ë°”ìœ ì‹œê°„: {busiest_hour['hour']}ì‹œ ({busiest_hour['trip_count']:,}ê±´)")
    print(f"ê°€ì¥ í•œê°€í•œ ì‹œê°„: {slowest_hour['hour']}ì‹œ ({slowest_hour['trip_count']:,}ê±´)")
    print(f"í‰ê·  ìš”ê¸ˆ: \${hourly_stats['avg_fare'].mean():.2f}")
    print(f"í‰ê·  ì´ë™ ê±°ë¦¬: {hourly_stats['avg_distance'].mean():.2f} ë§ˆì¼")

# ===========================================
# ì‹¤í–‰
# ===========================================

if __name__ == "__main__":
    print("=== ì‹œê°„ëŒ€ë³„ ìš´í–‰ íŒ¨í„´ ë¶„ì„ ===\\n")

    # íŠ¹ì„± ì¶”ì¶œ
    df = extract_time_features(df)

    # ë¶„ì„ ì‹¤í–‰
    hourly = analyze_by_hour(df)
    daily = analyze_by_day(df)
    peaks = find_peak_hours(df)

    # ê²°ê³¼ ì¶œë ¥
    visualize_patterns(hourly, daily)

    print("\\n=== í”¼í¬ ì‹œê°„ ë¶„ì„ ===")
    print(f"ì „ì²´ í”¼í¬ ì‹œê°„: {peaks['overall_peak_hours']}")
    print(f"í‰ì¼ í”¼í¬ ì‹œê°„: {peaks['weekday_peak_hours']}")
    print(f"ì£¼ë§ í”¼í¬ ì‹œê°„: {peaks['weekend_peak_hours']}")
    print(f"\\nì¶œí‡´ê·¼ ì‹œê°„ ë¹„ì¤‘:")
    print(f"  ì•„ì¹¨ (7-9ì‹œ): {peaks['commute_analysis']['morning_rush_pct']}%")
    print(f"  ì €ë… (17-19ì‹œ): {peaks['commute_analysis']['evening_rush_pct']}%")
    print(f"  ê¸°íƒ€: {peaks['commute_analysis']['off_peak_pct']}%")
`,
      instructions: 'ì‹œê°„ëŒ€ë³„ ìš´í–‰ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì‹œê°„ íŠ¹ì„±ì„ ì¶”ì¶œí•˜ê³ , ì‹œê°„ëŒ€/ìš”ì¼ë³„ ìš´í–‰ íŒ¨í„´ê³¼ í”¼í¬ ì‹œê°„ì„ ì‹ë³„í•©ë‹ˆë‹¤.',
      keyPoints: [
        'dt accessorë¡œ ì‹œê°„ íŠ¹ì„± íš¨ìœ¨ì  ì¶”ì¶œ',
        'groupby().agg()ë¡œ ë‹¤ì¤‘ ì§‘ê³„',
        'í‰ì¼/ì£¼ë§ íŒ¨í„´ ì°¨ì´ ë¶„ì„',
        'ì¶œí‡´ê·¼ ì‹œê°„ vs ë¹„í”¼í¬ ì‹œê°„ ë¹„êµ'
      ]
    }
  },

  // Task 5: ì§€ì—­ë³„ ë¶„ì„ (Code)
  {
    id: 'w2d5-task5',
    title: 'Part 3: ì§€ì—­ë³„ ìš”ê¸ˆ ë¶„ì„',
    type: 'code',
    duration: 25,
    content: {
      objectives: [
        'Top ìŠ¹ì°¨/í•˜ì°¨ ì§€ì—­ ì‹ë³„',
        'ì§€ì—­ë³„ í‰ê·  ìš”ê¸ˆ ë¶„ì„',
        'ì§€ì—­ ê°„ ì´ë™ íŒ¨í„´ ë¶„ì„',
        'OD(Origin-Destination) ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±'
      ],
      starterCode: `import pandas as pd
import numpy as np

# ë°ì´í„° ë¡œë”©
df = pd.read_parquet('yellow_tripdata_2022-01.parquet',
    columns=['PULocationID', 'DOLocationID', 'fare_amount',
             'tip_amount', 'total_amount', 'trip_distance'])

print(f"ë°ì´í„° í¬ê¸°: {len(df):,} í–‰")

# ì§€ì—­ ID â†’ ì´ë¦„ ë§¤í•‘ (ì¼ë¶€ ì£¼ìš” ì§€ì—­)
ZONE_NAMES = {
    1: 'Newark Airport', 132: 'JFK Airport', 138: 'LaGuardia Airport',
    161: 'Midtown Center', 162: 'Midtown East', 163: 'Midtown North',
    164: 'Midtown South', 186: 'Penn Station', 230: 'Times Sq',
    234: 'Union Sq', 237: 'Upper East Side N', 236: 'Upper East Side S',
    239: 'Upper West Side N', 238: 'Upper West Side S',
    79: 'East Village', 114: 'Greenwich Village', 125: 'Hudson Sq'
}

# ===========================================
# Task 1: Top ìŠ¹ì°¨/í•˜ì°¨ ì§€ì—­
# ===========================================

def analyze_popular_zones(df, top_n=10):
    """ì¸ê¸° ìŠ¹ì°¨/í•˜ì°¨ ì§€ì—­ ë¶„ì„"""
    # TODO: êµ¬í˜„
    # 1. Top N ìŠ¹ì°¨ ì§€ì—­ (ìš´í–‰ ê±´ìˆ˜ ê¸°ì¤€)
    # 2. Top N í•˜ì°¨ ì§€ì—­
    # 3. ì§€ì—­ëª… ë§¤í•‘
    pass

# ===========================================
# Task 2: ì§€ì—­ë³„ í‰ê·  ìš”ê¸ˆ
# ===========================================

def analyze_zone_fares(df, min_trips=100):
    """ì§€ì—­ë³„ ìš”ê¸ˆ ë¶„ì„"""
    # TODO: êµ¬í˜„
    # 1. ìŠ¹ì°¨ ì§€ì—­ë³„ í‰ê·  ìš”ê¸ˆ, í‰ê·  íŒ, í‰ê·  ê±°ë¦¬
    # 2. ìµœì†Œ ìš´í–‰ ê±´ìˆ˜ ì´ìƒì¸ ì§€ì—­ë§Œ í¬í•¨
    # 3. ê³ ìš”ê¸ˆ ì§€ì—­ vs ì €ìš”ê¸ˆ ì§€ì—­ ë¹„êµ
    pass

# ===========================================
# Task 3: OD ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
# ===========================================

def create_od_matrix(df, top_n=10):
    """Origin-Destination ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
    # TODO: êµ¬í˜„
    # 1. ìƒìœ„ Nê°œ ì§€ì—­ ì„ ì •
    # 2. ìŠ¹ì°¨-í•˜ì°¨ ì§€ì—­ ìŒë³„ ìš´í–‰ ê±´ìˆ˜
    # 3. í”¼ë²— í…Œì´ë¸”ë¡œ ë§¤íŠ¸ë¦­ìŠ¤ í˜•íƒœ ë³€í™˜
    pass

# ===========================================
# Task 4: ê³µí•­ ë¶„ì„
# ===========================================

def analyze_airports(df):
    """ê³µí•­ ê´€ë ¨ ìš´í–‰ ë¶„ì„"""
    # TODO: êµ¬í˜„
    # ê³µí•­ ID: JFK(132), LaGuardia(138), Newark(1)
    # 1. ê³µí•­ TO/FROM ìš´í–‰ ë¹„ìœ¨
    # 2. ê³µí•­ë³„ í‰ê·  ìš”ê¸ˆ
    # 3. ê³µí•­ ì´ìš© ì‹œê°„ëŒ€ íŒ¨í„´
    pass

# ===========================================
# Task 5: ê²°ê³¼ ì¶œë ¥
# ===========================================

def print_zone_analysis(popular, fares, od_matrix, airport):
    """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
    # TODO: ê²°ê³¼ í¬ë§·íŒ… ë° ì¶œë ¥
    pass

# ===========================================
# ì‹¤í–‰
# ===========================================

if __name__ == "__main__":
    print("=== ì§€ì—­ë³„ ìš”ê¸ˆ ë¶„ì„ ===\\n")

    popular = analyze_popular_zones(df)
    fares = analyze_zone_fares(df)
    od = create_od_matrix(df)
    airport = analyze_airports(df)

    print_zone_analysis(popular, fares, od, airport)
`,
      solutionCode: `import pandas as pd
import numpy as np

df = pd.read_parquet('yellow_tripdata_2022-01.parquet',
    columns=['PULocationID', 'DOLocationID', 'fare_amount',
             'tip_amount', 'total_amount', 'trip_distance',
             'tpep_pickup_datetime'])

print(f"ë°ì´í„° í¬ê¸°: {len(df):,} í–‰")

ZONE_NAMES = {
    1: 'Newark Airport', 132: 'JFK Airport', 138: 'LaGuardia Airport',
    161: 'Midtown Center', 162: 'Midtown East', 163: 'Midtown North',
    164: 'Midtown South', 186: 'Penn Station', 230: 'Times Sq',
    234: 'Union Sq', 237: 'Upper East Side N', 236: 'Upper East Side S',
    239: 'Upper West Side N', 238: 'Upper West Side S',
    79: 'East Village', 114: 'Greenwich Village', 125: 'Hudson Sq'
}

def get_zone_name(zone_id):
    return ZONE_NAMES.get(zone_id, f'Zone {zone_id}')

# ===========================================
# Task 1: Top ìŠ¹ì°¨/í•˜ì°¨ ì§€ì—­
# ===========================================

def analyze_popular_zones(df, top_n=10):
    """ì¸ê¸° ìŠ¹ì°¨/í•˜ì°¨ ì§€ì—­ ë¶„ì„"""
    # Top ìŠ¹ì°¨ ì§€ì—­
    pickup_counts = df['PULocationID'].value_counts().head(top_n).reset_index()
    pickup_counts.columns = ['zone_id', 'trip_count']
    pickup_counts['zone_name'] = pickup_counts['zone_id'].apply(get_zone_name)
    pickup_counts['pct'] = (pickup_counts['trip_count'] / len(df) * 100).round(2)

    # Top í•˜ì°¨ ì§€ì—­
    dropoff_counts = df['DOLocationID'].value_counts().head(top_n).reset_index()
    dropoff_counts.columns = ['zone_id', 'trip_count']
    dropoff_counts['zone_name'] = dropoff_counts['zone_id'].apply(get_zone_name)
    dropoff_counts['pct'] = (dropoff_counts['trip_count'] / len(df) * 100).round(2)

    return {'pickup': pickup_counts, 'dropoff': dropoff_counts}

# ===========================================
# Task 2: ì§€ì—­ë³„ í‰ê·  ìš”ê¸ˆ
# ===========================================

def analyze_zone_fares(df, min_trips=100):
    """ì§€ì—­ë³„ ìš”ê¸ˆ ë¶„ì„"""
    zone_stats = df.groupby('PULocationID').agg({
        'fare_amount': ['count', 'mean'],
        'tip_amount': 'mean',
        'trip_distance': 'mean',
        'total_amount': 'mean'
    }).round(2)

    zone_stats.columns = ['trip_count', 'avg_fare', 'avg_tip', 'avg_distance', 'avg_total']
    zone_stats = zone_stats.reset_index()

    # ìµœì†Œ ìš´í–‰ ê±´ìˆ˜ í•„í„°
    zone_stats = zone_stats[zone_stats['trip_count'] >= min_trips]
    zone_stats['zone_name'] = zone_stats['PULocationID'].apply(get_zone_name)

    # ì •ë ¬
    zone_stats_sorted = zone_stats.sort_values('avg_fare', ascending=False)

    return {
        'all': zone_stats,
        'top_fare': zone_stats_sorted.head(10),
        'low_fare': zone_stats_sorted.tail(10)
    }

# ===========================================
# Task 3: OD ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
# ===========================================

def create_od_matrix(df, top_n=10):
    """Origin-Destination ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±"""
    # ìƒìœ„ ìŠ¹ì°¨ ì§€ì—­ ì„ ì •
    top_pickups = df['PULocationID'].value_counts().head(top_n).index.tolist()
    top_dropoffs = df['DOLocationID'].value_counts().head(top_n).index.tolist()
    top_zones = list(set(top_pickups + top_dropoffs))[:top_n]

    # í•„í„°ë§
    df_filtered = df[
        df['PULocationID'].isin(top_zones) &
        df['DOLocationID'].isin(top_zones)
    ]

    # í”¼ë²— í…Œì´ë¸”
    od_matrix = df_filtered.pivot_table(
        values='fare_amount',
        index='PULocationID',
        columns='DOLocationID',
        aggfunc='count',
        fill_value=0
    )

    # ì§€ì—­ëª…ìœ¼ë¡œ ì¸ë±ìŠ¤/ì»¬ëŸ¼ ë³€í™˜
    od_matrix.index = [get_zone_name(z) for z in od_matrix.index]
    od_matrix.columns = [get_zone_name(z) for z in od_matrix.columns]

    return od_matrix

# ===========================================
# Task 4: ê³µí•­ ë¶„ì„
# ===========================================

def analyze_airports(df):
    """ê³µí•­ ê´€ë ¨ ìš´í–‰ ë¶„ì„"""
    airport_ids = [132, 138, 1]  # JFK, LaGuardia, Newark
    airport_names = {132: 'JFK', 138: 'LaGuardia', 1: 'Newark'}

    results = {}
    total_trips = len(df)

    for aid, aname in airport_names.items():
        # TO ê³µí•­
        to_airport = df[df['DOLocationID'] == aid]
        # FROM ê³µí•­
        from_airport = df[df['PULocationID'] == aid]

        results[aname] = {
            'to_count': len(to_airport),
            'to_pct': round(100 * len(to_airport) / total_trips, 2),
            'from_count': len(from_airport),
            'from_pct': round(100 * len(from_airport) / total_trips, 2),
            'to_avg_fare': round(to_airport['fare_amount'].mean(), 2),
            'from_avg_fare': round(from_airport['fare_amount'].mean(), 2),
            'to_avg_distance': round(to_airport['trip_distance'].mean(), 2),
            'from_avg_distance': round(from_airport['trip_distance'].mean(), 2)
        }

    return results

# ===========================================
# Task 5: ê²°ê³¼ ì¶œë ¥
# ===========================================

def print_zone_analysis(popular, fares, od_matrix, airport):
    """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""

    # Top ìŠ¹ì°¨ ì§€ì—­
    print("=== Top 10 ìŠ¹ì°¨ ì§€ì—­ ===")
    for _, row in popular['pickup'].iterrows():
        print(f"  {row['zone_name']:<25} {row['trip_count']:>10,} ({row['pct']:.1f}%)")

    # Top í•˜ì°¨ ì§€ì—­
    print("\\n=== Top 10 í•˜ì°¨ ì§€ì—­ ===")
    for _, row in popular['dropoff'].iterrows():
        print(f"  {row['zone_name']:<25} {row['trip_count']:>10,} ({row['pct']:.1f}%)")

    # ê³ ìš”ê¸ˆ/ì €ìš”ê¸ˆ ì§€ì—­
    print("\\n=== ê³ ìš”ê¸ˆ ì§€ì—­ (ìƒìœ„ 5) ===")
    for _, row in fares['top_fare'].head(5).iterrows():
        print(f"  {row['zone_name']:<25} í‰ê·  \${row['avg_fare']:>7.2f} (ê±°ë¦¬: {row['avg_distance']:.1f}mi)")

    print("\\n=== ì €ìš”ê¸ˆ ì§€ì—­ (í•˜ìœ„ 5) ===")
    for _, row in fares['low_fare'].head(5).iterrows():
        print(f"  {row['zone_name']:<25} í‰ê·  \${row['avg_fare']:>7.2f} (ê±°ë¦¬: {row['avg_distance']:.1f}mi)")

    # OD ë§¤íŠ¸ë¦­ìŠ¤
    print("\\n=== OD ë§¤íŠ¸ë¦­ìŠ¤ (ìƒìœ„ ì§€ì—­ ê°„ ìš´í–‰ ê±´ìˆ˜) ===")
    print(od_matrix.to_string())

    # ê³µí•­ ë¶„ì„
    print("\\n=== ê³µí•­ ìš´í–‰ ë¶„ì„ ===")
    for airport_name, stats in airport.items():
        print(f"\\n{airport_name}:")
        print(f"  TO ê³µí•­: {stats['to_count']:,}ê±´ ({stats['to_pct']}%), í‰ê·  \${stats['to_avg_fare']:.2f}")
        print(f"  FROM ê³µí•­: {stats['from_count']:,}ê±´ ({stats['from_pct']}%), í‰ê·  \${stats['from_avg_fare']:.2f}")

# ===========================================
# ì‹¤í–‰
# ===========================================

if __name__ == "__main__":
    print("=== ì§€ì—­ë³„ ìš”ê¸ˆ ë¶„ì„ ===\\n")

    popular = analyze_popular_zones(df)
    fares = analyze_zone_fares(df)
    od = create_od_matrix(df)
    airport = analyze_airports(df)

    print_zone_analysis(popular, fares, od, airport)
`,
      instructions: 'ì§€ì—­ë³„ ìš´í–‰ íŒ¨í„´ê³¼ ìš”ê¸ˆì„ ë¶„ì„í•©ë‹ˆë‹¤. ì¸ê¸° ìŠ¹í•˜ì°¨ ì§€ì—­, ì§€ì—­ë³„ ìš”ê¸ˆ ì°¨ì´, OD ë§¤íŠ¸ë¦­ìŠ¤, ê³µí•­ ì´ìš© íŒ¨í„´ì„ íŒŒì•…í•©ë‹ˆë‹¤.',
      keyPoints: [
        'value_counts()ë¡œ ë¹ˆë„ ë¶„ì„',
        'pivot_table()ë¡œ OD ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±',
        'ê³µí•­ íŠ¹ìˆ˜ ì§€ì—­ ë³„ë„ ë¶„ì„',
        'ì§€ì—­ID â†’ ì§€ì—­ëª… ë§¤í•‘ìœ¼ë¡œ ê°€ë…ì„± í–¥ìƒ'
      ]
    }
  },

  // Task 6: ìµœì¢… í”„ë¡œì íŠ¸ (Code)
  {
    id: 'w2d5-task6',
    title: 'Part 4: ì¢…í•© ë¶„ì„ íŒŒì´í”„ë¼ì¸',
    type: 'code',
    duration: 30,
    content: {
      objectives: [
        'ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ í†µí•©',
        'pandas vs Polars ì„±ëŠ¥ ë¹„êµ',
        'ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±',
        'ì¸ì‚¬ì´íŠ¸ ë„ì¶œ'
      ],
      starterCode: `import pandas as pd
import polars as pl
import numpy as np
import time
from typing import Dict, Any

# ===========================================
# ìµœì¢… í”„ë¡œì íŠ¸: NYC Taxi ì¢…í•© ë¶„ì„ íŒŒì´í”„ë¼ì¸
# ===========================================

class NYCTaxiAnalyzer:
    """NYC Taxi ë°ì´í„° ì¢…í•© ë¶„ì„ê¸°"""

    def __init__(self, filepath: str):
        self.filepath = filepath
        self.df = None
        self.results = {}
        self.performance = {}

    # ===========================================
    # ë°ì´í„° ë¡œë”©
    # ===========================================

    def load_data(self, backend='pandas'):
        """ë°ì´í„° ë¡œë”©"""
        # TODO: pandasì™€ polars ëª¨ë‘ ì§€ì›
        pass

    # ===========================================
    # ì‹œê°„ëŒ€ë³„ ë¶„ì„
    # ===========================================

    def analyze_time_patterns(self) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„"""
        # TODO: ì´ì „ íƒœìŠ¤í¬ì˜ ë¶„ì„ í†µí•©
        pass

    # ===========================================
    # ì§€ì—­ë³„ ë¶„ì„
    # ===========================================

    def analyze_zones(self) -> Dict[str, Any]:
        """ì§€ì—­ë³„ ë¶„ì„"""
        # TODO: ì´ì „ íƒœìŠ¤í¬ì˜ ë¶„ì„ í†µí•©
        pass

    # ===========================================
    # íŒ ë¶„ì„ (ì„ íƒ ê³¼ì œ)
    # ===========================================

    def analyze_tips(self) -> Dict[str, Any]:
        """íŒ íŒ¨í„´ ë¶„ì„"""
        # TODO: êµ¬í˜„
        # 1. íŒ ë¶„í¬ ë¶„ì„
        # 2. íŒ ë¹„ìœ¨ ê³„ì‚° (tip_amount / fare_amount)
        # 3. íŒì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì¸ ë¶„ì„
        pass

    # ===========================================
    # pandas vs Polars ë¹„êµ (ì„ íƒ ê³¼ì œ)
    # ===========================================

    def compare_backends(self) -> Dict[str, Any]:
        """pandas vs Polars ì„±ëŠ¥ ë¹„êµ"""
        # TODO: êµ¬í˜„
        # 1. ë™ì¼í•œ ë¶„ì„ì„ ë‘ ë°±ì—”ë“œë¡œ ìˆ˜í–‰
        # 2. ì‹¤í–‰ ì‹œê°„ ë¹„êµ
        # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
        pass

    # ===========================================
    # ë¦¬í¬íŠ¸ ìƒì„±
    # ===========================================

    def generate_report(self) -> str:
        """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        # TODO: ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•œ ë¦¬í¬íŠ¸
        pass

    # ===========================================
    # ë©”ì¸ ì‹¤í–‰
    # ===========================================

    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=== NYC Taxi ì¢…í•© ë¶„ì„ ì‹œì‘ ===\\n")

        # 1. ë°ì´í„° ë¡œë”©
        print("[1/5] ë°ì´í„° ë¡œë”©...")
        self.load_data()

        # 2. ì‹œê°„ëŒ€ë³„ ë¶„ì„
        print("[2/5] ì‹œê°„ëŒ€ë³„ ë¶„ì„...")
        self.results['time'] = self.analyze_time_patterns()

        # 3. ì§€ì—­ë³„ ë¶„ì„
        print("[3/5] ì§€ì—­ë³„ ë¶„ì„...")
        self.results['zones'] = self.analyze_zones()

        # 4. íŒ ë¶„ì„
        print("[4/5] íŒ ë¶„ì„...")
        self.results['tips'] = self.analyze_tips()

        # 5. ë°±ì—”ë“œ ë¹„êµ
        print("[5/5] ë°±ì—”ë“œ ë¹„êµ...")
        self.results['backend'] = self.compare_backends()

        # ë¦¬í¬íŠ¸ ìƒì„±
        print("\\n" + "=" * 60)
        print(self.generate_report())

        return self.results

# ===========================================
# ì‹¤í–‰
# ===========================================

if __name__ == "__main__":
    analyzer = NYCTaxiAnalyzer('yellow_tripdata_2022-01.parquet')
    results = analyzer.run_full_analysis()
`,
      solutionCode: `import pandas as pd
import polars as pl
import numpy as np
import time
import tracemalloc
from typing import Dict, Any

class NYCTaxiAnalyzer:
    """NYC Taxi ë°ì´í„° ì¢…í•© ë¶„ì„ê¸°"""

    COLUMNS = [
        'tpep_pickup_datetime', 'tpep_dropoff_datetime',
        'passenger_count', 'trip_distance',
        'PULocationID', 'DOLocationID', 'payment_type',
        'fare_amount', 'tip_amount', 'total_amount'
    ]

    def __init__(self, filepath: str):
        self.filepath = filepath
        self.df = None
        self.df_polars = None
        self.results = {}
        self.performance = {}

    def load_data(self, backend='pandas'):
        """ë°ì´í„° ë¡œë”©"""
        start = time.time()
        tracemalloc.start()

        if backend == 'pandas':
            self.df = pd.read_parquet(self.filepath, columns=self.COLUMNS)
            # ì‹œê°„ íŠ¹ì„± ì¶”ê°€
            self.df['hour'] = self.df['tpep_pickup_datetime'].dt.hour
            self.df['day_of_week'] = self.df['tpep_pickup_datetime'].dt.dayofweek
            self.df['is_weekend'] = self.df['day_of_week'].isin([5, 6])
        else:
            self.df_polars = pl.read_parquet(self.filepath, columns=self.COLUMNS)

        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        self.performance['load'] = {
            'time': time.time() - start,
            'memory_mb': peak / 1024**2
        }

        print(f"  ë¡œë”© ì™„ë£Œ: {len(self.df):,} í–‰")
        print(f"  ì‹œê°„: {self.performance['load']['time']:.2f}ì´ˆ")
        print(f"  ë©”ëª¨ë¦¬: {self.performance['load']['memory_mb']:.1f} MB")

        return self

    def analyze_time_patterns(self) -> Dict[str, Any]:
        """ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„"""
        start = time.time()

        hourly = self.df.groupby('hour').agg({
            'fare_amount': ['count', 'mean'],
            'trip_distance': 'mean',
            'tip_amount': 'mean'
        }).round(2)
        hourly.columns = ['trips', 'avg_fare', 'avg_distance', 'avg_tip']

        daily = self.df.groupby('day_of_week').agg({
            'fare_amount': ['count', 'mean']
        }).round(2)
        daily.columns = ['trips', 'avg_fare']

        # í”¼í¬ ì‹œê°„
        peak_hour = hourly['trips'].idxmax()
        slow_hour = hourly['trips'].idxmin()

        self.performance['time_analysis'] = time.time() - start

        return {
            'hourly': hourly,
            'daily': daily,
            'peak_hour': peak_hour,
            'slow_hour': slow_hour,
            'weekday_avg_trips': daily.loc[0:4, 'trips'].mean(),
            'weekend_avg_trips': daily.loc[5:6, 'trips'].mean()
        }

    def analyze_zones(self) -> Dict[str, Any]:
        """ì§€ì—­ë³„ ë¶„ì„"""
        start = time.time()

        # Top 10 ìŠ¹ì°¨ ì§€ì—­
        top_pickups = self.df['PULocationID'].value_counts().head(10)

        # Top 10 í•˜ì°¨ ì§€ì—­
        top_dropoffs = self.df['DOLocationID'].value_counts().head(10)

        # ì§€ì—­ë³„ í‰ê·  ìš”ê¸ˆ
        zone_fares = self.df.groupby('PULocationID').agg({
            'fare_amount': ['count', 'mean']
        }).round(2)
        zone_fares.columns = ['trips', 'avg_fare']
        zone_fares = zone_fares[zone_fares['trips'] >= 100].sort_values('avg_fare', ascending=False)

        self.performance['zone_analysis'] = time.time() - start

        return {
            'top_pickups': top_pickups,
            'top_dropoffs': top_dropoffs,
            'high_fare_zones': zone_fares.head(5),
            'low_fare_zones': zone_fares.tail(5)
        }

    def analyze_tips(self) -> Dict[str, Any]:
        """íŒ íŒ¨í„´ ë¶„ì„"""
        start = time.time()

        # í˜„ê¸ˆ ê²°ì œ ì œì™¸ (íŒ 0ì¸ ê²½ìš°ê°€ ë§ìŒ)
        df_card = self.df[self.df['payment_type'] == 1]

        # íŒ ë¹„ìœ¨ ê³„ì‚°
        df_card = df_card.copy()
        df_card['tip_pct'] = (df_card['tip_amount'] / df_card['fare_amount'] * 100).clip(0, 100)

        # íŒ ë¶„í¬
        tip_stats = {
            'mean': df_card['tip_amount'].mean(),
            'median': df_card['tip_amount'].median(),
            'std': df_card['tip_amount'].std(),
            'pct_25': df_card['tip_amount'].quantile(0.25),
            'pct_75': df_card['tip_amount'].quantile(0.75)
        }

        # íŒ ë¹„ìœ¨ ë¶„í¬
        tip_pct_stats = {
            'mean_pct': df_card['tip_pct'].mean(),
            'median_pct': df_card['tip_pct'].median()
        }

        # ì‹œê°„ëŒ€ë³„ íŒ
        hourly_tip = df_card.groupby(df_card['tpep_pickup_datetime'].dt.hour)['tip_pct'].mean()

        # ê±°ë¦¬ë³„ íŒ
        df_card['distance_bin'] = pd.cut(df_card['trip_distance'], bins=[0, 2, 5, 10, 100], labels=['Short', 'Medium', 'Long', 'Very Long'])
        distance_tip = df_card.groupby('distance_bin')['tip_pct'].mean()

        self.performance['tip_analysis'] = time.time() - start

        return {
            'tip_stats': tip_stats,
            'tip_pct_stats': tip_pct_stats,
            'hourly_tip_pct': hourly_tip,
            'distance_tip_pct': distance_tip
        }

    def compare_backends(self) -> Dict[str, Any]:
        """pandas vs Polars ì„±ëŠ¥ ë¹„êµ"""
        results = {}

        # pandas í…ŒìŠ¤íŠ¸
        start = time.time()
        _ = self.df.groupby(['hour', 'day_of_week']).agg({
            'fare_amount': 'mean',
            'trip_distance': 'mean',
            'tip_amount': 'sum'
        })
        results['pandas_groupby'] = time.time() - start

        # Polars í…ŒìŠ¤íŠ¸
        df_pl = pl.from_pandas(self.df)
        start = time.time()
        _ = df_pl.group_by(['hour', 'day_of_week']).agg([
            pl.col('fare_amount').mean(),
            pl.col('trip_distance').mean(),
            pl.col('tip_amount').sum()
        ])
        results['polars_groupby'] = time.time() - start

        results['speedup'] = results['pandas_groupby'] / results['polars_groupby']

        return results

    def generate_report(self) -> str:
        """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("=" * 60)
        report.append("NYC Taxi ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("=" * 60)

        # ë°ì´í„° ê°œìš”
        report.append(f"\\nğŸ“Š ë°ì´í„° ê°œìš”")
        report.append(f"  - ì´ ìš´í–‰ ê±´ìˆ˜: {len(self.df):,}")
        report.append(f"  - ë¶„ì„ ê¸°ê°„: {self.df['tpep_pickup_datetime'].min().date()} ~ {self.df['tpep_pickup_datetime'].max().date()}")

        # ì‹œê°„ íŒ¨í„´
        time_results = self.results.get('time', {})
        report.append(f"\\nâ° ì‹œê°„ íŒ¨í„´")
        report.append(f"  - í”¼í¬ ì‹œê°„: {time_results.get('peak_hour')}ì‹œ")
        report.append(f"  - í•œê°€í•œ ì‹œê°„: {time_results.get('slow_hour')}ì‹œ")
        report.append(f"  - í‰ì¼ í‰ê· : {time_results.get('weekday_avg_trips', 0):,.0f}ê±´/ì¼")
        report.append(f"  - ì£¼ë§ í‰ê· : {time_results.get('weekend_avg_trips', 0):,.0f}ê±´/ì¼")

        # ì§€ì—­ ë¶„ì„
        zone_results = self.results.get('zones', {})
        report.append(f"\\nğŸ“ ì§€ì—­ ë¶„ì„")
        if 'top_pickups' in zone_results:
            top_zone = zone_results['top_pickups'].index[0]
            report.append(f"  - ê°€ì¥ ì¸ê¸° ìŠ¹ì°¨ ì§€ì—­: Zone {top_zone}")
        if 'high_fare_zones' in zone_results:
            report.append(f"  - ê³ ìš”ê¸ˆ ì§€ì—­: {zone_results['high_fare_zones'].index[0]}")

        # íŒ ë¶„ì„
        tip_results = self.results.get('tips', {})
        if 'tip_stats' in tip_results:
            report.append(f"\\nğŸ’° íŒ ë¶„ì„")
            report.append(f"  - í‰ê·  íŒ: \${tip_results['tip_stats']['mean']:.2f}")
            report.append(f"  - ì¤‘ì•™ê°’: \${tip_results['tip_stats']['median']:.2f}")
            report.append(f"  - í‰ê·  íŒ ë¹„ìœ¨: {tip_results['tip_pct_stats']['mean_pct']:.1f}%")

        # ì„±ëŠ¥ ë¹„êµ
        backend_results = self.results.get('backend', {})
        if 'speedup' in backend_results:
            report.append(f"\\nâš¡ ì„±ëŠ¥ ë¹„êµ (pandas vs Polars)")
            report.append(f"  - pandas: {backend_results['pandas_groupby']:.4f}ì´ˆ")
            report.append(f"  - Polars: {backend_results['polars_groupby']:.4f}ì´ˆ")
            report.append(f"  - ì†ë„ í–¥ìƒ: {backend_results['speedup']:.1f}ë°°")

        # ì²˜ë¦¬ ì‹œê°„
        report.append(f"\\nâ±ï¸ ì²˜ë¦¬ ì‹œê°„")
        for key, val in self.performance.items():
            if isinstance(val, dict):
                report.append(f"  - {key}: {val.get('time', 0):.2f}ì´ˆ")
            else:
                report.append(f"  - {key}: {val:.2f}ì´ˆ")

        report.append("\\n" + "=" * 60)
        return "\\n".join(report)

    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("=== NYC Taxi ì¢…í•© ë¶„ì„ ì‹œì‘ ===\\n")

        print("[1/5] ë°ì´í„° ë¡œë”©...")
        self.load_data()

        print("[2/5] ì‹œê°„ëŒ€ë³„ ë¶„ì„...")
        self.results['time'] = self.analyze_time_patterns()

        print("[3/5] ì§€ì—­ë³„ ë¶„ì„...")
        self.results['zones'] = self.analyze_zones()

        print("[4/5] íŒ ë¶„ì„...")
        self.results['tips'] = self.analyze_tips()

        print("[5/5] ë°±ì—”ë“œ ë¹„êµ...")
        self.results['backend'] = self.compare_backends()

        print("\\n" + self.generate_report())

        return self.results

if __name__ == "__main__":
    analyzer = NYCTaxiAnalyzer('yellow_tripdata_2022-01.parquet')
    results = analyzer.run_full_analysis()
`,
      instructions: 'ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ NYC Taxi ë°ì´í„° ì¢…í•© ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì™„ì„±í•©ë‹ˆë‹¤. ì‹œê°„ëŒ€ë³„, ì§€ì—­ë³„, íŒ ë¶„ì„ê³¼ pandas/Polars ì„±ëŠ¥ ë¹„êµë¥¼ í¬í•¨í•©ë‹ˆë‹¤.',
      keyPoints: [
        'í´ë˜ìŠ¤ ê¸°ë°˜ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì„¤ê³„',
        'ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ results ë”•ì…”ë„ˆë¦¬ì— ì €ì¥',
        'ì„±ëŠ¥ ì¸¡ì •ìœ¼ë¡œ ë³‘ëª© ì§€ì  íŒŒì•…',
        'ìµœì¢… ë¦¬í¬íŠ¸ë¡œ ì¸ì‚¬ì´íŠ¸ ì „ë‹¬'
      ]
    }
  },

  // Task 7: í”„ë¡œì íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸
  {
    id: 'w2d5-task7',
    title: 'í”„ë¡œì íŠ¸ ì œì¶œ ì²´í¬ë¦¬ìŠ¤íŠ¸',
    type: 'reading',
    duration: 5,
    content: {
      objectives: [
        'í”„ë¡œì íŠ¸ ì™„ë£Œ í™•ì¸',
        'ì œì¶œ ìš”êµ¬ì‚¬í•­ ì ê²€',
        'í‰ê°€ ê¸°ì¤€ ìµœì¢… í™•ì¸',
        'ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´'
      ],
      markdown: `
# í”„ë¡œì íŠ¸ ì œì¶œ ì²´í¬ë¦¬ìŠ¤íŠ¸

## í•„ìˆ˜ ê³¼ì œ (70ì )

### 1. ë°ì´í„° ë¡œë”© ìµœì í™” (20ì )
- [ ] í•„ìš” ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ë¡œë”©
- [ ] dtype ìµœì í™” ì ìš©
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 4GB ì´í•˜
- [ ] ë¡œë”© ì‹œê°„/ë©”ëª¨ë¦¬ ì¸¡ì • ê²°ê³¼ í¬í•¨

### 2. ì‹œê°„ëŒ€ë³„ ë¶„ì„ (25ì )
- [ ] ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ (hour, day_of_week, is_weekend)
- [ ] ì‹œê°„ëŒ€ë³„ ìš´í–‰ ê±´ìˆ˜/í‰ê·  ìš”ê¸ˆ
- [ ] ìš”ì¼ë³„ íŒ¨í„´ ë¶„ì„
- [ ] í”¼í¬ ì‹œê°„ ì‹ë³„

### 3. ì§€ì—­ë³„ ë¶„ì„ (25ì )
- [ ] Top 10 ìŠ¹ì°¨/í•˜ì°¨ ì§€ì—­
- [ ] ì§€ì—­ë³„ í‰ê·  ìš”ê¸ˆ ë¶„ì„
- [ ] OD ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
- [ ] ê³µí•­ ê´€ë ¨ ë¶„ì„ (ì„ íƒ)

## ì„ íƒ ê³¼ì œ (30ì )

### 4. íŒ ë¶„ì„ (15ì )
- [ ] íŒ ê¸ˆì•¡ ë¶„í¬ ë¶„ì„
- [ ] íŒ ë¹„ìœ¨ ê³„ì‚° ë° ë¶„ì„
- [ ] íŒì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì¸ ë¶„ì„
- [ ] ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

### 5. pandas vs Polars ë¹„êµ (15ì )
- [ ] ë™ì¼ ë¶„ì„ ë‘ ë°±ì—”ë“œë¡œ ìˆ˜í–‰
- [ ] ì‹¤í–‰ ì‹œê°„ ë¹„êµ
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
- [ ] ì½”ë“œ ë³µì¡ë„ ë¹„êµ

## ì œì¶œ í˜•ì‹

### íŒŒì¼ êµ¬ì¡°
\`\`\`
project/
â”œâ”€â”€ README.md           # í”„ë¡œì íŠ¸ ì„¤ëª…
â”œâ”€â”€ analysis.py         # ë©”ì¸ ë¶„ì„ ì½”ë“œ
â”œâ”€â”€ data_loader.py      # ë°ì´í„° ë¡œë”© ëª¨ë“ˆ
â”œâ”€â”€ visualizations.py   # ì‹œê°í™” ì½”ë“œ (ì„ íƒ)
â”œâ”€â”€ report.md           # ë¶„ì„ ë¦¬í¬íŠ¸
â””â”€â”€ requirements.txt    # ì˜ì¡´ì„±
\`\`\`

### README.md í•„ìˆ˜ í•­ëª©
- í”„ë¡œì íŠ¸ ê°œìš”
- ì‹¤í–‰ ë°©ë²•
- ì£¼ìš” ë°œê²¬ì  ìš”ì•½
- ì„±ëŠ¥ ìµœì í™” ê²°ê³¼

## í‰ê°€ ê¸°ì¤€ ìƒì„¸

| í•­ëª© | ë°°ì  | ìš°ìˆ˜ ê¸°ì¤€ |
|------|------|-----------|
| ì½”ë“œ í’ˆì§ˆ | 20% | í•¨ìˆ˜í™”, í´ë˜ìŠ¤í™”, ì£¼ì„ |
| ì„±ëŠ¥ ìµœì í™” | 30% | ë©”ëª¨ë¦¬ 50%+ ì ˆì•½, ë¹ ë¥¸ ì‹¤í–‰ |
| ë¶„ì„ ê¹Šì´ | 30% | ë‹¤ì–‘í•œ ì¸ì‚¬ì´íŠ¸, ë…¼ë¦¬ì  í•´ì„ |
| ë¬¸ì„œí™” | 20% | ëª…í™•í•œ ì„¤ëª…, ì¬í˜„ ê°€ëŠ¥ |

## ì¶”ê°€ ê°œì„  ì•„ì´ë””ì–´

ë„ì „í•´ë³¼ ë§Œí•œ í™•ì¥ ê³¼ì œ:

1. **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**
   - Streamlit/Dashë¡œ ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”

2. **ì˜ˆì¸¡ ëª¨ë¸**
   - ìš”ê¸ˆ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•
   - íŒ ì˜ˆì¸¡ ëª¨ë¸

3. **ì§€ë¦¬ ì‹œê°í™”**
   - foliumìœ¼ë¡œ ì§€ë„ ì‹œê°í™”
   - íˆíŠ¸ë§µ ìƒì„±

4. **ì‹œê³„ì—´ ë¶„ì„**
   - ì¼ë³„/ì£¼ë³„ íŠ¸ë Œë“œ ë¶„ì„
   - ê³„ì ˆì„± íŒ¨í„´ íƒì§€

5. **ëŒ€ìš©ëŸ‰ í™•ì¥**
   - 1ë…„ì¹˜ ë°ì´í„° ì²˜ë¦¬
   - Dask/Spark í™œìš©
      `,
      keyPoints: [
        'í•„ìˆ˜ ê³¼ì œ 70ì  + ì„ íƒ ê³¼ì œ 30ì ',
        'ë©”ëª¨ë¦¬ 4GB ì´í•˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ í•µì‹¬',
        'ì½”ë“œ í’ˆì§ˆê³¼ ë¬¸ì„œí™”ë„ í‰ê°€ ëŒ€ìƒ',
        'ì¶”ê°€ ê°œì„ ìœ¼ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ í–¥ìƒ'
      ]
    }
  }
]
