// Week 8 Day 2: ë°ì´í„° ìˆ˜ì§‘ ë° ì •ì œ

import type { Day } from './types'
import { createVideoTask, createCodeTask, createReadingTask } from './types'

const task1 = createVideoTask('w8d2-data-sources', 'ë„ë©”ì¸ë³„ ë°ì´í„° ì†ŒìŠ¤', 20, {
  introduction: `
## ë„ë©”ì¸ë³„ ë°ì´í„° ì†ŒìŠ¤

### ë‰´ìŠ¤/ë¯¸ë””ì–´ ë°ì´í„° ì†ŒìŠ¤

| ì†ŒìŠ¤ | ì ‘ê·¼ ë°©ì‹ | ì œí•œ ì‚¬í•­ |
|------|----------|----------|
| ë„¤ì´ë²„ ë‰´ìŠ¤ API | REST API | ì¼ 25,000ê±´ |
| RSS í”¼ë“œ | XML íŒŒì‹± | ìµœì‹  ê¸°ì‚¬ë§Œ |
| ì›¹ ìŠ¤í¬ë˜í•‘ | BeautifulSoup | robots.txt ì¤€ìˆ˜ |

### ê¸ˆìœµ ë°ì´í„° ì†ŒìŠ¤

| ì†ŒìŠ¤ | ì ‘ê·¼ ë°©ì‹ | ì œí•œ ì‚¬í•­ |
|------|----------|----------|
| DART ì „ìê³µì‹œ | REST API | ë¬´ë£Œ |
| Yahoo Finance | yfinance ë¼ì´ë¸ŒëŸ¬ë¦¬ | ë¹„ê³µì‹ |
| KRX í•œêµ­ê±°ë˜ì†Œ | ì›¹ ìŠ¤í¬ë˜í•‘ | ì œí•œì  |

### ê¸°ìˆ  ë°ì´í„° ì†ŒìŠ¤

| ì†ŒìŠ¤ | ì ‘ê·¼ ë°©ì‹ | ì œí•œ ì‚¬í•­ |
|------|----------|----------|
| GitHub API | REST/GraphQL | ì‹œê°„ë‹¹ 5,000ê±´ |
| StackOverflow | API | ì¼ 10,000ê±´ |
| npm Registry | REST API | ë¬´ì œí•œ |

### API ì¸ì¦ ë°©ì‹

1. **API Key**: ë„¤ì´ë²„, OpenAI
2. **OAuth**: GitHub
3. **í† í° ê¸°ë°˜**: DART
4. **ë¬´ì¸ì¦**: RSS, ê³µê°œ API
`,
  keyPoints: ['ë„ë©”ì¸ë³„ ì£¼ìš” ë°ì´í„° ì†ŒìŠ¤', 'API ì œí•œ ì‚¬í•­ ìˆ™ì§€', 'ì¸ì¦ ë°©ì‹ë³„ êµ¬í˜„'],
  practiceGoal: 'í”„ë¡œì íŠ¸ ë°ì´í„° ì†ŒìŠ¤ ê²°ì •',
})

const task2 = createCodeTask('w8d2-collector', 'ì‹¤ìŠµ: ë°ì´í„° ìˆ˜ì§‘ê¸° êµ¬í˜„', 60, {
  introduction: `
## ì™œ ë°°ìš°ëŠ”ê°€?

**ë¬¸ì œ**: ì—¬ëŸ¬ API(ë„¤ì´ë²„ ë‰´ìŠ¤, GitHub, RSS)ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ë•Œ ì¤‘ë³µ ì½”ë“œë¥¼ ì–´ë–»ê²Œ ì¤„ì¼ê¹Œ?

**ë¹„ìœ **: ë°ì´í„° ìˆ˜ì§‘ê¸° = ì „ê¸° í”ŒëŸ¬ê·¸ í‘œì¤€
- ëª¨ë“  ì „ìê¸°ê¸°ê°€ ê°™ì€ í”ŒëŸ¬ê·¸ë¥¼ ì”€ â†’ ë²½ ì½˜ì„¼íŠ¸ í•˜ë‚˜ë¡œ ì¶©ë¶„
- ëª¨ë“  ìˆ˜ì§‘ê¸°ê°€ ê°™ì€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì”€ â†’ ê³µí†µ ë¡œì§(rate limiting, retry) ì¬ì‚¬ìš©

---

## ë°ì´í„° ìˆ˜ì§‘ê¸° êµ¬í˜„

### ê¸°ë³¸ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤

\`\`\`python
# data/collectors/base.py

from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
from typing import List
import time

# ğŸ“Œ Step 1: ë°ì´í„° êµ¬ì¡° ì •ì˜
@dataclass
class CollectedItem:
    """ìˆ˜ì§‘ëœ ë°ì´í„° ì•„ì´í…œ"""
    id: str
    source: str
    title: str
    content: str
    url: str
    published_date: datetime

# ğŸ“Œ Step 2: ê¸°ë³¸ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤ (ì¶”ìƒ í´ë˜ìŠ¤)
class BaseCollector(ABC):
    """ë°ì´í„° ìˆ˜ì§‘ê¸° ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(self, name: str, rate_limit: float = 1.0):
        self.name = name
        self.rate_limit = rate_limit  # ì´ˆë‹¹ ìš”ì²­ ìˆ˜
        self.last_request_time = 0

    def _rate_limit(self):
        """Rate limiting ì ìš©"""
        elapsed = time.time() - self.last_request_time
        wait_time = (1 / self.rate_limit) - elapsed
        if wait_time > 0:
            time.sleep(wait_time)
        self.last_request_time = time.time()

    @abstractmethod
    def collect(self, query: str, limit: int = 100) -> List[CollectedItem]:
        """ë°ì´í„° ìˆ˜ì§‘ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        pass

    # ğŸ“Œ Step 3: ì¬ì‹œë„ ë¡œì§
    def collect_with_retry(self, query: str, limit: int = 100) -> List[CollectedItem]:
        """ì¬ì‹œë„ ë¡œì§ í¬í•¨ ìˆ˜ì§‘"""
        for attempt in range(3):
            try:
                self._rate_limit()
                return self.collect(query, limit)
            except Exception as e:
                if attempt < 2:
                    time.sleep(2 ** attempt)  # Exponential backoff
        return []

# ğŸ“Œ Step 4: ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° êµ¬í˜„
import requests

class NaverNewsCollector(BaseCollector):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    BASE_URL = "https://openapi.naver.com/v1/search/news.json"

    def __init__(self, client_id: str, client_secret: str):
        super().__init__("naver_news", rate_limit=10)
        self.headers = {
            "X-Naver-Client-Id": client_id,
            "X-Naver-Client-Secret": client_secret
        }

    def collect(self, query: str, limit: int = 100) -> List[CollectedItem]:
        items = []
        params = {"query": query, "display": min(limit, 100), "sort": "date"}

        response = requests.get(self.BASE_URL, headers=self.headers, params=params)
        response.raise_for_status()
        data = response.json()

        for item in data.get("items", []):
            items.append(CollectedItem(
                id=item.get("link"),
                source="naver_news",
                title=self._clean_html(item.get("title", "")),
                content=self._clean_html(item.get("description", "")),
                url=item.get("link", ""),
                published_date=datetime.now()
            ))

        return items[:limit]

    def _clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±°"""
        import re
        return re.sub(r'<[^>]+>', '', text)
\`\`\`
`,
  keyPoints: [
    'ğŸ—ï¸ ì¶”ìƒ í´ë˜ìŠ¤ë¡œ ê³µí†µ ë¡œì§(rate limit, retry) ë¶„ë¦¬',
    'â±ï¸ Rate limiting ìë™ ì ìš© - API ì œí•œ ì¤€ìˆ˜',
    'ğŸ”„ Exponential backoff ì¬ì‹œë„ - ì•ˆì •ì„± í–¥ìƒ',
    'ğŸ”Œ ì„œë¸Œí´ë˜ìŠ¤ëŠ” collect()ë§Œ êµ¬í˜„ - ì¬ì‚¬ìš©ì„±',
  ],
  practiceGoal: 'ë„ë©”ì¸ì— ë§ëŠ” ìˆ˜ì§‘ê¸° êµ¬í˜„',
  commonPitfalls: `
## ğŸ’¥ Common Pitfalls (ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜)

### 1. Rate Limiting ë¬´ì‹œ â†’ API ì°¨ë‹¨
**ì¦ìƒ**: "429 Too Many Requests" ë˜ëŠ” API í‚¤ ì •ì§€

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: Rate limiting ì—†ì´ ë¹ ë¥´ê²Œ í˜¸ì¶œ
for query in queries:
    items.extend(collector.collect(query))  # ì´ˆë‹¹ 100ê°œ ìš”ì²­!

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: Rate limiting ì ìš©
class BaseCollector:
    def __init__(self, rate_limit: float = 1.0):  # ì´ˆë‹¹ 1ìš”ì²­
        self.rate_limit = rate_limit
        self.last_request_time = 0

    def _rate_limit(self):
        elapsed = time.time() - self.last_request_time
        wait_time = (1 / self.rate_limit) - elapsed
        if wait_time > 0:
            time.sleep(wait_time)
        self.last_request_time = time.time()
\`\`\`

ğŸ’¡ **ê¸°ì–µí•  ì **: ëª¨ë“  API í˜¸ì¶œ ì „ _rate_limit() í˜¸ì¶œ í•„ìˆ˜

### 2. API ì¸ì¦ ì •ë³´ í•˜ë“œì½”ë”©
**ì¦ìƒ**: GitHubì— í‘¸ì‹œ í›„ API í‚¤ ë…¸ì¶œ

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: ì½”ë“œì— ì§ì ‘ ì…ë ¥
collector = NaverNewsCollector(
    client_id="ABC123",  # ì½”ë“œì— ë…¸ì¶œ!
    client_secret="XYZ789"
)

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©
import os
from dotenv import load_dotenv
load_dotenv()

collector = NaverNewsCollector(
    client_id=os.getenv("NAVER_CLIENT_ID"),
    client_secret=os.getenv("NAVER_CLIENT_SECRET")
)
\`\`\`

ğŸ’¡ **ê¸°ì–µí•  ì **: .env íŒŒì¼ ì‚¬ìš©, .gitignoreì— .env ì¶”ê°€

### 3. ì˜ˆì™¸ ì²˜ë¦¬ ëˆ„ë½ìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨
**ì¦ìƒ**: í•˜ë‚˜ì˜ API ì˜¤ë¥˜ë¡œ ì „ì²´ ìˆ˜ì§‘ ì‹¤íŒ¨

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: ì˜ˆì™¸ ì²˜ë¦¬ ì—†ìŒ
def collect_all(queries):
    results = []
    for query in queries:
        items = collector.collect(query)  # í•˜ë‚˜ ì‹¤íŒ¨í•˜ë©´ ì „ì²´ ì¤‘ë‹¨
        results.extend(items)
    return results

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ê°œë³„ ì˜ˆì™¸ ì²˜ë¦¬ + ë¡œê¹…
def collect_all(queries):
    results = []
    failed = []
    for query in queries:
        try:
            items = collector.collect_with_retry(query)
            results.extend(items)
        except Exception as e:
            failed.append({"query": query, "error": str(e)})
            continue  # ë‹¤ìŒ ì¿¼ë¦¬ ê³„ì†
    print(f"ì„±ê³µ: {len(results)}, ì‹¤íŒ¨: {len(failed)}")
    return results, failed
\`\`\`

ğŸ’¡ **ê¸°ì–µí•  ì **: ê°œë³„ í•­ëª© ì‹¤íŒ¨ëŠ” ë¡œê¹…í•˜ê³  ê³„ì† ì§„í–‰, ì „ì²´ ì‹¤íŒ¨ë§Œ ì¤‘ë‹¨
`,
  codeExample: `# ğŸ“Œ Step 5: ìˆ˜ì§‘ê¸° ì‚¬ìš©
collector = NaverNewsCollector(
    client_id=os.getenv("NAVER_CLIENT_ID"),
    client_secret=os.getenv("NAVER_CLIENT_SECRET")
)

items = collector.collect_with_retry("ì‚¼ì„±ì „ì", limit=50)
print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(items)}ê°œ")
print(f"ì²« ë²ˆì§¸ ë‰´ìŠ¤: {items[0].title}")`,
})

const task3 = createCodeTask('w8d2-cleaner', 'ì‹¤ìŠµ: ë°ì´í„° ì •ì œê¸° êµ¬í˜„', 50, {
  introduction: `
## ë°ì´í„° ì •ì œê¸° êµ¬í˜„

### í…ìŠ¤íŠ¸ ì •ì œ

\`\`\`python
# data/processors/cleaner.py

import re
from typing import List
from dataclasses import dataclass

@dataclass
class CleaningResult:
    """ì •ì œ ê²°ê³¼"""
    original: str
    cleaned: str
    changes: List[str]

class TextCleaner:
    """í…ìŠ¤íŠ¸ ì •ì œê¸°"""

    def __init__(self):
        self.patterns = [
            (r'<[^>]+>', '', 'HTML íƒœê·¸ ì œê±°'),
            (r'&[a-zA-Z]+;', ' ', 'HTML ì—”í‹°í‹° ì œê±°'),
            (r'http[s]?://\\S+', '', 'URL ì œê±°'),
            (r'\\s+', ' ', 'ë‹¤ì¤‘ ê³µë°± ì •ë¦¬'),
            (r'^\\s+|\\s+$', '', 'ì•ë’¤ ê³µë°± ì œê±°'),
        ]

    def clean(self, text: str) -> CleaningResult:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        changes = []
        cleaned = text

        for pattern, replacement, description in self.patterns:
            if re.search(pattern, cleaned):
                cleaned = re.sub(pattern, replacement, cleaned)
                changes.append(description)

        return CleaningResult(
            original=text,
            cleaned=cleaned,
            changes=changes
        )

    def clean_batch(self, texts: List[str]) -> List[CleaningResult]:
        """ë°°ì¹˜ ì •ì œ"""
        return [self.clean(text) for text in texts]
\`\`\`

### ì¤‘ë³µ ì œê±°

\`\`\`python
# data/processors/deduplicator.py

from typing import List, Tuple
import hashlib
from difflib import SequenceMatcher

class Deduplicator:
    """ì¤‘ë³µ ë°ì´í„° ì œê±°ê¸°"""

    def __init__(self, similarity_threshold: float = 0.9):
        self.threshold = similarity_threshold
        self.seen_hashes = set()

    def _get_hash(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„±"""
        normalized = text.lower().strip()
        return hashlib.md5(normalized.encode()).hexdigest()

    def _similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

    def is_duplicate(self, text: str) -> bool:
        """ì •í™•íˆ ê°™ì€ í…ìŠ¤íŠ¸ ì¤‘ë³µ ê²€ì‚¬"""
        hash_val = self._get_hash(text)
        if hash_val in self.seen_hashes:
            return True
        self.seen_hashes.add(hash_val)
        return False

    def deduplicate(
        self,
        items: List[dict],
        text_field: str = "content"
    ) -> Tuple[List[dict], List[dict]]:
        """ì¤‘ë³µ ì œê±° (ìœ ì‚¬ë„ ê¸°ë°˜)"""
        unique = []
        duplicates = []

        for item in items:
            text = item.get(text_field, "")

            # ì •í™•í•œ ì¤‘ë³µ
            if self.is_duplicate(text):
                duplicates.append(item)
                continue

            # ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ
            is_similar = False
            for unique_item in unique:
                if self._similarity(text, unique_item.get(text_field, "")) > self.threshold:
                    is_similar = True
                    duplicates.append(item)
                    break

            if not is_similar:
                unique.append(item)

        return unique, duplicates
\`\`\`

### ë°ì´í„° ê²€ì¦

\`\`\`python
# data/processors/validator.py

from typing import List, Tuple
from dataclasses import dataclass, field

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    is_valid: bool
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)

class DataValidator:
    """ë°ì´í„° ê²€ì¦ê¸°"""

    def __init__(self, min_content_length: int = 50):
        self.min_content_length = min_content_length

    def validate_item(self, item: dict) -> ValidationResult:
        """ë‹¨ì¼ ì•„ì´í…œ ê²€ì¦"""
        errors = []
        warnings = []

        # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
        required_fields = ["id", "title", "content"]
        for field in required_fields:
            if not item.get(field):
                errors.append(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")

        # ì½˜í…ì¸  ê¸¸ì´ ê²€ì‚¬
        content = item.get("content", "")
        if len(content) < self.min_content_length:
            warnings.append(f"ì½˜í…ì¸  ê¸¸ì´ ë¶€ì¡±: {len(content)}ì")

        # URL í˜•ì‹ ê²€ì‚¬
        url = item.get("url", "")
        if url and not url.startswith(("http://", "https://")):
            errors.append(f"ì˜ëª»ëœ URL í˜•ì‹: {url}")

        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings
        )

    def validate_batch(
        self,
        items: List[dict]
    ) -> Tuple[List[dict], List[dict], List[ValidationResult]]:
        """ë°°ì¹˜ ê²€ì¦"""
        valid_items = []
        invalid_items = []
        results = []

        for item in items:
            result = self.validate_item(item)
            results.append(result)

            if result.is_valid:
                valid_items.append(item)
            else:
                invalid_items.append(item)

        return valid_items, invalid_items, results
\`\`\`
`,
  keyPoints: ['ì •ê·œì‹ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì •ì œ', 'í•´ì‹œ + ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°', 'í•„ìˆ˜ í•„ë“œ ê²€ì¦'],
  practiceGoal: 'ë°ì´í„° ì •ì œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•',
  codeExample: `# ì •ì œ íŒŒì´í”„ë¼ì¸
cleaner = TextCleaner()
deduplicator = Deduplicator(similarity_threshold=0.85)
validator = DataValidator(min_content_length=100)

# 1. ì •ì œ
cleaned_items = [cleaner.clean(item["content"]).cleaned for item in raw_items]

# 2. ì¤‘ë³µ ì œê±°
unique, duplicates = deduplicator.deduplicate(items)
print(f"ì¤‘ë³µ ì œê±°: {len(duplicates)}ê°œ")

# 3. ê²€ì¦
valid, invalid, _ = validator.validate_batch(unique)
print(f"ìœ íš¨ ë°ì´í„°: {len(valid)}ê°œ")`,
})

const task4 = createCodeTask('w8d2-entity-extraction', 'ì‹¤ìŠµ: ì—”í‹°í‹° ì¶”ì¶œ', 50, {
  introduction: `
## ì™œ ë°°ìš°ëŠ”ê°€?

**ë¬¸ì œ**: ë¹„êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°(ì¸ë¬¼, ê¸°ê´€, ì¥ì†Œ)ì™€ ê´€ê³„ë¥¼ ì–´ë–»ê²Œ ìë™ ì¶”ì¶œí• ê¹Œ?

**ë¹„ìœ **: ì—”í‹°í‹° ì¶”ì¶œ = í˜•ê´‘íœìœ¼ë¡œ í•µì‹¬ ë‹¨ì–´ í‘œì‹œ
- ì‚¬ëŒì´ ê¸€ì„ ì½ìœ¼ë©° í˜•ê´‘íœìœ¼ë¡œ ì¤‘ìš” ë‹¨ì–´ í‘œì‹œ
- LLMì´ í…ìŠ¤íŠ¸ë¥¼ ì½ìœ¼ë©° ì—”í‹°í‹°/ê´€ê³„ íƒœê¹…

---

## ì—”í‹°í‹° ì¶”ì¶œ

### LLM ê¸°ë°˜ NER

\`\`\`python
# data/processors/extractor.py

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from typing import List, Dict
import json

class EntityExtractor:
    """LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œê¸°"""

    def __init__(self, openai_key: str):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            api_key=openai_key
        )

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‰´ìŠ¤ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì¶”ì¶œí•  ì—”í‹°í‹° ìœ í˜•:
- Person: ì¸ë¬¼ (ì´ë¦„, ì§ì±… í¬í•¨)
- Organization: ê¸°ì—…, ê¸°ê´€, ë‹¨ì²´
- Location: ì¥ì†Œ, ì§€ì—­
- Event: ì´ë²¤íŠ¸, ì‚¬ê±´

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "entities": [
        {{"type": "Person", "name": "í™ê¸¸ë™", "role": "CEO"}},
        {{"type": "Organization", "name": "ì‚¼ì„±ì „ì"}},
        ...
    ]
}}"""),
            ("human", "í…ìŠ¤íŠ¸: {text}")
        ])

        self.chain = self.prompt | self.llm

    def extract(self, text: str) -> List[Dict]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ"""
        try:
            result = self.chain.invoke({"text": text[:2000]})  # í† í° ì œí•œ
            data = json.loads(result.content)
            return data.get("entities", [])
        except Exception as e:
            print(f"ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []

    def extract_batch(
        self,
        texts: List[str],
        batch_size: int = 5
    ) -> List[List[Dict]]:
        """ë°°ì¹˜ ì—”í‹°í‹° ì¶”ì¶œ"""
        all_entities = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]

            for text in batch:
                entities = self.extract(text)
                all_entities.append(entities)

        return all_entities
\`\`\`

### ê´€ê³„ ì¶”ì¶œ

\`\`\`python
# data/processors/relation_extractor.py

class RelationExtractor:
    """LLM ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œê¸°"""

    def __init__(self, openai_key: str, ontology_relations: List[str]):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            api_key=openai_key
        )
        self.valid_relations = ontology_relations

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """í…ìŠ¤íŠ¸ì™€ ì—”í‹°í‹° ëª©ë¡ì—ì„œ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

í—ˆìš©ëœ ê´€ê³„ ìœ í˜•:
{relations}

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
    "relations": [
        {{"subject": "ì‚¼ì„±ì „ì", "predicate": "affiliated_with", "object": "ì´ì¬ìš©"}},
        ...
    ]
}}

ê·œì¹™:
- í—ˆìš©ëœ ê´€ê³„ ìœ í˜•ë§Œ ì‚¬ìš©
- í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê´€ê³„ë§Œ ì¶”ì¶œ
- subjectì™€ objectëŠ” ì—”í‹°í‹° ëª©ë¡ì— ìˆì–´ì•¼ í•¨"""),
            ("human", "í…ìŠ¤íŠ¸: {text}\\n\\nì—”í‹°í‹°: {entities}")
        ])

        self.chain = self.prompt | self.llm

    def extract(
        self,
        text: str,
        entities: List[Dict]
    ) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê´€ê³„ ì¶”ì¶œ"""
        try:
            entity_names = [e.get("name") for e in entities]
            result = self.chain.invoke({
                "text": text[:2000],
                "entities": json.dumps(entity_names, ensure_ascii=False),
                "relations": ", ".join(self.valid_relations)
            })
            data = json.loads(result.content)
            return data.get("relations", [])
        except Exception as e:
            print(f"ê´€ê³„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
\`\`\`

### Triple ìƒì„±

\`\`\`python
# data/processors/triple_builder.py

from dataclasses import dataclass
from typing import List
import hashlib

@dataclass
class Triple:
    """RDF Triple"""
    subject: str      # URI
    predicate: str    # URI
    object: str       # URI
    confidence: float
    source_url: str

class TripleBuilder:
    """Triple ìƒì„±ê¸°"""

    def __init__(self, namespace: str = "mykg"):
        self.namespace = namespace

    def _to_uri(self, entity_type: str, name: str) -> str:
        """ì—”í‹°í‹°ë¥¼ URIë¡œ ë³€í™˜"""
        safe_name = name.replace(" ", "_").replace("/", "_")
        return f"{self.namespace}:{entity_type}_{safe_name}"

    def build_triples(
        self,
        entities: List[Dict],
        relations: List[Dict],
        source_url: str,
        base_confidence: float = 0.8
    ) -> List[Triple]:
        """ì—”í‹°í‹°ì™€ ê´€ê³„ì—ì„œ Triple ìƒì„±"""
        triples = []

        # ì—”í‹°í‹° â†’ URI ë§¤í•‘
        entity_map = {}
        for entity in entities:
            uri = self._to_uri(entity["type"], entity["name"])
            entity_map[entity["name"]] = uri

        # ê´€ê³„ â†’ Triple
        for rel in relations:
            subject = entity_map.get(rel["subject"])
            obj = entity_map.get(rel["object"])

            if subject and obj:
                triples.append(Triple(
                    subject=subject,
                    predicate=f"{self.namespace}:{rel['predicate']}",
                    object=obj,
                    confidence=base_confidence,
                    source_url=source_url
                ))

        return triples
\`\`\`
`,
  keyPoints: ['GPT-4o-minië¡œ NER ìˆ˜í–‰', 'ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ê´€ê³„ ì¶”ì¶œ', 'URI í˜•ì‹ Triple ìƒì„±'],
  practiceGoal: 'ì—”í‹°í‹° ë° ê´€ê³„ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•',
  codeExample: `# ì „ì²´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
extractor = EntityExtractor(openai_key)
rel_extractor = RelationExtractor(openai_key, ["mentions", "affiliated_with"])
triple_builder = TripleBuilder("news")

# ë‰´ìŠ¤ì—ì„œ Triple ìƒì„±
entities = extractor.extract(news_content)
relations = rel_extractor.extract(news_content, entities)
triples = triple_builder.build_triples(entities, relations, news_url)

print(f"ì¶”ì¶œëœ Triple: {len(triples)}ê°œ")`,
})

const task5 = createReadingTask('w8d2-pipeline', 'ë°ì´í„° íŒŒì´í”„ë¼ì¸ í†µí•©', 30, {
  introduction: `
## ë°ì´í„° íŒŒì´í”„ë¼ì¸ í†µí•©

### ì „ì²´ íŒŒì´í”„ë¼ì¸

\`\`\`python
# scripts/run_pipeline.py

from data.collectors.news import NaverNewsCollector
from data.processors.cleaner import TextCleaner
from data.processors.deduplicator import Deduplicator
from data.processors.validator import DataValidator
from data.processors.extractor import EntityExtractor
from data.processors.relation_extractor import RelationExtractor
from data.processors.triple_builder import TripleBuilder
import os
from dotenv import load_dotenv

load_dotenv()

class DataPipeline:
    """í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸"""

    def __init__(self):
        # ìˆ˜ì§‘ê¸°
        self.collector = NaverNewsCollector(
            os.getenv("NAVER_CLIENT_ID"),
            os.getenv("NAVER_CLIENT_SECRET")
        )

        # ì •ì œê¸°
        self.cleaner = TextCleaner()
        self.deduplicator = Deduplicator()
        self.validator = DataValidator()

        # ì¶”ì¶œê¸°
        openai_key = os.getenv("OPENAI_API_KEY")
        self.entity_extractor = EntityExtractor(openai_key)
        self.relation_extractor = RelationExtractor(
            openai_key,
            ["mentions", "affiliated_with", "competes_with"]
        )
        self.triple_builder = TripleBuilder("news")

    def run(self, query: str, limit: int = 50) -> list:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print(f"[1/5] ë°ì´í„° ìˆ˜ì§‘: {query}")
        items = self.collector.collect_with_retry(query, limit)
        print(f"  â†’ {len(items)}ê°œ ìˆ˜ì§‘")

        print("[2/5] í…ìŠ¤íŠ¸ ì •ì œ")
        for item in items:
            result = self.cleaner.clean(item.content)
            item.content = result.cleaned
        print(f"  â†’ ì •ì œ ì™„ë£Œ")

        print("[3/5] ì¤‘ë³µ ì œê±°")
        items_dict = [{"id": i.id, "content": i.content} for i in items]
        unique, duplicates = self.deduplicator.deduplicate(items_dict)
        print(f"  â†’ {len(unique)}ê°œ ìœ ë‹ˆí¬ ({len(duplicates)}ê°œ ì¤‘ë³µ)")

        print("[4/5] ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ")
        all_triples = []
        for item in items[:10]:  # ë¹„ìš© ì œí•œ
            entities = self.entity_extractor.extract(item.content)
            relations = self.relation_extractor.extract(item.content, entities)
            triples = self.triple_builder.build_triples(
                entities, relations, item.url
            )
            all_triples.extend(triples)
        print(f"  â†’ {len(all_triples)}ê°œ Triple ìƒì„±")

        print("[5/5] ê²€ì¦")
        # Triple ê²€ì¦ ë¡œì§...

        return all_triples

if __name__ == "__main__":
    pipeline = DataPipeline()
    triples = pipeline.run("ì‚¼ì„±ì „ì", limit=20)

    print("\\n=== ìƒì„±ëœ Triple ===")
    for t in triples[:10]:
        print(f"  {t.subject} â†’ {t.predicate} â†’ {t.object}")
\`\`\`

### ë°°ì¹˜ ì²˜ë¦¬

\`\`\`python
# ì¼ì¼ ë°°ì¹˜ ì²˜ë¦¬
QUERIES = ["ì‚¼ì„±ì „ì", "SKí•˜ì´ë‹‰ìŠ¤", "í˜„ëŒ€ìë™ì°¨", "LGí™”í•™"]

for query in QUERIES:
    triples = pipeline.run(query, limit=20)
    # DB ì €ì¥
    save_triples_to_neo4j(triples)
\`\`\`

### ëª¨ë‹ˆí„°ë§

| ì§€í‘œ | ëª©í‘œ |
|------|------|
| ìˆ˜ì§‘ ì„±ê³µë¥  | > 95% |
| ì •ì œ í›„ ìœ íš¨ìœ¨ | > 80% |
| ì—”í‹°í‹° ì¶”ì¶œìœ¨ | > 70% |
| Triple ìƒì„±ìœ¨ | > 50% |
`,
  keyPoints: ['5ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°', 'ë‹¨ê³„ë³„ ë¡œê¹…ê³¼ í†µê³„', 'ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›'],
  practiceGoal: 'í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•',
})

export const day2DataCollection: Day = {
  slug: 'data-collection',
  title: 'ë°ì´í„° ìˆ˜ì§‘ ë° ì •ì œ',
  totalDuration: 210,
  tasks: [task1, task2, task3, task4, task5],
}
