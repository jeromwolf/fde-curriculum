// Week 8 Day 4: GraphRAG í†µí•©

import type { Day } from './types'
import { createVideoTask, createCodeTask, createReadingTask } from './types'

const task1 = createVideoTask('w8d4-integration-arch', 'GraphRAG í†µí•© ì•„í‚¤í…ì²˜', 25, {
  introduction: `
## GraphRAG í†µí•© ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì„±

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ì‚¬ìš©ì ì§ˆë¬¸                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Query Router                            â”‚
â”‚    (ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜: êµ¬ì¡°ì  vs ì˜ë¯¸ì  vs í•˜ì´ë¸Œë¦¬ë“œ)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text2Cypher   â”‚  â”‚ Vector Search  â”‚  â”‚ Hybrid Search  â”‚
â”‚  (êµ¬ì¡°ì  ì§ˆë¬¸)  â”‚  â”‚  (ì˜ë¯¸ì  ì§ˆë¬¸) â”‚  â”‚   (ë³µí•© ì§ˆë¬¸)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Context Aggregator                     â”‚
â”‚           (ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° ì •ì œ)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Response Generator                      â”‚
â”‚            (LLM ê¸°ë°˜ ì‘ë‹µ ìƒì„±)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜

| ìœ í˜• | ì˜ˆì‹œ | ì²˜ë¦¬ ë°©ì‹ |
|------|------|----------|
| êµ¬ì¡°ì  | "ì‚¼ì„±ì „ìì˜ ê²½ìŸì‚¬ëŠ”?" | Text2Cypher |
| ì˜ë¯¸ì  | "ë°˜ë„ì²´ ì‹œì¥ ì „ë§ì€?" | Vector Search |
| í•˜ì´ë¸Œë¦¬ë“œ | "ì‚¼ì„±ì „ì ê´€ë ¨ ìµœì‹  ë™í–¥ì€?" | ë‘˜ ë‹¤ |

### ì»´í¬ë„ŒíŠ¸ ì±…ì„

1. **Query Router**: ì§ˆë¬¸ ë¶„ë¥˜ ë° ë¼ìš°íŒ…
2. **Text2Cypher**: êµ¬ì¡°ì  ì¿¼ë¦¬ ìƒì„±/ì‹¤í–‰
3. **Vector Search**: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰
4. **Context Aggregator**: ê²°ê³¼ ë³‘í•©/ìˆœìœ„í™”
5. **Response Generator**: ìì—°ì–´ ì‘ë‹µ ìƒì„±
`,
  keyPoints: ['Query Routerë¡œ ì§ˆë¬¸ ë¶„ë¥˜', 'êµ¬ì¡°ì /ì˜ë¯¸ì  ê²€ìƒ‰ ë³‘í–‰', 'ê²°ê³¼ í†µí•© í›„ ì‘ë‹µ ìƒì„±'],
  practiceGoal: 'GraphRAG í†µí•© ì•„í‚¤í…ì²˜ ì´í•´',
})

const task2 = createCodeTask('w8d4-query-router', 'ì‹¤ìŠµ: Query Router êµ¬í˜„', 50, {
  introduction: `
## Query Router êµ¬í˜„

### LLM ê¸°ë°˜ ë¼ìš°í„°

\`\`\`python
# rag/router.py

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from enum import Enum
from typing import Dict

class QueryType(Enum):
    STRUCTURAL = "structural"    # êµ¬ì¡°ì  ì§ˆë¬¸ â†’ Text2Cypher
    SEMANTIC = "semantic"        # ì˜ë¯¸ì  ì§ˆë¬¸ â†’ Vector Search
    HYBRID = "hybrid"            # ë³µí•© ì§ˆë¬¸ â†’ ë‘˜ ë‹¤

class QueryRouter:
    """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ë° ë¼ìš°íŒ…"""

    def __init__(self, openai_key: str):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            api_key=openai_key
        )

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìœ í˜•ì„ ë¶„ë¥˜í•˜ì„¸ìš”.

ë¶„ë¥˜ ê¸°ì¤€:
- structural: íŠ¹ì • ì—”í‹°í‹° ê°„ ê´€ê³„, ì†ì„± ì¡°íšŒ (ì˜ˆ: "ì‚¼ì„± ê²½ìŸì‚¬", "Aì™€ Bì˜ ê´€ê³„")
- semantic: ê°œë…, ì„¤ëª…, ìš”ì•½ ìš”ì²­ (ì˜ˆ: "ë°˜ë„ì²´ë€?", "ì‹œì¥ ì „ë§")
- hybrid: íŠ¹ì • ì—”í‹°í‹° + ì„¤ëª…/ë¶„ì„ (ì˜ˆ: "ì‚¼ì„±ì „ì ìµœì‹  ë™í–¥")

JSONìœ¼ë¡œ ì‘ë‹µ:
{{"type": "structural" | "semantic" | "hybrid", "reason": "ë¶„ë¥˜ ì´ìœ "}}
"""),
            ("human", "ì§ˆë¬¸: {question}")
        ])

        self.chain = self.prompt | self.llm

    def route(self, question: str) -> Dict:
        """ì§ˆë¬¸ ë¶„ë¥˜"""
        try:
            result = self.chain.invoke({"question": question})
            import json
            data = json.loads(result.content)
            return {
                "type": QueryType(data["type"]),
                "reason": data.get("reason", "")
            }
        except Exception as e:
            # ê¸°ë³¸ê°’: í•˜ì´ë¸Œë¦¬ë“œ
            return {"type": QueryType.HYBRID, "reason": f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}"}

    def route_with_keywords(self, question: str) -> Dict:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ë¼ìš°íŒ… (LLM í˜¸ì¶œ ì—†ìŒ)"""
        structural_keywords = ["ê²½ìŸì‚¬", "ê´€ê³„", "ì—°ê²°", "ì†Œì†", "ëˆ„ê°€", "ì–´ë””"]
        semantic_keywords = ["ì „ë§", "ë¶„ì„", "ì„¤ëª…", "ì˜ë¯¸", "ë¬´ì—‡", "ì™œ"]

        has_structural = any(kw in question for kw in structural_keywords)
        has_semantic = any(kw in question for kw in semantic_keywords)

        if has_structural and has_semantic:
            return {"type": QueryType.HYBRID, "reason": "ë³µí•© í‚¤ì›Œë“œ"}
        elif has_structural:
            return {"type": QueryType.STRUCTURAL, "reason": "êµ¬ì¡°ì  í‚¤ì›Œë“œ"}
        elif has_semantic:
            return {"type": QueryType.SEMANTIC, "reason": "ì˜ë¯¸ì  í‚¤ì›Œë“œ"}
        else:
            return {"type": QueryType.HYBRID, "reason": "ê¸°ë³¸ê°’"}
\`\`\`

### ë¼ìš°íŒ… ê²°ê³¼ì— ë”°ë¥¸ ì²˜ë¦¬

\`\`\`python
# rag/graphrag.py

class GraphRAGEngine:
    def __init__(self, router, text2cypher, vector_search, llm):
        self.router = router
        self.text2cypher = text2cypher
        self.vector_search = vector_search
        self.llm = llm

    def query(self, question: str) -> Dict:
        # 1. ë¼ìš°íŒ…
        route_result = self.router.route(question)
        query_type = route_result["type"]

        contexts = []

        # 2. ìœ í˜•ë³„ ê²€ìƒ‰
        if query_type in [QueryType.STRUCTURAL, QueryType.HYBRID]:
            cypher_result = self.text2cypher.query(question)
            if cypher_result.get("success"):
                contexts.append({
                    "source": "text2cypher",
                    "data": cypher_result["results"],
                    "cypher": cypher_result["cypher"]
                })

        if query_type in [QueryType.SEMANTIC, QueryType.HYBRID]:
            vector_result = self.vector_search.search(question)
            contexts.append({
                "source": "vector_search",
                "data": vector_result
            })

        # 3. ì‘ë‹µ ìƒì„±
        response = self._generate_response(question, contexts)

        return {
            "question": question,
            "query_type": query_type.value,
            "contexts": contexts,
            "response": response
        }
\`\`\`
`,
  keyPoints: ['LLM ê¸°ë°˜ ì§ˆë¬¸ ë¶„ë¥˜', 'í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ë¼ìš°íŒ…', 'ìœ í˜•ë³„ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸'],
  practiceGoal: 'Query Router êµ¬í˜„',
  commonPitfalls: `
## ğŸ’¥ Common Pitfalls (ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜)

### 1. ë¼ìš°íŒ… ì‹¤íŒ¨ ì‹œ Fallback ì—†ìŒ
**ì¦ìƒ**: JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ì „ì²´ ì¿¼ë¦¬ ì‹¤íŒ¨

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: ì˜ˆì™¸ ì²˜ë¦¬ ì—†ìŒ
def route(self, question: str) -> Dict:
    result = self.chain.invoke({"question": question})
    data = json.loads(result.content)  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ crash!
    return {"type": QueryType(data["type"])}

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: Fallback ì²˜ë¦¬
def route(self, question: str) -> Dict:
    try:
        result = self.chain.invoke({"question": question})
        data = json.loads(result.content)
        return {"type": QueryType(data["type"]), "reason": data.get("reason")}
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        # ê¸°ë³¸ê°’: í•˜ì´ë¸Œë¦¬ë“œ (ë‘˜ ë‹¤ ê²€ìƒ‰)
        return {"type": QueryType.HYBRID, "reason": f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}"}
\`\`\`

ğŸ’¡ **ê¸°ì–µí•  ì **: LLM ì‘ë‹µì€ í•­ìƒ íŒŒì‹± ì‹¤íŒ¨ ê°€ëŠ¥ì„± ê³ ë ¤, Fallback í•„ìˆ˜

### 2. ë¼ìš°í„° + ê²€ìƒ‰ ì´ì¤‘ LLM í˜¸ì¶œ â†’ ì§€ì—°
**ì¦ìƒ**: ì‘ë‹µ ì‹œê°„ 3ì´ˆ ì´ìƒ

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: í•­ìƒ LLM ë¼ìš°í„° ì‚¬ìš©
def query(self, question: str):
    route = self.router.route(question)  # LLM í˜¸ì¶œ 1
    cypher = self.text2cypher.generate(question)  # LLM í˜¸ì¶œ 2
    # ì´ 2íšŒ LLM í˜¸ì¶œ = 2ì´ˆ+

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ë¼ìš°íŒ… ìš°ì„ 
def query(self, question: str):
    # 1. ë¹ ë¥¸ í‚¤ì›Œë“œ ë¼ìš°íŒ… ì‹œë„ (LLM í˜¸ì¶œ ì—†ìŒ)
    route = self.router.route_with_keywords(question)

    # 2. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ LLM ë¼ìš°íŒ…
    if route["reason"] == "ê¸°ë³¸ê°’":
        route = self.router.route(question)  # LLM í˜¸ì¶œ

    # 3. ê²€ìƒ‰ ìˆ˜í–‰
    ...
\`\`\`

ğŸ’¡ **ê¸°ì–µí•  ì **: ëª…í™•í•œ íŒ¨í„´ì€ í‚¤ì›Œë“œ ë¼ìš°íŒ…, ëª¨í˜¸í•  ë•Œë§Œ LLM ì‚¬ìš©

### 3. Hybrid ê²€ìƒ‰ ì‹œ ê²°ê³¼ ì¤‘ë³µ
**ì¦ìƒ**: ê°™ì€ ì—”í‹°í‹°ê°€ Text2Cypherì™€ Vector ê²°ê³¼ì— ëª¨ë‘ í¬í•¨

\`\`\`python
# âŒ ì˜ëª»ëœ ì˜ˆì‹œ: ë‹¨ìˆœ ë³‘í•©
contexts = cypher_results + vector_results  # ì¤‘ë³µ í¬í•¨!

# âœ… ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: í†µí•© ì‹œ ì¤‘ë³µ ì œê±°
def aggregate(self, contexts: List[Dict]) -> List[ContextItem]:
    items = []
    for ctx in contexts:
        items.extend(self._process_results(ctx))

    # ì¤‘ë³µ ì œê±° (ì½˜í…ì¸  í•´ì‹œ ê¸°ë°˜)
    items = self._deduplicate(items)

    # ì ìˆ˜ìˆœ ì •ë ¬
    items.sort(key=lambda x: x.score, reverse=True)
    return items
\`\`\`

ğŸ’¡ **ê¸°ì–µí•  ì **: Context Aggregatorì—ì„œ ë°˜ë“œì‹œ ì¤‘ë³µ ì œê±° ìˆ˜í–‰
`,
  codeExample: `# ë¼ìš°í„° í…ŒìŠ¤íŠ¸
router = QueryRouter(openai_key)

# êµ¬ì¡°ì  ì§ˆë¬¸
result = router.route("ì‚¼ì„±ì „ìì˜ ê²½ìŸì‚¬ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?")
# {"type": QueryType.STRUCTURAL, "reason": "íŠ¹ì • ì—”í‹°í‹° ê´€ê³„ ì¡°íšŒ"}

# ì˜ë¯¸ì  ì§ˆë¬¸
result = router.route("ë°˜ë„ì²´ ì‚°ì—…ì˜ ë¯¸ë˜ ì „ë§ì€?")
# {"type": QueryType.SEMANTIC, "reason": "ê°œë…/ì „ë§ ìš”ì²­"}`,
})

const task3 = createCodeTask('w8d4-context-aggregator', 'ì‹¤ìŠµ: Context Aggregator êµ¬í˜„', 45, {
  introduction: `
## Context Aggregator êµ¬í˜„

### ê²°ê³¼ í†µí•©

\`\`\`python
# rag/aggregator.py

from typing import List, Dict
from dataclasses import dataclass

@dataclass
class ContextItem:
    content: str
    source: str
    score: float
    metadata: Dict = None

class ContextAggregator:
    """ê²€ìƒ‰ ê²°ê³¼ í†µí•©ê¸°"""

    def __init__(self, max_tokens: int = 3000):
        self.max_tokens = max_tokens

    def aggregate(self, contexts: List[Dict]) -> List[ContextItem]:
        """ì—¬ëŸ¬ ì†ŒìŠ¤ì˜ ê²°ê³¼ í†µí•©"""
        items = []

        for ctx in contexts:
            source = ctx.get("source", "unknown")
            data = ctx.get("data", [])

            if source == "text2cypher":
                items.extend(self._process_cypher_results(data))
            elif source == "vector_search":
                items.extend(self._process_vector_results(data))

        # ì¤‘ë³µ ì œê±°
        items = self._deduplicate(items)

        # ì ìˆ˜ìˆœ ì •ë ¬
        items.sort(key=lambda x: x.score, reverse=True)

        # í† í° ì œí•œ
        items = self._limit_by_tokens(items)

        return items

    def _process_cypher_results(self, data: List) -> List[ContextItem]:
        """Cypher ê²°ê³¼ ì²˜ë¦¬"""
        items = []
        for row in data:
            content = self._row_to_text(row)
            items.append(ContextItem(
                content=content,
                source="neo4j",
                score=1.0,  # êµ¬ì¡°ì  ê²°ê³¼ëŠ” ë†’ì€ ì‹ ë¢°ë„
                metadata=row
            ))
        return items

    def _process_vector_results(self, data: List) -> List[ContextItem]:
        """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬"""
        items = []
        for result in data:
            items.append(ContextItem(
                content=result.get("content", ""),
                source="vector",
                score=result.get("score", 0.5),
                metadata=result.get("metadata", {})
            ))
        return items

    def _row_to_text(self, row: Dict) -> str:
        """Neo4j í–‰ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        for key, value in row.items():
            if value:
                parts.append(f"{key}: {value}")
        return ", ".join(parts)

    def _deduplicate(self, items: List[ContextItem]) -> List[ContextItem]:
        """ì¤‘ë³µ ì œê±° (ìœ ì‚¬ ì½˜í…ì¸ )"""
        seen = set()
        unique = []

        for item in items:
            # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
            key = item.content[:100].lower()
            if key not in seen:
                seen.add(key)
                unique.append(item)

        return unique

    def _limit_by_tokens(self, items: List[ContextItem]) -> List[ContextItem]:
        """í† í° ì œí•œ"""
        total_chars = 0
        limited = []

        for item in items:
            item_chars = len(item.content)
            if total_chars + item_chars > self.max_tokens * 4:  # ëŒ€ëµì  ë³€í™˜
                break
            total_chars += item_chars
            limited.append(item)

        return limited

    def format_for_prompt(self, items: List[ContextItem]) -> str:
        """í”„ë¡¬í”„íŠ¸ìš© í¬ë§·íŒ…"""
        if not items:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        parts = []
        for i, item in enumerate(items, 1):
            parts.append(f"[{i}] ({item.source}) {item.content}")

        return "\\n\\n".join(parts)
\`\`\`
`,
  keyPoints: ['ì†ŒìŠ¤ë³„ ê²°ê³¼ ì²˜ë¦¬', 'ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ì •ë ¬', 'í† í° ì œí•œ ê´€ë¦¬'],
  practiceGoal: 'Context Aggregator êµ¬í˜„',
  codeExample: `# ì»¨í…ìŠ¤íŠ¸ í†µí•©
aggregator = ContextAggregator(max_tokens=3000)

contexts = [
    {"source": "text2cypher", "data": [{"name": "SKí•˜ì´ë‹‰ìŠ¤", "relation": "competes_with"}]},
    {"source": "vector_search", "data": [{"content": "ì‚¼ì„±ì „ìëŠ”...", "score": 0.85}]}
]

items = aggregator.aggregate(contexts)
prompt_context = aggregator.format_for_prompt(items)`,
})

const task4 = createCodeTask('w8d4-response-generator', 'ì‹¤ìŠµ: Response Generator êµ¬í˜„', 45, {
  introduction: `
## Response Generator êµ¬í˜„

### ìì—°ì–´ ì‘ë‹µ ìƒì„±

\`\`\`python
# rag/response.py

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from typing import List, Dict

class ResponseGenerator:
    """ìì—°ì–´ ì‘ë‹µ ìƒì„±ê¸°"""

    def __init__(self, openai_key: str):
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=openai_key
        )

        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ Knowledge Graph ê¸°ë°˜ Q&A ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©
2. ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤" ë¼ê³  ë‹µë³€
3. ê´€ê³„ë‚˜ ì‚¬ì‹¤ì„ ëª…í™•í•˜ê²Œ ì„¤ëª…
4. ê°€ëŠ¥í•˜ë©´ êµ¬ì¡°í™”ëœ í˜•ì‹ (ëª©ë¡, í‘œ) ì‚¬ìš©
5. ì¶œì²˜ ë²ˆí˜¸ [1], [2] ë“±ì„ ì¸ìš©"""),
            ("human", """ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:""")
        ])

        self.chain = self.prompt | self.llm

    def generate(
        self,
        question: str,
        context: str,
        metadata: Dict = None
    ) -> Dict:
        """ì‘ë‹µ ìƒì„±"""
        try:
            result = self.chain.invoke({
                "question": question,
                "context": context
            })

            return {
                "answer": result.content,
                "sources": metadata.get("sources", []) if metadata else [],
                "confidence": self._estimate_confidence(context, result.content)
            }
        except Exception as e:
            return {
                "answer": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                "sources": [],
                "confidence": 0.0
            }

    def _estimate_confidence(self, context: str, answer: str) -> float:
        """ì‘ë‹µ ì‹ ë¢°ë„ ì¶”ì •"""
        # ì»¨í…ìŠ¤íŠ¸ê°€ í’ë¶€í• ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
        if not context or "ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤" in context:
            return 0.3

        # ë‹µë³€ ê¸¸ì´ì™€ ì¸ìš© ìˆ˜ ê¸°ë°˜
        citation_count = answer.count("[")
        if citation_count >= 3:
            return 0.9
        elif citation_count >= 1:
            return 0.7
        else:
            return 0.5

    def generate_with_explanation(
        self,
        question: str,
        context: str,
        cypher_query: str = None
    ) -> Dict:
        """ì„¤ëª… í¬í•¨ ì‘ë‹µ ìƒì„±"""
        response = self.generate(question, context)

        explanation_parts = []

        if cypher_query:
            explanation_parts.append(f"**ì‚¬ìš©ëœ ì¿¼ë¦¬:**\\n\`\`\`cypher\\n{cypher_query}\\n\`\`\`")

        explanation_parts.append(f"**ì‹ ë¢°ë„:** {response['confidence']*100:.0f}%")

        response["explanation"] = "\\n\\n".join(explanation_parts)
        return response
\`\`\`

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

\`\`\`python
# rag/streaming.py

from langchain_core.callbacks import StreamingStdOutCallbackHandler

class StreamingResponseGenerator(ResponseGenerator):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±ê¸°"""

    def __init__(self, openai_key: str):
        super().__init__(openai_key)
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            api_key=openai_key,
            streaming=True,
            callbacks=[StreamingStdOutCallbackHandler()]
        )

    async def generate_stream(self, question: str, context: str):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        async for chunk in self.chain.astream({
            "question": question,
            "context": context
        }):
            yield chunk.content
\`\`\`
`,
  keyPoints: ['ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±', 'ì¶œì²˜ ì¸ìš© ê·œì¹™', 'ì‹ ë¢°ë„ ì¶”ì •', 'ìŠ¤íŠ¸ë¦¬ë° ì§€ì›'],
  practiceGoal: 'Response Generator êµ¬í˜„',
  codeExample: `# ì‘ë‹µ ìƒì„±
generator = ResponseGenerator(openai_key)

response = generator.generate_with_explanation(
    question="ì‚¼ì„±ì „ìì˜ ì£¼ìš” ê²½ìŸì‚¬ëŠ”?",
    context=prompt_context,
    cypher_query="MATCH (c:Company {name:'ì‚¼ì„±ì „ì'})-[:COMPETES_WITH]->(x) RETURN x"
)

print(response["answer"])
print(response["explanation"])`,
})

const task5 = createCodeTask('w8d4-full-pipeline', 'ì‹¤ìŠµ: ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©', 50, {
  introduction: `
## ì „ì²´ GraphRAG íŒŒì´í”„ë¼ì¸

### í†µí•© ì—”ì§„

\`\`\`python
# rag/graphrag.py

from .router import QueryRouter, QueryType
from .aggregator import ContextAggregator
from .response import ResponseGenerator
from kg.query import KGQueryEngine
from kg.vector_index import KGVectorIndex

class DomainGraphRAG:
    """ë„ë©”ì¸ íŠ¹í™” GraphRAG ì—”ì§„"""

    def __init__(
        self,
        neo4j_url: str,
        neo4j_user: str,
        neo4j_password: str,
        openai_key: str
    ):
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.router = QueryRouter(openai_key)
        self.query_engine = KGQueryEngine(neo4j_url, neo4j_user, neo4j_password)
        self.vector_index = KGVectorIndex(
            neo4j_url, neo4j_user, neo4j_password, openai_key
        )
        self.aggregator = ContextAggregator()
        self.generator = ResponseGenerator(openai_key)

        # Text2Cypher ì—”ì§„ (Week 7ì—ì„œ êµ¬í˜„)
        from .text2cypher import Text2CypherEngine
        self.text2cypher = Text2CypherEngine(
            neo4j_url, neo4j_user, neo4j_password, openai_key
        )

    def query(self, question: str) -> dict:
        """ì§ˆë¬¸ ì²˜ë¦¬ ì „ì²´ íŒŒì´í”„ë¼ì¸"""

        # 1. ë¼ìš°íŒ…
        route = self.router.route(question)
        query_type = route["type"]

        contexts = []
        cypher_query = None

        # 2. êµ¬ì¡°ì  ê²€ìƒ‰
        if query_type in [QueryType.STRUCTURAL, QueryType.HYBRID]:
            t2c_result = self.text2cypher.query(question)
            if t2c_result.get("success"):
                contexts.append({
                    "source": "text2cypher",
                    "data": t2c_result["results"]
                })
                cypher_query = t2c_result.get("cypher")

        # 3. ì˜ë¯¸ì  ê²€ìƒ‰
        if query_type in [QueryType.SEMANTIC, QueryType.HYBRID]:
            vector_results = self.vector_index.similarity_search(question, k=5)
            contexts.append({
                "source": "vector_search",
                "data": vector_results
            })

        # 4. ì»¨í…ìŠ¤íŠ¸ í†µí•©
        items = self.aggregator.aggregate(contexts)
        formatted_context = self.aggregator.format_for_prompt(items)

        # 5. ì‘ë‹µ ìƒì„±
        response = self.generator.generate_with_explanation(
            question=question,
            context=formatted_context,
            cypher_query=cypher_query
        )

        return {
            "question": question,
            "query_type": query_type.value,
            "routing_reason": route.get("reason"),
            "contexts_count": len(items),
            "answer": response["answer"],
            "explanation": response.get("explanation"),
            "confidence": response.get("confidence"),
            "cypher": cypher_query
        }

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.query_engine.close()


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv

    load_dotenv()

    rag = DomainGraphRAG(
        neo4j_url=os.getenv("NEO4J_URI"),
        neo4j_user=os.getenv("NEO4J_USER"),
        neo4j_password=os.getenv("NEO4J_PASSWORD"),
        openai_key=os.getenv("OPENAI_API_KEY")
    )

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    questions = [
        "ì‚¼ì„±ì „ìì˜ ì£¼ìš” ê²½ìŸì‚¬ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
        "ë°˜ë„ì²´ ì‚°ì—…ì˜ ìµœê·¼ ë™í–¥ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ì‚¼ì„±ì „ì ê´€ë ¨ ìµœì‹  ë‰´ìŠ¤ ìš”ì•½"
    ]

    for q in questions:
        print(f"\\nì§ˆë¬¸: {q}")
        result = rag.query(q)
        print(f"ìœ í˜•: {result['query_type']}")
        print(f"ë‹µë³€: {result['answer'][:200]}...")
        print(f"ì‹ ë¢°ë„: {result['confidence']*100:.0f}%")

    rag.close()
\`\`\`
`,
  keyPoints: ['5ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ í†µí•©', 'ì»´í¬ë„ŒíŠ¸ ì¡°í•©', 'ë¦¬ì†ŒìŠ¤ ê´€ë¦¬'],
  practiceGoal: 'ì „ì²´ GraphRAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•',
  codeExample: `# GraphRAG ì‹¤í–‰
rag = DomainGraphRAG(neo4j_url, neo4j_user, neo4j_password, openai_key)

result = rag.query("ì‚¼ì„±ì „ìì™€ SKí•˜ì´ë‹‰ìŠ¤ì˜ ê²½ìŸ ê´€ê³„ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”")

print(f"ìœ í˜•: {result['query_type']}")  # hybrid
print(f"ë‹µë³€: {result['answer']}")
print(f"Cypher: {result['cypher']}")
print(f"ì‹ ë¢°ë„: {result['confidence']*100}%")`,
})

const task6 = createReadingTask('w8d4-evaluation', 'GraphRAG í‰ê°€', 25, {
  introduction: `
## GraphRAG í‰ê°€

### í‰ê°€ ì§€í‘œ

| ì§€í‘œ | ì„¤ëª… | ì¸¡ì • ë°©ë²• |
|------|------|----------|
| ì •í™•ë„ | ë‹µë³€ì˜ ì‚¬ì‹¤ì  ì •í™•ì„± | ìˆ˜ë™ í‰ê°€ |
| ê´€ë ¨ì„± | ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ê´€ë ¨ì„± | LLM í‰ê°€ |
| ì™„ì „ì„± | í•„ìš”í•œ ì •ë³´ í¬í•¨ ì—¬ë¶€ | ì²´í¬ë¦¬ìŠ¤íŠ¸ |
| ì‘ë‹µ ì‹œê°„ | ì „ì²´ íŒŒì´í”„ë¼ì¸ ì§€ì—° | ìë™ ì¸¡ì • |
| ë¼ìš°íŒ… ì •í™•ë„ | ì§ˆë¬¸ ë¶„ë¥˜ ì •í™•ì„± | ìˆ˜ë™ ë ˆì´ë¸”ë§ |

### ìë™ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

\`\`\`python
# evaluation/evaluator.py

class GraphRAGEvaluator:
    def __init__(self, rag_engine, test_cases: List[Dict]):
        self.rag = rag_engine
        self.test_cases = test_cases

    def evaluate(self) -> Dict:
        results = []

        for case in self.test_cases:
            question = case["question"]
            expected_type = case.get("expected_type")
            expected_entities = case.get("expected_entities", [])

            # ì‹¤í–‰
            import time
            start = time.time()
            result = self.rag.query(question)
            latency = time.time() - start

            # í‰ê°€
            metrics = {
                "question": question,
                "latency": latency,
                "type_correct": result["query_type"] == expected_type,
                "entities_found": self._count_entities(
                    result["answer"], expected_entities
                ),
                "confidence": result.get("confidence", 0)
            }
            results.append(metrics)

        return self._aggregate_results(results)

    def _count_entities(self, answer: str, expected: List[str]) -> int:
        return sum(1 for e in expected if e in answer)

    def _aggregate_results(self, results: List[Dict]) -> Dict:
        return {
            "total_cases": len(results),
            "avg_latency": sum(r["latency"] for r in results) / len(results),
            "type_accuracy": sum(r["type_correct"] for r in results) / len(results),
            "avg_confidence": sum(r["confidence"] for r in results) / len(results)
        }
\`\`\`

### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì˜ˆì‹œ

\`\`\`python
TEST_CASES = [
    {
        "question": "ì‚¼ì„±ì „ìì˜ ê²½ìŸì‚¬ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
        "expected_type": "structural",
        "expected_entities": ["SKí•˜ì´ë‹‰ìŠ¤", "Intel", "TSMC"]
    },
    {
        "question": "ë°˜ë„ì²´ ì‚°ì—… ì „ë§ì€?",
        "expected_type": "semantic",
        "expected_entities": []
    },
    {
        "question": "ì‚¼ì„±ì „ì ìµœì‹  ë‰´ìŠ¤",
        "expected_type": "hybrid",
        "expected_entities": ["ì‚¼ì„±ì „ì"]
    }
]
\`\`\`

### ì„±ëŠ¥ ëª©í‘œ

| ì§€í‘œ | ëª©í‘œ |
|------|------|
| í‰ê·  ì‘ë‹µ ì‹œê°„ | < 3ì´ˆ |
| ë¼ìš°íŒ… ì •í™•ë„ | > 85% |
| ì—”í‹°í‹° ì¶”ì¶œë¥  | > 70% |
| í‰ê·  ì‹ ë¢°ë„ | > 0.7 |
`,
  keyPoints: ['5ê°€ì§€ í‰ê°€ ì§€í‘œ', 'ìë™ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸', 'ì„±ëŠ¥ ëª©í‘œ ì„¤ì •'],
  practiceGoal: 'GraphRAG ì‹œìŠ¤í…œ í‰ê°€ ë°©ë²• ì´í•´',
})

export const day4GraphragIntegration: Day = {
  slug: 'graphrag-integration',
  title: 'GraphRAG í†µí•©',
  totalDuration: 240,
  tasks: [task1, task2, task3, task4, task5, task6],
}
