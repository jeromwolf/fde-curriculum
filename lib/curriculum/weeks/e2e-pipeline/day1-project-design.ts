import { Task } from '../../types'

export const day1Tasks: Task[] = [
  {
    id: 'w8d1t1',
    title: 'Phase 1 ì¢…í•© í”„ë¡œì íŠ¸ ì†Œê°œ',
    type: 'video',
    duration: 25,
    content: {
      videoUrl: '',
      transcript: `# Phase 1 ì¢…í•© í”„ë¡œì íŠ¸: E2E ë°ì´í„° íŒŒì´í”„ë¼ì¸

## ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ

ì§€ë‚œ 7ì£¼ê°„ ë°°ìš´ ëª¨ë“  ê¸°ìˆ ì„ í†µí•©í•˜ì—¬ **ì‹¤ì œ ìš´ì˜ í™˜ê²½ ìˆ˜ì¤€ì˜ End-to-End ë°ì´í„° íŒŒì´í”„ë¼ì¸**ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì´ í”„ë¡œì íŠ¸ëŠ” Phase 1ì˜ ëª¨ë“  ì—­ëŸ‰ì„ ê²€ì¦í•˜ëŠ” **ìº¡ìŠ¤í†¤ í”„ë¡œì íŠ¸**ì…ë‹ˆë‹¤.

## ğŸ“š í™œìš© ê¸°ìˆ  ìŠ¤íƒ

| Week | ê¸°ìˆ  | í”„ë¡œì íŠ¸ ì ìš© |
|------|------|--------------|
| Week 1 | Python ì‹¬í™” | ë°ì´í„° ì²˜ë¦¬ ë¡œì§, í´ë˜ìŠ¤ ì„¤ê³„ |
| Week 2 | Pandas | ë°ì´í„° ì •ì œ, ë³€í™˜ |
| Week 3 | SQL ì‹¬í™” | ë°ì´í„° ëª¨ë¸ ì¿¼ë¦¬, ë¶„ì„ |
| Week 4 | ë°ì´í„° ëª¨ë¸ë§ | Star Schema, SCD |
| Week 5-6 | Spark | ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ |
| Week 7 | Airflow | íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ |

## ğŸ¢ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤

**ìŠ¤íƒ€íŠ¸ì—… Aì‚¬ì˜ ë°ì´í„° í”Œë«í¼ êµ¬ì¶•**

Aì‚¬ëŠ” ë¹ ë¥´ê²Œ ì„±ì¥í•˜ëŠ” SaaS ìŠ¤íƒ€íŠ¸ì—…ì…ë‹ˆë‹¤:
- ì›”ê°„ í™œì„± ì‚¬ìš©ì (MAU): 50ë§Œ ëª…
- ì¼ì¼ ì´ë²¤íŠ¸ ë°ì´í„°: 1ì–µ ê±´
- ë°ì´í„° ì†ŒìŠ¤: PostgreSQL, MongoDB, REST API, S3 ë¡œê·¸

**í˜„ì¬ ë¬¸ì œì :**
- ìˆ˜ë™ ë°ì´í„° ì¶”ì¶œ â†’ ë¶„ì„ ì§€ì—°
- ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë¹ˆë²ˆ
- ë¶€ì„œë³„ ë°ì´í„° ì‚¬ì¼ë¡œ
- ë¦¬í¬íŒ… ìë™í™” ë¶€ì¬

**í”„ë¡œì íŠ¸ ëª©í‘œ:**
- ìë™í™”ëœ ETL íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ êµ¬í˜„
- ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì œê³µ
- ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

## ğŸ“Š ë°ì´í„° ë„ë©”ì¸

### 1. ì‚¬ìš©ì ë°ì´í„° (PostgreSQL)
\`\`\`sql
-- users í…Œì´ë¸”
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE,
    name VARCHAR(100),
    plan_type VARCHAR(20),  -- free, pro, enterprise
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);

-- subscriptions í…Œì´ë¸”
CREATE TABLE subscriptions (
    subscription_id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users,
    plan_type VARCHAR(20),
    start_date DATE,
    end_date DATE,
    mrr DECIMAL(10,2),  -- Monthly Recurring Revenue
    status VARCHAR(20)
);
\`\`\`

### 2. ì´ë²¤íŠ¸ ë°ì´í„° (S3 JSON ë¡œê·¸)
\`\`\`json
{
  "event_id": "evt_abc123",
  "user_id": 12345,
  "event_type": "page_view",
  "page": "/dashboard",
  "timestamp": "2024-01-15T10:30:00Z",
  "properties": {
    "device": "desktop",
    "browser": "chrome",
    "country": "KR"
  }
}
\`\`\`

### 3. ê²°ì œ ë°ì´í„° (REST API)
\`\`\`json
{
  "payment_id": "pay_xyz789",
  "user_id": 12345,
  "amount": 29000,
  "currency": "KRW",
  "status": "completed",
  "created_at": "2024-01-15T10:35:00Z"
}
\`\`\`

## ğŸ—ï¸ ëª©í‘œ ì•„í‚¤í…ì²˜

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Data Sources                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚PostgreSQLâ”‚  â”‚  S3 Logs â”‚  â”‚ REST API â”‚  â”‚ MongoDB  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Ingestion Layer                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Apache Airflow                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚  â”‚ Extract â”‚  â”‚ Extract â”‚  â”‚ Extract â”‚  â”‚ Extract â”‚      â”‚   â”‚
â”‚  â”‚  â”‚  Users  â”‚  â”‚  Events â”‚  â”‚Payments â”‚  â”‚  Docs   â”‚      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚            â”‚            â”‚            â”‚
           â–¼            â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Raw Data Lake                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    S3 / Delta Lake                        â”‚   â”‚
â”‚  â”‚  raw/users/  raw/events/  raw/payments/  raw/documents/  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Transformation Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Apache Spark                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚  â”‚ Cleanse â”‚  â”‚ Dedupe  â”‚  â”‚  Join   â”‚  â”‚ Compute â”‚      â”‚   â”‚
â”‚  â”‚  â”‚  Data   â”‚  â”‚  Data   â”‚  â”‚  Data   â”‚  â”‚ Metrics â”‚      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Data Warehouse                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 Star Schema (Delta Lake)                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚   â”‚
â”‚  â”‚  â”‚dim_user â”‚  â”‚dim_date â”‚  â”‚dim_plan â”‚  â”‚fact_    â”‚      â”‚   â”‚
â”‚  â”‚  â”‚         â”‚  â”‚         â”‚  â”‚         â”‚  â”‚events   â”‚      â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Data Marts                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Marketing  â”‚  â”‚   Product   â”‚  â”‚   Finance   â”‚              â”‚
â”‚  â”‚    Mart     â”‚  â”‚    Mart     â”‚  â”‚    Mart     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## ğŸ“… 5ì¼ í”„ë¡œì íŠ¸ ì¼ì •

| Day | ì£¼ì œ | ì‚°ì¶œë¬¼ |
|-----|------|--------|
| Day 1 | í”„ë¡œì íŠ¸ ì„¤ê³„ | ì•„í‚¤í…ì²˜ ë¬¸ì„œ, ERD, í”„ë¡œì íŠ¸ êµ¬ì¡° |
| Day 2 | ë°ì´í„° ìˆ˜ì§‘ | Extraction DAG, Raw Layer êµ¬í˜„ |
| Day 3 | ë°ì´í„° ë³€í™˜ | Spark ë³€í™˜ ë¡œì§, DW ìŠ¤í‚¤ë§ˆ |
| Day 4 | ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ | í†µí•© DAG, ëª¨ë‹ˆí„°ë§, ì•Œë¦¼ |
| Day 5 | í…ŒìŠ¤íŠ¸ & ë°œí‘œ | í…ŒìŠ¤íŠ¸ ì½”ë“œ, ë¬¸ì„œí™”, ë°œí‘œ ìë£Œ |

## ğŸ† í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ì„¸ë¶€ ê¸°ì¤€ |
|------|------|----------|
| ì•„í‚¤í…ì²˜ | 20% | í™•ì¥ì„±, ìœ ì§€ë³´ìˆ˜ì„±, ëª¨ë²” ì‚¬ë¡€ ì ìš© |
| ì½”ë“œ í’ˆì§ˆ | 25% | ê°€ë…ì„±, ëª¨ë“ˆí™”, ì—ëŸ¬ í•¸ë“¤ë§ |
| ë°ì´í„° í’ˆì§ˆ | 20% | ê²€ì¦ ë¡œì§, í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ |
| ìš´ì˜ ê°€ëŠ¥ì„± | 20% | ëª¨ë‹ˆí„°ë§, ì•Œë¦¼, ë¬¸ì„œí™” |
| ë°œí‘œ | 15% | ê¸°ìˆ  ì„¤ëª…ë ¥, ë°ëª¨, Q&A |

ì, ì´ì œ ì‹œì‘í•´ë´…ì‹œë‹¤! ğŸš€
`,
      objectives: [
        'E2E ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ ì´í•´í•œë‹¤',
        'Phase 1ì—ì„œ ë°°ìš´ ê¸°ìˆ ë“¤ì´ ì–´ë–»ê²Œ í†µí•©ë˜ëŠ”ì§€ íŒŒì•…í•œë‹¤',
        'ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì„ ê¸°ìˆ  ì•„í‚¤í…ì²˜ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ìµíŒë‹¤'
      ],
      keyPoints: [
        'E2E íŒŒì´í”„ë¼ì¸ì€ Extract â†’ Transform â†’ Load â†’ Serveì˜ íë¦„',
        'ê° ë ˆì´ì–´ëŠ” ëª…í™•í•œ ì±…ì„ì„ ê°€ì§€ë©° ë…ë¦½ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•´ì•¼ í•¨',
        'ë°ì´í„° í’ˆì§ˆê³¼ ëª¨ë‹ˆí„°ë§ì€ ì²˜ìŒë¶€í„° ì„¤ê³„ì— í¬í•¨ë˜ì–´ì•¼ í•¨'
      ]
    }
  },
  {
    id: 'w8d1t2',
    title: 'ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„',
    type: 'reading',
    duration: 30,
    content: {
      markdown: `# ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„

## ğŸ¯ ì„¤ê³„ ì›ì¹™

### Kimball ë°©ë²•ë¡  ì ìš©
- **ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ ì¤‘ì‹¬**: ì‚¬ìš©ì í–‰ë™, ê²°ì œ, êµ¬ë…
- **ì°¨ì› ëª¨ë¸ë§**: Star Schema ê¸°ë°˜
- **ì ì§„ì  êµ¬ì¶•**: í•µì‹¬ ì§€í‘œë¶€í„° ì‹œì‘

## ğŸ“Š Star Schema ì„¤ê³„

### Fact Tables

#### 1. fact_events (ì´ë²¤íŠ¸ íŒ©íŠ¸)
\`\`\`sql
CREATE TABLE fact_events (
    event_sk BIGINT,              -- Surrogate Key
    event_id VARCHAR(50),         -- Natural Key
    user_sk BIGINT,               -- FK to dim_user
    date_sk INTEGER,              -- FK to dim_date (YYYYMMDD)
    time_sk INTEGER,              -- FK to dim_time (HHMM)
    event_type_sk INTEGER,        -- FK to dim_event_type
    page_sk INTEGER,              -- FK to dim_page
    device_sk INTEGER,            -- FK to dim_device

    -- Measures
    session_duration_seconds INTEGER,
    page_views INTEGER DEFAULT 1,

    -- Metadata
    event_timestamp TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- íŒŒí‹°ì…”ë‹: ë‚ ì§œ ê¸°ì¤€
-- ì¸ë±ì‹±: user_sk, event_type_sk
\`\`\`

#### 2. fact_subscriptions (êµ¬ë… íŒ©íŠ¸)
\`\`\`sql
CREATE TABLE fact_subscriptions (
    subscription_sk BIGINT,
    subscription_id VARCHAR(50),
    user_sk BIGINT,
    plan_sk INTEGER,              -- FK to dim_plan
    start_date_sk INTEGER,
    end_date_sk INTEGER,

    -- Measures
    mrr DECIMAL(10,2),            -- Monthly Recurring Revenue
    arr DECIMAL(12,2),            -- Annual Recurring Revenue
    subscription_months INTEGER,

    -- Status
    status VARCHAR(20),           -- active, cancelled, expired
    churn_reason VARCHAR(100),

    -- Metadata
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
\`\`\`

#### 3. fact_payments (ê²°ì œ íŒ©íŠ¸)
\`\`\`sql
CREATE TABLE fact_payments (
    payment_sk BIGINT,
    payment_id VARCHAR(50),
    user_sk BIGINT,
    date_sk INTEGER,
    plan_sk INTEGER,

    -- Measures
    amount DECIMAL(10,2),
    amount_usd DECIMAL(10,2),     -- í™˜ì‚° ê¸ˆì•¡
    discount_amount DECIMAL(10,2),
    tax_amount DECIMAL(10,2),

    -- Attributes
    currency VARCHAR(3),
    payment_method VARCHAR(20),
    status VARCHAR(20),

    -- Metadata
    payment_timestamp TIMESTAMP,
    created_at TIMESTAMP
);
\`\`\`

### Dimension Tables

#### 1. dim_user (ì‚¬ìš©ì ì°¨ì›) - SCD Type 2
\`\`\`sql
CREATE TABLE dim_user (
    user_sk BIGINT,               -- Surrogate Key
    user_id INTEGER,              -- Natural Key

    -- Attributes
    email VARCHAR(255),
    name VARCHAR(100),
    signup_source VARCHAR(50),
    country VARCHAR(50),
    timezone VARCHAR(50),

    -- Current Plan (denormalized)
    current_plan VARCHAR(20),

    -- Derived Attributes
    user_segment VARCHAR(20),     -- new, active, power, churned
    lifetime_value DECIMAL(12,2),

    -- SCD Type 2 Columns
    effective_date DATE,
    expiration_date DATE,
    is_current BOOLEAN,

    -- Metadata
    created_at TIMESTAMP,
    updated_at TIMESTAMP
);
\`\`\`

#### 2. dim_date (ë‚ ì§œ ì°¨ì›)
\`\`\`sql
CREATE TABLE dim_date (
    date_sk INTEGER PRIMARY KEY,  -- YYYYMMDD
    full_date DATE,

    -- Calendar Attributes
    year INTEGER,
    quarter INTEGER,
    month INTEGER,
    month_name VARCHAR(20),
    week_of_year INTEGER,
    day_of_month INTEGER,
    day_of_week INTEGER,
    day_name VARCHAR(20),

    -- Fiscal Calendar (Optional)
    fiscal_year INTEGER,
    fiscal_quarter INTEGER,

    -- Flags
    is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    holiday_name VARCHAR(100)
);
\`\`\`

#### 3. dim_plan (êµ¬ë… í”Œëœ ì°¨ì›)
\`\`\`sql
CREATE TABLE dim_plan (
    plan_sk INTEGER PRIMARY KEY,
    plan_code VARCHAR(20),

    -- Attributes
    plan_name VARCHAR(50),
    plan_tier VARCHAR(20),        -- free, starter, pro, enterprise
    monthly_price DECIMAL(10,2),
    annual_price DECIMAL(10,2),

    -- Features (JSON or columns)
    max_users INTEGER,
    max_storage_gb INTEGER,
    features JSONB,

    -- Status
    is_active BOOLEAN,

    -- Metadata
    effective_date DATE,
    created_at TIMESTAMP
);
\`\`\`

#### 4. dim_event_type (ì´ë²¤íŠ¸ ìœ í˜• ì°¨ì›)
\`\`\`sql
CREATE TABLE dim_event_type (
    event_type_sk INTEGER PRIMARY KEY,
    event_type_code VARCHAR(50),

    -- Hierarchy
    event_category VARCHAR(50),   -- page_view, click, form, system
    event_subcategory VARCHAR(50),

    -- Attributes
    event_name VARCHAR(100),
    description TEXT,
    is_conversion BOOLEAN,
    conversion_value DECIMAL(10,2),

    -- Metadata
    created_at TIMESTAMP
);
\`\`\`

## ğŸ“ˆ ì§‘ê³„ í…Œì´ë¸” (Data Marts)

### 1. agg_daily_user_metrics
\`\`\`sql
CREATE TABLE agg_daily_user_metrics (
    date_sk INTEGER,
    user_sk BIGINT,

    -- Engagement Metrics
    page_views INTEGER,
    unique_sessions INTEGER,
    total_session_duration_seconds INTEGER,
    events_count INTEGER,

    -- Feature Usage
    feature_a_usage INTEGER,
    feature_b_usage INTEGER,

    -- Computed
    avg_session_duration DECIMAL(10,2),
    engagement_score DECIMAL(5,2),

    PRIMARY KEY (date_sk, user_sk)
);
\`\`\`

### 2. agg_daily_revenue
\`\`\`sql
CREATE TABLE agg_daily_revenue (
    date_sk INTEGER PRIMARY KEY,

    -- Revenue Metrics
    total_revenue DECIMAL(12,2),
    new_mrr DECIMAL(12,2),
    churned_mrr DECIMAL(12,2),
    net_mrr_change DECIMAL(12,2),

    -- Customer Metrics
    total_paying_users INTEGER,
    new_subscriptions INTEGER,
    churned_subscriptions INTEGER,

    -- Plan Breakdown
    free_users INTEGER,
    pro_users INTEGER,
    enterprise_users INTEGER,

    -- Computed
    arpu DECIMAL(10,2),           -- Average Revenue Per User
    conversion_rate DECIMAL(5,4)
);
\`\`\`

### 3. agg_cohort_retention
\`\`\`sql
CREATE TABLE agg_cohort_retention (
    cohort_month DATE,            -- ê°€ì… ì›”
    months_since_signup INTEGER,  -- 0, 1, 2, ...

    -- Cohort Size
    cohort_size INTEGER,

    -- Retention Metrics
    retained_users INTEGER,
    retention_rate DECIMAL(5,4),

    -- Revenue Metrics
    cohort_revenue DECIMAL(12,2),
    revenue_per_user DECIMAL(10,2),

    PRIMARY KEY (cohort_month, months_since_signup)
);
\`\`\`

## ğŸ”„ ë°ì´í„° íë¦„

\`\`\`
Raw Layer (S3)
    â”‚
    â–¼
Staging Layer (Spark)
    â”œâ”€â”€ stg_users
    â”œâ”€â”€ stg_events
    â”œâ”€â”€ stg_payments
    â””â”€â”€ stg_subscriptions
    â”‚
    â–¼
Intermediate Layer (Spark)
    â”œâ”€â”€ int_users_deduplicated
    â”œâ”€â”€ int_events_enriched
    â””â”€â”€ int_payments_validated
    â”‚
    â–¼
Dimensional Layer (Delta Lake)
    â”œâ”€â”€ dim_user (SCD Type 2)
    â”œâ”€â”€ dim_date
    â”œâ”€â”€ dim_plan
    â””â”€â”€ dim_event_type
    â”‚
    â–¼
Fact Layer (Delta Lake)
    â”œâ”€â”€ fact_events
    â”œâ”€â”€ fact_subscriptions
    â””â”€â”€ fact_payments
    â”‚
    â–¼
Aggregation Layer (Delta Lake)
    â”œâ”€â”€ agg_daily_user_metrics
    â”œâ”€â”€ agg_daily_revenue
    â””â”€â”€ agg_cohort_retention
\`\`\`

## âœ… ìŠ¤í‚¤ë§ˆ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ëª¨ë“  Fact í…Œì´ë¸”ì´ Date Dimensionì„ ì°¸ì¡°í•˜ëŠ”ê°€?
- [ ] Surrogate Keyê°€ ì¼ê´€ë˜ê²Œ ì‚¬ìš©ë˜ëŠ”ê°€?
- [ ] SCD Type 2ê°€ í•„ìš”í•œ ì°¨ì›ì— ì ìš©ë˜ì—ˆëŠ”ê°€?
- [ ] ì§‘ê³„ í…Œì´ë¸”ì´ ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆëŠ”ê°€?
- [ ] íŒŒí‹°ì…”ë‹ ì „ëµì´ ì¿¼ë¦¬ íŒ¨í„´ì— ë§ëŠ”ê°€?
`,
      externalLinks: [
        { title: 'Kimball Group - Dimensional Modeling', url: 'https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/' },
        { title: 'Star Schema Best Practices', url: 'https://docs.getdbt.com/guides/best-practices/how-we-structure/4-marts' }
      ]
    }
  },
  {
    id: 'w8d1t3',
    title: 'í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •',
    type: 'code',
    duration: 35,
    content: {
      instructions: `## í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •

E2E íŒŒì´í”„ë¼ì¸ í”„ë¡œì íŠ¸ì˜ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
ëª¨ë²” ì‚¬ë¡€ë¥¼ ë”°ë¥´ëŠ” êµ¬ì¡°ë¡œ í™•ì¥ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±**
   - ë ˆì´ì–´ë³„ ëª…í™•í•œ ë¶„ë¦¬
   - í…ŒìŠ¤íŠ¸, ì„¤ì •, ë¬¸ì„œ ë¶„ë¦¬

2. **ì„¤ì • íŒŒì¼ ì‘ì„±**
   - pyproject.toml / requirements.txt
   - Docker Compose
   - í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿

3. **ê³µí†µ ìœ í‹¸ë¦¬í‹°**
   - ë¡œê¹… ì„¤ì •
   - ì„¤ì • ê´€ë¦¬
   - ì—ëŸ¬ í•¸ë“¤ë§

### í…ŒìŠ¤íŠ¸ ë°©ë²•

\`\`\`bash
# í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
tree -L 3 e2e_pipeline/

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -e .

# ì„¤ì • í…ŒìŠ¤íŠ¸
python -c "from e2e_pipeline.config import settings; print(settings)"
\`\`\`
`,
      starterCode: `# í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
# setup_project.py

import os
from pathlib import Path

PROJECT_ROOT = Path("e2e_pipeline")

# ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ì˜
DIRECTORIES = [
    # ë©”ì¸ ì†ŒìŠ¤ ì½”ë“œ
    "src/e2e_pipeline",
    "src/e2e_pipeline/extractors",
    "src/e2e_pipeline/transformers",
    "src/e2e_pipeline/loaders",
    "src/e2e_pipeline/utils",
    "src/e2e_pipeline/models",

    # Airflow DAGs
    "dags",

    # Spark Jobs
    "spark_jobs",

    # SQL ìŠ¤í¬ë¦½íŠ¸
    "sql/ddl",
    "sql/dml",
    "sql/queries",

    # ì„¤ì • íŒŒì¼
    "config",

    # í…ŒìŠ¤íŠ¸
    "tests/unit",
    "tests/integration",
    "tests/fixtures",

    # ë¬¸ì„œ
    "docs",

    # ìŠ¤í¬ë¦½íŠ¸
    "scripts",
]

def create_project_structure():
    """í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
    # TODO: ë””ë ‰í† ë¦¬ ìƒì„±
    pass

def create_init_files():
    """__init__.py íŒŒì¼ ìƒì„±"""
    # TODO: Python íŒ¨í‚¤ì§€ ì´ˆê¸°í™” íŒŒì¼ ìƒì„±
    pass

def create_config_files():
    """ì„¤ì • íŒŒì¼ ìƒì„±"""
    # TODO: pyproject.toml, docker-compose.yml ë“± ìƒì„±
    pass

def create_utility_modules():
    """ê³µí†µ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ìƒì„±"""
    # TODO: config.py, logger.py, exceptions.py ìƒì„±
    pass


if __name__ == "__main__":
    create_project_structure()
    create_init_files()
    create_config_files()
    create_utility_modules()
    print("âœ… Project structure created successfully!")
`,
      solutionCode: `# í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
# setup_project.py

import os
from pathlib import Path

PROJECT_ROOT = Path("e2e_pipeline")

# ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ì˜
DIRECTORIES = [
    # ë©”ì¸ ì†ŒìŠ¤ ì½”ë“œ
    "src/e2e_pipeline",
    "src/e2e_pipeline/extractors",
    "src/e2e_pipeline/transformers",
    "src/e2e_pipeline/loaders",
    "src/e2e_pipeline/utils",
    "src/e2e_pipeline/models",
    "src/e2e_pipeline/validators",

    # Airflow DAGs
    "dags",
    "dags/utils",

    # Spark Jobs
    "spark_jobs/batch",
    "spark_jobs/streaming",

    # SQL ìŠ¤í¬ë¦½íŠ¸
    "sql/ddl",
    "sql/dml",
    "sql/queries",

    # ì„¤ì • íŒŒì¼
    "config/environments",

    # í…ŒìŠ¤íŠ¸
    "tests/unit/extractors",
    "tests/unit/transformers",
    "tests/unit/loaders",
    "tests/integration",
    "tests/fixtures",

    # ë¬¸ì„œ
    "docs/architecture",
    "docs/runbooks",

    # ìŠ¤í¬ë¦½íŠ¸
    "scripts",

    # ë°ì´í„° (ë¡œì»¬ ê°œë°œìš©)
    "data/raw",
    "data/processed",
]


def create_project_structure():
    """í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±"""
    for directory in DIRECTORIES:
        dir_path = PROJECT_ROOT / directory
        dir_path.mkdir(parents=True, exist_ok=True)
        print(f"Created: {dir_path}")


def create_init_files():
    """__init__.py íŒŒì¼ ìƒì„±"""
    python_dirs = [d for d in DIRECTORIES if "src/" in d or "tests/" in d]

    for directory in python_dirs:
        init_path = PROJECT_ROOT / directory / "__init__.py"
        if not init_path.exists():
            init_path.touch()
            print(f"Created: {init_path}")


def create_config_files():
    """ì„¤ì • íŒŒì¼ ìƒì„±"""

    # pyproject.toml
    pyproject_content = '''[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "e2e_pipeline"
version = "0.1.0"
description = "End-to-End Data Pipeline for SaaS Analytics"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}
authors = [
    {name = "Data Team", email = "data-team@company.com"}
]

dependencies = [
    "pyspark>=3.4.0",
    "apache-airflow>=2.8.0",
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "pyarrow>=14.0.0",
    "delta-spark>=2.4.0",
    "psycopg2-binary>=2.9.0",
    "boto3>=1.28.0",
    "requests>=2.31.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "python-dotenv>=1.0.0",
    "structlog>=23.1.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.11.0",
    "black>=23.7.0",
    "ruff>=0.0.280",
    "mypy>=1.4.0",
    "pre-commit>=3.3.0",
]

[tool.setuptools.packages.find]
where = ["src"]

[tool.black]
line-length = 88
target-version = ["py39", "py310", "py311"]

[tool.ruff]
line-length = 88
select = ["E", "F", "I", "N", "W"]

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true

[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "-v --cov=e2e_pipeline --cov-report=term-missing"
'''

    pyproject_path = PROJECT_ROOT / "pyproject.toml"
    pyproject_path.write_text(pyproject_content)
    print(f"Created: {pyproject_path}")

    # docker-compose.yml
    docker_compose_content = '''version: '3.8'

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile.airflow
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here
    - AIRFLOW__CORE__LOAD_EXAMPLES=false
    - AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key-here
    - AWS_ACCESS_KEY_ID=minioadmin
    - AWS_SECRET_ACCESS_KEY=minioadmin
    - AWS_ENDPOINT_URL=http://minio:9000
  volumes:
    - ./dags:/opt/airflow/dags
    - ./src:/opt/airflow/src
    - ./config:/opt/airflow/config
    - ./logs:/opt/airflow/logs
  depends_on:
    - postgres
    - minio

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/ddl:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  minio:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  spark-master:
    image: bitnami/spark:3.4
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./spark_jobs:/opt/spark-jobs
      - ./src:/opt/src

  spark-worker:
    image: bitnami/spark:3.4
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    depends_on:
      - spark-master

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8081:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db init
        airflow users create \\
          --username admin \\
          --password admin \\
          --firstname Admin \\
          --lastname User \\
          --role Admin \\
          --email admin@example.com

volumes:
  postgres_data:
  minio_data:
'''

    docker_compose_path = PROJECT_ROOT / "docker-compose.yml"
    docker_compose_path.write_text(docker_compose_content)
    print(f"Created: {docker_compose_path}")

    # .env.example
    env_example_content = '''# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=datawarehouse

# S3 / MinIO
AWS_ACCESS_KEY_ID=minioadmin
AWS_SECRET_ACCESS_KEY=minioadmin
AWS_ENDPOINT_URL=http://localhost:9000
S3_BUCKET=data-lake

# Spark
SPARK_MASTER=spark://localhost:7077

# Airflow
AIRFLOW__CORE__EXECUTOR=LocalExecutor

# API Keys
PAYMENT_API_KEY=your-api-key
PAYMENT_API_URL=https://api.payment-provider.com

# Environment
ENVIRONMENT=development
LOG_LEVEL=INFO
'''

    env_path = PROJECT_ROOT / ".env.example"
    env_path.write_text(env_example_content)
    print(f"Created: {env_path}")

    # .gitignore
    gitignore_content = '''# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
.env
.venv
env/
venv/

# IDE
.idea/
.vscode/
*.swp
*.swo

# Jupyter
.ipynb_checkpoints/

# Testing
.coverage
.pytest_cache/
htmlcov/

# Logs
logs/
*.log

# Data (local development)
data/raw/*
data/processed/*
!data/raw/.gitkeep
!data/processed/.gitkeep

# Secrets
.env
*.pem
*.key
credentials.json

# Airflow
airflow.db
airflow.cfg

# Spark
spark-warehouse/
metastore_db/
derby.log
'''

    gitignore_path = PROJECT_ROOT / ".gitignore"
    gitignore_path.write_text(gitignore_content)
    print(f"Created: {gitignore_path}")


def create_utility_modules():
    """ê³µí†µ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ìƒì„±"""

    # config.py
    config_content = '''"""Application configuration using Pydantic Settings."""

from functools import lru_cache
from typing import Optional

from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """Application settings loaded from environment variables."""

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
    )

    # Environment
    environment: str = Field(default="development")
    log_level: str = Field(default="INFO")

    # Database
    postgres_host: str = Field(default="localhost")
    postgres_port: int = Field(default=5432)
    postgres_user: str = Field(default="airflow")
    postgres_password: str = Field(default="airflow")
    postgres_db: str = Field(default="datawarehouse")

    # S3 / MinIO
    aws_access_key_id: str = Field(default="minioadmin")
    aws_secret_access_key: str = Field(default="minioadmin")
    aws_endpoint_url: Optional[str] = Field(default=None)
    s3_bucket: str = Field(default="data-lake")

    # Spark
    spark_master: str = Field(default="local[*]")

    # API
    payment_api_key: Optional[str] = Field(default=None)
    payment_api_url: str = Field(default="https://api.payment-provider.com")

    @property
    def postgres_url(self) -> str:
        """Generate PostgreSQL connection URL."""
        return (
            f"postgresql://{self.postgres_user}:{self.postgres_password}"
            f"@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"
        )

    @property
    def is_production(self) -> bool:
        """Check if running in production."""
        return self.environment == "production"


@lru_cache
def get_settings() -> Settings:
    """Get cached settings instance."""
    return Settings()


# Convenience alias
settings = get_settings()
'''

    config_path = PROJECT_ROOT / "src/e2e_pipeline/config.py"
    config_path.write_text(config_content)
    print(f"Created: {config_path}")

    # logger.py
    logger_content = '''"""Structured logging configuration."""

import logging
import sys
from typing import Any

import structlog


def setup_logging(log_level: str = "INFO") -> None:
    """Configure structured logging for the application."""

    # Configure structlog
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer() if log_level != "DEBUG"
                else structlog.dev.ConsoleRenderer(),
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

    # Configure standard logging
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, log_level.upper()),
    )


def get_logger(name: str) -> Any:
    """Get a structured logger instance."""
    return structlog.get_logger(name)


# Initialize logging on module import
setup_logging()
'''

    logger_path = PROJECT_ROOT / "src/e2e_pipeline/utils/logger.py"
    logger_path.write_text(logger_content)
    print(f"Created: {logger_path}")

    # exceptions.py
    exceptions_content = '''"""Custom exceptions for the E2E Pipeline."""


class PipelineError(Exception):
    """Base exception for pipeline errors."""
    pass


class ExtractionError(PipelineError):
    """Error during data extraction."""
    pass


class TransformationError(PipelineError):
    """Error during data transformation."""
    pass


class LoadingError(PipelineError):
    """Error during data loading."""
    pass


class ValidationError(PipelineError):
    """Error during data validation."""
    pass


class ConfigurationError(PipelineError):
    """Error in configuration."""
    pass


class ConnectionError(PipelineError):
    """Error connecting to external service."""
    pass


class DataQualityError(PipelineError):
    """Data quality check failed."""

    def __init__(self, message: str, failed_checks: list = None):
        super().__init__(message)
        self.failed_checks = failed_checks or []
'''

    exceptions_path = PROJECT_ROOT / "src/e2e_pipeline/utils/exceptions.py"
    exceptions_path.write_text(exceptions_content)
    print(f"Created: {exceptions_path}")

    # README.md
    readme_content = '''# E2E Data Pipeline

End-to-End Data Pipeline for SaaS Analytics

## Overview

This project implements a complete data pipeline that:
- Extracts data from PostgreSQL, S3, and REST APIs
- Transforms data using Apache Spark
- Loads data into a Star Schema data warehouse
- Orchestrates workflows with Apache Airflow

## Architecture

\`\`\`
Sources â†’ Ingestion â†’ Raw Layer â†’ Transformation â†’ DW â†’ Data Marts
\`\`\`

## Quick Start

\`\`\`bash
# Clone and setup
git clone <repo>
cd e2e_pipeline

# Start infrastructure
docker-compose up -d

# Install dependencies
pip install -e ".[dev]"

# Run tests
pytest
\`\`\`

## Project Structure

\`\`\`
e2e_pipeline/
â”œâ”€â”€ src/e2e_pipeline/     # Main source code
â”‚   â”œâ”€â”€ extractors/       # Data extraction modules
â”‚   â”œâ”€â”€ transformers/     # Spark transformation jobs
â”‚   â”œâ”€â”€ loaders/          # Data loading modules
â”‚   â””â”€â”€ utils/            # Utilities and helpers
â”œâ”€â”€ dags/                 # Airflow DAGs
â”œâ”€â”€ spark_jobs/           # Spark job scripts
â”œâ”€â”€ sql/                  # SQL scripts
â”œâ”€â”€ tests/                # Test suite
â”œâ”€â”€ docs/                 # Documentation
â””â”€â”€ config/               # Configuration files
\`\`\`

## Development

\`\`\`bash
# Format code
black src/ tests/

# Lint
ruff check src/ tests/

# Type check
mypy src/
\`\`\`

## License

MIT
'''

    readme_path = PROJECT_ROOT / "README.md"
    readme_path.write_text(readme_content)
    print(f"Created: {readme_path}")

    # Create .gitkeep files for empty directories
    gitkeep_dirs = ["data/raw", "data/processed"]
    for directory in gitkeep_dirs:
        gitkeep_path = PROJECT_ROOT / directory / ".gitkeep"
        gitkeep_path.touch()


if __name__ == "__main__":
    create_project_structure()
    create_init_files()
    create_config_files()
    create_utility_modules()
    print("\\nâœ… Project structure created successfully!")
    print(f"\\nNext steps:")
    print(f"1. cd {PROJECT_ROOT}")
    print(f"2. python -m venv .venv && source .venv/bin/activate")
    print(f"3. pip install -e '.[dev]'")
    print(f"4. docker-compose up -d")
`,
      hints: [
        "pathlib.Pathë¥¼ ì‚¬ìš©í•˜ë©´ OS ë…ë¦½ì ì¸ ê²½ë¡œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤",
        "pydantic-settingsë¥¼ ì‚¬ìš©í•˜ë©´ í™˜ê²½ ë³€ìˆ˜ë¥¼ íƒ€ì… ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        "structlogëŠ” JSON í˜•ì‹ì˜ êµ¬ì¡°í™”ëœ ë¡œê¹…ì„ ì œê³µí•©ë‹ˆë‹¤",
        "docker-composeì˜ x-ë¡œ ì‹œì‘í•˜ëŠ” í‚¤ëŠ” YAML ì•µì»¤ë¡œ ê³µí†µ ì„¤ì •ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w8d1t4',
    title: 'ì•„í‚¤í…ì²˜ ë¬¸ì„œ ì‘ì„±',
    type: 'code',
    duration: 30,
    content: {
      instructions: `## ì•„í‚¤í…ì²˜ ë¬¸ì„œ ì‘ì„± (ADR í˜•ì‹)

Architecture Decision Records (ADR) í˜•ì‹ìœ¼ë¡œ ì£¼ìš” ì„¤ê³„ ê²°ì •ì„ ë¬¸ì„œí™”í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **ADR í…œí”Œë¦¿ ì‘ì„±**
   - ì œëª©, ìƒíƒœ, ì»¨í…ìŠ¤íŠ¸, ê²°ì •, ê²°ê³¼

2. **ì£¼ìš” ê²°ì • ì‚¬í•­ ë¬¸ì„œí™”**
   - ë°ì´í„° ë ˆì´í¬ êµ¬ì¡° (Delta Lake)
   - ìŠ¤í‚¤ë§ˆ ì„¤ê³„ (Star Schema)
   - ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ (Airflow)

3. **ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨**
   - ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
   - ë°ì´í„° íë¦„
`,
      starterCode: `# docs/architecture/adr-template.md

"""
# ADR-XXX: [ì œëª©]

## ìƒíƒœ
[Proposed | Accepted | Deprecated | Superseded]

## ì»¨í…ìŠ¤íŠ¸
[ê²°ì •ì´ í•„ìš”í•œ ìƒí™©ê³¼ ë°°ê²½]

## ê²°ì •
[ë‚´ë¦° ê²°ì •ê³¼ ê·¸ ì´ìœ ]

## ê²°ê³¼
[ì˜ˆìƒë˜ëŠ” ê²°ê³¼ì™€ íŠ¸ë ˆì´ë“œì˜¤í”„]

## ì°¸ê³ 
[ê´€ë ¨ ë¬¸ì„œ, ë§í¬]
"""

# TODO: ADR-001: ë°ì´í„° ë ˆì´í¬ êµ¬ì¡° ì‘ì„±
# TODO: ADR-002: ìŠ¤í‚¤ë§ˆ ì„¤ê³„ ë°©ë²•ë¡  ì‘ì„±
# TODO: ADR-003: ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ ì„ íƒ ì‘ì„±
`,
      solutionCode: `# docs/architecture/adr-001-data-lake-structure.md
ADR_001 = """
# ADR-001: Delta Lake ê¸°ë°˜ ë°ì´í„° ë ˆì´í¬ êµ¬ì¡°

## ìƒíƒœ
Accepted

## ì»¨í…ìŠ¤íŠ¸
SaaS ë¶„ì„ í”Œë«í¼ì„ ìœ„í•œ ë°ì´í„° ë ˆì´í¬ë¥¼ êµ¬ì¶•í•´ì•¼ í•©ë‹ˆë‹¤.
ìš”êµ¬ì‚¬í•­:
- ëŒ€ìš©ëŸ‰ ì´ë²¤íŠ¸ ë°ì´í„° (ì¼ 1ì–µ ê±´) ì²˜ë¦¬
- ACID íŠ¸ëœì­ì…˜ ì§€ì› (ë°ì´í„° ì •í•©ì„±)
- ì‹œê°„ ì—¬í–‰ (Time Travel) ê¸°ëŠ¥
- ìŠ¤í‚¤ë§ˆ ì§„í™” ì§€ì›
- ë¹„ìš© íš¨ìœ¨ì ì¸ ìŠ¤í† ë¦¬ì§€

ê³ ë ¤ëœ ëŒ€ì•ˆ:
1. Apache Hive + Parquet
2. Apache Iceberg
3. Delta Lake
4. Apache Hudi

## ê²°ì •
**Delta Lake**ë¥¼ ë°ì´í„° ë ˆì´í¬ í¬ë§·ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.

ì´ìœ :
1. **Spark ë„¤ì´í‹°ë¸Œ í†µí•©**: PySparkì™€ ì™„ë²½í•œ í˜¸í™˜ì„±
2. **ACID íŠ¸ëœì­ì…˜**: ë™ì‹œ ì“°ê¸°/ì½ê¸° ì‹œ ë°ì´í„° ì •í•©ì„± ë³´ì¥
3. **Time Travel**: íŠ¹ì • ì‹œì  ë°ì´í„° ì¡°íšŒ, ë¡¤ë°± ê°€ëŠ¥
4. **ìŠ¤í‚¤ë§ˆ ì§„í™”**: ì»¬ëŸ¼ ì¶”ê°€/ë³€ê²½ ì‹œ í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€
5. **OPTIMIZE & Z-ORDER**: ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™” ë‚´ì¥
6. **ì»¤ë®¤ë‹ˆí‹° & ë¬¸ì„œ**: í™œë°œí•œ ìƒíƒœê³„, í’ë¶€í•œ ë ˆí¼ëŸ°ìŠ¤

## ê²°ê³¼

### ê¸ì •ì 
- ë°ì´í„° ë ˆì´í¬ì˜ ACID ë³´ì¥
- ì¦ë¶„ ì²˜ë¦¬ (MERGE) ìš©ì´
- ìš´ì˜ ë³µì¡ë„ ê°ì†Œ (ë¡¤ë°±, ê°ì‚¬ ë¡œê·¸)

### ë¶€ì •ì 
- Delta Lake ë²„ì „ ê´€ë¦¬ í•„ìš”
- ì¼ë¶€ BI ë„êµ¬ì™€ì˜ í˜¸í™˜ì„± í™•ì¸ í•„ìš”
- ë©”íƒ€ë°ì´í„° ì˜¤ë²„í—¤ë“œ (ì‘ì€ íŒŒì¼ ë¬¸ì œ)

### ì™„í™” ë°©ì•ˆ
- Auto Compaction í™œì„±í™”
- ì£¼ê¸°ì  OPTIMIZE ì‹¤í–‰
- BI ë„êµ¬ìš© ë·° ë ˆì´ì–´ êµ¬ì„±

## ë ˆì´ì–´ êµ¬ì¡°

\`\`\`
s3://data-lake/
â”œâ”€â”€ raw/                    # Bronze Layer
â”‚   â”œâ”€â”€ users/
â”‚   â”‚   â””â”€â”€ dt=YYYY-MM-DD/
â”‚   â”œâ”€â”€ events/
â”‚   â”‚   â””â”€â”€ dt=YYYY-MM-DD/hour=HH/
â”‚   â””â”€â”€ payments/
â”‚       â””â”€â”€ dt=YYYY-MM-DD/
â”œâ”€â”€ staging/                # Silver Layer (Delta)
â”‚   â”œâ”€â”€ stg_users/
â”‚   â”œâ”€â”€ stg_events/
â”‚   â””â”€â”€ stg_payments/
â”œâ”€â”€ warehouse/              # Gold Layer (Delta)
â”‚   â”œâ”€â”€ dim_user/
â”‚   â”œâ”€â”€ dim_date/
â”‚   â”œâ”€â”€ fact_events/
â”‚   â””â”€â”€ fact_payments/
â””â”€â”€ marts/                  # Platinum Layer (Delta)
    â”œâ”€â”€ marketing/
    â”œâ”€â”€ product/
    â””â”€â”€ finance/
\`\`\`

## ì°¸ê³ 
- [Delta Lake Documentation](https://docs.delta.io/)
- [Databricks Lakehouse Architecture](https://www.databricks.com/glossary/data-lakehouse)
"""

# docs/architecture/adr-002-dimensional-modeling.md
ADR_002 = """
# ADR-002: Kimball Star Schema ê¸°ë°˜ ì°¨ì› ëª¨ë¸ë§

## ìƒíƒœ
Accepted

## ì»¨í…ìŠ¤íŠ¸
ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì˜ ìŠ¤í‚¤ë§ˆ ì„¤ê³„ ë°©ë²•ë¡ ì„ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

ìš”êµ¬ì‚¬í•­:
- ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€ì˜ ì…€í”„ ì„œë¹„ìŠ¤ ë¶„ì„
- BI ë„êµ¬ (Tableau, Looker) í˜¸í™˜ì„±
- ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™”
- ì ì§„ì  í™•ì¥ ê°€ëŠ¥

ê³ ë ¤ëœ ëŒ€ì•ˆ:
1. Kimball Star Schema
2. Inmon 3NF (ì •ê·œí™”)
3. Data Vault 2.0
4. Wide Table (ë¹„ì •ê·œí™”)

## ê²°ì •
**Kimball Star Schema**ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

ì´ìœ :
1. **ë¶„ì„ ì¹œí™”ì **: BI ë„êµ¬ì™€ ìµœì ì˜ í˜¸í™˜ì„±
2. **ì¿¼ë¦¬ ë‹¨ìˆœì„±**: ì¡°ì¸ ìµœì†Œí™”ë¡œ ë¹ ë¥¸ ì§‘ê³„
3. **ë¹„ì¦ˆë‹ˆìŠ¤ í”„ë¡œì„¸ìŠ¤ ì¤‘ì‹¬**: ì´í•´í•˜ê¸° ì‰¬ìš´ ëª¨ë¸
4. **ì ì§„ì  êµ¬ì¶•**: ì£¼ì œ ì˜ì—­ë³„ ë…ë¦½ ê°œë°œ ê°€ëŠ¥
5. **ê²€ì¦ëœ ë°©ë²•ë¡ **: 30ë…„ ì´ìƒì˜ ì‚°ì—… í‘œì¤€

## ê²°ê³¼

### ìŠ¤í‚¤ë§ˆ êµ¬ì„±

Fact Tables:
- fact_events: ì‚¬ìš©ì í–‰ë™ ì´ë²¤íŠ¸ (Grain: ê°œë³„ ì´ë²¤íŠ¸)
- fact_subscriptions: êµ¬ë… ìƒíƒœ (Grain: êµ¬ë… ë³€ê²½)
- fact_payments: ê²°ì œ (Grain: ê°œë³„ ê²°ì œ)

Dimension Tables:
- dim_user: SCD Type 2 (íˆìŠ¤í† ë¦¬ ì¶”ì )
- dim_date: ë‚ ì§œ ê³„ì¸µ
- dim_plan: êµ¬ë… í”Œëœ
- dim_event_type: ì´ë²¤íŠ¸ ìœ í˜• ê³„ì¸µ

### SCD (Slowly Changing Dimension) ì „ëµ

| Dimension | SCD Type | ì´ìœ  |
|-----------|----------|------|
| dim_user | Type 2 | ì‚¬ìš©ì ì„¸ê·¸ë¨¼íŠ¸ ë³€ê²½ ì´ë ¥ í•„ìš” |
| dim_plan | Type 1 | ìµœì‹  ê°€ê²©ë§Œ í•„ìš” |
| dim_date | N/A | ì •ì  ë°ì´í„° |
| dim_event_type | Type 1 | ë³€ê²½ ë¹ˆë„ ë‚®ìŒ |

### íŠ¸ë ˆì´ë“œì˜¤í”„

ê¸ì •ì :
- ë¶„ì„ê°€ ìƒì‚°ì„± í–¥ìƒ
- ì¿¼ë¦¬ ì„±ëŠ¥ ìš°ìˆ˜
- ëª…í™•í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸

ë¶€ì •ì :
- ë°ì´í„° ì¤‘ë³µ (ì €ì¥ ê³µê°„ ì¦ê°€)
- ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ì œí•œì 
- ë³µì¡í•œ ë³€í™˜ ë¡œì§ í•„ìš”

## ì°¸ê³ 
- [The Data Warehouse Toolkit](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/)
"""

# docs/architecture/adr-003-orchestration.md
ADR_003 = """
# ADR-003: Apache Airflow ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

## ìƒíƒœ
Accepted

## ì»¨í…ìŠ¤íŠ¸
ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.

ìš”êµ¬ì‚¬í•­:
- DAG (Directed Acyclic Graph) ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°
- Python ë„¤ì´í‹°ë¸Œ ì§€ì›
- ìŠ¤ì¼€ì¤„ë§ ë° ì˜ì¡´ì„± ê´€ë¦¬
- ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
- í™•ì¥ì„± (ìˆ˜ë°± ê°œ DAG)

ê³ ë ¤ëœ ëŒ€ì•ˆ:
1. Apache Airflow
2. Prefect
3. Dagster
4. Luigi
5. AWS Step Functions

## ê²°ì •
**Apache Airflow 2.8+**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.

ì´ìœ :
1. **ì‚°ì—… í‘œì¤€**: ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬
2. **í’ë¶€í•œ Operator**: AWS, GCP, Spark ë“± ë‹¤ì–‘í•œ í†µí•©
3. **TaskFlow API**: í˜„ëŒ€ì ì¸ Python ë°ì½”ë ˆì´í„° ê¸°ë°˜ DAG
4. **í™œë°œí•œ ì»¤ë®¤ë‹ˆí‹°**: ë¬¸ì œ í•´ê²° ë¦¬ì†ŒìŠ¤ í’ë¶€
5. **ê´€ë¦¬í˜• ì„œë¹„ìŠ¤**: MWAA, Cloud Composer ë“± ì˜µì…˜

ë²„ì „ ì„ íƒ ì´ìœ  (2.8+):
- Dataset ê¸°ë°˜ DAG íŠ¸ë¦¬ê±°
- Dynamic Task Mapping ê°œì„ 
- ë³´ì•ˆ íŒ¨ì¹˜ ë° ì•ˆì •ì„±

## ê²°ê³¼

### DAG êµ¬ì¡°

\`\`\`
e2e_pipeline/
â”œâ”€â”€ dag_extract_users.py      # ì‚¬ìš©ì ë°ì´í„° ì¶”ì¶œ
â”œâ”€â”€ dag_extract_events.py     # ì´ë²¤íŠ¸ ë°ì´í„° ì¶”ì¶œ
â”œâ”€â”€ dag_extract_payments.py   # ê²°ì œ ë°ì´í„° ì¶”ì¶œ
â”œâ”€â”€ dag_transform_staging.py  # Staging ë³€í™˜
â”œâ”€â”€ dag_transform_warehouse.py # DW ì ì¬
â”œâ”€â”€ dag_aggregate_marts.py    # ì§‘ê³„ í…Œì´ë¸” ìƒì„±
â””â”€â”€ dag_daily_pipeline.py     # í†µí•© DAG
\`\`\`

### ìŠ¤ì¼€ì¤„ ì „ëµ

| DAG | ìŠ¤ì¼€ì¤„ | ì˜ì¡´ì„± |
|-----|--------|--------|
| extract_* | 02:00 UTC | ì—†ìŒ |
| transform_staging | Dataset | extract_* ì™„ë£Œ |
| transform_warehouse | Dataset | staging ì™„ë£Œ |
| aggregate_marts | 06:00 UTC | warehouse ì™„ë£Œ |

### íŠ¸ë ˆì´ë“œì˜¤í”„

ê¸ì •ì :
- ê²€ì¦ëœ ì•ˆì •ì„±
- í’ë¶€í•œ ëª¨ë‹ˆí„°ë§ UI
- ë‹¤ì–‘í•œ ë°°í¬ ì˜µì…˜

ë¶€ì •ì :
- ìŠ¤ì¼€ì¤„ëŸ¬ ë‹¨ì¼ ì¥ì• ì  (HA êµ¬ì„± í•„ìš”)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ
- ëŸ¬ë‹ ì»¤ë¸Œ

### ì™„í™” ë°©ì•ˆ
- Celery/Kubernetes Executorë¡œ í™•ì¥
- ì£¼ê¸°ì  DAG ì •ë¦¬ (30ì¼ ì´ìƒ ê¸°ë¡ ì‚­ì œ)
- ì•Œë¦¼ ì„¤ì • (Slack, PagerDuty)

## ì°¸ê³ 
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Astronomer Guides](https://www.astronomer.io/guides/)
"""

def write_adr_files():
    """Write ADR files to docs directory."""
    from pathlib import Path

    docs_dir = Path("e2e_pipeline/docs/architecture")
    docs_dir.mkdir(parents=True, exist_ok=True)

    (docs_dir / "adr-001-data-lake-structure.md").write_text(ADR_001.strip())
    (docs_dir / "adr-002-dimensional-modeling.md").write_text(ADR_002.strip())
    (docs_dir / "adr-003-orchestration.md").write_text(ADR_003.strip())

    print("âœ… ADR documents created successfully!")

if __name__ == "__main__":
    write_adr_files()
`,
      hints: [
        "ADRì€ ë‚˜ì¤‘ì— 'ì™œ ì´ë ‡ê²Œ ê²°ì •í–ˆì§€?'ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë¬¸ì„œì…ë‹ˆë‹¤",
        "ê³ ë ¤ëœ ëŒ€ì•ˆê³¼ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ëª…ì‹œí•˜ë©´ ê²°ì •ì˜ ë§¥ë½ì„ ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤",
        "ìƒíƒœ í•„ë“œë¡œ ê²°ì •ì´ ì•„ì§ ìœ íš¨í•œì§€ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤",
        "Markdown í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ë©´ GitHubì—ì„œ ë°”ë¡œ ë Œë”ë§ë©ë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w8d1t5',
    title: 'Day 1 ì„¤ê³„ ê²€í†  í€´ì¦ˆ',
    type: 'quiz',
    duration: 15,
    content: {
      questions: [
        {
          question: 'Star Schemaì—ì„œ Fact í…Œì´ë¸”ê³¼ Dimension í…Œì´ë¸”ì˜ ê´€ê³„ëŠ”?',
          options: [
            'Factê°€ Dimensionì„ ì°¸ì¡°í•œë‹¤ (Many-to-One)',
            'Dimensionì´ Factë¥¼ ì°¸ì¡°í•œë‹¤ (One-to-Many)',
            'Factì™€ Dimensionì€ Many-to-Many ê´€ê³„',
            'Factì™€ Dimensionì€ ë…ë¦½ì ì´ë‹¤'
          ],
          answer: 0,
          explanation: 'Star Schemaì—ì„œ ì¤‘ì•™ì˜ Fact í…Œì´ë¸”ì´ ì£¼ë³€ì˜ Dimension í…Œì´ë¸”ì„ ì™¸ë˜í‚¤ë¡œ ì°¸ì¡°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë³„ ëª¨ì–‘ì˜ êµ¬ì¡°ê°€ í˜•ì„±ë©ë‹ˆë‹¤.'
        },
        {
          question: 'SCD Type 2ì˜ íŠ¹ì§•ì€?',
          options: [
            'ë³€ê²½ ì‹œ ê¸°ì¡´ ê°’ì„ ë®ì–´ì“´ë‹¤',
            'ë³€ê²½ ì´ë ¥ì„ ìƒˆë¡œìš´ í–‰ìœ¼ë¡œ ì¶”ê°€í•œë‹¤',
            'ë³€ê²½ëœ ê°’ë§Œ ë³„ë„ í…Œì´ë¸”ì— ì €ì¥í•œë‹¤',
            'ë³€ê²½ì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ”ë‹¤'
          ],
          answer: 1,
          explanation: 'SCD Type 2ëŠ” ë³€ê²½ì´ ë°œìƒí•  ë•Œ ê¸°ì¡´ í–‰ì„ ë§Œë£Œì‹œí‚¤ê³  ìƒˆë¡œìš´ í–‰ì„ ì¶”ê°€í•˜ì—¬ ì „ì²´ ì´ë ¥ì„ ë³´ì¡´í•©ë‹ˆë‹¤. effective_date, expiration_date, is_current ì»¬ëŸ¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.'
        },
        {
          question: 'Delta Lakeì˜ ì£¼ìš” ì¥ì ì´ ì•„ë‹Œ ê²ƒì€?',
          options: [
            'ACID íŠ¸ëœì­ì…˜ ì§€ì›',
            'Time Travel (ë²„ì „ ê´€ë¦¬)',
            'ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬',
            'ìŠ¤í‚¤ë§ˆ ì§„í™” ì§€ì›'
          ],
          answer: 2,
          explanation: 'Delta LakeëŠ” ë°°ì¹˜ ì²˜ë¦¬ì— ìµœì í™”ëœ ë ˆì´í¬í•˜ìš°ìŠ¤ í¬ë§·ì…ë‹ˆë‹¤. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì€ Kafka, Flink ë“±ì´ ë” ì í•©í•©ë‹ˆë‹¤. Delta LakeëŠ” Structured Streamingê³¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, ìˆœìˆ˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ëŠ” ì•„ë‹™ë‹ˆë‹¤.'
        },
        {
          question: 'ADR (Architecture Decision Record)ì— ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•˜ëŠ” í•­ëª©ì€?',
          options: [
            'êµ¬í˜„ ì½”ë“œ',
            'ê²°ì •ì˜ ì»¨í…ìŠ¤íŠ¸ì™€ ì´ìœ ',
            'í…ŒìŠ¤íŠ¸ ê²°ê³¼',
            'ë°°í¬ ì¼ì •'
          ],
          answer: 1,
          explanation: 'ADRì˜ í•µì‹¬ì€ "ì™œ ì´ ê²°ì •ì„ ë‚´ë ¸ëŠ”ê°€"ë¥¼ ê¸°ë¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸(ìƒí™©), ê²°ì •(ì„ íƒ), ê²°ê³¼(íŠ¸ë ˆì´ë“œì˜¤í”„)ê°€ í•„ìˆ˜ í•­ëª©ì…ë‹ˆë‹¤.'
        },
        {
          question: 'ë°ì´í„° ë ˆì´í¬ì˜ Bronze-Silver-Gold ë ˆì´ì–´ êµ¬ì¡°ì—ì„œ Silver ë ˆì´ì–´ì˜ ì—­í• ì€?',
          options: [
            'ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥',
            'ì •ì œ ë° í‘œì¤€í™”ëœ ë°ì´í„° ì €ì¥',
            'ë¹„ì¦ˆë‹ˆìŠ¤ ì§‘ê³„ ë°ì´í„° ì €ì¥',
            'ì™¸ë¶€ ì‹œìŠ¤í…œì— ì œê³µí•˜ëŠ” API ë°ì´í„°'
          ],
          answer: 1,
          explanation: 'BronzeëŠ” ì›ë³¸ ë°ì´í„°, SilverëŠ” ì •ì œ/í‘œì¤€í™”ëœ ë°ì´í„°, GoldëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì§‘ê³„/ë§ˆíŠ¸ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. Silver ë ˆì´ì–´ì—ì„œ ì¤‘ë³µ ì œê±°, ìŠ¤í‚¤ë§ˆ ì •ê·œí™”, ë°ì´í„° íƒ€ì… ë³€í™˜ ë“±ì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.'
        }
      ]
    }
  }
]
