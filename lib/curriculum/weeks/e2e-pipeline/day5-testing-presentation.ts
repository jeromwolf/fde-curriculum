import { Task } from '../../types'

export const day5Tasks: Task[] = [
  {
    id: 'w8d5t1',
    title: 'í†µí•© í…ŒìŠ¤íŠ¸ ì „ëµ',
    type: 'reading',
    duration: 25,
    content: {
      markdown: `# í†µí•© í…ŒìŠ¤íŠ¸ ì „ëµ

## ğŸ¯ í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ

\`\`\`
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  E2E    â”‚  ì ìŒ, ëŠë¦¼, ë¹„ìš© ë†’ìŒ
          â”Œâ”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”
          â”‚Integrationâ”‚
         â”Œâ”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”
         â”‚    Unit     â”‚  ë§ìŒ, ë¹ ë¦„, ë¹„ìš© ë‚®ìŒ
        â”Œâ”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”
        â”‚  Static Check â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## ğŸ“Š í…ŒìŠ¤íŠ¸ ê³„ì¸µë³„ ì „ëµ

### 1. Static Checks (ì •ì  ë¶„ì„)
- **Black/Ruff**: ì½”ë“œ í¬ë§¤íŒ… & ë¦°íŒ…
- **mypy**: íƒ€ì… ì²´í¬
- **pre-commit hooks**: ì»¤ë°‹ ì „ ìë™ ê²€ì‚¬

\`\`\`yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/psf/black
    rev: 23.7.0
    hooks:
      - id: black
  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.0.280
    hooks:
      - id: ruff
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.4.0
    hooks:
      - id: mypy
\`\`\`

### 2. Unit Tests (ë‹¨ìœ„ í…ŒìŠ¤íŠ¸)
- ê°œë³„ í•¨ìˆ˜/í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸
- Mockì„ ì‚¬ìš©í•œ ì™¸ë¶€ ì˜ì¡´ì„± ê²©ë¦¬
- ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ ëª©í‘œ

\`\`\`python
# tests/unit/transformers/test_staging_users.py

import pytest
from pyspark.sql import SparkSession
from e2e_pipeline.transformers.staging.users import StagingUserTransformer


class TestStagingUserTransformer:
    @pytest.fixture(scope="session")
    def spark(self):
        return SparkSession.builder \\
            .master("local[2]") \\
            .appName("test") \\
            .getOrCreate()

    def test_normalize_email(self, spark):
        # Given
        data = [{"user_id": 1, "email": "  TEST@EXAMPLE.COM  "}]
        df = spark.createDataFrame(data)

        # When
        transformer = StagingUserTransformer(spark, {})
        result = transformer.transform(df)

        # Then
        assert result.first()["email"] == "test@example.com"

    def test_deduplicate_users(self, spark):
        # Given
        data = [
            {"user_id": 1, "email": "a@test.com", "updated_at": "2024-01-01"},
            {"user_id": 1, "email": "b@test.com", "updated_at": "2024-01-02"},
        ]
        df = spark.createDataFrame(data)

        # When
        transformer = StagingUserTransformer(spark, {})
        result = transformer.transform(df)

        # Then
        assert result.count() == 1
        assert result.first()["email"] == "b@test.com"
\`\`\`

### 3. Integration Tests (í†µí•© í…ŒìŠ¤íŠ¸)
- ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ ê°„ ìƒí˜¸ì‘ìš© í…ŒìŠ¤íŠ¸
- ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤/S3 ì‚¬ìš© (í…ŒìŠ¤íŠ¸ í™˜ê²½)
- Docker Composeë¡œ í™˜ê²½ êµ¬ì„±

\`\`\`python
# tests/integration/test_extract_transform_flow.py

import pytest
from datetime import date

class TestExtractTransformFlow:
    @pytest.fixture(scope="class")
    def test_environment(self, docker_compose_up):
        """Start test environment with Docker Compose."""
        yield

    def test_extract_to_staging(self, test_environment, spark):
        # Given: Raw data in S3
        raw_data = [{"user_id": 1, "email": "test@example.com"}]
        save_to_s3(raw_data, "raw/users/dt=2024-01-01")

        # When: Run extract and transform
        run_extraction_dag("2024-01-01")
        run_transformation_dag("2024-01-01")

        # Then: Verify staging data
        staging = spark.read.format("delta").load("staging/stg_users")
        assert staging.count() == 1
\`\`\`

### 4. E2E Tests (End-to-End í…ŒìŠ¤íŠ¸)
- ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
- í”„ë¡œë•ì…˜ ìœ ì‚¬ í™˜ê²½
- ì¼ì¼ ë°°ì¹˜ ì‹œë®¬ë ˆì´ì…˜

\`\`\`python
# tests/e2e/test_daily_pipeline.py

import pytest
from airflow.models import DagBag

class TestDailyPipeline:
    def test_daily_pipeline_e2e(self, airflow_test_env):
        # Given: Test data prepared
        prepare_test_data("2024-01-01")

        # When: Trigger daily pipeline
        dagbag = DagBag()
        dag = dagbag.get_dag("daily_pipeline")
        dag.test(execution_date="2024-01-01")

        # Then: Verify all tables populated
        assert_table_has_data("staging/stg_users")
        assert_table_has_data("warehouse/dim_user")
        assert_table_has_data("warehouse/fact_events")
        assert_quality_checks_passed("2024-01-01")
\`\`\`

## ğŸ”§ í…ŒìŠ¤íŠ¸ í”½ìŠ¤ì²˜

### conftest.py
\`\`\`python
# tests/conftest.py

import pytest
from pyspark.sql import SparkSession
import docker

@pytest.fixture(scope="session")
def spark():
    """Create Spark session for tests."""
    return SparkSession.builder \\
        .master("local[2]") \\
        .appName("pytest") \\
        .config("spark.sql.shuffle.partitions", "2") \\
        .getOrCreate()

@pytest.fixture(scope="session")
def docker_compose_up():
    """Start Docker Compose services."""
    import subprocess
    subprocess.run(["docker-compose", "-f", "docker-compose.test.yml", "up", "-d"])
    yield
    subprocess.run(["docker-compose", "-f", "docker-compose.test.yml", "down"])

@pytest.fixture
def sample_users(spark):
    """Create sample user data."""
    return spark.createDataFrame([
        {"user_id": 1, "email": "alice@example.com", "name": "Alice"},
        {"user_id": 2, "email": "bob@example.com", "name": "Bob"},
    ])
\`\`\`

## ğŸ“ˆ ì»¤ë²„ë¦¬ì§€ ëª©í‘œ

| ê³„ì¸µ | ëª©í‘œ ì»¤ë²„ë¦¬ì§€ | ìš°ì„ ìˆœìœ„ |
|------|--------------|----------|
| Transformers | 90% | ë†’ìŒ |
| Extractors | 80% | ë†’ìŒ |
| DAGs | 70% | ì¤‘ê°„ |
| Utils | 80% | ì¤‘ê°„ |

## ğŸš€ CI/CD í†µí•©

\`\`\`yaml
# .github/workflows/test.yml

name: Test Pipeline

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Run static checks
        run: |
          black --check src/ tests/
          ruff check src/ tests/
          mypy src/

      - name: Run unit tests
        run: pytest tests/unit/ -v --cov=e2e_pipeline --cov-report=xml

      - name: Run integration tests
        run: |
          docker-compose -f docker-compose.test.yml up -d
          pytest tests/integration/ -v
          docker-compose -f docker-compose.test.yml down

      - name: Upload coverage
        uses: codecov/codecov-action@v3
\`\`\`
`,
      externalLinks: [
        { title: 'pytest Documentation', url: 'https://docs.pytest.org/' },
        { title: 'Testing Spark Applications', url: 'https://spark.apache.org/docs/latest/testing-spark.html' },
        { title: 'Airflow Testing Guide', url: 'https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#testing-a-dag' }
      ]
    }
  },
  {
    id: 'w8d5t2',
    title: 'ë¬¸ì„œí™” & ë°œí‘œ ìë£Œ ì‘ì„±',
    type: 'reading',
    duration: 30,
    content: {
      markdown: `# ë¬¸ì„œí™” & ë°œí‘œ ìë£Œ ì‘ì„±

## ğŸ“š í”„ë¡œì íŠ¸ ë¬¸ì„œ êµ¬ì¡°

\`\`\`
docs/
â”œâ”€â”€ README.md              # í”„ë¡œì íŠ¸ ê°œìš”, ë¹ ë¥¸ ì‹œì‘
â”œâ”€â”€ ARCHITECTURE.md        # ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
â”œâ”€â”€ SETUP.md               # í™˜ê²½ ì„¤ì • ê°€ì´ë“œ
â”œâ”€â”€ DATA_DICTIONARY.md     # í…Œì´ë¸”/ì»¬ëŸ¼ ì •ì˜
â”œâ”€â”€ RUNBOOKS.md            # ìš´ì˜ ê°€ì´ë“œ
â””â”€â”€ architecture/
    â”œâ”€â”€ adr-001-*.md       # Architecture Decision Records
    â”œâ”€â”€ adr-002-*.md
    â””â”€â”€ diagrams/
        â”œâ”€â”€ system-overview.png
        â””â”€â”€ data-flow.png
\`\`\`

## ğŸ“– README.md í…œí”Œë¦¿

\`\`\`markdown
# E2E Data Pipeline

SaaS ë¶„ì„ í”Œë«í¼ì„ ìœ„í•œ End-to-End ë°ì´í„° íŒŒì´í”„ë¼ì¸

## ğŸ¯ Overview

- ì¼ì¼ 1ì–µ ê±´ ì´ë²¤íŠ¸ ì²˜ë¦¬
- PostgreSQL, S3, REST APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
- Star Schema ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤
- Airflow ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

## ğŸ—ï¸ Architecture

![System Architecture](docs/architecture/diagrams/system-overview.png)

### Tech Stack
| Component | Technology |
|-----------|------------|
| Orchestration | Apache Airflow 2.8 |
| Processing | Apache Spark 3.4 |
| Storage | Delta Lake 2.4 |
| Database | PostgreSQL 15 |

## ğŸš€ Quick Start

\`\`\`bash
# Clone
git clone https://github.com/company/e2e-pipeline.git
cd e2e-pipeline

# Start infrastructure
docker-compose up -d

# Install dependencies
pip install -e ".[dev]"

# Run tests
pytest

# Access Airflow UI
open http://localhost:8081  # admin/admin
\`\`\`

## ğŸ“Š Data Model

### Fact Tables
- **fact_events**: ì‚¬ìš©ì í–‰ë™ ì´ë²¤íŠ¸
- **fact_payments**: ê²°ì œ íŠ¸ëœì­ì…˜

### Dimension Tables
- **dim_user**: ì‚¬ìš©ì (SCD Type 2)
- **dim_date**: ë‚ ì§œ
- **dim_plan**: êµ¬ë… í”Œëœ

## ğŸ“… Daily Pipeline

| Time (UTC) | Stage | Duration |
|------------|-------|----------|
| 02:00 | Extract | ~1 hour |
| 03:00 | Transform Staging | ~1.5 hours |
| 04:30 | Transform Warehouse | ~1 hour |
| 05:30 | Quality Checks | ~30 min |
| 06:00 | Complete | - |

## ğŸ”§ Configuration

See [SETUP.md](docs/SETUP.md) for detailed configuration options.

## ğŸ“ˆ Monitoring

- Airflow UI: http://localhost:8081
- Spark UI: http://localhost:8080
- Slack: #data-alerts

## ğŸ§ª Testing

\`\`\`bash
# Unit tests
pytest tests/unit/ -v

# Integration tests
pytest tests/integration/ -v

# Coverage report
pytest --cov=e2e_pipeline --cov-report=html
\`\`\`

## ğŸ“„ License

MIT License
\`\`\`

## ğŸ¤ ë°œí‘œ ìë£Œ êµ¬ì¡° (15ë¶„)

### 1. ì†Œê°œ (2ë¶„)
- í”„ë¡œì íŠ¸ ë°°ê²½ ë° ëª©í‘œ
- ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ìš”ì•½

### 2. ì•„í‚¤í…ì²˜ (3ë¶„)
- ì‹œìŠ¤í…œ ê°œìš” ë‹¤ì´ì–´ê·¸ë¨
- ê¸°ìˆ  ìŠ¤íƒ ì„ íƒ ì´ìœ 
- ë°ì´í„° íë¦„

### 3. í•µì‹¬ êµ¬í˜„ (5ë¶„)
- Extractor íŒ¨í„´
- Spark ë³€í™˜ ë¡œì§
- SCD Type 2 êµ¬í˜„
- ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬

### 4. ë°ëª¨ (3ë¶„)
- Airflow DAG ì‹¤í–‰
- ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸
- Slack ì•Œë¦¼

### 5. ê²°ë¡  & Q&A (2ë¶„)
- ë°°ìš´ ì 
- í–¥í›„ ê°œì„  ì‚¬í•­
- ì§ˆë¬¸

## ğŸ–¼ï¸ ë°œí‘œ ìŠ¬ë¼ì´ë“œ ì˜ˆì‹œ

### ìŠ¬ë¼ì´ë“œ 1: íƒ€ì´í‹€
\`\`\`
E2E ë°ì´í„° íŒŒì´í”„ë¼ì¸
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SaaS ë¶„ì„ í”Œë«í¼ êµ¬ì¶•

[Your Name]
Phase 1 Capstone Project
\`\`\`

### ìŠ¬ë¼ì´ë“œ 2: ë¬¸ì œ ì •ì˜
\`\`\`
As-Is (ë¬¸ì œì )
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ ìˆ˜ë™ ë°ì´í„° ì¶”ì¶œ
â€¢ ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ
â€¢ ë¶€ì„œë³„ ì‚¬ì¼ë¡œ

To-Be (ëª©í‘œ)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ ìë™í™”ëœ ETL
â€¢ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
â€¢ í†µí•© DW
\`\`\`

### ìŠ¬ë¼ì´ë“œ 3: ì•„í‚¤í…ì²˜
\`\`\`
[ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì´ë¯¸ì§€]

Sources â†’ Ingestion â†’ Staging â†’ Warehouse â†’ Marts
   â”‚          â”‚           â”‚          â”‚          â”‚
PostgreSQL  Airflow    Spark     Delta     BI Tools
   S3       Sensors   Transforms  Lake
   API
\`\`\`

### ìŠ¬ë¼ì´ë“œ 4: ê¸°ìˆ  ìŠ¤íƒ
\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Apache Airflow 2.8          â”‚
â”‚   (Orchestration & Scheduling)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Apache Spark 3.4            â”‚
â”‚      (Batch Processing)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Delta Lake 2.4            â”‚
â”‚   (ACID Storage, Time Travel)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         PostgreSQL 15              â”‚
â”‚      (Source & Metadata)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

### ìŠ¬ë¼ì´ë“œ 5: í•µì‹¬ êµ¬í˜„
\`\`\`
1. Extractor Pattern
   â€¢ BaseExtractor ì¶”ìƒ í´ë˜ìŠ¤
   â€¢ PostgreSQL, S3, REST API êµ¬í˜„ì²´
   â€¢ Watermark ê¸°ë°˜ ì¦ë¶„ ì¶”ì¶œ

2. Spark Transformations
   â€¢ Staging: ì •ì œ, ì¤‘ë³µ ì œê±°
   â€¢ Dimension: SCD Type 2
   â€¢ Fact: ì¡°ì¸, ì§‘ê³„

3. Quality Checks
   â€¢ Not Null, Unique, Range
   â€¢ Freshness, Referential Integrity
\`\`\`

### ìŠ¬ë¼ì´ë“œ 6: ë°ëª¨
\`\`\`
[Airflow DAG ìŠ¤í¬ë¦°ìƒ·]

Daily Pipeline ì‹¤í–‰ ê²°ê³¼:
â€¢ Extract: âœ… 15,000 rows
â€¢ Transform: âœ… 3 tables
â€¢ Quality: âœ… 98% passed
â€¢ Duration: 2h 15m
\`\`\`

### ìŠ¬ë¼ì´ë“œ 7: ê²°ë¡ 
\`\`\`
ë°°ìš´ ì 
â”€â”€â”€â”€â”€â”€â”€
â€¢ E2E íŒŒì´í”„ë¼ì¸ ì„¤ê³„
â€¢ Spark + Delta Lake í™œìš©
â€¢ Airflow ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
â€¢ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

í–¥í›„ ê³„íš
â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¶”ê°€
â€¢ ML Feature Store ì—°ë™
â€¢ ëª¨ë‹ˆí„°ë§ ê³ ë„í™”
\`\`\`

## âœ… ë°œí‘œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] README.md ì™„ì„±
- [ ] ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì¤€ë¹„
- [ ] ë°ëª¨ ì‹œë‚˜ë¦¬ì˜¤ ì—°ìŠµ
- [ ] ë°œí‘œ ìŠ¬ë¼ì´ë“œ ì‘ì„±
- [ ] ì˜ˆìƒ ì§ˆë¬¸ ì¤€ë¹„
- [ ] ì‹œê°„ ë§ì¶¤ ì—°ìŠµ (15ë¶„)
`,
      externalLinks: [
        { title: 'Technical Writing Guide', url: 'https://developers.google.com/tech-writing' },
        { title: 'Diagrams.net (draw.io)', url: 'https://app.diagrams.net/' }
      ]
    }
  },
  {
    id: 'w8d5t3',
    title: 'ìµœì¢… í”„ë¡œì íŠ¸ ì œì¶œ ì²´í¬ë¦¬ìŠ¤íŠ¸',
    type: 'reading',
    duration: 20,
    content: {
      markdown: `# ìµœì¢… í”„ë¡œì íŠ¸ ì œì¶œ ì²´í¬ë¦¬ìŠ¤íŠ¸

## âœ… ì½”ë“œ ì™„ì„±ë„

### Extractors
- [ ] BaseExtractor ì¶”ìƒ í´ë˜ìŠ¤
- [ ] PostgreSQLExtractor (ì¦ë¶„ ì¶”ì¶œ, ë°°ì¹˜)
- [ ] S3JSONExtractor (ë‚ ì§œ íŒŒí‹°ì…˜, ë³‘ë ¬ ì²˜ë¦¬)
- [ ] RESTAPIExtractor (í˜ì´ì§•, Rate Limiting)
- [ ] WatermarkManager

### Transformers
- [ ] StagingUserTransformer
- [ ] StagingEventTransformer
- [ ] StagingPaymentTransformer
- [ ] DimUserTransformer (SCD Type 2)
- [ ] FactEventsTransformer

### DAGs
- [ ] dag_extract_all.py
- [ ] dag_transform_warehouse.py
- [ ] dag_data_quality.py
- [ ] dag_daily_pipeline.py

### Utils
- [ ] config.py (Pydantic Settings)
- [ ] logger.py (Structured Logging)
- [ ] exceptions.py
- [ ] notifications.py
- [ ] metrics.py

## âœ… í…ŒìŠ¤íŠ¸

- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] DAG ê²€ì¦ í…ŒìŠ¤íŠ¸
- [ ] ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì •ì˜

## âœ… ì¸í”„ë¼

- [ ] docker-compose.yml ì™„ì„±
- [ ] í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿ (.env.example)
- [ ] Spark + Delta Lake ì„¤ì •
- [ ] Airflow ì—°ê²° ì„¤ì • (Connections)

## âœ… ë¬¸ì„œí™”

- [ ] README.md
- [ ] ARCHITECTURE.md
- [ ] DATA_DICTIONARY.md
- [ ] ADR ë¬¸ì„œ (3ê°œ ì´ìƒ)
- [ ] ì¸ë¼ì¸ ì½”ë“œ ì£¼ì„

## âœ… ë°ì´í„° í’ˆì§ˆ

- [ ] Not Null ê²€ì‚¬
- [ ] Unique ê²€ì‚¬
- [ ] Value Range ê²€ì‚¬
- [ ] Freshness ê²€ì‚¬
- [ ] í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±

## âœ… ìš´ì˜

- [ ] Slack ì•Œë¦¼ í†µí•©
- [ ] SLA ì„¤ì •
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ (ì¬ì‹œë„, ì½œë°±)
- [ ] ë¡œê¹… í‘œì¤€í™”

## ğŸ“Š í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ì²´í¬í¬ì¸íŠ¸ |
|------|------|-----------|
| **ì•„í‚¤í…ì²˜ (20%)** | | |
| - ë ˆì´ì–´ ë¶„ë¦¬ | 5% | Bronze/Silver/Gold êµ¬ì¡° |
| - í™•ì¥ì„± | 5% | ëª¨ë“ˆí™”, ì„¤ì • ë¶„ë¦¬ |
| - ëª¨ë²” ì‚¬ë¡€ | 10% | ADR, ì„¤ê³„ íŒ¨í„´ ì ìš© |
| **ì½”ë“œ í’ˆì§ˆ (25%)** | | |
| - ê°€ë…ì„± | 10% | PEP8, íƒ€ì… íŒíŠ¸ |
| - ëª¨ë“ˆí™” | 10% | ì¶”ìƒí™”, ì¬ì‚¬ìš©ì„± |
| - ì—ëŸ¬ í•¸ë“¤ë§ | 5% | ì˜ˆì™¸ ì²˜ë¦¬, ë¡œê¹… |
| **ë°ì´í„° í’ˆì§ˆ (20%)** | | |
| - ê²€ì¦ ë¡œì§ | 10% | ë‹¤ì–‘í•œ ê²€ì‚¬ ì¢…ë¥˜ |
| - í…ŒìŠ¤íŠ¸ | 10% | ì»¤ë²„ë¦¬ì§€ 80%+ |
| **ìš´ì˜ ê°€ëŠ¥ì„± (20%)** | | |
| - ëª¨ë‹ˆí„°ë§ | 10% | ì•Œë¦¼, ë©”íŠ¸ë¦­ |
| - ë¬¸ì„œí™” | 10% | README, ê°€ì´ë“œ |
| **ë°œí‘œ (15%)** | | |
| - ê¸°ìˆ  ì„¤ëª…ë ¥ | 5% | ëª…í™•í•œ ì „ë‹¬ |
| - ë°ëª¨ | 5% | ì‹¤í–‰ ê°€ëŠ¥í•œ ì‹œì—° |
| - Q&A | 5% | ì§ˆë¬¸ ëŒ€ì‘ë ¥ |

## ğŸ¯ ì œì¶œ ë°©ë²•

### 1. GitHub Repository
\`\`\`bash
# ìµœì¢… ì»¤ë°‹
git add .
git commit -m "feat: Phase 1 Capstone - E2E Data Pipeline complete"
git push origin main

# íƒœê·¸ ìƒì„±
git tag -a v1.0.0 -m "Phase 1 Capstone Project"
git push origin v1.0.0
\`\`\`

### 2. ì œì¶œ ë‚´ìš©
- GitHub Repository URL
- ë°œí‘œ ìŠ¬ë¼ì´ë“œ (PDF)
- ë°ëª¨ ì˜ìƒ (ì„ íƒ)

### 3. ë§ˆê°ì¼
- ì½”ë“œ ì œì¶œ: Day 5 23:59
- ë°œí‘œ: Day 5 ë°œí‘œ ì„¸ì…˜

## ğŸ’¡ ë§ˆì§€ë§‰ ì ê²€

\`\`\`bash
# 1. ì½”ë“œ í¬ë§¤íŒ…
black src/ tests/
ruff check src/ tests/ --fix

# 2. íƒ€ì… ì²´í¬
mypy src/

# 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/ -v --cov=e2e_pipeline

# 4. DAG ê²€ì¦
python -c "from airflow.models import DagBag; db=DagBag('dags/'); print(db.import_errors)"

# 5. Docker ì‹¤í–‰ í™•ì¸
docker-compose up -d
docker-compose ps

# 6. ë¬¸ì„œ ìµœì¢… í™•ì¸
cat README.md
\`\`\`

## ğŸ† ìš°ìˆ˜ í”„ë¡œì íŠ¸ ê¸°ì¤€

- ëª¨ë“  ì²´í¬ë¦¬ìŠ¤íŠ¸ ì™„ë£Œ
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 90% ì´ìƒ
- ì¶”ê°€ ê¸°ëŠ¥ êµ¬í˜„ (ì˜ˆ: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°, ML ì—°ë™)
- ì°½ì˜ì ì¸ ë¬¸ì œ í•´ê²°
- ë›°ì–´ë‚œ ë°œí‘œë ¥

ì¶•í•˜í•©ë‹ˆë‹¤! Phase 1ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤! ğŸ‰
`,
      externalLinks: [
        { title: 'Git Tagging', url: 'https://git-scm.com/book/en/v2/Git-Basics-Tagging' },
        { title: 'GitHub Releases', url: 'https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository' }
      ]
    }
  },
  {
    id: 'w8d5t4',
    title: 'ìµœì¢… í”„ë¡œì íŠ¸ í€´ì¦ˆ',
    type: 'quiz',
    duration: 15,
    content: {
      questions: [
        {
          question: 'Phase 1ì—ì„œ ë°°ìš´ ê¸°ìˆ  ìŠ¤íƒ ì¤‘ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì˜ ACID íŠ¸ëœì­ì…˜ì„ ì§€ì›í•˜ëŠ” ê²ƒì€?',
          options: [
            'Apache Spark',
            'Delta Lake',
            'Apache Airflow',
            'PostgreSQL'
          ],
          answer: 1,
          explanation: 'Delta LakeëŠ” ë°ì´í„° ë ˆì´í¬ì— ACID íŠ¸ëœì­ì…˜ì„ ì œê³µí•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ìŠ¤í† ë¦¬ì§€ ë ˆì´ì–´ì…ë‹ˆë‹¤. Sparkì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì‹ ë¢°ì„± ìˆëŠ” ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.'
        },
        {
          question: 'E2E íŒŒì´í”„ë¼ì¸ì—ì„œ Watermarkì˜ ì—­í• ì€?',
          options: [
            'ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•œë‹¤',
            'ì¦ë¶„ ì¶”ì¶œì˜ ì‹œì‘ì ì„ ê²°ì •í•œë‹¤',
            'íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•œë‹¤',
            'ë°ì´í„°ë¥¼ ì•”í˜¸í™”í•œë‹¤'
          ],
          answer: 1,
          explanation: 'WatermarkëŠ” ë§ˆì§€ë§‰ ì„±ê³µ ì¶”ì¶œ ì‹œì ì„ ê¸°ë¡í•˜ì—¬, ë‹¤ìŒ ì¶”ì¶œ ì‹œ ë³€ê²½ë¶„ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ íš¨ìœ¨ì ì¸ ì¦ë¶„ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.'
        },
        {
          question: 'Star Schemaì—ì„œ SCD Type 2ë¥¼ ì ìš©í•˜ëŠ” í…Œì´ë¸”ì€?',
          options: [
            'Fact í…Œì´ë¸”',
            'Dimension í…Œì´ë¸”',
            'Staging í…Œì´ë¸”',
            'Raw í…Œì´ë¸”'
          ],
          answer: 1,
          explanation: 'SCD (Slowly Changing Dimension) Type 2ëŠ” Dimension í…Œì´ë¸”ì— ì ìš©ë˜ì–´, ì°¨ì› ì†ì„±ì˜ ë³€ê²½ ì´ë ¥ì„ ìƒˆë¡œìš´ í–‰ìœ¼ë¡œ ì¶”ì í•©ë‹ˆë‹¤. Fact í…Œì´ë¸”ì€ íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.'
        },
        {
          question: 'Airflowì—ì„œ ì—¬ëŸ¬ DAGë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” ê°€ì¥ ì í•©í•œ ë°©ë²•ì€?',
          options: [
            'PythonOperator ë‚´ì—ì„œ ì§ì ‘ í˜¸ì¶œ',
            'TriggerDagRunOperator + wait_for_completion',
            'BashOperatorë¡œ airflow trigger_dag ì‹¤í–‰',
            'SubDagOperator ì‚¬ìš©'
          ],
          answer: 1,
          explanation: 'TriggerDagRunOperatorì— wait_for_completion=Trueë¥¼ ì„¤ì •í•˜ë©´, íŠ¸ë¦¬ê±°ëœ DAGê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ DAG ê°„ ì˜ì¡´ì„±ì„ ëª…í™•íˆ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.'
        },
        {
          question: 'ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ì—ì„œ "Freshness" ê²€ì‚¬ì˜ ëª©ì ì€?',
          options: [
            'ë°ì´í„°ì— NULL ê°’ì´ ì—†ëŠ”ì§€ í™•ì¸',
            'ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœì¸ì§€ í™•ì¸',
            'ë°ì´í„°ì— ì¤‘ë³µì´ ì—†ëŠ”ì§€ í™•ì¸',
            'ë°ì´í„° íƒ€ì…ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸'
          ],
          answer: 1,
          explanation: 'Freshness ê²€ì‚¬ëŠ” ë°ì´í„°ì˜ ìµœì‹  íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í—ˆìš© ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì–´ì œ ë‚ ì§œ ë°ì´í„°ê°€ ìˆì–´ì•¼ í•˜ëŠ”ë° 3ì¼ ì „ ë°ì´í„°ë§Œ ìˆë‹¤ë©´ ë¬¸ì œê°€ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.'
        }
      ]
    }
  },
  {
    id: 'w8d5t5',
    title: 'Phase 1 íšŒê³  & ë‹¤ìŒ ë‹¨ê³„',
    type: 'reading',
    duration: 15,
    content: {
      markdown: `# Phase 1 íšŒê³  & ë‹¤ìŒ ë‹¨ê³„

## ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤!

**Phase 1: ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ**ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!

8ì£¼ê°„ì˜ ì—¬ì •ì„ í†µí•´ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì˜ í•µì‹¬ ì—­ëŸ‰ì„ ê°–ì¶”ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“š Phase 1 í•™ìŠµ ìš”ì•½

### Week 1: Python ì‹¬í™”
- Iterator, Generator, Decorator
- Context Manager, ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°
- ì½”ë“œ í’ˆì§ˆ ë„êµ¬ (Black, Ruff, mypy)

### Week 2: Pandas & ë°ì´í„° ì²˜ë¦¬
- DataFrame ê³ ê¸‰ í™œìš©
- ì‹œê³„ì—´ ë¶„ì„, í”¼ë²—, ë©€í‹°ì¸ë±ìŠ¤
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê¸°ë²•

### Week 3: SQL ì‹¬í™”
- Window Functions, CTE
- ì¿¼ë¦¬ ìµœì í™”, ì‹¤í–‰ ê³„íš
- ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬

### Week 4: ë°ì´í„° ëª¨ë¸ë§
- ì •ê·œí™”, ë¹„ì •ê·œí™”
- Star Schema, Snowflake Schema
- SCD (Slowly Changing Dimension)

### Week 5-6: Apache Spark
- DataFrame API, Catalyst Optimizer
- Structured Streaming, Delta Lake
- ì„±ëŠ¥ íŠœë‹, Spark UI ë¶„ì„

### Week 7: Apache Airflow
- DAG ì„¤ê³„, Operators
- TaskFlow API, XCom
- ìŠ¤ì¼€ì¤„ë§, ì—ëŸ¬ í•¸ë“¤ë§

### Week 8: E2E Pipeline Project
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ë° êµ¬í˜„
- ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
- ë¬¸ì„œí™” ë° ë°œí‘œ

## ğŸ› ï¸ ìŠµë“í•œ ê¸°ìˆ  ìŠ¤íƒ

\`\`\`
Programming:    Python, SQL
Processing:     Pandas, PySpark, Delta Lake
Orchestration:  Apache Airflow
Storage:        PostgreSQL, S3, Delta Lake
Quality:        Great Expectations, Custom Validators
DevOps:         Docker, GitHub Actions, pytest
\`\`\`

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: Phase 2 Preview

### Phase 2: ML & ë°ì´í„° ë¶„ì„ (8ì£¼)

| Week | ì£¼ì œ |
|------|------|
| 1 | í†µê³„ ê¸°ì´ˆ & EDA |
| 2 | Feature Engineering |
| 3 | ì§€ë„ í•™ìŠµ (Classification, Regression) |
| 4 | ë¹„ì§€ë„ í•™ìŠµ (Clustering, Dimensionality Reduction) |
| 5 | MLflow & ëª¨ë¸ ê´€ë¦¬ |
| 6 | ëª¨ë¸ ì„œë¹™ & API |
| 7 | A/B í…ŒìŠ¤íŠ¸ & ì¸ê³¼ ì¶”ë¡  |
| 8 | ML Pipeline Project |

### Phase 2ì—ì„œ ë°°ìš¸ ë‚´ìš©
- Scikit-learn, XGBoost, LightGBM
- MLflow ì‹¤í—˜ ì¶”ì 
- FastAPI ëª¨ë¸ ì„œë¹™
- Feature Store êµ¬ì¶•

## ğŸ’ª ê³„ì† ì„±ì¥í•˜ê¸°

### ê¶Œì¥ í•™ìŠµ ìë£Œ
1. **ì±…**: "Designing Data-Intensive Applications" - Martin Kleppmann
2. **ì±…**: "Fundamentals of Data Engineering" - Joe Reis & Matt Housley
3. **ê°•ì˜**: Databricks Academy (ë¬´ë£Œ ì½”ìŠ¤)
4. **ìê²©ì¦**: Databricks Data Engineer Associate

### ì»¤ë®¤ë‹ˆí‹°
- Apache Airflow Slack
- Delta Lake GitHub Discussions
- í•œêµ­ ë°ì´í„° ì—”ì§€ë‹ˆì–´ ì»¤ë®¤ë‹ˆí‹°

### ì‹¤ì „ ê²½í—˜
- ê°œì¸ í”„ë¡œì íŠ¸ (ê³µê°œ ë°ì´í„°ì…‹ í™œìš©)
- ì˜¤í”ˆì†ŒìŠ¤ ê¸°ì—¬ (Airflow, Spark)
- ë¸”ë¡œê·¸ ì‘ì„± (í•™ìŠµ ë‚´ìš© ì •ë¦¬)

## ğŸ† Phase 1 ì¸ì¦

Phase 1 ìº¡ìŠ¤í†¤ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ íšë“í•©ë‹ˆë‹¤:

- **Phase 1 ìˆ˜ë£Œ ë°°ì§€**: Data Engineering Foundations
- **GitHub ì¸ì¦**: í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸
- **Phase 2 ì ‘ê·¼ ê¶Œí•œ**: ML & ë°ì´í„° ë¶„ì„ ê³¼ì •

## ğŸ’¬ í”¼ë“œë°±

Phase 1 ê²½í—˜ì„ ê³µìœ í•´ì£¼ì„¸ìš”:
- ê°€ì¥ ë„ì›€ì´ ë˜ì—ˆë˜ ë‚´ìš©
- ì–´ë ¤ì› ë˜ ë¶€ë¶„
- ê°œì„  ì œì•ˆ

í”¼ë“œë°±ì€ ì»¤ë¦¬í˜ëŸ¼ ê°œì„ ì— ë°˜ì˜ë©ë‹ˆë‹¤.

---

**ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! Phase 2ì—ì„œ ë‹¤ì‹œ ë§Œë‚˜ìš”! ğŸš€**
`,
      externalLinks: [
        { title: 'Designing Data-Intensive Applications', url: 'https://dataintensive.net/' },
        { title: 'Databricks Academy', url: 'https://www.databricks.com/learn/training/home' },
        { title: 'Apache Airflow Slack', url: 'https://apache-airflow.slack.com/' }
      ]
    }
  },
  {
    id: 'w8d5-challenge',
    type: 'challenge',
    title: 'ì£¼ê°„ ë„ì „ê³¼ì œ: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ í™•ì¥',
    duration: 60,
    content: {
      instructions: `# ì£¼ê°„ ë„ì „ê³¼ì œ: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ í™•ì¥

## ğŸ¯ ëª©í‘œ
Phase 1ì—ì„œ ë°°ìš´ **ë°°ì¹˜ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**ì„ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**ìœ¼ë¡œ í™•ì¥í•˜ì„¸ìš”. Spark Structured Streamingê³¼ Kafkaë¥¼ í™œìš©í•˜ì—¬ ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ëŠ” ì¦‰ì‹œ ì²˜ë¦¬ë˜ëŠ” Near Real-Time íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

## ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤
ê¸°ì¡´ Daily Batch íŒŒì´í”„ë¼ì¸ì— ë”í•´ ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤:
- **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**: ìš´ì˜íŒ€ì´ ë¶„ ë‹¨ìœ„ë¡œ í•µì‹¬ ì§€í‘œë¥¼ ëª¨ë‹ˆí„°ë§
- **ì‹¤ì‹œê°„ ì•Œë¦¼**: ì´ìƒ í–‰ë™(ë†’ì€ ì˜¤ë¥˜ìœ¨, ê¸‰ê²©í•œ íŠ¸ë˜í”½ ì¦ê°€) ì¦‰ì‹œ ê°ì§€
- **Fresh Data**: ë°ì´í„° ë ˆì´í„´ì‹œë¥¼ 24ì‹œê°„ì—ì„œ 5ë¶„ ì´ë‚´ë¡œ ë‹¨ì¶•

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

### Part 1: ìŠ¤íŠ¸ë¦¬ë° ì¸í”„ë¼ (20ì )
1. **Kafka ì„¤ì •**
   - í† í”½ ì„¤ê³„ (events, users, payments)
   - íŒŒí‹°ì…˜ ì „ëµ (user_id ê¸°ì¤€)
   - Retention ì„¤ì •

2. **Producer êµ¬í˜„**
   - ê¸°ì¡´ Extractorë¥¼ Kafka Producerë¡œ ë³€í™˜
   - JSON ì§ë ¬í™”
   - ì—ëŸ¬ í•¸ë“¤ë§ (ì¬ì‹œë„, DLQ)

### Part 2: Spark Structured Streaming (30ì )
1. **ìŠ¤íŠ¸ë¦¼ ì†Œë¹„**
   - Kafka Source ì„¤ì •
   - Checkpoint ê´€ë¦¬
   - Trigger ì„¤ì • (processingTime)

2. **ìŠ¤íŠ¸ë¦¼ ë³€í™˜**
   - ì´ë²¤íŠ¸ íŒŒì‹± ë° ì •ì œ
   - Watermark ì„¤ì • (late data ì²˜ë¦¬)
   - ìœˆë„ìš° ì§‘ê³„ (5ë¶„ ë‹¨ìœ„)

3. **ì‹±í¬ ì„¤ì •**
   - Delta Lakeë¡œ ìŠ¤íŠ¸ë¦¬ë° ì“°ê¸°
   - ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ Redis ì €ì¥
   - ì•Œë¦¼ íŠ¸ë¦¬ê±° (ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ)

### Part 3: Lambda Architecture í†µí•© (25ì )
1. **Batch + Streaming í†µí•©**
   - Batch Layer: ê¸°ì¡´ Daily Pipeline (ì •í™•ì„±)
   - Speed Layer: ì‹¤ì‹œê°„ Pipeline (ì†ë„)
   - Serving Layer: ë‘ ê²°ê³¼ ë³‘í•© View

2. **ë°ì´í„° ì¼ê´€ì„±**
   - Exactly-once ë³´ì¥ ì „ëµ
   - Late Data ì²˜ë¦¬ ë¡œì§
   - ë°°ì¹˜/ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ë¹„êµ ê²€ì¦

### Part 4: ëª¨ë‹ˆí„°ë§ & ìš´ì˜ (25ì )
1. **ìŠ¤íŠ¸ë¦¬ë° ë©”íŠ¸ë¦­**
   - ì²˜ë¦¬ëŸ‰ (records/sec)
   - ì§€ì—° ì‹œê°„ (end-to-end latency)
   - Consumer Lag

2. **ì•Œë¦¼ ì„¤ì •**
   - ì²˜ë¦¬ ì§€ì—° ì•Œë¦¼
   - ì˜¤ë¥˜ìœ¨ ì„ê³„ì¹˜ ì•Œë¦¼
   - Kafka Consumer Lag ì•Œë¦¼

3. **ìš´ì˜ ëŒ€ì‹œë³´ë“œ**
   - Spark Streaming UI ì„¤ì •
   - Kafka Metrics ëª¨ë‹ˆí„°ë§
   - ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ì§€í‘œ

## ğŸ† í‰ê°€ ê¸°ì¤€

| í•­ëª© | ë°°ì  | ì„¸ë¶€ ê¸°ì¤€ |
|------|------|-----------|
| ìŠ¤íŠ¸ë¦¬ë° ì¸í”„ë¼ | 20ì  | Kafka ì„¤ì •(10), Producer(10) |
| Structured Streaming | 30ì  | ìŠ¤íŠ¸ë¦¼ ë³€í™˜(15), Watermark/Window(15) |
| Lambda Architecture | 25ì  | í†µí•© ì„¤ê³„(15), ì¼ê´€ì„±(10) |
| ëª¨ë‹ˆí„°ë§ & ìš´ì˜ | 25ì  | ë©”íŠ¸ë¦­(15), ì•Œë¦¼(10) |

## ğŸ’¡ íŒíŠ¸

### Kafka Docker Compose
\`\`\`yaml
# docker-compose.streaming.yml
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
\`\`\`

### Kafka Producer ì˜ˆì‹œ
\`\`\`python
from confluent_kafka import Producer
import json

def delivery_report(err, msg):
    if err is not None:
        print(f"Delivery failed: {err}")
    else:
        print(f"Message delivered to {msg.topic()} [{msg.partition()}]")

producer = Producer({'bootstrap.servers': 'localhost:9092'})

# ì´ë²¤íŠ¸ ë°œí–‰
event = {"user_id": 123, "event_type": "page_view", "timestamp": "2024-01-01T10:00:00Z"}
producer.produce(
    topic='events',
    key=str(event['user_id']),
    value=json.dumps(event),
    callback=delivery_report
)
producer.flush()
\`\`\`

### Spark Structured Streaming
\`\`\`python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder \\
    .appName("StreamingPipeline") \\
    .config("spark.sql.streaming.schemaInference", "true") \\
    .getOrCreate()

# Kafka ì†ŒìŠ¤
events_df = spark.readStream \\
    .format("kafka") \\
    .option("kafka.bootstrap.servers", "localhost:9092") \\
    .option("subscribe", "events") \\
    .option("startingOffsets", "latest") \\
    .load()

# JSON íŒŒì‹±
event_schema = StructType([
    StructField("user_id", IntegerType()),
    StructField("event_type", StringType()),
    StructField("timestamp", TimestampType())
])

parsed_df = events_df \\
    .select(from_json(col("value").cast("string"), event_schema).alias("data")) \\
    .select("data.*")

# Watermark ì„¤ì • (5ë¶„ ì§€ì—° í—ˆìš©)
watermarked_df = parsed_df \\
    .withWatermark("timestamp", "5 minutes")

# 5ë¶„ ìœˆë„ìš° ì§‘ê³„
windowed_counts = watermarked_df \\
    .groupBy(
        window(col("timestamp"), "5 minutes"),
        col("event_type")
    ) \\
    .count()

# Delta Lakeë¡œ ìŠ¤íŠ¸ë¦¬ë° ì“°ê¸°
query = windowed_counts.writeStream \\
    .format("delta") \\
    .outputMode("append") \\
    .option("checkpointLocation", "/tmp/checkpoint/events") \\
    .trigger(processingTime="1 minute") \\
    .start("/data/streaming/event_counts")

query.awaitTermination()
\`\`\`

### ì‹¤ì‹œê°„ ì•Œë¦¼ ì˜ˆì‹œ
\`\`\`python
def alert_on_threshold(batch_df, batch_id):
    """ë°°ì¹˜ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ì•Œë¦¼ ë¡œì§"""
    error_rate = batch_df.filter(col("event_type") == "error").count() / batch_df.count()

    if error_rate > 0.05:  # 5% ì´ˆê³¼ ì‹œ ì•Œë¦¼
        send_slack_alert(f"High error rate detected: {error_rate:.2%}")

# foreachBatchë¡œ ì»¤ìŠ¤í…€ ë¡œì§ ì‹¤í–‰
query = parsed_df.writeStream \\
    .foreachBatch(alert_on_threshold) \\
    .trigger(processingTime="1 minute") \\
    .start()
\`\`\`

## ğŸ”— ì°¸ê³  ìë£Œ
- [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [Delta Lake Streaming](https://docs.delta.io/latest/delta-streaming.html)
- [Lambda Architecture](http://lambda-architecture.net/)
`,
      starterCode: `"""
Week 8 ë„ì „ê³¼ì œ: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ í™•ì¥
===============================================
ëª©í‘œ: ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ì„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í™•ì¥
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from typing import Dict, Any
import json

# ===========================================
# Part 1: Kafka Producer
# ===========================================

class EventProducer:
    """ì´ë²¤íŠ¸ë¥¼ Kafkaë¡œ ë°œí–‰í•˜ëŠ” Producer"""

    def __init__(self, bootstrap_servers: str = "localhost:9092"):
        self.bootstrap_servers = bootstrap_servers
        self.producer = None

    def connect(self):
        """Kafka ì—°ê²°"""
        # TODO: confluent_kafka Producer ì´ˆê¸°í™”
        pass

    def send_event(self, topic: str, event: Dict[str, Any]):
        """ì´ë²¤íŠ¸ ë°œí–‰"""
        # TODO: JSON ì§ë ¬í™” ë° Kafka ë°œí–‰
        pass

    def close(self):
        """ì—°ê²° ì¢…ë£Œ"""
        # TODO: flush ë° ì—°ê²° ì¢…ë£Œ
        pass

# ===========================================
# Part 2: Spark Structured Streaming
# ===========================================

class StreamingPipeline:
    """Spark Structured Streaming íŒŒì´í”„ë¼ì¸"""

    EVENT_SCHEMA = StructType([
        StructField("user_id", IntegerType(), True),
        StructField("event_type", StringType(), True),
        StructField("page_url", StringType(), True),
        StructField("session_id", StringType(), True),
        StructField("timestamp", TimestampType(), True)
    ])

    def __init__(self, spark: SparkSession, kafka_servers: str = "localhost:9092"):
        self.spark = spark
        self.kafka_servers = kafka_servers
        self.stream = None

    def read_kafka_stream(self, topic: str):
        """Kafka ìŠ¤íŠ¸ë¦¼ ì½ê¸°"""
        # TODO: Kafka Source ì„¤ì •
        # - subscribe
        # - startingOffsets
        pass

    def parse_events(self, df):
        """JSON íŒŒì‹± ë° ìŠ¤í‚¤ë§ˆ ì ìš©"""
        # TODO: from_jsonìœ¼ë¡œ íŒŒì‹±
        pass

    def add_watermark(self, df, delay: str = "5 minutes"):
        """Watermark ì„¤ì •"""
        # TODO: withWatermark ì ìš©
        pass

    def aggregate_by_window(self, df, window_duration: str = "5 minutes"):
        """ìœˆë„ìš° ì§‘ê³„"""
        # TODO: window í•¨ìˆ˜ë¡œ ì§‘ê³„
        pass

    def write_to_delta(self, df, path: str, checkpoint: str):
        """Delta Lakeë¡œ ìŠ¤íŠ¸ë¦¬ë° ì“°ê¸°"""
        # TODO: writeStream ì„¤ì •
        pass

# ===========================================
# Part 3: ì‹¤ì‹œê°„ ì•Œë¦¼
# ===========================================

class AlertManager:
    """ì‹¤ì‹œê°„ ì•Œë¦¼ ê´€ë¦¬"""

    def __init__(self, error_threshold: float = 0.05):
        self.error_threshold = error_threshold
        self.alerts = []

    def check_error_rate(self, batch_df, batch_id: int):
        """ì˜¤ë¥˜ìœ¨ ì²´í¬ ë° ì•Œë¦¼"""
        # TODO: ì˜¤ë¥˜ìœ¨ ê³„ì‚° ë° ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ì•Œë¦¼
        pass

    def check_traffic_spike(self, batch_df, batch_id: int, spike_threshold: float = 2.0):
        """íŠ¸ë˜í”½ ê¸‰ì¦ ê°ì§€"""
        # TODO: ì´ì „ ìœˆë„ìš° ëŒ€ë¹„ ê¸‰ì¦ ê°ì§€
        pass

    def send_alert(self, message: str, severity: str = "warning"):
        """ì•Œë¦¼ ë°œì†¡"""
        # TODO: Slack/ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡
        print(f"[{severity.upper()}] {message}")
        self.alerts.append({"message": message, "severity": severity})

# ===========================================
# Part 4: Lambda Architecture í†µí•©
# ===========================================

class LambdaArchitecture:
    """Lambda Architecture í†µí•© ë ˆì´ì–´"""

    def __init__(self, spark: SparkSession):
        self.spark = spark

    def merge_batch_and_stream(self, batch_path: str, stream_path: str):
        """ë°°ì¹˜ì™€ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ë³‘í•©"""
        # TODO: UNION ë˜ëŠ” MERGE ë¡œì§
        pass

    def create_serving_view(self, view_name: str):
        """ì„œë¹™ ë ˆì´ì–´ View ìƒì„±"""
        # TODO: ìµœì‹  ë°ì´í„° ìš°ì„  View
        pass

    def validate_consistency(self, batch_df, stream_df):
        """ë°°ì¹˜/ìŠ¤íŠ¸ë¦¬ë° ì¼ê´€ì„± ê²€ì¦"""
        # TODO: ê²°ê³¼ ë¹„êµ ë¡œì§
        pass

# ===========================================
# Part 5: ë©”íŠ¸ë¦­ ìˆ˜ì§‘
# ===========================================

class StreamingMetrics:
    """ìŠ¤íŠ¸ë¦¬ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

    def __init__(self):
        self.metrics = {}

    def collect_from_query(self, query):
        """StreamingQueryì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        # TODO: lastProgressì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        pass

    def get_processing_rate(self) -> float:
        """ì²˜ë¦¬ìœ¨ (records/sec)"""
        # TODO: inputRowsPerSecond ë°˜í™˜
        pass

    def get_latency(self) -> float:
        """ì²˜ë¦¬ ì§€ì—° ì‹œê°„ (ms)"""
        # TODO: triggerExecution ì‹œê°„ ë°˜í™˜
        pass

    def report(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ë¦¬í¬íŠ¸"""
        return self.metrics

# ===========================================
# ë©”ì¸ ì‹¤í–‰
# ===========================================

def create_spark_session():
    """ìŠ¤íŠ¸ë¦¬ë°ìš© Spark ì„¸ì…˜ ìƒì„±"""
    return SparkSession.builder \\
        .appName("StreamingChallenge") \\
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \\
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \\
        .config("spark.sql.streaming.schemaInference", "true") \\
        .master("local[*]") \\
        .getOrCreate()


if __name__ == "__main__":
    print("=" * 60)
    print("ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ë„ì „ê³¼ì œ")
    print("=" * 60)

    # Spark ì„¸ì…˜ ìƒì„±
    spark = create_spark_session()

    # ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = StreamingPipeline(spark)

    # ì•Œë¦¼ ê´€ë¦¬ì ì´ˆê¸°í™”
    alert_manager = AlertManager(error_threshold=0.05)

    # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    metrics = StreamingMetrics()

    # TODO: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë¡œì§ êµ¬í˜„
    # 1. Kafka ìŠ¤íŠ¸ë¦¼ ì½ê¸°
    # 2. ì´ë²¤íŠ¸ íŒŒì‹± ë° ë³€í™˜
    # 3. ìœˆë„ìš° ì§‘ê³„
    # 4. Delta Lake ì“°ê¸°
    # 5. ì•Œë¦¼ ì„¤ì •
    # 6. ë©”íŠ¸ë¦­ ìˆ˜ì§‘

    print("\\níŒŒì´í”„ë¼ì¸ì´ ì‹œì‘ë˜ë©´ Kafka ë©”ì‹œì§€ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤...")
`,
      hints: [
        'withWatermarkëŠ” groupBy ì „ì— ì ìš©í•´ì•¼ í•¨',
        'foreachBatchë¡œ ë°°ì¹˜ë§ˆë‹¤ ì»¤ìŠ¤í…€ ë¡œì§ ì‹¤í–‰ ê°€ëŠ¥',
        'checkpointLocationì€ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ë§ˆë‹¤ ê³ ìœ í•´ì•¼ í•¨',
        'outputMode: appendëŠ” ìƒˆ ë°ì´í„°ë§Œ, completeëŠ” ì „ì²´ ê²°ê³¼',
        'trigger(processingTime="1 minute")ë¡œ ì²˜ë¦¬ ì£¼ê¸° ì„¤ì •',
        'Kafka Consumer LagëŠ” í† í”½ offset ì°¨ì´ë¡œ ê³„ì‚°'
      ]
    }
  }
]
