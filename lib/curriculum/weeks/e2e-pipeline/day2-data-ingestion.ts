import { Task } from '../../types'

export const day2Tasks: Task[] = [
  {
    id: 'w8d2t1',
    title: 'ë°ì´í„° ì†ŒìŠ¤ë³„ Extractor ì„¤ê³„',
    type: 'video',
    duration: 25,
    content: {
      videoUrl: '',
      transcript: `# ë°ì´í„° ì†ŒìŠ¤ë³„ Extractor ì„¤ê³„

## ðŸŽ¯ ì˜¤ëŠ˜ì˜ ëª©í‘œ

ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” Extractorë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡

| ì†ŒìŠ¤ | ìœ í˜• | ë°ì´í„° | ì¶”ì¶œ ë°©ì‹ |
|------|------|--------|----------|
| PostgreSQL | RDBMS | users, subscriptions | Full/Incremental |
| S3 | Object Storage | event logs (JSON) | Partition Scan |
| REST API | HTTP | payments | Pagination |

## ðŸ—ï¸ Extractor ì•„í‚¤í…ì²˜

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 BaseExtractor (ABC)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  + extract() -> DataFrame                        â”‚â”‚
â”‚  â”‚  + validate() -> bool                            â”‚â”‚
â”‚  â”‚  + get_watermark() -> datetime                   â”‚â”‚
â”‚  â”‚  + save_to_raw() -> str                          â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚                 â”‚
           â–¼                â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚PostgresSQL   â”‚ â”‚   S3Json     â”‚ â”‚  RestAPI     â”‚
    â”‚  Extractor   â”‚ â”‚  Extractor   â”‚ â”‚  Extractor   â”‚
    â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
    â”‚ + query      â”‚ â”‚ + prefix     â”‚ â”‚ + endpoint   â”‚
    â”‚ + batch_size â”‚ â”‚ + pattern    â”‚ â”‚ + auth       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`

## ðŸ“Š ì¶”ì¶œ ì „ëžµ

### 1. Full Extract (ì „ì²´ ì¶”ì¶œ)
- ìž‘ì€ Dimension í…Œì´ë¸”ì— ì í•©
- ë§¤ë²ˆ ì „ì²´ ë°ì´í„° êµì²´
- ì˜ˆ: dim_plan, dim_event_type

### 2. Incremental Extract (ì¦ë¶„ ì¶”ì¶œ)
- ëŒ€ìš©ëŸ‰ Fact í…Œì´ë¸”ì— í•„ìˆ˜
- Watermark ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½ë¶„ë§Œ ì¶”ì¶œ
- ì˜ˆ: fact_events, fact_payments

### 3. CDC (Change Data Capture)
- ì‹¤ì‹œê°„ì— ê°€ê¹Œìš´ ë°ì´í„° ë™ê¸°í™”
- Debezium, AWS DMS ë“± í™œìš©
- ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë°°ì¹˜ ì¦ë¶„ìœ¼ë¡œ ëŒ€ì²´

## ðŸ”§ í•µì‹¬ íŒ¨í„´

### Watermark ê´€ë¦¬

\`\`\`python
class WatermarkManager:
    def __init__(self, state_store: str):
        self.state_store = state_store

    def get_last_watermark(self, source: str) -> datetime:
        # ë§ˆì§€ë§‰ ì„±ê³µ ì¶”ì¶œ ì‹œì  ì¡°íšŒ
        pass

    def update_watermark(self, source: str, watermark: datetime):
        # ì¶”ì¶œ ì™„ë£Œ í›„ ì›Œí„°ë§ˆí¬ ì—…ë°ì´íŠ¸
        pass
\`\`\`

### ìž¬ì‹œë„ ë° ì—ëŸ¬ í•¸ë“¤ë§

\`\`\`python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=60)
)
def extract_with_retry(extractor):
    return extractor.extract()
\`\`\`

## ðŸ“ ì˜¤ëŠ˜ êµ¬í˜„í•  íŒŒì¼

\`\`\`
src/e2e_pipeline/extractors/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py           # BaseExtractor ABC
â”œâ”€â”€ postgres.py       # PostgreSQL Extractor
â”œâ”€â”€ s3_json.py        # S3 JSON Extractor
â”œâ”€â”€ rest_api.py       # REST API Extractor
â””â”€â”€ watermark.py      # Watermark Manager
\`\`\`

ìž, ì´ì œ êµ¬í˜„ì„ ì‹œìž‘í•©ì‹œë‹¤! ðŸ’»
`,
      objectives: [
        'ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” íŒ¨í„´ì„ ìµížŒë‹¤',
        'ì¦ë¶„ ì¶”ì¶œê³¼ Watermark ê´€ë¦¬ ë°©ë²•ì„ ì´í•´í•œë‹¤',
        'ìž¬ì‹œë„ ë° ì—ëŸ¬ í•¸ë“¤ë§ íŒ¨í„´ì„ ì ìš©í•œë‹¤'
      ],
      keyPoints: [
        'BaseExtractor ì¶”ìƒ í´ëž˜ìŠ¤ë¡œ ê³µí†µ ì¸í„°íŽ˜ì´ìŠ¤ ì •ì˜',
        'Watermarkë¥¼ í†µí•´ ì¦ë¶„ ì¶”ì¶œì˜ ì‹œìž‘ì  ê´€ë¦¬',
        'ê° ì†ŒìŠ¤ë³„ íŠ¹ì„±ì— ë§žëŠ” ì¶”ì¶œ ì „ëžµ ì„ íƒ'
      ]
    }
  },
  {
    id: 'w8d2t2',
    title: 'BaseExtractor & PostgreSQL Extractor êµ¬í˜„',
    type: 'code',
    duration: 45,
    content: {
      instructions: `## BaseExtractor & PostgreSQL Extractor êµ¬í˜„

ì¶”ìƒ ë² ì´ìŠ¤ í´ëž˜ìŠ¤ì™€ PostgreSQL ë°ì´í„° ì¶”ì¶œê¸°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **BaseExtractor (ABC)**
   - extract(), validate(), save_to_raw() ì¶”ìƒ ë©”ì„œë“œ
   - ê³µí†µ ë¡œê¹… ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘

2. **PostgreSQLExtractor**
   - ì „ì²´ ì¶”ì¶œ / ì¦ë¶„ ì¶”ì¶œ ì§€ì›
   - ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê¸°ë°˜ ì²­í¬ ì²˜ë¦¬
   - ì—°ê²° í’€ ê´€ë¦¬

3. **í…ŒìŠ¤íŠ¸**
   - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìž‘ì„±
   - Mockì„ í™œìš©í•œ DB ì—°ê²° í…ŒìŠ¤íŠ¸
`,
      starterCode: `# src/e2e_pipeline/extractors/base.py

from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Dict, Optional
import pandas as pd

class BaseExtractor(ABC):
    """Base class for all data extractors."""

    def __init__(self, source_name: str, config: Dict[str, Any]):
        self.source_name = source_name
        self.config = config
        # TODO: ë¡œê±° ì´ˆê¸°í™”

    @abstractmethod
    def extract(self, **kwargs) -> pd.DataFrame:
        """Extract data from source."""
        pass

    @abstractmethod
    def validate(self, df: pd.DataFrame) -> bool:
        """Validate extracted data."""
        pass

    def save_to_raw(self, df: pd.DataFrame, path: str) -> str:
        """Save DataFrame to raw layer."""
        # TODO: S3ì— Parquetìœ¼ë¡œ ì €ìž¥
        pass


# src/e2e_pipeline/extractors/postgres.py

class PostgreSQLExtractor(BaseExtractor):
    """Extract data from PostgreSQL database."""

    def __init__(
        self,
        source_name: str,
        connection_string: str,
        table_name: str,
        # TODO: ì¶”ê°€ íŒŒë¼ë¯¸í„°
    ):
        # TODO: ì´ˆê¸°í™” êµ¬í˜„
        pass

    def extract(
        self,
        incremental: bool = False,
        watermark_column: str = None,
        last_watermark: datetime = None,
    ) -> pd.DataFrame:
        """Extract data from PostgreSQL."""
        # TODO: êµ¬í˜„
        pass

    def validate(self, df: pd.DataFrame) -> bool:
        """Validate extracted data."""
        # TODO: êµ¬í˜„
        pass
`,
      solutionCode: `# src/e2e_pipeline/extractors/base.py

from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Dict, List, Optional
import pandas as pd
from pathlib import Path

from e2e_pipeline.utils.logger import get_logger
from e2e_pipeline.utils.exceptions import ExtractionError, ValidationError


class BaseExtractor(ABC):
    """Base class for all data extractors."""

    def __init__(self, source_name: str, config: Dict[str, Any]):
        self.source_name = source_name
        self.config = config
        self.logger = get_logger(f"extractor.{source_name}")
        self._extraction_stats = {
            "rows_extracted": 0,
            "extraction_time_seconds": 0,
            "errors": [],
        }

    @abstractmethod
    def extract(self, **kwargs) -> pd.DataFrame:
        """Extract data from source.

        Returns:
            pd.DataFrame: Extracted data
        """
        pass

    @abstractmethod
    def validate(self, df: pd.DataFrame) -> bool:
        """Validate extracted data.

        Args:
            df: DataFrame to validate

        Returns:
            bool: True if valid
        """
        pass

    def save_to_raw(
        self,
        df: pd.DataFrame,
        bucket: str,
        prefix: str,
        partition_cols: Optional[List[str]] = None,
    ) -> str:
        """Save DataFrame to raw layer in S3.

        Args:
            df: DataFrame to save
            bucket: S3 bucket name
            prefix: S3 key prefix
            partition_cols: Columns to partition by

        Returns:
            str: S3 path where data was saved
        """
        import boto3
        from io import BytesIO
        import pyarrow as pa
        import pyarrow.parquet as pq

        self.logger.info(
            "Saving to raw layer",
            bucket=bucket,
            prefix=prefix,
            rows=len(df),
        )

        # Generate path with timestamp
        timestamp = datetime.utcnow().strftime("%Y-%m-%dT%H-%M-%S")
        s3_key = f"{prefix}/{timestamp}/data.parquet"

        # Convert to Parquet
        table = pa.Table.from_pandas(df)
        buffer = BytesIO()
        pq.write_table(table, buffer)
        buffer.seek(0)

        # Upload to S3
        s3_client = boto3.client("s3")
        s3_client.upload_fileobj(buffer, bucket, s3_key)

        full_path = f"s3://{bucket}/{s3_key}"
        self.logger.info("Saved to S3", path=full_path)

        return full_path

    def get_stats(self) -> Dict[str, Any]:
        """Get extraction statistics."""
        return self._extraction_stats


# src/e2e_pipeline/extractors/postgres.py

from datetime import datetime
from typing import Any, Dict, List, Optional
import pandas as pd
from contextlib import contextmanager
import time

from sqlalchemy import create_engine, text
from sqlalchemy.pool import QueuePool

from e2e_pipeline.extractors.base import BaseExtractor
from e2e_pipeline.utils.exceptions import ExtractionError, ValidationError
from e2e_pipeline.utils.logger import get_logger


class PostgreSQLExtractor(BaseExtractor):
    """Extract data from PostgreSQL database."""

    def __init__(
        self,
        source_name: str,
        connection_string: str,
        table_name: str,
        schema: str = "public",
        batch_size: int = 10000,
        required_columns: Optional[List[str]] = None,
    ):
        """Initialize PostgreSQL extractor.

        Args:
            source_name: Unique identifier for this source
            connection_string: PostgreSQL connection URL
            table_name: Table to extract from
            schema: Database schema
            batch_size: Number of rows per batch
            required_columns: Columns that must exist
        """
        super().__init__(source_name, {"table": table_name, "schema": schema})

        self.connection_string = connection_string
        self.table_name = table_name
        self.schema = schema
        self.batch_size = batch_size
        self.required_columns = required_columns or []

        # Create connection pool
        self.engine = create_engine(
            connection_string,
            poolclass=QueuePool,
            pool_size=5,
            max_overflow=10,
            pool_pre_ping=True,  # Test connection health
        )

        self.logger = get_logger(f"extractor.postgres.{source_name}")

    @contextmanager
    def get_connection(self):
        """Get database connection from pool."""
        conn = self.engine.connect()
        try:
            yield conn
        finally:
            conn.close()

    def extract(
        self,
        incremental: bool = False,
        watermark_column: Optional[str] = None,
        last_watermark: Optional[datetime] = None,
        columns: Optional[List[str]] = None,
        where_clause: Optional[str] = None,
    ) -> pd.DataFrame:
        """Extract data from PostgreSQL.

        Args:
            incremental: If True, extract only new/updated rows
            watermark_column: Column to use for incremental extraction
            last_watermark: Last extraction timestamp
            columns: Specific columns to extract (default: all)
            where_clause: Additional WHERE conditions

        Returns:
            pd.DataFrame: Extracted data
        """
        start_time = time.time()
        self.logger.info(
            "Starting extraction",
            table=self.table_name,
            incremental=incremental,
            last_watermark=last_watermark,
        )

        try:
            # Build query
            select_cols = ", ".join(columns) if columns else "*"
            query = f"SELECT {select_cols} FROM {self.schema}.{self.table_name}"

            conditions = []

            # Add incremental condition
            if incremental and watermark_column and last_watermark:
                conditions.append(
                    f"{watermark_column} > :last_watermark"
                )

            # Add custom where clause
            if where_clause:
                conditions.append(f"({where_clause})")

            if conditions:
                query += " WHERE " + " AND ".join(conditions)

            query += f" ORDER BY {watermark_column}" if watermark_column else ""

            self.logger.debug("Executing query", query=query)

            # Execute with batching for large tables
            all_data = []

            with self.get_connection() as conn:
                # Get total count first
                count_query = f"SELECT COUNT(*) FROM ({query}) AS subquery"
                params = {"last_watermark": last_watermark} if last_watermark else {}
                result = conn.execute(text(count_query), params)
                total_rows = result.scalar()

                self.logger.info(f"Total rows to extract: {total_rows}")

                # Batch extraction
                offset = 0
                while offset < total_rows:
                    batch_query = f"{query} LIMIT {self.batch_size} OFFSET {offset}"
                    df_batch = pd.read_sql(
                        text(batch_query),
                        conn,
                        params=params,
                    )
                    all_data.append(df_batch)
                    offset += self.batch_size

                    self.logger.debug(
                        f"Extracted batch",
                        offset=offset,
                        batch_size=len(df_batch),
                    )

            # Combine all batches
            df = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()

            # Update stats
            extraction_time = time.time() - start_time
            self._extraction_stats = {
                "rows_extracted": len(df),
                "extraction_time_seconds": round(extraction_time, 2),
                "errors": [],
            }

            self.logger.info(
                "Extraction completed",
                rows=len(df),
                duration_seconds=round(extraction_time, 2),
            )

            return df

        except Exception as e:
            self.logger.error(f"Extraction failed: {e}")
            self._extraction_stats["errors"].append(str(e))
            raise ExtractionError(f"Failed to extract from {self.table_name}: {e}")

    def validate(self, df: pd.DataFrame) -> bool:
        """Validate extracted data.

        Checks:
        - Not empty (warning, not error)
        - Required columns exist
        - No duplicate primary keys (if specified)

        Args:
            df: DataFrame to validate

        Returns:
            bool: True if valid

        Raises:
            ValidationError: If critical validation fails
        """
        self.logger.info("Validating extracted data", rows=len(df))

        # Check if empty
        if df.empty:
            self.logger.warning("Extracted DataFrame is empty")
            return True  # Empty is valid, just a warning

        # Check required columns
        missing_cols = set(self.required_columns) - set(df.columns)
        if missing_cols:
            raise ValidationError(
                f"Missing required columns: {missing_cols}"
            )

        # Check for nulls in critical columns
        for col in self.required_columns:
            null_count = df[col].isnull().sum()
            if null_count > 0:
                self.logger.warning(
                    f"Column {col} has {null_count} null values"
                )

        self.logger.info("Validation passed")
        return True

    def get_current_watermark(self, watermark_column: str) -> Optional[datetime]:
        """Get the maximum value of watermark column.

        Args:
            watermark_column: Column name for watermark

        Returns:
            datetime: Maximum watermark value
        """
        query = f"SELECT MAX({watermark_column}) FROM {self.schema}.{self.table_name}"

        with self.get_connection() as conn:
            result = conn.execute(text(query))
            max_value = result.scalar()

        return max_value

    def close(self):
        """Close database connection pool."""
        self.engine.dispose()
        self.logger.info("Connection pool closed")


# tests/unit/extractors/test_postgres.py

import pytest
from unittest.mock import Mock, patch, MagicMock
import pandas as pd
from datetime import datetime


class TestPostgreSQLExtractor:
    """Tests for PostgreSQLExtractor."""

    @pytest.fixture
    def mock_engine(self):
        """Create mock SQLAlchemy engine."""
        with patch("e2e_pipeline.extractors.postgres.create_engine") as mock:
            yield mock

    @pytest.fixture
    def extractor(self, mock_engine):
        """Create extractor instance with mocked engine."""
        from e2e_pipeline.extractors.postgres import PostgreSQLExtractor

        return PostgreSQLExtractor(
            source_name="test_users",
            connection_string="postgresql://test:test@localhost/test",
            table_name="users",
            required_columns=["user_id", "email"],
        )

    def test_validate_with_required_columns(self, extractor):
        """Test validation passes with required columns."""
        df = pd.DataFrame({
            "user_id": [1, 2, 3],
            "email": ["a@test.com", "b@test.com", "c@test.com"],
            "name": ["A", "B", "C"],
        })

        assert extractor.validate(df) is True

    def test_validate_missing_required_columns(self, extractor):
        """Test validation fails when required columns missing."""
        from e2e_pipeline.utils.exceptions import ValidationError

        df = pd.DataFrame({
            "user_id": [1, 2, 3],
            "name": ["A", "B", "C"],
            # Missing 'email' column
        })

        with pytest.raises(ValidationError) as exc_info:
            extractor.validate(df)

        assert "email" in str(exc_info.value)

    def test_validate_empty_dataframe(self, extractor):
        """Test validation passes for empty DataFrame."""
        df = pd.DataFrame()

        # Empty is valid (warning, not error)
        assert extractor.validate(df) is True

    def test_get_stats(self, extractor):
        """Test extraction statistics."""
        stats = extractor.get_stats()

        assert "rows_extracted" in stats
        assert "extraction_time_seconds" in stats
        assert "errors" in stats
`,
      hints: [
        "SQLAlchemyì˜ QueuePoolì„ ì‚¬ìš©í•˜ë©´ ì—°ê²°ì„ ìž¬ì‚¬ìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤",
        "contextmanagerë¥¼ ì‚¬ìš©í•˜ë©´ ì—°ê²°ì„ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤",
        "pd.read_sqlì— text()ë¥¼ ì‚¬ìš©í•˜ë©´ parameterized queryê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤",
        "ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì œì–´í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w8d2t3',
    title: 'S3 JSON & REST API Extractor êµ¬í˜„',
    type: 'code',
    duration: 40,
    content: {
      instructions: `## S3 JSON & REST API Extractor êµ¬í˜„

S3ì—ì„œ JSON ë¡œê·¸ë¥¼ ì½ê³ , REST APIì—ì„œ ë°ì´í„°ë¥¼ íŽ˜ì´ì§•ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” Extractorë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **S3JSONExtractor**
   - ë‚ ì§œ íŒŒí‹°ì…˜ ê¸°ë°˜ íŒŒì¼ ìŠ¤ìº”
   - JSON Lines í˜•ì‹ ì§€ì›
   - ë³‘ë ¬ íŒŒì¼ ì²˜ë¦¬

2. **RESTAPIExtractor**
   - íŽ˜ì´ì§• ì²˜ë¦¬ (cursor / offset)
   - Rate limiting
   - ì¸ì¦ (API Key / OAuth)
`,
      starterCode: `# src/e2e_pipeline/extractors/s3_json.py

from datetime import datetime, date
from typing import List, Optional
import pandas as pd

from e2e_pipeline.extractors.base import BaseExtractor


class S3JSONExtractor(BaseExtractor):
    """Extract JSON data from S3."""

    def __init__(
        self,
        source_name: str,
        bucket: str,
        prefix: str,
        # TODO: ì¶”ê°€ íŒŒë¼ë¯¸í„°
    ):
        # TODO: ì´ˆê¸°í™”
        pass

    def extract(
        self,
        start_date: date,
        end_date: date,
    ) -> pd.DataFrame:
        """Extract JSON files for date range."""
        # TODO: êµ¬í˜„
        pass


# src/e2e_pipeline/extractors/rest_api.py

class RESTAPIExtractor(BaseExtractor):
    """Extract data from REST API with pagination."""

    def __init__(
        self,
        source_name: str,
        base_url: str,
        endpoint: str,
        # TODO: ì¶”ê°€ íŒŒë¼ë¯¸í„°
    ):
        # TODO: ì´ˆê¸°í™”
        pass

    def extract(
        self,
        params: dict = None,
    ) -> pd.DataFrame:
        """Extract all pages from API."""
        # TODO: êµ¬í˜„
        pass
`,
      solutionCode: `# src/e2e_pipeline/extractors/s3_json.py

from datetime import datetime, date, timedelta
from typing import Any, Dict, List, Optional
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
from io import BytesIO
import time

import boto3
from botocore.config import Config

from e2e_pipeline.extractors.base import BaseExtractor
from e2e_pipeline.utils.exceptions import ExtractionError
from e2e_pipeline.utils.logger import get_logger


class S3JSONExtractor(BaseExtractor):
    """Extract JSON data from S3 with date partitioning."""

    def __init__(
        self,
        source_name: str,
        bucket: str,
        prefix: str,
        file_pattern: str = "*.json",
        partition_format: str = "dt=%Y-%m-%d",
        max_workers: int = 4,
        endpoint_url: Optional[str] = None,
    ):
        """Initialize S3 JSON extractor.

        Args:
            source_name: Unique identifier for this source
            bucket: S3 bucket name
            prefix: Base prefix for files
            file_pattern: Glob pattern for files
            partition_format: Date partition format string
            max_workers: Number of parallel file readers
            endpoint_url: Custom S3 endpoint (for MinIO)
        """
        super().__init__(source_name, {"bucket": bucket, "prefix": prefix})

        self.bucket = bucket
        self.prefix = prefix
        self.file_pattern = file_pattern
        self.partition_format = partition_format
        self.max_workers = max_workers

        # Configure S3 client with retries
        config = Config(
            retries={"max_attempts": 3, "mode": "adaptive"},
            max_pool_connections=max_workers * 2,
        )

        self.s3_client = boto3.client(
            "s3",
            endpoint_url=endpoint_url,
            config=config,
        )

        self.logger = get_logger(f"extractor.s3.{source_name}")

    def _list_files(self, start_date: date, end_date: date) -> List[str]:
        """List all files in date range.

        Args:
            start_date: Start date (inclusive)
            end_date: End date (inclusive)

        Returns:
            List of S3 keys
        """
        files = []
        current_date = start_date

        while current_date <= end_date:
            partition = current_date.strftime(self.partition_format)
            full_prefix = f"{self.prefix}/{partition}/"

            self.logger.debug(f"Listing files in {full_prefix}")

            paginator = self.s3_client.get_paginator("list_objects_v2")
            for page in paginator.paginate(Bucket=self.bucket, Prefix=full_prefix):
                for obj in page.get("Contents", []):
                    key = obj["Key"]
                    if key.endswith(".json") or key.endswith(".jsonl"):
                        files.append(key)

            current_date += timedelta(days=1)

        self.logger.info(f"Found {len(files)} files to process")
        return files

    def _read_json_file(self, key: str) -> List[Dict[str, Any]]:
        """Read a single JSON/JSONL file from S3.

        Args:
            key: S3 object key

        Returns:
            List of records
        """
        try:
            response = self.s3_client.get_object(Bucket=self.bucket, Key=key)
            content = response["Body"].read().decode("utf-8")

            # Handle both JSON array and JSON Lines
            if key.endswith(".jsonl"):
                records = [json.loads(line) for line in content.strip().split("\\n") if line]
            else:
                data = json.loads(content)
                records = data if isinstance(data, list) else [data]

            self.logger.debug(f"Read {len(records)} records from {key}")
            return records

        except Exception as e:
            self.logger.error(f"Failed to read {key}: {e}")
            return []

    def extract(
        self,
        start_date: date,
        end_date: date,
        flatten_nested: bool = True,
    ) -> pd.DataFrame:
        """Extract JSON files for date range.

        Args:
            start_date: Start date (inclusive)
            end_date: End date (inclusive)
            flatten_nested: If True, flatten nested JSON structures

        Returns:
            pd.DataFrame: Combined data from all files
        """
        start_time = time.time()
        self.logger.info(
            "Starting S3 extraction",
            start_date=str(start_date),
            end_date=str(end_date),
        )

        try:
            # List all files
            files = self._list_files(start_date, end_date)

            if not files:
                self.logger.warning("No files found for date range")
                return pd.DataFrame()

            # Read files in parallel
            all_records = []

            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_key = {
                    executor.submit(self._read_json_file, key): key
                    for key in files
                }

                for future in as_completed(future_to_key):
                    key = future_to_key[future]
                    try:
                        records = future.result()
                        all_records.extend(records)
                    except Exception as e:
                        self.logger.error(f"Error processing {key}: {e}")

            # Convert to DataFrame
            df = pd.DataFrame(all_records)

            # Flatten nested structures if requested
            if flatten_nested and not df.empty:
                df = pd.json_normalize(all_records, sep="_")

            # Update stats
            extraction_time = time.time() - start_time
            self._extraction_stats = {
                "rows_extracted": len(df),
                "files_processed": len(files),
                "extraction_time_seconds": round(extraction_time, 2),
                "errors": [],
            }

            self.logger.info(
                "Extraction completed",
                rows=len(df),
                files=len(files),
                duration_seconds=round(extraction_time, 2),
            )

            return df

        except Exception as e:
            self.logger.error(f"Extraction failed: {e}")
            raise ExtractionError(f"Failed to extract from S3: {e}")

    def validate(self, df: pd.DataFrame) -> bool:
        """Validate extracted JSON data."""
        if df.empty:
            self.logger.warning("Extracted DataFrame is empty")
            return True

        # Check for required event fields
        required_fields = ["event_id", "user_id", "event_type", "timestamp"]
        missing = set(required_fields) - set(df.columns)

        if missing:
            self.logger.warning(f"Missing expected fields: {missing}")

        return True


# src/e2e_pipeline/extractors/rest_api.py

from datetime import datetime
from typing import Any, Dict, List, Optional, Callable
import pandas as pd
import time

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from e2e_pipeline.extractors.base import BaseExtractor
from e2e_pipeline.utils.exceptions import ExtractionError
from e2e_pipeline.utils.logger import get_logger


class RESTAPIExtractor(BaseExtractor):
    """Extract data from REST API with pagination and rate limiting."""

    def __init__(
        self,
        source_name: str,
        base_url: str,
        endpoint: str,
        auth_type: str = "api_key",
        api_key: Optional[str] = None,
        api_key_header: str = "X-API-Key",
        page_size: int = 100,
        max_pages: Optional[int] = None,
        rate_limit_per_second: float = 10.0,
        pagination_type: str = "offset",  # 'offset' or 'cursor'
    ):
        """Initialize REST API extractor.

        Args:
            source_name: Unique identifier for this source
            base_url: API base URL
            endpoint: API endpoint path
            auth_type: Authentication type ('api_key', 'bearer', 'basic')
            api_key: API key or token
            api_key_header: Header name for API key
            page_size: Number of records per page
            max_pages: Maximum pages to fetch (None for all)
            rate_limit_per_second: Maximum requests per second
            pagination_type: 'offset' or 'cursor'
        """
        super().__init__(source_name, {"base_url": base_url, "endpoint": endpoint})

        self.base_url = base_url.rstrip("/")
        self.endpoint = endpoint
        self.page_size = page_size
        self.max_pages = max_pages
        self.rate_limit_per_second = rate_limit_per_second
        self.pagination_type = pagination_type

        # Setup session with retries
        self.session = requests.Session()
        retries = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retries))
        self.session.mount("http://", HTTPAdapter(max_retries=retries))

        # Setup authentication
        if auth_type == "api_key" and api_key:
            self.session.headers[api_key_header] = api_key
        elif auth_type == "bearer" and api_key:
            self.session.headers["Authorization"] = f"Bearer {api_key}"

        self.logger = get_logger(f"extractor.api.{source_name}")
        self._last_request_time = 0

    def _rate_limit(self):
        """Apply rate limiting between requests."""
        min_interval = 1.0 / self.rate_limit_per_second
        elapsed = time.time() - self._last_request_time

        if elapsed < min_interval:
            sleep_time = min_interval - elapsed
            time.sleep(sleep_time)

        self._last_request_time = time.time()

    def _fetch_page(
        self,
        params: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Fetch a single page from API.

        Args:
            params: Query parameters

        Returns:
            API response as dict
        """
        self._rate_limit()

        url = f"{self.base_url}{self.endpoint}"

        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.RequestException as e:
            self.logger.error(f"API request failed: {e}")
            raise ExtractionError(f"API request failed: {e}")

    def extract(
        self,
        params: Optional[Dict[str, Any]] = None,
        data_key: str = "data",
        cursor_key: str = "next_cursor",
        total_key: str = "total",
    ) -> pd.DataFrame:
        """Extract all pages from API.

        Args:
            params: Additional query parameters
            data_key: Key in response containing data array
            cursor_key: Key for cursor pagination
            total_key: Key for total count

        Returns:
            pd.DataFrame: Combined data from all pages
        """
        start_time = time.time()
        self.logger.info("Starting API extraction", endpoint=self.endpoint)

        try:
            all_records = []
            page = 0
            cursor = None
            total_fetched = 0

            while True:
                # Build request params
                request_params = {"limit": self.page_size}

                if params:
                    request_params.update(params)

                if self.pagination_type == "offset":
                    request_params["offset"] = page * self.page_size
                elif self.pagination_type == "cursor" and cursor:
                    request_params["cursor"] = cursor

                # Fetch page
                self.logger.debug(f"Fetching page {page + 1}")
                response = self._fetch_page(request_params)

                # Extract data
                data = response.get(data_key, [])
                if not data:
                    self.logger.info("No more data, stopping pagination")
                    break

                all_records.extend(data)
                total_fetched += len(data)
                page += 1

                self.logger.debug(
                    f"Page {page}: fetched {len(data)} records, total: {total_fetched}"
                )

                # Check cursor for next page
                if self.pagination_type == "cursor":
                    cursor = response.get(cursor_key)
                    if not cursor:
                        break

                # Check if we've fetched all
                if len(data) < self.page_size:
                    break

                # Check max pages limit
                if self.max_pages and page >= self.max_pages:
                    self.logger.info(f"Reached max pages limit: {self.max_pages}")
                    break

            # Convert to DataFrame
            df = pd.DataFrame(all_records)

            # Update stats
            extraction_time = time.time() - start_time
            self._extraction_stats = {
                "rows_extracted": len(df),
                "pages_fetched": page,
                "extraction_time_seconds": round(extraction_time, 2),
                "errors": [],
            }

            self.logger.info(
                "Extraction completed",
                rows=len(df),
                pages=page,
                duration_seconds=round(extraction_time, 2),
            )

            return df

        except Exception as e:
            self.logger.error(f"Extraction failed: {e}")
            raise ExtractionError(f"Failed to extract from API: {e}")

    def validate(self, df: pd.DataFrame) -> bool:
        """Validate extracted API data."""
        if df.empty:
            self.logger.warning("Extracted DataFrame is empty")

        return True

    def close(self):
        """Close the session."""
        self.session.close()


# src/e2e_pipeline/extractors/watermark.py

from datetime import datetime
from typing import Optional, Dict, Any
import json
from pathlib import Path

import boto3

from e2e_pipeline.utils.logger import get_logger


class WatermarkManager:
    """Manage extraction watermarks for incremental processing."""

    def __init__(
        self,
        storage_type: str = "s3",
        bucket: Optional[str] = None,
        prefix: str = "watermarks",
        local_path: Optional[str] = None,
    ):
        """Initialize watermark manager.

        Args:
            storage_type: 's3' or 'local'
            bucket: S3 bucket for watermarks
            prefix: S3 prefix or local directory
            local_path: Path for local storage
        """
        self.storage_type = storage_type
        self.bucket = bucket
        self.prefix = prefix
        self.local_path = Path(local_path) if local_path else Path("./watermarks")
        self.logger = get_logger("watermark_manager")

        if storage_type == "local":
            self.local_path.mkdir(parents=True, exist_ok=True)

    def _get_key(self, source: str) -> str:
        """Generate storage key for source."""
        return f"{self.prefix}/{source}.json"

    def get_watermark(self, source: str) -> Optional[datetime]:
        """Get last successful watermark for source.

        Args:
            source: Source identifier

        Returns:
            Last watermark datetime or None
        """
        try:
            if self.storage_type == "s3":
                s3 = boto3.client("s3")
                response = s3.get_object(
                    Bucket=self.bucket,
                    Key=self._get_key(source),
                )
                data = json.loads(response["Body"].read())

            else:  # local
                file_path = self.local_path / f"{source}.json"
                if not file_path.exists():
                    return None
                data = json.loads(file_path.read_text())

            watermark_str = data.get("last_watermark")
            if watermark_str:
                return datetime.fromisoformat(watermark_str)

            return None

        except Exception as e:
            self.logger.warning(f"Could not get watermark for {source}: {e}")
            return None

    def update_watermark(
        self,
        source: str,
        watermark: datetime,
        metadata: Optional[Dict[str, Any]] = None,
    ):
        """Update watermark for source.

        Args:
            source: Source identifier
            watermark: New watermark value
            metadata: Additional metadata to store
        """
        data = {
            "source": source,
            "last_watermark": watermark.isoformat(),
            "updated_at": datetime.utcnow().isoformat(),
            "metadata": metadata or {},
        }

        try:
            if self.storage_type == "s3":
                s3 = boto3.client("s3")
                s3.put_object(
                    Bucket=self.bucket,
                    Key=self._get_key(source),
                    Body=json.dumps(data),
                    ContentType="application/json",
                )

            else:  # local
                file_path = self.local_path / f"{source}.json"
                file_path.write_text(json.dumps(data, indent=2))

            self.logger.info(
                f"Updated watermark for {source}",
                watermark=watermark.isoformat(),
            )

        except Exception as e:
            self.logger.error(f"Failed to update watermark for {source}: {e}")
            raise
`,
      hints: [
        "ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ë©´ S3 íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì½ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤",
        "requests.Sessionì˜ Retryë¥¼ ì„¤ì •í•˜ë©´ ì¼ì‹œì  ì˜¤ë¥˜ì— ìžë™ ìž¬ì‹œë„ë©ë‹ˆë‹¤",
        "Rate limitingì€ time.sleep()ìœ¼ë¡œ ê°„ë‹¨ížˆ êµ¬í˜„í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤",
        "WatermarkëŠ” S3ë‚˜ ë¡œì»¬ íŒŒì¼ì— JSONìœ¼ë¡œ ì €ìž¥í•˜ë©´ ë©ë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w8d2t4',
    title: 'Extraction DAG êµ¬í˜„',
    type: 'code',
    duration: 35,
    content: {
      instructions: `## Airflow Extraction DAG êµ¬í˜„

3ê°œì˜ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” Airflow DAGë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

### êµ¬í˜„ ìš”êµ¬ì‚¬í•­

1. **dag_extract_all.py**
   - 3ê°œ ì¶”ì¶œ Taskë¥¼ ë³‘ë ¬ ì‹¤í–‰
   - TaskFlow API ì‚¬ìš©
   - ì—ëŸ¬ ì‹œ Slack ì•Œë¦¼

2. **ì˜ì¡´ì„± ê´€ë¦¬**
   - Extract ì™„ë£Œ í›„ Raw Layer ì €ìž¥
   - Watermark ì—…ë°ì´íŠ¸
`,
      starterCode: `# dags/dag_extract_all.py

from datetime import datetime, timedelta
from airflow.decorators import dag, task, task_group

default_args = {
    'owner': 'data-team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}


@dag(
    dag_id='extract_all_sources',
    description='Extract data from all sources',
    schedule='0 2 * * *',  # ë§¤ì¼ 02:00 UTC
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    tags=['extract', 'e2e-pipeline'],
)
def extract_all_sources():
    """Extract data from PostgreSQL, S3, and REST API."""

    # TODO: Task êµ¬í˜„
    pass


dag = extract_all_sources()
`,
      solutionCode: `# dags/dag_extract_all.py

from datetime import datetime, timedelta
from typing import Dict, Any

from airflow.decorators import dag, task, task_group
from airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook
from airflow.models import Variable

default_args = {
    'owner': 'data-team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
}


def slack_failure_callback(context):
    """Send Slack notification on failure."""
    task_instance = context['task_instance']
    dag_id = context['dag'].dag_id
    exception = context.get('exception', 'Unknown error')

    message = f"""
:red_circle: *Extraction Failed*
â€¢ DAG: {dag_id}
â€¢ Task: {task_instance.task_id}
â€¢ Error: {str(exception)[:200]}
    """

    try:
        hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
        hook.send(text=message)
    except Exception:
        pass


@dag(
    dag_id='extract_all_sources',
    description='Extract data from all sources to raw layer',
    schedule='0 2 * * *',  # ë§¤ì¼ 02:00 UTC
    start_date=datetime(2024, 1, 1),
    catchup=False,
    default_args=default_args,
    on_failure_callback=slack_failure_callback,
    tags=['extract', 'e2e-pipeline'],
    dagrun_timeout=timedelta(hours=2),
)
def extract_all_sources():
    """Extract data from PostgreSQL, S3, and REST API."""

    @task_group(group_id='extract_sources')
    def extract_all():
        """Extract from all sources in parallel."""

        @task(
            retries=3,
            retry_delay=timedelta(minutes=5),
        )
        def extract_users(**context) -> Dict[str, Any]:
            """Extract users and subscriptions from PostgreSQL."""
            import sys
            sys.path.insert(0, '/opt/airflow/src')

            from e2e_pipeline.extractors.postgres import PostgreSQLExtractor
            from e2e_pipeline.extractors.watermark import WatermarkManager
            from e2e_pipeline.config import settings

            execution_date = context['logical_date']
            target_date = (execution_date - timedelta(days=1)).strftime('%Y-%m-%d')

            # Initialize extractor
            extractor = PostgreSQLExtractor(
                source_name='users',
                connection_string=settings.postgres_url,
                table_name='users',
                required_columns=['user_id', 'email', 'created_at'],
            )

            # Get watermark
            watermark_mgr = WatermarkManager(
                storage_type='s3',
                bucket=settings.s3_bucket,
                prefix='watermarks',
            )
            last_watermark = watermark_mgr.get_watermark('users')

            # Extract
            df = extractor.extract(
                incremental=True,
                watermark_column='updated_at',
                last_watermark=last_watermark,
            )

            # Validate
            extractor.validate(df)

            # Save to raw layer
            if not df.empty:
                path = extractor.save_to_raw(
                    df,
                    bucket=settings.s3_bucket,
                    prefix=f'raw/users/dt={target_date}',
                )

                # Update watermark
                new_watermark = df['updated_at'].max()
                watermark_mgr.update_watermark(
                    'users',
                    new_watermark,
                    metadata={'rows': len(df), 'path': path},
                )
            else:
                path = None

            extractor.close()

            return {
                'source': 'users',
                'rows': len(df),
                'path': path,
                'target_date': target_date,
            }

        @task(
            retries=3,
            retry_delay=timedelta(minutes=5),
        )
        def extract_events(**context) -> Dict[str, Any]:
            """Extract event logs from S3."""
            import sys
            sys.path.insert(0, '/opt/airflow/src')

            from datetime import date
            from e2e_pipeline.extractors.s3_json import S3JSONExtractor
            from e2e_pipeline.config import settings

            execution_date = context['logical_date']
            target_date = (execution_date - timedelta(days=1)).date()

            # Initialize extractor
            extractor = S3JSONExtractor(
                source_name='events',
                bucket='source-logs',  # Source bucket
                prefix='event-logs',
                partition_format='dt=%Y-%m-%d',
                max_workers=4,
            )

            # Extract yesterday's events
            df = extractor.extract(
                start_date=target_date,
                end_date=target_date,
                flatten_nested=True,
            )

            # Validate
            extractor.validate(df)

            # Save to raw layer (our data lake)
            if not df.empty:
                path = extractor.save_to_raw(
                    df,
                    bucket=settings.s3_bucket,
                    prefix=f'raw/events/dt={target_date}',
                )
            else:
                path = None

            return {
                'source': 'events',
                'rows': len(df),
                'files_processed': extractor.get_stats().get('files_processed', 0),
                'path': path,
                'target_date': str(target_date),
            }

        @task(
            retries=3,
            retry_delay=timedelta(minutes=5),
        )
        def extract_payments(**context) -> Dict[str, Any]:
            """Extract payments from REST API."""
            import sys
            sys.path.insert(0, '/opt/airflow/src')

            from e2e_pipeline.extractors.rest_api import RESTAPIExtractor
            from e2e_pipeline.config import settings

            execution_date = context['logical_date']
            target_date = (execution_date - timedelta(days=1)).strftime('%Y-%m-%d')

            # Initialize extractor
            extractor = RESTAPIExtractor(
                source_name='payments',
                base_url=settings.payment_api_url,
                endpoint='/v1/payments',
                auth_type='api_key',
                api_key=settings.payment_api_key,
                page_size=100,
                rate_limit_per_second=5.0,
                pagination_type='cursor',
            )

            # Extract with date filter
            df = extractor.extract(
                params={
                    'created_after': f'{target_date}T00:00:00Z',
                    'created_before': f'{target_date}T23:59:59Z',
                },
                data_key='payments',
                cursor_key='next_cursor',
            )

            # Validate
            extractor.validate(df)

            # Save to raw layer
            if not df.empty:
                path = extractor.save_to_raw(
                    df,
                    bucket=settings.s3_bucket,
                    prefix=f'raw/payments/dt={target_date}',
                )
            else:
                path = None

            extractor.close()

            return {
                'source': 'payments',
                'rows': len(df),
                'pages_fetched': extractor.get_stats().get('pages_fetched', 0),
                'path': path,
                'target_date': target_date,
            }

        # Return all extraction results
        return [extract_users(), extract_events(), extract_payments()]

    @task
    def validate_extractions(results: list) -> Dict[str, Any]:
        """Validate all extraction results."""
        import logging
        logger = logging.getLogger(__name__)

        total_rows = 0
        sources_status = {}
        warnings = []

        for result in results:
            source = result['source']
            rows = result['rows']
            total_rows += rows
            sources_status[source] = {
                'rows': rows,
                'path': result.get('path'),
                'status': 'success' if rows > 0 else 'empty',
            }

            if rows == 0:
                warnings.append(f"{source}: No data extracted")

        logger.info(f"Extraction summary: {total_rows} total rows")

        return {
            'status': 'warning' if warnings else 'success',
            'total_rows': total_rows,
            'sources': sources_status,
            'warnings': warnings,
        }

    @task
    def send_summary(validation_result: Dict[str, Any], **context):
        """Send extraction summary to Slack."""
        execution_date = context['logical_date']

        status_emoji = ':white_check_mark:' if validation_result['status'] == 'success' else ':warning:'

        sources_summary = '\\n'.join([
            f"â€¢ {source}: {info['rows']:,} rows"
            for source, info in validation_result['sources'].items()
        ])

        message = f"""
{status_emoji} *Daily Extraction Complete*
:calendar: Date: {execution_date.strftime('%Y-%m-%d')}

*Sources:*
{sources_summary}

*Total:* {validation_result['total_rows']:,} rows
        """

        if validation_result['warnings']:
            message += f"\\n*Warnings:* {', '.join(validation_result['warnings'])}"

        try:
            hook = SlackWebhookHook(slack_webhook_conn_id='slack_webhook')
            hook.send(text=message)
        except Exception as e:
            print(f"Failed to send Slack message: {e}")

    # DAG flow
    extraction_results = extract_all()
    validation = validate_extractions(extraction_results)
    send_summary(validation)


dag = extract_all_sources()
`,
      hints: [
        "task_groupì„ ì‚¬ìš©í•˜ë©´ ê´€ë ¨ Taskë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤",
        "sys.path.insertë¡œ src ë””ë ‰í† ë¦¬ë¥¼ ìž„í¬íŠ¸ ê²½ë¡œì— ì¶”ê°€í•©ë‹ˆë‹¤",
        "logical_dateëŠ” ì‹¤í–‰ ëŒ€ìƒ ë‚ ì§œ, execution_dateì™€ ë™ì¼í•©ë‹ˆë‹¤",
        "ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë©´ ë‹¤ìŒ Taskì—ì„œ ë°°ì—´ë¡œ ë°›ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤"
      ]
    }
  },
  {
    id: 'w8d2t5',
    title: 'Day 2 ë°ì´í„° ìˆ˜ì§‘ í€´ì¦ˆ',
    type: 'quiz',
    duration: 10,
    content: {
      questions: [
        {
          question: 'ì¦ë¶„ ì¶”ì¶œ(Incremental Extract)ì—ì„œ Watermarkì˜ ì—­í• ì€?',
          options: [
            'ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•œë‹¤',
            'ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œì ì„ ê¸°ë¡í•˜ì—¬ ë³€ê²½ë¶„ë§Œ ì¶”ì¶œí•œë‹¤',
            'ë°ì´í„°ë¥¼ ì••ì¶•í•œë‹¤',
            'ì¤‘ë³µ ë°ì´í„°ë¥¼ ì œê±°í•œë‹¤'
          ],
          answer: 1,
          explanation: 'WatermarkëŠ” ë§ˆì§€ë§‰ ì„±ê³µ ì¶”ì¶œ ì‹œì ì„ ì €ìž¥í•˜ì—¬, ë‹¤ìŒ ì¶”ì¶œ ì‹œ ê·¸ ì´í›„ ë³€ê²½ëœ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìžˆê²Œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì „ì²´ ì¶”ì¶œ ëŒ€ë¹„ ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•©ë‹ˆë‹¤.'
        },
        {
          question: 'REST API ì¶”ì¶œì—ì„œ Rate Limitingì´ í•„ìš”í•œ ì´ìœ ëŠ”?',
          options: [
            'ë°ì´í„° í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•´',
            'API ì„œë²„ì˜ 429 ì—ëŸ¬ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´',
            'ë°ì´í„° ì••ì¶•ë¥ ì„ ë†’ì´ê¸° ìœ„í•´',
            'ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì„ ì ˆì•½í•˜ê¸° ìœ„í•´'
          ],
          answer: 1,
          explanation: 'API ì„œë²„ëŠ” ê³¼ë„í•œ ìš”ì²­ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ Rate Limitì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ë¥¼ ì´ˆê³¼í•˜ë©´ 429 Too Many Requests ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤. í´ë¼ì´ì–¸íŠ¸ì—ì„œ Rate Limitingì„ ì ìš©í•˜ë©´ ì´ë¥¼ ë°©ì§€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.'
        },
        {
          question: 'SQLAlchemy QueuePoolì˜ ìž¥ì ì€?',
          options: [
            'ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ìºì‹±í•œë‹¤',
            'ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ìž¬ì‚¬ìš©í•˜ì—¬ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì¸ë‹¤',
            'ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•œë‹¤',
            'íŠ¸ëžœìž­ì…˜ì„ ìžë™ìœ¼ë¡œ ë¡¤ë°±í•œë‹¤'
          ],
          answer: 1,
          explanation: 'Connection Poolì€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ë¯¸ë¦¬ ìƒì„±í•´ë‘ê³  ìž¬ì‚¬ìš©í•©ë‹ˆë‹¤. ë§¤ë²ˆ ì—°ê²°ì„ ìƒˆë¡œ ë§Œë“œëŠ” ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.'
        },
        {
          question: 'ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ S3 íŒŒì¼ ë³‘ë ¬ ì½ê¸°ì˜ ìž¥ì ì€?',
          options: [
            'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì¸ë‹¤',
            'I/O ëŒ€ê¸° ì‹œê°„ì„ ê²¹ì³ì„œ ì „ì²´ ì‹œê°„ì„ ë‹¨ì¶•í•œë‹¤',
            'CPU ì‚¬ìš©ë¥ ì„ ë†’ì¸ë‹¤',
            'íŒŒì¼ ì••ì¶•ë¥ ì„ ë†’ì¸ë‹¤'
          ],
          answer: 1,
          explanation: 'S3 íŒŒì¼ ì½ê¸°ëŠ” I/O ë°”ìš´ë“œ ìž‘ì—…ìž…ë‹ˆë‹¤. ì—¬ëŸ¬ íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì½ìœ¼ë©´ ë„¤íŠ¸ì›Œí¬ ëŒ€ê¸° ì‹œê°„ì´ ê²¹ì³ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ì´ í¬ê²Œ ë‹¨ì¶•ë©ë‹ˆë‹¤.'
        }
      ]
    }
  }
]
